{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radio-villa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sustained-discussion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "religious-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased', use_fast=False)\n",
    "#model = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regulation-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tok(['hälsa hälsan hälsans'], return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True)\n",
    "#inputs = tok(X[:10].values.tolist(), return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dress-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = model(**inputs)\n",
    "#outputs = outputs['pooler_output'].detach().numpy().reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "specific-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('Init called')\n",
    "        self.model_name = 'KB/bert-base-swedish-cased'\n",
    "        self.Bert = AutoModel.from_pretrained(self.model_name)\n",
    "        self.Tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.batch_size = 50\n",
    "        \n",
    "        #cuda_enabled = torch.cuda.is_available()\n",
    "        #if cuda_enabled:\n",
    "        #    device = torch.device('cuda')\n",
    "        #    self.batch_size = 25\n",
    "        #else:\n",
    "        #    device = torch.device('cpu')\n",
    "        #    self.batch_size = 20\n",
    "\n",
    "        #self.Bert.to(device)\n",
    "        #print(f'We are running on device: {device}')\n",
    "        \n",
    "        #Freeze the Bert model layers\n",
    "        for param in self.Bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        print('Fit called')\n",
    "        \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def partial_fit(self, X, y=None):\n",
    "        print('Partial Fit called')\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        print('Transform Called')\n",
    "        \n",
    "        #Check device\n",
    "        device = self.Bert.device\n",
    "        \n",
    "        #Preprocess data\n",
    "        X = X.str.replace('\\n','')\n",
    "        X = X.str.replace('\\r','')\n",
    "        X = X.str.lower()\n",
    "        X = X.values.tolist()\n",
    "       \n",
    "        # Transform input tokens. This is most efficient if done in one batch \n",
    "        inputs = self.Tokenizer(X, return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True)\n",
    "\n",
    "        # Run Bert model, We must mini batch this in order to not overflow the memory of the system\n",
    "        transformed = []\n",
    "        \n",
    "        batches = int(np.ceil(len(X) / self.batch_size))\n",
    "        for batchId in range(batches):\n",
    "            print(f'Running batch {batchId+1}/{batches}')\n",
    "        \n",
    "            inputs_batch = {}\n",
    "            for key in inputs.keys():\n",
    "                inputs_batch[key] = inputs[key][batchId * self.batch_size:(batchId + 1) * self.batch_size].to(device)\n",
    "            \n",
    "            inputs_batch\n",
    "            outputs = self.Bert(**inputs_batch)\n",
    "            #inputs = self.Tokenizer(X[batchId * self.batch_size:(batchId + 1) * self.batch_size].values.tolist(), return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True).to(device)\n",
    "            #outputs = self.Bert(**inputs)\n",
    "            #outputs = outputs['pooler_output'].detach().numpy()\n",
    "            \n",
    "            outputs = outputs['pooler_output'].to('cpu').detach().numpy()\n",
    "            print(f'output shape: {outputs.shape}')\n",
    "            transformed.extend(outputs)\n",
    "        \n",
    "        transformed = np.array(transformed)\n",
    "        print(f'transformed.shape: {transformed.shape}')\n",
    "        \n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "marked-citizenship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([    \n",
    "            ('nlpTransformer', NLPTransformer()),\n",
    "            ('clf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-oxygen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interpreted-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "boxed-syntax",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-01876a210620>:3: DtypeWarning: Columns (1,3,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,39,41,43,45,47,49,51,52,54,56,57,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/CRMIncidents_Anonymized_Complete_Table.csv')\n"
     ]
    }
   ],
   "source": [
    "#Test data anonymized\n",
    "#df = pd.read_csv('../data/CRMIncidents_Anonymized.csv')\n",
    "df = pd.read_csv('../data/CRMIncidents_Anonymized_Complete_Table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "altered-overhead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CRMIncidentId</th>\n",
       "      <th>IncidentId</th>\n",
       "      <th>LineId</th>\n",
       "      <th>Linje</th>\n",
       "      <th>JourneyId</th>\n",
       "      <th>TurNummer</th>\n",
       "      <th>Trafikslag</th>\n",
       "      <th>Ankomstdag</th>\n",
       "      <th>...</th>\n",
       "      <th>Enhet</th>\n",
       "      <th>Queue_SK</th>\n",
       "      <th>Kö</th>\n",
       "      <th>ModifiedOn</th>\n",
       "      <th>ContactId</th>\n",
       "      <th>Contact_SK</th>\n",
       "      <th>iBID</th>\n",
       "      <th>IsActive</th>\n",
       "      <th>TicketId</th>\n",
       "      <th>Beskrivning_Anonymized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>673326.0</td>\n",
       "      <td>77DE23B5-43D8-E811-80F6-005056B63599</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2018-10-29 11:33:19</td>\n",
       "      <td>586769.0</td>\n",
       "      <td>C0BDFF48-2C0B-E811-80F1-005056B64D75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hej,\\r\\r\\n \\r\\r\\nHar nu fått tag i föraren som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>673354.0</td>\n",
       "      <td>EBD68435-41D8-E811-80F6-005056B63599</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2018-11-08 08:17:09</td>\n",
       "      <td>331145.0</td>\n",
       "      <td>E429FD9A-5DEC-E411-80D6-0050569071BE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buss 000 00:00\\r\\r\\n\\r\\r\\nKristianstad Hästtor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>673617.0</td>\n",
       "      <td>3ED0AA37-16D9-E811-80F4-005056B62B18</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2021-10-07 08:49:10</td>\n",
       "      <td>726145.0</td>\n",
       "      <td>12F4B35F-16D9-E811-80F4-005056B62B18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Skadeanmälan för påkörning av bil bakifrån vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>673807.0</td>\n",
       "      <td>AAA91AC9-E0D8-E811-80F5-005056B64D75</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2018-11-08 10:36:12</td>\n",
       "      <td>603794.0</td>\n",
       "      <td>56A30A33-A32D-E811-80F2-005056B62B18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hej, \\r\\r\\nVarför heter en av hållplatserna i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>673850.0</td>\n",
       "      <td>5F1165E6-63D9-E811-80F5-005056B64D75</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2019-11-27 15:06:13</td>\n",
       "      <td>22229.0</td>\n",
       "      <td>383EAEB1-1DEB-E411-80D8-005056903A38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hej!\\r\\r\\nHar en fråga som gäller busskurerna ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1 Unnamed: 0  CRMIncidentId  \\\n",
       "0             0          0       673326.0   \n",
       "1             1          1       673354.0   \n",
       "2             2          2       673617.0   \n",
       "3             3          3       673807.0   \n",
       "4             4          4       673850.0   \n",
       "\n",
       "                             IncidentId  LineId Linje  JourneyId TurNummer  \\\n",
       "0  77DE23B5-43D8-E811-80F6-005056B63599    -1.0   NaN       -1.0       NaN   \n",
       "1  EBD68435-41D8-E811-80F6-005056B63599    -1.0   NaN       -1.0       NaN   \n",
       "2  3ED0AA37-16D9-E811-80F4-005056B62B18    -1.0   NaN       -1.0       NaN   \n",
       "3  AAA91AC9-E0D8-E811-80F5-005056B64D75    -1.0   NaN       -1.0       NaN   \n",
       "4  5F1165E6-63D9-E811-80F5-005056B64D75    -1.0   NaN       -1.0       NaN   \n",
       "\n",
       "  Trafikslag  Ankomstdag  ...       Enhet Queue_SK                  Kö  \\\n",
       "0        NaN  2018-10-25  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "1        NaN  2018-10-25  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "2        NaN  2018-10-26  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "3        NaN  2018-10-26  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "4        NaN  2018-10-26  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "\n",
       "            ModifiedOn ContactId                            Contact_SK iBID  \\\n",
       "0  2018-10-29 11:33:19  586769.0  C0BDFF48-2C0B-E811-80F1-005056B64D75  NaN   \n",
       "1  2018-11-08 08:17:09  331145.0  E429FD9A-5DEC-E411-80D6-0050569071BE  NaN   \n",
       "2  2021-10-07 08:49:10  726145.0  12F4B35F-16D9-E811-80F4-005056B62B18  NaN   \n",
       "3  2018-11-08 10:36:12  603794.0  56A30A33-A32D-E811-80F2-005056B62B18  NaN   \n",
       "4  2019-11-27 15:06:13   22229.0  383EAEB1-1DEB-E411-80D8-005056903A38  NaN   \n",
       "\n",
       "  IsActive TicketId                             Beskrivning_Anonymized  \n",
       "0     True      NaN  Hej,\\r\\r\\n \\r\\r\\nHar nu fått tag i föraren som...  \n",
       "1     True      NaN  Buss 000 00:00\\r\\r\\n\\r\\r\\nKristianstad Hästtor...  \n",
       "2     True      NaN  Skadeanmälan för påkörning av bil bakifrån vid...  \n",
       "3     True      NaN  Hej, \\r\\r\\nVarför heter en av hållplatserna i ...  \n",
       "4     True      NaN  Hej!\\r\\r\\nHar en fråga som gäller busskurerna ...  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "strategic-junction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'CRMIncidentId', 'IncidentId', 'LineId',\n",
       "       'Linje', 'JourneyId', 'TurNummer', 'Trafikslag', 'Ankomstdag',\n",
       "       'Händelsedatum', 'Hanteratdatum', 'Ärendenummer', 'KategoriId11',\n",
       "       'Kategori11', 'KategoriId12', 'Kategori12', 'KategoriId13',\n",
       "       'Kategori13', 'KategoriId21', 'Kategori21', 'KategoriId22',\n",
       "       'Kategori22', 'KategoriId23', 'Kategori23', 'KategoriId31',\n",
       "       'Kategori31', 'KategoriId32', 'Kategori32', 'KategoriId33',\n",
       "       'Kategori33', 'KategoriId41', 'Kategori41', 'KategoriId42',\n",
       "       'Kategori42', 'KategoriId43', 'Kategori43', 'Titel', 'CaseType_SK',\n",
       "       'Ärendetyp', 'CaseOrigin_SK', 'Ursprung', 'Priority_SK', 'Prioritet',\n",
       "       'IncidentStage_SK', 'ÄrendeStatus', 'Owner_SK', 'Handläggare',\n",
       "       'BusinessUnit_SK', 'Enhet', 'Queue_SK', 'Kö', 'ModifiedOn', 'ContactId',\n",
       "       'Contact_SK', 'iBID', 'IsActive', 'TicketId', 'Beskrivning_Anonymized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assumed-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove nans, select relevant columns\n",
    "df = df[['Ärendetyp', 'Beskrivning_Anonymized', 'Prioritet', 'Ankomstdag']]\n",
    "df = df[~df['Ärendetyp'].isna()]\n",
    "df = df[~df['Beskrivning_Anonymized'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "organized-caution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([288232.,  88707.,      0.,   5276.,      0., 160978., 799256.,\n",
       "             0.,   6202.,   2816.]),\n",
       " array([0. , 0.6, 1.2, 1.8, 2.4, 3. , 3.6, 4.2, 4.8, 5.4, 6. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(df['Ärendetyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fitting-cosmetic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Förseningsersättning    799256\n",
       "Klagomål                288232\n",
       "Fråga                   160978\n",
       "Synpunkt/Önskemål        88707\n",
       "Beröm                     6202\n",
       "Skada                     5276\n",
       "Avvikelse                 2816\n",
       "Name: Ärendetyp, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Ärendetyp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "organic-politics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1351467, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "assumed-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df['Ärendetyp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thirty-music",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Klagomål', 'Synpunkt/Önskemål', 'Skada', 'Fråga',\n",
       "       'Förseningsersättning', 'Beröm', 'Avvikelse'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "declared-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a balanced training set\n",
    "desiredCount = 20000\n",
    "#desiredCount = 1500\n",
    "dfBalanced = None\n",
    "for category in categories:\n",
    "    sample = df[df['Ärendetyp'] == category].sample(n=desiredCount, replace=True, random_state=42)\n",
    "    if dfBalanced is None:\n",
    "        dfBalanced = sample\n",
    "    else:\n",
    "        dfBalanced = pd.concat([dfBalanced, sample], ignore_index=True)\n",
    "\n",
    "\n",
    "#Next random shuffle of all rows\n",
    "df = dfBalanced.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "seven-venture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df[:10000] #Max for this computer\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "artificial-lounge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20000., 20000.,     0., 20000.,     0., 20000., 20000.,     0.,\n",
       "        20000., 20000.]),\n",
       " array([0. , 0.6, 1.2, 1.8, 2.4, 3. , 3.6, 4.2, 4.8, 5.4, 6. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(df['Ärendetyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "external-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Beskrivning_Anonymized'].str.lower() #Lower case to slim the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tracked-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(df['Ärendetyp'], columns=['Ärendetyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "romance-thickness",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = Y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "thirty-product",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Avvikelse', 'Beröm', 'Fråga', 'Förseningsersättning', 'Klagomål',\n",
       "       'Skada', 'Synpunkt/Önskemål'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "returning-range",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "grateful-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transformer = NLPTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "crude-sister",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sticky-transcription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "established-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.to_pickle('targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rapid-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 3/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 4/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 5/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 6/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 7/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 8/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 9/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 10/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 11/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 12/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 13/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 14/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 15/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 16/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 17/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 18/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 19/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 20/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 21/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 22/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 23/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 24/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 25/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 26/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 27/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 28/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 29/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 30/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 31/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 32/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 33/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 34/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 35/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 36/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 37/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 38/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 39/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 40/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 41/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 42/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 43/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 44/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 45/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 46/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 47/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 48/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 49/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 50/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 51/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 52/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 53/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 54/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 55/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 56/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 57/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 58/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 59/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 60/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 61/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 62/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 63/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 64/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 65/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 66/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 67/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 68/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 69/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 70/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 71/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 72/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 73/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 74/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 75/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 76/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 77/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 78/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 79/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 80/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 81/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 82/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 83/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 84/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 85/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 86/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 87/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 88/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 89/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 90/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 91/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 92/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 93/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 94/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 95/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 96/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 97/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 98/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 99/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 100/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 101/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 102/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 103/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 104/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 105/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 106/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 107/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 108/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 109/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 110/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 111/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 112/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 113/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 114/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 115/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 116/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 117/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 118/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 119/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 120/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 121/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 122/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 123/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 124/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 125/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 126/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 127/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 128/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 129/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 130/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 131/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 132/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 133/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 134/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 135/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 136/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 137/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 138/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 139/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 140/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 141/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 142/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 143/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 144/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 145/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 146/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 147/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 148/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 149/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 150/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 151/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 152/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 153/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 154/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 155/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 156/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 157/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 158/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 159/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 160/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 161/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 162/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 163/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 164/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 165/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 166/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 167/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 168/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 169/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 170/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 171/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 172/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 173/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 174/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 175/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 176/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 177/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 178/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 179/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 180/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 181/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 182/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 183/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 184/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 185/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 186/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 187/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 188/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 189/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 190/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 191/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 192/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 193/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 194/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 195/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 196/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 197/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 198/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 199/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 200/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 201/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 202/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 203/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 204/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 205/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 206/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 207/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 208/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 209/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 210/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 211/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 212/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 213/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 214/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 215/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 216/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 217/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 218/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 219/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 220/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 221/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 222/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 223/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 224/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 225/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 226/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 227/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 228/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 229/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 230/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 231/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 232/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 233/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 234/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 235/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 236/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 237/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 238/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 239/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 240/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 241/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 242/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 243/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 244/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 245/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 246/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 247/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 248/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 249/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 250/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 251/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 252/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 253/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 254/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 255/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 256/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 257/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 258/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 259/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 260/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 261/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 262/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 263/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 264/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 265/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 266/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 267/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 268/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 269/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 270/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 271/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 272/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 273/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 274/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 275/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 276/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 277/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 278/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 279/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 280/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 281/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 282/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 283/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 284/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 285/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 286/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 287/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 288/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 289/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 290/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 291/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 292/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 293/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 294/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 295/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 296/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 297/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 298/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 299/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 300/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 301/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 302/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 303/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 304/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 305/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 306/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 307/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 308/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 309/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 310/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 311/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 312/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 313/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 314/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 315/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 316/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 317/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 318/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 319/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 320/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 321/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 322/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 323/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 324/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 325/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 326/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 327/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 328/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 329/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 330/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 331/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 332/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 333/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 334/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 335/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 336/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 337/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 338/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 339/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 340/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 341/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 342/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 343/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 344/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 345/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 346/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 347/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 348/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 349/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 350/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 351/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 352/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 353/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 354/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 355/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 356/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 357/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 358/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 359/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 360/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 361/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 362/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 363/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 364/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 365/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 366/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 367/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 368/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 369/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 370/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 371/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 372/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 373/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 374/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 375/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 376/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 377/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 378/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 379/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 380/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 381/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 382/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 383/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 384/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 385/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 386/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 387/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 388/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 389/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 390/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 391/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 392/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 393/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 394/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 395/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 396/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 397/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 398/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 399/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 400/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 401/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 402/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 403/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 404/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 405/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 406/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 407/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 408/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 409/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 410/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 411/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 412/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 413/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 414/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 415/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 416/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 417/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 418/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 419/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 420/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 421/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 422/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 423/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 424/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 425/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 426/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 427/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 428/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 429/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 430/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 431/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 432/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 433/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 434/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 435/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 436/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 437/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 438/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 439/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 440/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 441/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 442/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 443/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 444/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 445/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 446/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 447/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 448/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 449/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 450/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 451/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 452/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 453/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 454/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 455/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 456/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 457/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 458/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 459/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 460/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 461/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 462/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 463/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 464/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 465/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 466/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 467/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 468/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 469/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 470/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 471/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 472/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 473/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 474/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 475/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 476/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 477/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 478/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 479/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 480/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 481/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 482/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 483/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 484/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 485/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 486/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 487/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 488/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 489/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 490/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 491/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 492/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 493/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 494/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 495/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 496/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 497/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 498/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 499/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 500/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 501/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 502/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 503/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 504/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 505/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 506/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 507/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 508/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 509/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 510/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 511/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 512/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 513/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 514/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 515/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 516/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 517/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 518/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 519/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 520/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 521/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 522/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 523/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 524/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 525/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 526/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 527/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 528/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 529/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 530/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 531/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 532/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 533/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 534/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 535/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 536/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 537/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 538/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 539/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 540/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 541/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 542/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 543/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 544/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 545/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 546/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 547/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 548/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 549/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 550/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 551/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 552/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 553/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 554/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 555/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 556/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 557/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 558/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 559/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 560/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 561/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 562/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 563/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 564/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 565/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 566/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 567/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 568/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 569/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 570/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 571/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 572/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 573/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 574/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 575/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 576/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 577/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 578/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 579/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 580/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 581/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 582/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 583/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 584/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 585/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 586/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 587/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 588/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 589/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 590/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 591/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 592/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 593/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 594/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 595/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 596/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 597/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 598/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 599/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 600/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 601/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 602/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 603/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 604/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 605/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 606/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 607/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 608/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 609/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 610/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 611/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 612/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 613/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 614/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 615/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 616/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 617/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 618/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 619/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 620/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 621/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 622/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 623/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 624/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 625/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 626/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 627/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 628/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 629/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 630/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 631/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 632/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 633/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 634/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 635/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 636/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 637/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 638/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 639/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 640/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 641/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 642/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 643/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 644/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 645/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 646/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 647/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 648/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 649/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 650/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 651/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 652/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 653/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 654/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 655/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 656/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 657/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 658/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 659/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 660/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 661/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 662/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 663/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 664/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 665/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 666/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 667/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 668/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 669/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 670/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 671/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 672/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 673/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 674/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 675/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 676/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 677/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 678/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 679/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 680/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 681/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 682/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 683/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 684/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 685/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 686/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 687/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 688/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 689/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 690/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 691/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 692/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 693/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 694/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 695/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 696/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 697/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 698/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 699/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 700/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 701/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 702/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 703/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 704/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 705/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 706/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 707/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 708/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 709/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 710/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 711/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 712/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 713/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 714/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 715/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 716/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 717/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 718/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 719/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 720/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 721/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 722/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 723/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 724/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 725/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 726/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 727/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 728/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 729/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 730/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 731/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 732/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 733/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 734/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 735/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 736/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 737/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 738/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 739/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 740/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 741/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 742/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 743/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 744/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 745/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 746/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 747/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 748/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 749/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 750/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 751/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 752/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 753/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 754/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 755/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 756/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 757/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 758/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 759/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 760/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 761/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 762/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 763/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 764/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 765/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 766/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 767/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 768/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 769/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 770/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 771/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 772/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 773/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 774/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 775/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 776/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 777/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 778/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 779/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 780/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 781/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 782/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 783/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 784/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 785/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 786/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 787/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 788/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 789/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 790/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 791/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 792/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 793/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 794/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 795/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 796/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 797/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 798/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 799/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 800/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 801/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 802/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 803/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 804/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 805/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 806/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 807/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 808/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 809/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 810/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 811/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 812/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 813/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 814/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 815/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 816/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 817/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 818/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 819/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 820/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 821/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 822/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 823/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 824/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 825/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 826/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 827/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 828/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 829/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 830/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 831/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 832/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 833/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 834/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 835/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 836/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 837/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 838/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 839/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 840/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 841/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 842/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 843/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 844/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 845/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 846/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 847/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 848/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 849/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 850/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 851/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 852/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 853/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 854/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 855/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 856/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 857/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 858/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 859/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 860/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 861/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 862/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 863/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 864/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 865/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 866/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 867/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 868/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 869/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 870/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 871/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 872/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 873/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 874/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 875/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 876/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 877/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 878/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 879/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 880/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 881/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 882/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 883/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 884/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 885/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 886/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 887/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 888/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 889/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 890/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 891/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 892/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 893/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 894/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 895/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 896/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 897/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 898/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 899/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 900/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 901/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 902/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 903/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 904/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 905/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 906/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 907/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 908/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 909/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 910/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 911/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 912/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 913/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 914/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 915/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 916/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 917/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 918/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 919/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 920/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 921/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 922/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 923/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 924/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 925/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 926/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 927/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 928/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 929/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 930/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 931/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 932/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 933/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 934/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 935/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 936/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 937/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 938/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 939/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 940/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 941/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 942/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 943/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 944/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 945/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 946/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 947/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 948/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 949/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 950/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 951/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 952/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 953/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 954/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 955/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 956/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 957/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 958/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 959/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 960/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 961/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 962/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 963/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 964/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 965/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 966/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 967/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 968/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 969/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 970/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 971/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 972/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 973/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 974/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 975/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 976/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 977/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 978/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 979/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 980/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 981/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 982/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 983/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 984/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 985/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 986/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 987/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 988/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 989/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 990/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 991/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 992/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 993/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 994/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 995/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 996/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 997/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 998/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 999/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1000/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1001/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1002/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1003/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1004/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1005/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1006/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1007/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1008/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1009/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1010/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1011/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1012/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1013/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1014/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1015/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1016/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1017/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1018/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1019/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1020/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1021/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1022/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1023/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1024/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1025/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1026/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1027/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1028/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1029/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1030/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1031/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1032/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1033/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1034/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1035/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1036/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1037/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1038/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1039/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1040/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1041/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1042/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1043/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1044/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1045/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1046/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1047/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1048/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1049/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1050/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1051/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 1052/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1053/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1054/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1055/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1056/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1057/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1058/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1059/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1060/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1061/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1062/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1063/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1064/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1065/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1066/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1067/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1068/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1069/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1070/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1071/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1072/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1073/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1074/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1075/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1076/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1077/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1078/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1079/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1080/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1081/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1082/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1083/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1084/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1085/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1086/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1087/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1088/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1089/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1090/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1091/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1092/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1093/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1094/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1095/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1096/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1097/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1098/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1099/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1100/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1101/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1102/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1103/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1104/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1105/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1106/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1107/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1108/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1109/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1110/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1111/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1112/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1113/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1114/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1115/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1116/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1117/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1118/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1119/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1120/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1121/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1122/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1123/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1124/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1125/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1126/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1127/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1128/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1129/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1130/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1131/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1132/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1133/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1134/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1135/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1136/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1137/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1138/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1139/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1140/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1141/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1142/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1143/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1144/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1145/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1146/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1147/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1148/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1149/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1150/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1151/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1152/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1153/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1154/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1155/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1156/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1157/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1158/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1159/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1160/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1161/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1162/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1163/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1164/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1165/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1166/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1167/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1168/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1169/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1170/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1171/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1172/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1173/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1174/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1175/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1176/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1177/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1178/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1179/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1180/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1181/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1182/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1183/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1184/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1185/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1186/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1187/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1188/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1189/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1190/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1191/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1192/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1193/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1194/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1195/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1196/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1197/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1198/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1199/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1200/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1201/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1202/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1203/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1204/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1205/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1206/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1207/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1208/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1209/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1210/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1211/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1212/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1213/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1214/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1215/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1216/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1217/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1218/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1219/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1220/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1221/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1222/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 1223/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1224/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1225/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1226/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1227/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1228/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1229/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1230/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1231/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1232/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1233/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1234/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1235/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1236/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1237/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1238/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1239/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1240/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1241/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1242/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1243/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1244/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1245/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1246/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1247/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1248/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1249/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1250/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1251/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1252/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1253/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1254/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1255/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1256/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1257/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1258/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1259/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1260/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1261/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1262/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1263/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1264/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1265/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1266/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1267/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1268/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1269/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1270/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1271/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1272/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1273/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1274/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1275/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1276/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1277/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1278/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1279/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1280/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1281/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1282/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1283/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1284/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1285/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1286/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1287/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1288/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1289/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1290/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1291/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1292/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1293/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1294/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1295/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1296/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1297/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1298/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1299/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1300/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1301/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1302/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1303/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1304/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1305/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1306/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1307/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1308/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1309/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1310/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1311/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1312/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1313/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1314/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1315/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1316/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1317/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1318/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1319/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1320/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1321/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1322/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1323/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1324/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1325/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1326/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1327/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1328/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1329/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1330/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1331/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1332/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1333/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1334/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1335/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1336/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1337/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1338/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1339/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1340/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1341/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1342/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1343/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1344/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1345/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1346/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1347/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1348/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1349/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1350/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1351/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1352/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1353/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1354/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1355/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1356/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1357/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1358/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1359/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1360/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1361/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1362/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1363/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1364/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1365/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1366/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1367/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1368/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1369/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1370/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1371/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1372/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1373/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1374/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1375/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1376/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1377/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1378/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1379/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1380/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1381/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1382/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1383/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1384/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1385/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1386/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1387/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1388/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1389/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1390/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1391/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1392/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1393/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 1394/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1395/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1396/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1397/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1398/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1399/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1400/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1401/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1402/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1403/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1404/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1405/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1406/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1407/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1408/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1409/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1410/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1411/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1412/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1413/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1414/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1415/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1416/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1417/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1418/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1419/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1420/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1421/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1422/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1423/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1424/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1425/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1426/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1427/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1428/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1429/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1430/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1431/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1432/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1433/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1434/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1435/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1436/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1437/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1438/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1439/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1440/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1441/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1442/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1443/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1444/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1445/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1446/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1447/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1448/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1449/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1450/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1451/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1452/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1453/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1454/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1455/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1456/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1457/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1458/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1459/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1460/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1461/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1462/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1463/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1464/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1465/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1466/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1467/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1468/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1469/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1470/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1471/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1472/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1473/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1474/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1475/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1476/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1477/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1478/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1479/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1480/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1481/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1482/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1483/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1484/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1485/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1486/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1487/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1488/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1489/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1490/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1491/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1492/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1493/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1494/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1495/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1496/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1497/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1498/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1499/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1500/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1501/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1502/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1503/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1504/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1505/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1506/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1507/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1508/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1509/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1510/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1511/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1512/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1513/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1514/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1515/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1516/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1517/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1518/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1519/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1520/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1521/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1522/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1523/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1524/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1525/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1526/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1527/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1528/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1529/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1530/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1531/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1532/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1533/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1534/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1535/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1536/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1537/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1538/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1539/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1540/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1541/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1542/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1543/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1544/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1545/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1546/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1547/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1548/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1549/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1550/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1551/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1552/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1553/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1554/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1555/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1556/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1557/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1558/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1559/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1560/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1561/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1562/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1563/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1564/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 1565/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1566/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1567/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1568/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1569/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1570/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1571/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1572/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1573/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1574/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1575/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1576/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1577/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1578/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1579/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1580/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1581/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1582/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1583/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1584/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1585/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1586/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1587/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1588/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1589/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1590/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1591/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1592/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1593/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1594/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1595/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1596/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1597/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1598/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1599/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1600/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1601/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1602/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1603/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1604/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1605/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1606/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1607/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1608/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1609/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1610/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1611/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1612/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1613/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1614/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1615/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1616/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1617/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1618/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1619/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1620/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1621/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1622/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1623/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1624/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1625/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1626/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1627/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1628/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1629/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1630/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1631/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1632/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1633/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1634/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1635/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1636/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1637/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1638/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1639/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1640/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1641/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1642/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1643/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1644/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1645/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1646/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1647/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1648/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1649/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1650/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1651/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1652/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1653/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1654/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1655/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1656/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1657/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1658/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1659/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1660/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1661/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1662/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1663/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1664/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1665/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1666/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1667/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1668/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1669/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1670/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1671/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1672/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1673/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1674/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1675/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1676/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1677/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1678/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1679/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1680/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1681/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1682/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1683/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1684/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1685/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1686/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1687/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1688/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1689/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1690/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1691/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1692/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1693/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1694/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1695/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1696/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1697/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1698/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1699/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1700/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1701/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1702/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1703/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1704/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1705/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1706/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1707/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1708/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1709/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1710/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1711/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1712/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1713/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1714/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1715/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1716/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1717/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1718/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1719/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1720/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1721/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1722/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1723/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1724/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1725/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1726/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1727/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1728/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1729/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1730/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1731/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1732/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1733/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1734/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1735/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 1736/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1737/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1738/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1739/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1740/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1741/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1742/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1743/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1744/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1745/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1746/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1747/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1748/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1749/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1750/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1751/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1752/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1753/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1754/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1755/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1756/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1757/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1758/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1759/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1760/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1761/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1762/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1763/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1764/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1765/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1766/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1767/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1768/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1769/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1770/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1771/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1772/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1773/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1774/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1775/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1776/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1777/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1778/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1779/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1780/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1781/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1782/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1783/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1784/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1785/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1786/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1787/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1788/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1789/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1790/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1791/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1792/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1793/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1794/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1795/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1796/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1797/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1798/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1799/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1800/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1801/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1802/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1803/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1804/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1805/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1806/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1807/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1808/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1809/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1810/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1811/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1812/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1813/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1814/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1815/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1816/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1817/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1818/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1819/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1820/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1821/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1822/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1823/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1824/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1825/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1826/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1827/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1828/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1829/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1830/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1831/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1832/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1833/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1834/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1835/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1836/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1837/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1838/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1839/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1840/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1841/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1842/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1843/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1844/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1845/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1846/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1847/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1848/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1849/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1850/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1851/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1852/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1853/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1854/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1855/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1856/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1857/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1858/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1859/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1860/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1861/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1862/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1863/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1864/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1865/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1866/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1867/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1868/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1869/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1870/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1871/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1872/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1873/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1874/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1875/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1876/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1877/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1878/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1879/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1880/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1881/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1882/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1883/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1884/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1885/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1886/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1887/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1888/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1889/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1890/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1891/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1892/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1893/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1894/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1895/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1896/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1897/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1898/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1899/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1900/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1901/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1902/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1903/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1904/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1905/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1906/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 1907/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1908/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1909/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1910/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1911/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1912/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1913/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1914/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1915/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1916/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1917/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1918/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1919/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1920/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1921/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1922/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1923/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1924/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1925/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1926/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1927/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1928/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1929/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1930/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1931/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1932/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1933/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1934/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1935/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1936/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1937/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1938/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1939/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1940/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1941/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1942/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1943/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1944/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1945/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1946/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1947/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1948/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1949/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1950/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1951/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1952/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1953/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1954/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1955/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1956/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1957/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1958/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1959/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1960/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1961/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1962/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1963/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1964/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1965/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1966/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1967/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1968/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1969/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1970/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1971/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1972/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1973/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1974/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1975/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1976/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1977/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1978/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1979/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1980/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1981/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1982/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1983/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1984/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1985/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1986/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1987/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1988/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1989/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1990/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1991/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1992/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1993/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1994/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1995/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1996/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1997/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1998/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 1999/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2000/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2001/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2002/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2003/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2004/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2005/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2006/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2007/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2008/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2009/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2010/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2011/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2012/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2013/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2014/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2015/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2016/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2017/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2018/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2019/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2020/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2021/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2022/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2023/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2024/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2025/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2026/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2027/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2028/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2029/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2030/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2031/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2032/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2033/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2034/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2035/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2036/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2037/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2038/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2039/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2040/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2041/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2042/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2043/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2044/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2045/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2046/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2047/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2048/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2049/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2050/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2051/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2052/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2053/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2054/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2055/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2056/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2057/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2058/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2059/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2060/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2061/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2062/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2063/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2064/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2065/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2066/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2067/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2068/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2069/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2070/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2071/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2072/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2073/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2074/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2075/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2076/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2077/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 2078/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2079/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2080/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2081/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2082/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2083/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2084/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2085/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2086/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2087/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2088/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2089/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2090/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2091/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2092/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2093/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2094/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2095/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2096/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2097/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2098/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2099/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2100/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2101/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2102/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2103/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2104/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2105/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2106/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2107/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2108/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2109/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2110/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2111/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2112/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2113/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2114/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2115/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2116/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2117/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2118/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2119/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2120/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2121/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2122/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2123/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2124/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2125/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2126/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2127/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2128/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2129/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2130/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2131/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2132/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2133/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2134/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2135/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2136/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2137/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2138/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2139/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2140/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2141/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2142/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2143/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2144/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2145/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2146/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2147/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2148/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2149/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2150/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2151/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2152/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2153/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2154/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2155/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2156/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2157/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2158/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2159/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2160/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2161/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2162/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2163/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2164/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2165/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2166/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2167/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2168/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2169/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2170/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2171/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2172/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2173/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2174/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2175/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2176/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2177/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2178/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2179/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2180/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2181/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2182/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2183/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2184/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2185/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2186/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2187/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2188/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2189/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2190/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2191/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2192/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2193/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2194/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2195/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2196/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2197/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2198/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2199/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2200/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2201/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2202/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2203/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2204/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2205/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2206/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2207/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2208/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2209/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2210/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2211/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2212/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2213/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2214/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2215/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2216/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2217/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2218/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2219/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2220/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2221/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2222/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2223/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2224/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2225/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2226/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2227/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2228/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2229/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2230/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2231/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2232/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2233/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2234/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2235/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2236/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2237/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2238/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2239/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2240/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2241/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2242/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2243/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2244/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2245/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2246/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2247/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2248/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 2249/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2250/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2251/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2252/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2253/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2254/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2255/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2256/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2257/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2258/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2259/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2260/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2261/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2262/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2263/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2264/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2265/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2266/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2267/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2268/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2269/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2270/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2271/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2272/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2273/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2274/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2275/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2276/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2277/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2278/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2279/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2280/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2281/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2282/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2283/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2284/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2285/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2286/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2287/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2288/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2289/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2290/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2291/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2292/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2293/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2294/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2295/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2296/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2297/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2298/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2299/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2300/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2301/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2302/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2303/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2304/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2305/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2306/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2307/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2308/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2309/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2310/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2311/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2312/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2313/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2314/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2315/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2316/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2317/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2318/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2319/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2320/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2321/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2322/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2323/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2324/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2325/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2326/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2327/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2328/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2329/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2330/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2331/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2332/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2333/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2334/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2335/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2336/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2337/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2338/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2339/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2340/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2341/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2342/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2343/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2344/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2345/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2346/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2347/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2348/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2349/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2350/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2351/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2352/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2353/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2354/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2355/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2356/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2357/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2358/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2359/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2360/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2361/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2362/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2363/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2364/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2365/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2366/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2367/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2368/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2369/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2370/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2371/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2372/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2373/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2374/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2375/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2376/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2377/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2378/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2379/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2380/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2381/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2382/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2383/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2384/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2385/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2386/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2387/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2388/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2389/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2390/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2391/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2392/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2393/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2394/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2395/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2396/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2397/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2398/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2399/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2400/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2401/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2402/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2403/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2404/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2405/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2406/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2407/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2408/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2409/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2410/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2411/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2412/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2413/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2414/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2415/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2416/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2417/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2418/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2419/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 2420/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2421/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2422/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2423/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2424/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2425/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2426/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2427/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2428/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2429/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2430/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2431/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2432/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2433/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2434/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2435/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2436/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2437/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2438/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2439/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2440/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2441/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2442/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2443/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2444/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2445/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2446/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2447/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2448/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2449/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2450/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2451/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2452/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2453/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2454/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2455/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2456/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2457/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2458/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2459/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2460/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2461/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2462/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2463/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2464/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2465/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2466/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2467/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2468/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2469/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2470/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2471/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2472/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2473/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2474/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2475/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2476/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2477/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2478/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2479/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2480/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2481/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2482/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2483/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2484/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2485/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2486/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2487/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2488/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2489/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2490/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2491/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2492/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2493/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2494/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2495/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2496/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2497/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2498/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2499/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2500/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2501/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2502/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2503/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2504/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2505/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2506/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2507/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2508/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2509/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2510/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2511/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2512/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2513/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2514/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2515/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2516/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2517/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2518/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2519/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2520/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2521/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2522/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2523/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2524/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2525/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2526/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2527/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2528/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2529/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2530/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2531/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2532/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2533/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2534/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2535/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2536/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2537/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2538/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2539/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2540/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2541/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2542/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2543/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2544/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2545/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2546/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2547/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2548/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2549/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2550/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2551/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2552/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2553/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2554/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2555/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2556/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2557/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2558/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2559/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2560/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2561/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2562/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2563/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2564/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2565/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2566/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2567/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2568/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2569/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2570/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2571/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2572/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2573/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2574/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2575/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2576/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2577/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2578/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2579/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2580/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2581/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2582/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2583/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2584/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2585/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2586/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2587/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2588/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2589/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2590/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 2591/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2592/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2593/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2594/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2595/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2596/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2597/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2598/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2599/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2600/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2601/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2602/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2603/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2604/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2605/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2606/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2607/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2608/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2609/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2610/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2611/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2612/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2613/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2614/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2615/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2616/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2617/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2618/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2619/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2620/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2621/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2622/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2623/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2624/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2625/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2626/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2627/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2628/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2629/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2630/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2631/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2632/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2633/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2634/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2635/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2636/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2637/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2638/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2639/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2640/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2641/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2642/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2643/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2644/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2645/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2646/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2647/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2648/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2649/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2650/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2651/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2652/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2653/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2654/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2655/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2656/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2657/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2658/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2659/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2660/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2661/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2662/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2663/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2664/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2665/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2666/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2667/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2668/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2669/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2670/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2671/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2672/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2673/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2674/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2675/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2676/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2677/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2678/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2679/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2680/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2681/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2682/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2683/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2684/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2685/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2686/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2687/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2688/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2689/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2690/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2691/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2692/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2693/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2694/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2695/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2696/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2697/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2698/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2699/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2700/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2701/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2702/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2703/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2704/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2705/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2706/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2707/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2708/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2709/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2710/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2711/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2712/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2713/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2714/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2715/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2716/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2717/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2718/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2719/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2720/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2721/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2722/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2723/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2724/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2725/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2726/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2727/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2728/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2729/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2730/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2731/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2732/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2733/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2734/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2735/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2736/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2737/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2738/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2739/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2740/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2741/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2742/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2743/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2744/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2745/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2746/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2747/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2748/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2749/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2750/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2751/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2752/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2753/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2754/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2755/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2756/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2757/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2758/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2759/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2760/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2761/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 2762/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2763/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2764/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2765/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2766/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2767/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2768/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2769/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2770/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2771/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2772/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2773/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2774/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2775/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2776/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2777/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2778/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2779/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2780/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2781/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2782/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2783/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2784/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2785/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2786/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2787/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2788/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2789/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2790/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2791/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2792/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2793/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2794/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2795/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2796/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2797/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2798/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2799/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2800/2800\n",
      "output shape: (50, 768)\n",
      "transformed.shape: (140000, 768)\n"
     ]
    }
   ],
   "source": [
    "X_transformed = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sophisticated-recruitment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "southeast-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('transformed_data.pkl', X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tamil-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read back the data\n",
    "X_transformed = np.load('transformed_data.pkl.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metallic-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.read_pickle('targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "flexible-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note in case of pytorch CrossEntropyLoss, the targets should be the class index, not one hot encoded\n",
    "Y = Y.values.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dominican-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 6, 5, 1], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "original-simple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "sharing-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This could also be a target\n",
    "Y2 = pd.get_dummies(df['Prioritet'], columns=['Prioritet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-slave",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "opposite-hands",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hög</th>\n",
       "      <th>Normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hög  Normal\n",
       "0    0       1\n",
       "1    0       1\n",
       "2    1       0\n",
       "3    0       1\n",
       "4    0       1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "false-chuck",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    hej,\\r\\n \\r\\nhar nu fått tag i föraren som är ...\n",
       "1    buss 000 00:00\\r\\n\\r\\nkristianstad hästtorget ...\n",
       "2    skadeanmälan för påkörning av bil bakifrån vid...\n",
       "3    hej, \\r\\nvarför heter en av hållplatserna i lu...\n",
       "4    hej!\\r\\nhar en fråga som gäller busskurerna i ...\n",
       "Name: Beskrivning_Anonymized, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "european-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hej, \\r\\nvarför heter en av hållplatserna i ludvigsborg kvarndamms gatan i hörby kommun, skåne? dels så finns det ingen väg som heter så där, den heter kvarndamsvägen. dessutom är hållplatsen på ludvigsborgsvägen. xxxx ni vidarebefordra detta till rätt avdelning så det blir ändrat för att inte göra det förvirrat för resenärerna?\\r\\nhälsningar fredrik\\r\\r\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "endangered-handbook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "0          0      0      0                     0         1      0   \n",
       "1          0      0      0                     0         0      0   \n",
       "2          0      0      0                     0         0      1   \n",
       "3          0      0      0                     0         1      0   \n",
       "4          0      0      1                     0         0      0   \n",
       "\n",
       "   Synpunkt/Önskemål  \n",
       "0                  0  \n",
       "1                  1  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "smart-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tok('hej, Har nu fått tag i föraren som är', return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stunning-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  8819,    19,  1177,   346,   902,  1326,    31, 15367,    67,\n",
       "            54,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "monthly-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "arbitrary-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "vulnerable-plain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "broken-petroleum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "935           1      0      0                     0         0      0   \n",
       "1009          1      0      0                     0         0      0   \n",
       "262           0      0      0                     0         0      0   \n",
       "931           1      0      0                     0         0      0   \n",
       "814           0      1      0                     0         0      0   \n",
       "\n",
       "      Synpunkt/Önskemål  \n",
       "935                   0  \n",
       "1009                  0  \n",
       "262                   1  \n",
       "931                   0  \n",
       "814                   0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "automatic-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatcher():\n",
    "    def __init__(self, X, Y, batch_size=100):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.batchId = 0     \n",
    "        \n",
    "    def getBatchIterator(self):\n",
    "        self.batchId = 0\n",
    "        self.epochId = 0\n",
    "        \n",
    "        while True:\n",
    "            X_mini = self.X[self.batchId * self.batch_size:(self.batchId + 1) * self.batch_size]\n",
    "            Y_mini = self.Y[self.batchId * self.batch_size:(self.batchId + 1) * self.batch_size]\n",
    "            self.batchId += 1\n",
    "\n",
    "            if len(X_mini) < self.batch_size:               \n",
    "                self.batchId = 0\n",
    "                break\n",
    "            \n",
    "            yield X_mini, Y_mini\n",
    "            \n",
    "    def getBatchInfo(self):\n",
    "        return f'batch: {self.batchId}/{int(np.ceil(self.X.shape[0] / self.batch_size))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "destroyed-roots",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-fd16a8b523e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmMiniBatcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMiniBatcher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "mMiniBatcher = MiniBatcher(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "protecting-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchIterator = mMiniBatcher.getBatchIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "pressing-mustang",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit called\n",
      "Transform Called\n",
      "Running batch 1/168\n",
      "output shape: (50, 768)\n",
      "Running batch 2/168\n",
      "output shape: (50, 768)\n",
      "Running batch 3/168\n",
      "output shape: (50, 768)\n",
      "Running batch 4/168\n",
      "output shape: (50, 768)\n",
      "Running batch 5/168\n",
      "output shape: (50, 768)\n",
      "Running batch 6/168\n",
      "output shape: (50, 768)\n",
      "Running batch 7/168\n",
      "output shape: (50, 768)\n",
      "Running batch 8/168\n",
      "output shape: (50, 768)\n",
      "Running batch 9/168\n",
      "output shape: (50, 768)\n",
      "Running batch 10/168\n",
      "output shape: (50, 768)\n",
      "Running batch 11/168\n",
      "output shape: (50, 768)\n",
      "Running batch 12/168\n",
      "output shape: (50, 768)\n",
      "Running batch 13/168\n",
      "output shape: (50, 768)\n",
      "Running batch 14/168\n",
      "output shape: (50, 768)\n",
      "Running batch 15/168\n",
      "output shape: (50, 768)\n",
      "Running batch 16/168\n",
      "output shape: (50, 768)\n",
      "Running batch 17/168\n",
      "output shape: (50, 768)\n",
      "Running batch 18/168\n",
      "output shape: (50, 768)\n",
      "Running batch 19/168\n",
      "output shape: (50, 768)\n",
      "Running batch 20/168\n",
      "output shape: (50, 768)\n",
      "Running batch 21/168\n",
      "output shape: (50, 768)\n",
      "Running batch 22/168\n",
      "output shape: (50, 768)\n",
      "Running batch 23/168\n",
      "output shape: (50, 768)\n",
      "Running batch 24/168\n",
      "output shape: (50, 768)\n",
      "Running batch 25/168\n",
      "output shape: (50, 768)\n",
      "Running batch 26/168\n",
      "output shape: (50, 768)\n",
      "Running batch 27/168\n",
      "output shape: (50, 768)\n",
      "Running batch 28/168\n",
      "output shape: (50, 768)\n",
      "Running batch 29/168\n",
      "output shape: (50, 768)\n",
      "Running batch 30/168\n",
      "output shape: (50, 768)\n",
      "Running batch 31/168\n",
      "output shape: (50, 768)\n",
      "Running batch 32/168\n",
      "output shape: (50, 768)\n",
      "Running batch 33/168\n",
      "output shape: (50, 768)\n",
      "Running batch 34/168\n",
      "output shape: (50, 768)\n",
      "Running batch 35/168\n",
      "output shape: (50, 768)\n",
      "Running batch 36/168\n",
      "output shape: (50, 768)\n",
      "Running batch 37/168\n",
      "output shape: (50, 768)\n",
      "Running batch 38/168\n",
      "output shape: (50, 768)\n",
      "Running batch 39/168\n",
      "output shape: (50, 768)\n",
      "Running batch 40/168\n",
      "output shape: (50, 768)\n",
      "Running batch 41/168\n",
      "output shape: (50, 768)\n",
      "Running batch 42/168\n",
      "output shape: (50, 768)\n",
      "Running batch 43/168\n",
      "output shape: (50, 768)\n",
      "Running batch 44/168\n",
      "output shape: (50, 768)\n",
      "Running batch 45/168\n",
      "output shape: (50, 768)\n",
      "Running batch 46/168\n",
      "output shape: (50, 768)\n",
      "Running batch 47/168\n",
      "output shape: (50, 768)\n",
      "Running batch 48/168\n",
      "output shape: (50, 768)\n",
      "Running batch 49/168\n",
      "output shape: (50, 768)\n",
      "Running batch 50/168\n",
      "output shape: (50, 768)\n",
      "Running batch 51/168\n",
      "output shape: (50, 768)\n",
      "Running batch 52/168\n",
      "output shape: (50, 768)\n",
      "Running batch 53/168\n",
      "output shape: (50, 768)\n",
      "Running batch 54/168\n",
      "output shape: (50, 768)\n",
      "Running batch 55/168\n",
      "output shape: (50, 768)\n",
      "Running batch 56/168\n",
      "output shape: (50, 768)\n",
      "Running batch 57/168\n",
      "output shape: (50, 768)\n",
      "Running batch 58/168\n",
      "output shape: (50, 768)\n",
      "Running batch 59/168\n",
      "output shape: (50, 768)\n",
      "Running batch 60/168\n",
      "output shape: (50, 768)\n",
      "Running batch 61/168\n",
      "output shape: (50, 768)\n",
      "Running batch 62/168\n",
      "output shape: (50, 768)\n",
      "Running batch 63/168\n",
      "output shape: (50, 768)\n",
      "Running batch 64/168\n",
      "output shape: (50, 768)\n",
      "Running batch 65/168\n",
      "output shape: (50, 768)\n",
      "Running batch 66/168\n",
      "output shape: (50, 768)\n",
      "Running batch 67/168\n",
      "output shape: (50, 768)\n",
      "Running batch 68/168\n",
      "output shape: (50, 768)\n",
      "Running batch 69/168\n",
      "output shape: (50, 768)\n",
      "Running batch 70/168\n",
      "output shape: (50, 768)\n",
      "Running batch 71/168\n",
      "output shape: (50, 768)\n",
      "Running batch 72/168\n",
      "output shape: (50, 768)\n",
      "Running batch 73/168\n",
      "output shape: (50, 768)\n",
      "Running batch 74/168\n",
      "output shape: (50, 768)\n",
      "Running batch 75/168\n",
      "output shape: (50, 768)\n",
      "Running batch 76/168\n",
      "output shape: (50, 768)\n",
      "Running batch 77/168\n",
      "output shape: (50, 768)\n",
      "Running batch 78/168\n",
      "output shape: (50, 768)\n",
      "Running batch 79/168\n",
      "output shape: (50, 768)\n",
      "Running batch 80/168\n",
      "output shape: (50, 768)\n",
      "Running batch 81/168\n",
      "output shape: (50, 768)\n",
      "Running batch 82/168\n",
      "output shape: (50, 768)\n",
      "Running batch 83/168\n",
      "output shape: (50, 768)\n",
      "Running batch 84/168\n",
      "output shape: (50, 768)\n",
      "Running batch 85/168\n",
      "output shape: (50, 768)\n",
      "Running batch 86/168\n",
      "output shape: (50, 768)\n",
      "Running batch 87/168\n",
      "output shape: (50, 768)\n",
      "Running batch 88/168\n",
      "output shape: (50, 768)\n",
      "Running batch 89/168\n",
      "output shape: (50, 768)\n",
      "Running batch 90/168\n",
      "output shape: (50, 768)\n",
      "Running batch 91/168\n",
      "output shape: (50, 768)\n",
      "Running batch 92/168\n",
      "output shape: (50, 768)\n",
      "Running batch 93/168\n",
      "output shape: (50, 768)\n",
      "Running batch 94/168\n",
      "output shape: (50, 768)\n",
      "Running batch 95/168\n",
      "output shape: (50, 768)\n",
      "Running batch 96/168\n",
      "output shape: (50, 768)\n",
      "Running batch 97/168\n",
      "output shape: (50, 768)\n",
      "Running batch 98/168\n",
      "output shape: (50, 768)\n",
      "Running batch 99/168\n",
      "output shape: (50, 768)\n",
      "Running batch 100/168\n",
      "output shape: (50, 768)\n",
      "Running batch 101/168\n",
      "output shape: (50, 768)\n",
      "Running batch 102/168\n",
      "output shape: (50, 768)\n",
      "Running batch 103/168\n",
      "output shape: (50, 768)\n",
      "Running batch 104/168\n",
      "output shape: (50, 768)\n",
      "Running batch 105/168\n",
      "output shape: (50, 768)\n",
      "Running batch 106/168\n",
      "output shape: (50, 768)\n",
      "Running batch 107/168\n",
      "output shape: (50, 768)\n",
      "Running batch 108/168\n",
      "output shape: (50, 768)\n",
      "Running batch 109/168\n",
      "output shape: (50, 768)\n",
      "Running batch 110/168\n",
      "output shape: (50, 768)\n",
      "Running batch 111/168\n",
      "output shape: (50, 768)\n",
      "Running batch 112/168\n",
      "output shape: (50, 768)\n",
      "Running batch 113/168\n",
      "output shape: (50, 768)\n",
      "Running batch 114/168\n",
      "output shape: (50, 768)\n",
      "Running batch 115/168\n",
      "output shape: (50, 768)\n",
      "Running batch 116/168\n",
      "output shape: (50, 768)\n",
      "Running batch 117/168\n",
      "output shape: (50, 768)\n",
      "Running batch 118/168\n",
      "output shape: (50, 768)\n",
      "Running batch 119/168\n",
      "output shape: (50, 768)\n",
      "Running batch 120/168\n",
      "output shape: (50, 768)\n",
      "Running batch 121/168\n",
      "output shape: (50, 768)\n",
      "Running batch 122/168\n",
      "output shape: (50, 768)\n",
      "Running batch 123/168\n",
      "output shape: (50, 768)\n",
      "Running batch 124/168\n",
      "output shape: (50, 768)\n",
      "Running batch 125/168\n",
      "output shape: (50, 768)\n",
      "Running batch 126/168\n",
      "output shape: (50, 768)\n",
      "Running batch 127/168\n",
      "output shape: (50, 768)\n",
      "Running batch 128/168\n",
      "output shape: (50, 768)\n",
      "Running batch 129/168\n",
      "output shape: (50, 768)\n",
      "Running batch 130/168\n",
      "output shape: (50, 768)\n",
      "Running batch 131/168\n",
      "output shape: (50, 768)\n",
      "Running batch 132/168\n",
      "output shape: (50, 768)\n",
      "Running batch 133/168\n",
      "output shape: (50, 768)\n",
      "Running batch 134/168\n",
      "output shape: (50, 768)\n",
      "Running batch 135/168\n",
      "output shape: (50, 768)\n",
      "Running batch 136/168\n",
      "output shape: (50, 768)\n",
      "Running batch 137/168\n",
      "output shape: (50, 768)\n",
      "Running batch 138/168\n",
      "output shape: (50, 768)\n",
      "Running batch 139/168\n",
      "output shape: (50, 768)\n",
      "Running batch 140/168\n",
      "output shape: (50, 768)\n",
      "Running batch 141/168\n",
      "output shape: (50, 768)\n",
      "Running batch 142/168\n",
      "output shape: (50, 768)\n",
      "Running batch 143/168\n",
      "output shape: (50, 768)\n",
      "Running batch 144/168\n",
      "output shape: (50, 768)\n",
      "Running batch 145/168\n",
      "output shape: (50, 768)\n",
      "Running batch 146/168\n",
      "output shape: (50, 768)\n",
      "Running batch 147/168\n",
      "output shape: (50, 768)\n",
      "Running batch 148/168\n",
      "output shape: (50, 768)\n",
      "Running batch 149/168\n",
      "output shape: (50, 768)\n",
      "Running batch 150/168\n",
      "output shape: (50, 768)\n",
      "Running batch 151/168\n",
      "output shape: (50, 768)\n",
      "Running batch 152/168\n",
      "output shape: (50, 768)\n",
      "Running batch 153/168\n",
      "output shape: (50, 768)\n",
      "Running batch 154/168\n",
      "output shape: (50, 768)\n",
      "Running batch 155/168\n",
      "output shape: (50, 768)\n",
      "Running batch 156/168\n",
      "output shape: (50, 768)\n",
      "Running batch 157/168\n",
      "output shape: (50, 768)\n",
      "Running batch 158/168\n",
      "output shape: (50, 768)\n",
      "Running batch 159/168\n",
      "output shape: (50, 768)\n",
      "Running batch 160/168\n",
      "output shape: (50, 768)\n",
      "Running batch 161/168\n",
      "output shape: (50, 768)\n",
      "Running batch 162/168\n",
      "output shape: (50, 768)\n",
      "Running batch 163/168\n",
      "output shape: (50, 768)\n",
      "Running batch 164/168\n",
      "output shape: (50, 768)\n",
      "Running batch 165/168\n",
      "output shape: (50, 768)\n",
      "Running batch 166/168\n",
      "output shape: (50, 768)\n",
      "Running batch 167/168\n",
      "output shape: (50, 768)\n",
      "Running batch 168/168\n",
      "output shape: (50, 768)\n",
      "transformed.shape: (8400, 768)\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "#output = pipeline.fit(X_train[:25], Y_train[:25].values)\n",
    "#output = pipeline.fit(X_train[:1500], Y_train[:1500].values)\n",
    "output = pipeline.fit(X_train, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "contemporary-masters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/2\n",
      "output shape: (10, 768)\n",
      "Running batch 2/2\n",
      "output shape: (5, 768)\n",
      "transformed.shape: (15, 768)\n"
     ]
    }
   ],
   "source": [
    "probs = output.predict_proba(X_test[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "conceptual-position",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "significant-bottom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "521          0      0      1                     0         0      0   \n",
       "941          0      0      0                     0         0      0   \n",
       "741          0      0      1                     0         0      0   \n",
       "980          0      0      0                     0         0      0   \n",
       "411          0      0      0                     0         0      0   \n",
       "679          0      0      1                     0         0      0   \n",
       "673          0      0      0                     0         0      0   \n",
       "513          0      0      0                     0         1      0   \n",
       "773          0      0      0                     0         1      0   \n",
       "136          0      0      0                     0         0      0   \n",
       "889          0      0      1                     0         0      0   \n",
       "76           0      0      0                     1         0      0   \n",
       "739          0      0      0                     0         0      0   \n",
       "806          0      0      0                     0         0      0   \n",
       "939          0      0      0                     0         0      0   \n",
       "\n",
       "     Synpunkt/Önskemål  \n",
       "521                  0  \n",
       "941                  1  \n",
       "741                  0  \n",
       "980                  1  \n",
       "411                  1  \n",
       "679                  0  \n",
       "673                  1  \n",
       "513                  0  \n",
       "773                  0  \n",
       "136                  1  \n",
       "889                  0  \n",
       "76                   0  \n",
       "739                  1  \n",
       "806                  1  \n",
       "939                  1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "first-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will predict the classes for each row. The class with the highest probability is selected\n",
    "def PredictClasses(model, X):\n",
    "    probs = model.predict_proba(X)\n",
    "    probs = np.array(probs) #List to (N, num_classes, 2)\n",
    "    predictedClasses = np.argmax(probs[:,:,1].T, axis=1) #First index classifies it as 0, second as 1, Then get the max index for each row\n",
    "    \n",
    "    return predictedClasses\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "utility-drive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hej, \\r\\nvarför heter en av hållplatserna i ludvigsborg kvarndamms gatan i hörby kommun, skåne? dels så finns det ingen väg som heter så där, den heter kvarndamsvägen. dessutom är hållplatsen på ludvigsborgsvägen. xxxx ni vidarebefordra detta till rätt avdelning så det blir ändrat för att inte göra det förvirrat för resenärerna?\\r\\nhälsningar fredrik\\r\\r\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "designing-covering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/1\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (10, 768)\n"
     ]
    }
   ],
   "source": [
    "result = PredictClasses(pipeline, X_test[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cooked-bouquet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 6, 3, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "inner-sitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6033</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7051</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "2750          0      0      0                     1         0      0   \n",
       "7487          0      0      0                     1         0      0   \n",
       "5272          0      0      0                     1         0      0   \n",
       "5653          0      0      0                     1         0      0   \n",
       "3999          0      0      0                     1         0      0   \n",
       "6033          0      0      0                     1         0      0   \n",
       "582           0      0      0                     0         1      0   \n",
       "9930          0      0      0                     1         0      0   \n",
       "7051          0      0      0                     1         0      0   \n",
       "8158          0      0      0                     1         0      0   \n",
       "\n",
       "      Synpunkt/Önskemål  \n",
       "2750                  0  \n",
       "7487                  0  \n",
       "5272                  0  \n",
       "5653                  0  \n",
       "3999                  0  \n",
       "6033                  0  \n",
       "582                   0  \n",
       "9930                  0  \n",
       "7051                  0  \n",
       "8158                  0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "proud-effectiveness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 6, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "beneficial-crest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 4, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ost = Y_test[10:20].to_numpy().argmax(axis=1)\n",
    "ost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "proof-warning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "compressed-columbia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(ost, result,normalize='true')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "european-hanging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1ae8f022d00>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix=cm,).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "humanitarian-cisco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ost, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "entire-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-d119748ae7af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#results = PredictClasses(pipeline, X_test[:15])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPredictClasses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-d220fc1fb3d3>\u001b[0m in \u001b[0;36mPredictClasses\u001b[1;34m(model, X)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#This will predict the classes for each row. The class with the highest probability is selected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mPredictClasses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#List to (N, num_classes, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpredictedClasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#First index classifies it as 0, second as 1, Then get the max index for each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, **predict_proba_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_proba_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b00c47e656f5>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0minputs_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatchId\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchId\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pooler_output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'output shape: {outputs.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1020\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         )\n\u001b[1;32m-> 1022\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1023\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    609\u001b[0m                 )\n\u001b[0;32m    610\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    612\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 427\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1832\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1834\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1835\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#results = PredictClasses(pipeline, X_test[:15])\n",
    "results = PredictClasses(pipeline, X_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "distinguished-delight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 4, 6, 6, 6, 4, 4, 6, 4, 6, 4, 6, 6, 6, 6, 6, 2, 4, 6, 6, 2,\n",
       "       2, 4, 4, 4, 4, 6, 2, 4, 4, 6, 6, 6, 4, 4, 6, 2, 6, 4, 4, 4, 3, 6,\n",
       "       4, 6, 2, 6, 2, 6, 6, 2, 6, 6, 4, 6, 4, 4, 4, 6, 4, 4, 4, 6, 6, 6,\n",
       "       6, 2, 4, 6, 6, 6, 6, 6, 4, 4, 2, 4, 6, 4, 6, 4, 2, 6, 4, 2, 6, 6,\n",
       "       2, 2, 4, 4, 4, 4, 6, 4, 4, 4, 4, 6, 6, 4, 4, 2, 2, 6, 4, 4, 3, 6,\n",
       "       6, 4, 6, 6, 4, 4, 6, 6, 6, 4, 6, 6, 6, 2, 4, 6, 6, 4, 6, 6, 6, 6,\n",
       "       6, 6, 6, 4, 4, 4, 6, 4, 4, 6, 6, 4, 6, 2, 6, 4, 2, 4, 4, 2, 6, 2,\n",
       "       6, 6, 2, 6, 4, 6, 6, 4, 6, 6, 2, 6, 4, 6, 3, 4, 4, 4, 2, 6, 4, 4,\n",
       "       6, 2, 6, 2, 4, 6, 4, 2, 6, 2, 4, 6, 6, 6, 4, 6, 6, 4, 2, 6, 6, 2,\n",
       "       6, 2, 6], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "binary-pathology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 5, ..., 4, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.argmax(np.array(Y), axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "spiritual-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Avvikelse', 'Beröm', 'Fråga', 'Förseningsersättning', 'Klagomål',\n",
       "       'Skada', 'Synpunkt/Önskemål'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "integral-contents",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "furnished-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateModelUsingProbs(model, X_test, Y_test):\n",
    "    Y_pred = PredictClasses(model, X_test) #1D-array with classes\n",
    "    targets = np.argmax(np.array(Y_test), axis=1) #1D-array with classes\n",
    "    \n",
    "    total_hits = np.sum(np.sum(Y_pred == targets))\n",
    "    total_misses = np.sum(np.sum(Y_pred != targets))\n",
    "    total_accuracy = total_hits/(total_hits + total_misses)\n",
    "    print(f'Total Accuracy: {total_accuracy}')\n",
    "    #cm = confusion_matrix(targets, Y_pred, normalize='true')\n",
    "    #ConfusionMatrixDisplay(confusion_matrix=cm,).plot()\n",
    "    cm = confusion_matrix(targets, Y_pred, normalize='true', labels=list(range(Y_test.columns.shape[0])))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(Y_test.columns.shape[0]))).plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "surface-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model and print the accuracy\n",
    "\n",
    "def EvaluateModel(model, X_test, Y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    total_hits = np.sum(np.sum(Y_pred == Y_test))\n",
    "    total_misses = np.sum(np.sum(Y_pred != Y_test))\n",
    "    total_accuracy = total_hits/(total_hits + total_misses)\n",
    "       \n",
    "    target_names = [name for name in Y.columns]\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1scores = []\n",
    "    for (name, col) in zip(target_names, range(len(target_names))):\n",
    "        y_test = Y_test[name].values\n",
    "        y_pred = Y_pred[:, col]\n",
    "        \n",
    "        if(np.max(y_test) <= 1):\n",
    "            #Only one category\n",
    "            precisions.append(precision_score(y_test, y_pred))\n",
    "            recalls.append(recall_score(y_test, y_pred))\n",
    "            f1scores.append(f1_score(y_test, y_pred))\n",
    "        print(f'Category: {name}')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print('-'*42)\n",
    "     \n",
    "    \n",
    "    print(f'Total Accuracy: {total_accuracy}')\n",
    "    print(f'Average Precission: {np.average(precisions)}')\n",
    "    print(f'Average Recall: {np.average(recalls)}')\n",
    "    print(f'Average F1 Score: {np.average(f1scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "realistic-conflict",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/10\n",
      "output shape: (10, 768)\n",
      "Running batch 2/10\n",
      "output shape: (10, 768)\n",
      "Running batch 3/10\n",
      "output shape: (10, 768)\n",
      "Running batch 4/10\n",
      "output shape: (10, 768)\n",
      "Running batch 5/10\n",
      "output shape: (10, 768)\n",
      "Running batch 6/10\n",
      "output shape: (10, 768)\n",
      "Running batch 7/10\n",
      "output shape: (10, 768)\n",
      "Running batch 8/10\n",
      "output shape: (10, 768)\n",
      "Running batch 9/10\n",
      "output shape: (10, 768)\n",
      "Running batch 10/10\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (100, 768)\n",
      "Category: Avvikelse\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Beröm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        99\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.49      0.50      0.50       100\n",
      "weighted avg       0.98      0.99      0.99       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Fråga\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86        76\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.76       100\n",
      "   macro avg       0.38      0.50      0.43       100\n",
      "weighted avg       0.58      0.76      0.66       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Förseningsersättning\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        94\n",
      "           1       1.00      0.17      0.29         6\n",
      "\n",
      "    accuracy                           0.95       100\n",
      "   macro avg       0.97      0.58      0.63       100\n",
      "weighted avg       0.95      0.95      0.93       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Klagomål\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.94      0.78        67\n",
      "           1       0.33      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.50      0.50      0.44       100\n",
      "weighted avg       0.56      0.65      0.56       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Skada\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        99\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.49      0.50      0.50       100\n",
      "weighted avg       0.98      0.99      0.99       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Synpunkt/Önskemål\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82        65\n",
      "           1       0.79      0.31      0.45        35\n",
      "\n",
      "    accuracy                           0.73       100\n",
      "   macro avg       0.75      0.63      0.64       100\n",
      "weighted avg       0.74      0.73      0.69       100\n",
      "\n",
      "------------------------------------------\n",
      "Total Accuracy: 0.8671428571428571\n",
      "Average Precission: 0.30272108843537415\n",
      "Average Recall: 0.07736549165120594\n",
      "Average F1 Score: 0.11960828287358898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "EvaluateModel(pipeline, X_test[:100], Y_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "hazardous-austria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/42\n",
      "output shape: (50, 768)\n",
      "Running batch 2/42\n",
      "output shape: (50, 768)\n",
      "Running batch 3/42\n",
      "output shape: (50, 768)\n",
      "Running batch 4/42\n",
      "output shape: (50, 768)\n",
      "Running batch 5/42\n",
      "output shape: (50, 768)\n",
      "Running batch 6/42\n",
      "output shape: (50, 768)\n",
      "Running batch 7/42\n",
      "output shape: (50, 768)\n",
      "Running batch 8/42\n",
      "output shape: (50, 768)\n",
      "Running batch 9/42\n",
      "output shape: (50, 768)\n",
      "Running batch 10/42\n",
      "output shape: (50, 768)\n",
      "Running batch 11/42\n",
      "output shape: (50, 768)\n",
      "Running batch 12/42\n",
      "output shape: (50, 768)\n",
      "Running batch 13/42\n",
      "output shape: (50, 768)\n",
      "Running batch 14/42\n",
      "output shape: (50, 768)\n",
      "Running batch 15/42\n",
      "output shape: (50, 768)\n",
      "Running batch 16/42\n",
      "output shape: (50, 768)\n",
      "Running batch 17/42\n",
      "output shape: (50, 768)\n",
      "Running batch 18/42\n",
      "output shape: (50, 768)\n",
      "Running batch 19/42\n",
      "output shape: (50, 768)\n",
      "Running batch 20/42\n",
      "output shape: (50, 768)\n",
      "Running batch 21/42\n",
      "output shape: (50, 768)\n",
      "Running batch 22/42\n",
      "output shape: (50, 768)\n",
      "Running batch 23/42\n",
      "output shape: (50, 768)\n",
      "Running batch 24/42\n",
      "output shape: (50, 768)\n",
      "Running batch 25/42\n",
      "output shape: (50, 768)\n",
      "Running batch 26/42\n",
      "output shape: (50, 768)\n",
      "Running batch 27/42\n",
      "output shape: (50, 768)\n",
      "Running batch 28/42\n",
      "output shape: (50, 768)\n",
      "Running batch 29/42\n",
      "output shape: (50, 768)\n",
      "Running batch 30/42\n",
      "output shape: (50, 768)\n",
      "Running batch 31/42\n",
      "output shape: (50, 768)\n",
      "Running batch 32/42\n",
      "output shape: (50, 768)\n",
      "Running batch 33/42\n",
      "output shape: (50, 768)\n",
      "Running batch 34/42\n",
      "output shape: (50, 768)\n",
      "Running batch 35/42\n",
      "output shape: (50, 768)\n",
      "Running batch 36/42\n",
      "output shape: (50, 768)\n",
      "Running batch 37/42\n",
      "output shape: (50, 768)\n",
      "Running batch 38/42\n",
      "output shape: (50, 768)\n",
      "Running batch 39/42\n",
      "output shape: (50, 768)\n",
      "Running batch 40/42\n",
      "output shape: (50, 768)\n",
      "Running batch 41/42\n",
      "output shape: (50, 768)\n",
      "Running batch 42/42\n",
      "output shape: (50, 768)\n",
      "transformed.shape: (2100, 768)\n",
      "Total Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EvaluateModelUsingProbs(pipeline, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "illegal-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/80\n",
      "output shape: (10, 768)\n",
      "Running batch 2/80\n",
      "output shape: (10, 768)\n",
      "Running batch 3/80\n",
      "output shape: (10, 768)\n",
      "Running batch 4/80\n",
      "output shape: (10, 768)\n",
      "Running batch 5/80\n",
      "output shape: (10, 768)\n",
      "Running batch 6/80\n",
      "output shape: (10, 768)\n",
      "Running batch 7/80\n",
      "output shape: (10, 768)\n",
      "Running batch 8/80\n",
      "output shape: (10, 768)\n",
      "Running batch 9/80\n",
      "output shape: (10, 768)\n",
      "Running batch 10/80\n",
      "output shape: (10, 768)\n",
      "Running batch 11/80\n",
      "output shape: (10, 768)\n",
      "Running batch 12/80\n",
      "output shape: (10, 768)\n",
      "Running batch 13/80\n",
      "output shape: (10, 768)\n",
      "Running batch 14/80\n",
      "output shape: (10, 768)\n",
      "Running batch 15/80\n",
      "output shape: (10, 768)\n",
      "Running batch 16/80\n",
      "output shape: (10, 768)\n",
      "Running batch 17/80\n",
      "output shape: (10, 768)\n",
      "Running batch 18/80\n",
      "output shape: (10, 768)\n",
      "Running batch 19/80\n",
      "output shape: (10, 768)\n",
      "Running batch 20/80\n",
      "output shape: (10, 768)\n",
      "Running batch 21/80\n",
      "output shape: (10, 768)\n",
      "Running batch 22/80\n",
      "output shape: (10, 768)\n",
      "Running batch 23/80\n",
      "output shape: (10, 768)\n",
      "Running batch 24/80\n",
      "output shape: (10, 768)\n",
      "Running batch 25/80\n",
      "output shape: (10, 768)\n",
      "Running batch 26/80\n",
      "output shape: (10, 768)\n",
      "Running batch 27/80\n",
      "output shape: (10, 768)\n",
      "Running batch 28/80\n",
      "output shape: (10, 768)\n",
      "Running batch 29/80\n",
      "output shape: (10, 768)\n",
      "Running batch 30/80\n",
      "output shape: (10, 768)\n",
      "Running batch 31/80\n",
      "output shape: (10, 768)\n",
      "Running batch 32/80\n",
      "output shape: (10, 768)\n",
      "Running batch 33/80\n",
      "output shape: (10, 768)\n",
      "Running batch 34/80\n",
      "output shape: (10, 768)\n",
      "Running batch 35/80\n",
      "output shape: (10, 768)\n",
      "Running batch 36/80\n",
      "output shape: (10, 768)\n",
      "Running batch 37/80\n",
      "output shape: (10, 768)\n",
      "Running batch 38/80\n",
      "output shape: (10, 768)\n",
      "Running batch 39/80\n",
      "output shape: (10, 768)\n",
      "Running batch 40/80\n",
      "output shape: (10, 768)\n",
      "Running batch 41/80\n",
      "output shape: (10, 768)\n",
      "Running batch 42/80\n",
      "output shape: (10, 768)\n",
      "Running batch 43/80\n",
      "output shape: (10, 768)\n",
      "Running batch 44/80\n",
      "output shape: (10, 768)\n",
      "Running batch 45/80\n",
      "output shape: (10, 768)\n",
      "Running batch 46/80\n",
      "output shape: (10, 768)\n",
      "Running batch 47/80\n",
      "output shape: (10, 768)\n",
      "Running batch 48/80\n",
      "output shape: (10, 768)\n",
      "Running batch 49/80\n",
      "output shape: (10, 768)\n",
      "Running batch 50/80\n",
      "output shape: (10, 768)\n",
      "Running batch 51/80\n",
      "output shape: (10, 768)\n",
      "Running batch 52/80\n",
      "output shape: (10, 768)\n",
      "Running batch 53/80\n",
      "output shape: (10, 768)\n",
      "Running batch 54/80\n",
      "output shape: (10, 768)\n",
      "Running batch 55/80\n",
      "output shape: (10, 768)\n",
      "Running batch 56/80\n",
      "output shape: (10, 768)\n",
      "Running batch 57/80\n",
      "output shape: (10, 768)\n",
      "Running batch 58/80\n",
      "output shape: (10, 768)\n",
      "Running batch 59/80\n",
      "output shape: (10, 768)\n",
      "Running batch 60/80\n",
      "output shape: (10, 768)\n",
      "Running batch 61/80\n",
      "output shape: (10, 768)\n",
      "Running batch 62/80\n",
      "output shape: (10, 768)\n",
      "Running batch 63/80\n",
      "output shape: (10, 768)\n",
      "Running batch 64/80\n",
      "output shape: (10, 768)\n",
      "Running batch 65/80\n",
      "output shape: (10, 768)\n",
      "Running batch 66/80\n",
      "output shape: (10, 768)\n",
      "Running batch 67/80\n",
      "output shape: (10, 768)\n",
      "Running batch 68/80\n",
      "output shape: (10, 768)\n",
      "Running batch 69/80\n",
      "output shape: (10, 768)\n",
      "Running batch 70/80\n",
      "output shape: (10, 768)\n",
      "Running batch 71/80\n",
      "output shape: (10, 768)\n",
      "Running batch 72/80\n",
      "output shape: (10, 768)\n",
      "Running batch 73/80\n",
      "output shape: (10, 768)\n",
      "Running batch 74/80\n",
      "output shape: (10, 768)\n",
      "Running batch 75/80\n",
      "output shape: (10, 768)\n",
      "Running batch 76/80\n",
      "output shape: (10, 768)\n",
      "Running batch 77/80\n",
      "output shape: (10, 768)\n",
      "Running batch 78/80\n",
      "output shape: (10, 768)\n",
      "Running batch 79/80\n",
      "output shape: (10, 768)\n",
      "Running batch 80/80\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (800, 768)\n",
      "Total Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "EvaluateModelUsingProbs(pipeline, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "intermediate-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hej,\\r\\r\\n \\r\\r\\nhar nu fått tag i föraren som är i grekland på semester.\\r\\r\\n \\r\\r\\nden unga killen som frågade fick svaret av föraren att det är läge xxxx som gäller på knutpunkten.\\r\\r\\n\\r\\r\\ndetta gav föraren resenären efter att ha själv letat upp det på skånetrafikens app.\\r\\r\\nmotparten lämnar tyvärr en del information till sin egen fördel.\\r\\r\\nföraren försökte att hjälpa så gott han kunde.\\r\\r\\n \\r\\r\\n\\r\\r\\nmed vänlig hälsning,\\r\\r\\n  xxxx prata\\r\\r\\n\\r\\r\\ngruppchef\\r\\r\\n \\r\\r\\nnobina sverige ab\\r\\r\\n\\r\\r\\nadress: streetname_replaced 00, 000 00 helsingborg\\r\\r\\nmobil:  +00 00 000 00 00\\r\\r\\ndirekt: +00 0 000 000 00\\r\\r\\n\\r\\r\\nväxel:  +00 0 000 000 00\\r\\r\\n\\r\\r\\nhemsida:\\r\\r\\nwww.nobina.com\\r\\r\\n \\r\\r\\nnobina sverige ab, org.nr 000000-0000 | styrelsens säte - stockholm\\r\\r\\n\\r\\r\\np please consider the environment before printing this e-mail.\\r\\r\\n\\r\\r\\n \\r\\r\\n\\r\\r\\nfrån: xxxx prata \\r\\r\\n\\r\\r\\nskickat: den 00 oktober 0000 00:00\\r\\r\\n\\r\\r\\ntill: email@address.replaced email@address.replaced\\r\\r\\n\\r\\r\\nkopia: xxxx xxxx email@address.replaced\\r\\r\\n\\r\\r\\nämne: vb: för kännedom ärendenummer: st-000000-w0k0 epostnr:00000000\\r\\r\\n\\r\\r\\n \\r\\r\\nhej,\\r\\r\\n \\r\\r\\nföraren är på semester och är åter 00/00.\\r\\r\\n\\r\\r\\nhar försökt att nå föraren som inte svarar.\\r\\r\\nmen stämmer händelsen så är det verkligen inte ett bra beteende ifrån vår förare.\\r\\r\\njag kommer att ta upp detta ärendet när han är tillbaka.\\r\\r\\n \\r\\r\\n\\r\\r\\nmed vänlig hälsning,\\r\\r\\n  xxxx prata\\r\\r\\n\\r\\r\\ngruppchef\\r\\r\\n \\r\\r\\nnobina sverige ab\\r\\r\\n\\r\\r\\nadress: streetname_replaced 00, 000 00 helsingborg\\r\\r\\nmobil:  +00 00 000 00 00\\r\\r\\ndirekt: +00 0 000 000 00\\r\\r\\n\\r\\r\\nväxel:  +00 0 000 000 00\\r\\r\\n\\r\\r\\nhemsida:\\r\\r\\nwww.nobina.com\\r\\r\\n \\r\\r\\nnobina sverige ab, org.nr 000000-0000 | styrelsens säte - stockholm\\r\\r\\n\\r\\r\\np please consider the environment before printing this e-mail.\\r\\r\\n\\r\\r\\n \\r\\r\\n\\r\\r\\nfrån: xxxx xxxx \\r\\r\\n\\r\\r\\nskickat: den 00 oktober 0000 00:00\\r\\r\\n\\r\\r\\ntill: xxxx prata email@address.replaced\\r\\r\\n\\r\\r\\nämne: vb: för kännedom ärendenummer: st-000000-w0k0 epostnr:00000000\\r\\r\\n\\r\\r\\n \\r\\r\\nhej xxxx xxxx r har fått ett klagomål som jag lagt in i oms\\r\\r\\n \\r\\r\\nmed vänlig hälsning,\\r\\r\\n  xxxx  xxxx \\r\\r\\nkams-chef\\r\\r\\n(kvalitet, arbetsmiljö, miljö, säkerhet)\\r\\r\\n \\r\\r\\nnobina sverige ab\\r\\r\\n\\r\\r\\nadress: streetname_replaced 00, 000 00 helsingborg\\r\\r\\nmobil:  +00 00 000 00 00\\r\\r\\ndirekt: +00 00 000 00 00\\r\\r\\n\\r\\r\\nväxel:  +00 0 000 000 00\\r\\r\\n\\r\\r\\nhemsida:\\r\\r\\nwww.nobina.com\\r\\r\\n \\r\\r\\nnobina sverige ab, org.nr 000000-0000 | styrelsens säte - stockholm\\r\\r\\n\\r\\r\\np please consider the environment before printing this e-mail.\\r\\r\\n \\r\\r\\nfrån: xxxx xxxx email@address.replaced\\r\\r\\n\\r\\r\\nskickat: den 00 oktober 0000 00:00\\r\\r\\n\\r\\r\\ntill: xxxx xxxx email@address.replaced\\r\\r\\n\\r\\r\\nämne: för kännedom ärendenummer: st-000000-w0k0 epostnr:00000000\\r\\r\\n \\r\\r\\nhej! \\r\\r\\ndetta är ett ärende från kund som vi tagit emot och som vi känner att även du behöver få kännedom om.\\r\\r\\nhälsningar\\r\\r\\nskånetrafikens kundtjänst\\r\\r\\n \\r\\r\\ninformation om ärendet \\r\\r\\närenderubrik: bemötande\\u2009\\r\\r\\närendetyp: \\u2009klagomål\\r\\r\\n\\u2009händelsedatum: 0000-00-00 00:00\\u2009\\r\\r\\nkundens beskrivning: xxxx borde nog ge era chaufförer i helsingborg en lektion i service!!! här frågar man en busschaufför om hans buss går till streetname_replaced och han snäser av: var vill du? xxxx  svarar: vi vill till streetname_replaced!!\\r\\r\\n\\r\\r\\nhan: jag är ingen taxi så kan inte svara dig???? va fan är detta?? \\r\\r\\n\\r\\r\\nchauffören det gäller körde följande buss från centralen ca 00.00 idag söndag! bussen reg nr:\\r\\r\\n\\r\\r\\nhoppas ni tar tag i detta!!😡😡\\r\\r\\n\\r\\r\\nvet inte vilken avgång det rör sig om men reg.nr är xbc 000\\u2009\\r\\r\\ntrafikslag: \\u2009\\r\\r\\nlinje: \\u2009\\r\\r\\ntur: \\u2009\\r\\r\\navgångstid planerad: \\u2009\\r\\r\\navgångstid verklig: \\u2009\\r\\r\\nlinjetext:',\n",
       "       'buss 000 00:00\\r\\r\\n\\r\\r\\nkristianstad hästtorget  - xxxx streetname_replaced \\r\\r\\n\\r\\r\\ninget ljud på automatiska utropen utanför bussen, som berättar vilket nummer på bussen och vilken slutstation linjen har. \\r\\r\\n\\r\\r\\nbra volym på automatiska hållplats utropen inne i bussen.\\r\\r\\n\\r\\r\\nbad föraren om hjälp av med rullatorn, jag var tvungen att säga till två gånger innan föraren förstod.\\r\\r\\n\\r\\r\\nnär föraren förstod fick jag utmärkt hjälp ut genom främre dörren. \\r\\r\\n xxxx 000 00:00\\r\\r\\n xxxx streetname_replaced - kristianstad streetname_replaced \\r\\r\\n\\r\\r\\ninget ljud på automatiska utropen utanför bussen, som berättar vilket nummer på bussen\\r\\r\\n och vilken slutstation linjen har. \\r\\r\\n\\r\\r\\ninget ljud på automatiska hållplats utropen inne i bussen.\\r\\r\\n\\r\\r\\nbad föraren om hjälp av med rullatorn, jag var tvungen att säga till två gånger innan\\r\\r\\n föraren förstod.\\r\\r\\n\\r\\r\\nnär föraren förstod fick jag utmärkt hjälp ut genom främre dörren. \\r\\r\\n\\r\\r\\nmed vänlig hälsningar.\\r\\r\\n xxxx johnson'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "subtle-webster",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dense-textbook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jag vill ha ersättning för min biljett, då tåg...\n",
       "1                      bussen hade dålig air condition\n",
       "2    beskrivning i avic:\\r\\r\\n xxxx sjukresebeställ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "filled-helmet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/1\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (10, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 1, 4, 4, 6, 6, 4, 2, 5, 0], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictClasses(pipeline, X[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "postal-search",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "10          0      0      1                     0         0      0   \n",
       "11          0      0      0                     0         0      0   \n",
       "12          0      0      0                     0         0      0   \n",
       "13          0      0      0                     0         0      0   \n",
       "14          0      0      0                     0         0      0   \n",
       "15          0      0      0                     0         1      0   \n",
       "16          0      0      0                     0         1      0   \n",
       "17          0      0      1                     0         0      0   \n",
       "18          0      0      0                     0         1      0   \n",
       "19          0      0      0                     0         1      0   \n",
       "\n",
       "    Synpunkt/Önskemål  \n",
       "10                  0  \n",
       "11                  1  \n",
       "12                  1  \n",
       "13                  1  \n",
       "14                  1  \n",
       "15                  0  \n",
       "16                  0  \n",
       "17                  0  \n",
       "18                  0  \n",
       "19                  0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "danish-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save, Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "productive-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "great-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'BertModel.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "sporting-mouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertModel.joblib']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dump(pipeline, modelName ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "improved-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedModel = load(modelName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pretty-failure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/2\n",
      "output shape: (10, 768)\n",
      "Running batch 2/2\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (20, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel.predict(X_test[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flying-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adolescent-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TorchNLP(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        #print('Init called')\n",
    "        self.model_name = 'KB/bert-base-swedish-cased'\n",
    "        self.Bert = AutoModel.from_pretrained(self.model_name)\n",
    "        self.Tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.n_classes = n_classes\n",
    "        self.TorchModel = nn.Sequential(nn.Linear(768, 256),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.42),\n",
    "                          nn.Linear(256, 128),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.42),              \n",
    "                          nn.Linear(128, n_classes))\n",
    "                          #nn.Softmax(dim=1)) Cannot use softmax here since nn.CrossEntropyLoss expects scores!\n",
    "        \n",
    "        #Freeze the Bert model layers\n",
    "        for param in self.Bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, X):\n",
    "        #print('Forward Called')\n",
    "       \n",
    "        #Check device\n",
    "        device = self.Bert.device\n",
    "        \n",
    "        # Transform input tokens. This is most efficient if done in one batch \n",
    "        X = self.Tokenizer(X.values.tolist(), return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True).to(device)\n",
    "        \n",
    "        X = self.Bert(**X)\n",
    "        X = X['pooler_output']\n",
    "        X = self.TorchModel(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            output = self.forward(X)\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "            top_class = top_class.to('cpu').numpy().reshape(-1,)\n",
    "            self.train()\n",
    "            return top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "underlying-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the slim version where the BERT features is already provided as input\n",
    "class TorchNLPLight(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        #print('Init called')\n",
    "        self.n_classes = n_classes\n",
    "        self.TorchModel = nn.Sequential(nn.Conv1d(1, 256, 3, stride=1, padding='same'),\n",
    "                          nn.Flatten(),\n",
    "                          nn.BatchNorm1d(256*768),              \n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.42),\n",
    "                          nn.Linear(256*768, 1024),\n",
    "                          nn.BatchNorm1d(1024),              \n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.42),\n",
    "                          nn.Linear(1024, 512),\n",
    "                          nn.BatchNorm1d(512),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.42),\n",
    "                          nn.Linear(512, 256),\n",
    "                          nn.BatchNorm1d(256),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.42),            \n",
    "                          nn.Linear(256, n_classes))\n",
    "                          #nn.Softmax(dim=1)) Cannot use softmax here since nn.CrossEntropyLoss expects scores!\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print('Forward Called')\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1]) #Reshape for Conv1d\n",
    "        X = self.TorchModel(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            output = self.forward(X)\n",
    "            #print(f'X.shape: {X.shape}')\n",
    "            #print(f'output.shape: {output.shape}')\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "            top_class = top_class.to('cpu').numpy().reshape(-1,)\n",
    "            #print(f'top_class.shape: {top_class.shape}')\n",
    "            self.train()\n",
    "            return top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "local-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testModel = nn.Conv1d(1, 768, 3, stride=1, padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "quick-serum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed[:5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "oriented-lloyd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X_transformed[:5].reshape(-1,768,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "extensive-throw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768, 768])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testModel(torch.tensor(X_transformed[:5].reshape(-1,1,768))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "humanitarian-captain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 589824])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testModel(torch.tensor(X_transformed[:5].reshape(-1,1,768))).view(5,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "accompanied-garlic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589824"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "blocked-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = Y.max() + 1\n",
    "torchModelLight = TorchNLPLight(n_classes = n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "seventh-adrian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "deluxe-indication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#n_classes = Y.shape[1] #In case of one hot encoded, which we don't have anymore\n",
    "n_classes = Y.max() + 1\n",
    "torchModel = TorchNLP(n_classes = n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "reduced-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Called\n"
     ]
    }
   ],
   "source": [
    "output = torchModel(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "welsh-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1394, 0.1599, 0.1548, 0.1324, 0.1248, 0.1408, 0.1479],\n",
       "        [0.1428, 0.1565, 0.1440, 0.1306, 0.1277, 0.1471, 0.1513],\n",
       "        [0.1410, 0.1569, 0.1521, 0.1396, 0.1235, 0.1379, 0.1489],\n",
       "        [0.1318, 0.1499, 0.1585, 0.1312, 0.1373, 0.1510, 0.1404],\n",
       "        [0.1423, 0.1517, 0.1463, 0.1242, 0.1348, 0.1532, 0.1476]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "understood-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mMiniBatcherTrain = MiniBatcher(X_train[:1000], Y_train[:1000], batch_size=15)\n",
    "#mMiniBatcherTest = MiniBatcher(X_test[:100], Y_test[:100], batch_size=15)\n",
    "mMiniBatcherTrain = MiniBatcher(X_train, Y_train, batch_size=100)\n",
    "mMiniBatcherTest = MiniBatcher(X_test[:500], Y_test[:500], batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aware-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([50325, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([512, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([2, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([128, 768]) True\n",
      "<class 'torch.Tensor'> torch.Size([128]) True\n",
      "<class 'torch.Tensor'> torch.Size([64, 128]) True\n",
      "<class 'torch.Tensor'> torch.Size([64]) True\n",
      "<class 'torch.Tensor'> torch.Size([7, 64]) True\n",
      "<class 'torch.Tensor'> torch.Size([7]) True\n"
     ]
    }
   ],
   "source": [
    "for param in torchModel.parameters():\n",
    "    print(type(param.data), param.size(), param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bronze-harvest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "cuda_enabled = torch.cuda.is_available()\n",
    "if cuda_enabled:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f'We are running on device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "serious-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "recreational-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install GPUtil\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "allied-projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM  |\n",
      "-------------------\n",
      "|  0 | 19% | 100% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 25% |  8% |\n"
     ]
    }
   ],
   "source": [
    "#free_gpu_cache()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "involved-workshop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1/15\n",
      "Batch loss: 1.954135537147522 batch: 1/840\n",
      "Batch loss: 1.9362761974334717 batch: 2/840\n",
      "Batch loss: 1.8906426429748535 batch: 3/840\n",
      "Batch loss: 1.8690556287765503 batch: 4/840\n",
      "Batch loss: 1.8202908039093018 batch: 5/840\n",
      "Batch loss: 1.835722804069519 batch: 6/840\n",
      "Batch loss: 1.8324123620986938 batch: 7/840\n",
      "Batch loss: 1.763242244720459 batch: 8/840\n",
      "Batch loss: 1.6997215747833252 batch: 9/840\n",
      "Batch loss: 1.6596179008483887 batch: 10/840\n",
      "Batch loss: 1.5897629261016846 batch: 11/840\n",
      "Batch loss: 1.539951205253601 batch: 12/840\n",
      "Batch loss: 1.4772002696990967 batch: 13/840\n",
      "Batch loss: 1.5468162298202515 batch: 14/840\n",
      "Batch loss: 1.5089606046676636 batch: 15/840\n",
      "Batch loss: 1.3836619853973389 batch: 16/840\n",
      "Batch loss: 1.4789026975631714 batch: 17/840\n",
      "Batch loss: 1.3889561891555786 batch: 18/840\n",
      "Batch loss: 1.5200637578964233 batch: 19/840\n",
      "Batch loss: 1.5299617052078247 batch: 20/840\n",
      "Batch loss: 1.3485318422317505 batch: 21/840\n",
      "Batch loss: 1.2940984964370728 batch: 22/840\n",
      "Batch loss: 1.5352778434753418 batch: 23/840\n",
      "Batch loss: 1.235742449760437 batch: 24/840\n",
      "Batch loss: 1.2986042499542236 batch: 25/840\n",
      "Batch loss: 1.2476935386657715 batch: 26/840\n",
      "Batch loss: 1.3339478969573975 batch: 27/840\n",
      "Batch loss: 1.2867741584777832 batch: 28/840\n",
      "Batch loss: 1.2653846740722656 batch: 29/840\n",
      "Batch loss: 1.0862339735031128 batch: 30/840\n",
      "Batch loss: 1.220850944519043 batch: 31/840\n",
      "Batch loss: 1.1114062070846558 batch: 32/840\n",
      "Batch loss: 1.1543275117874146 batch: 33/840\n",
      "Batch loss: 1.1870888471603394 batch: 34/840\n",
      "Batch loss: 1.1671419143676758 batch: 35/840\n",
      "Batch loss: 1.1610159873962402 batch: 36/840\n",
      "Batch loss: 1.3352938890457153 batch: 37/840\n",
      "Batch loss: 1.2488096952438354 batch: 38/840\n",
      "Batch loss: 1.264317274093628 batch: 39/840\n",
      "Batch loss: 1.0311836004257202 batch: 40/840\n",
      "Batch loss: 1.1945686340332031 batch: 41/840\n",
      "Batch loss: 1.0834273099899292 batch: 42/840\n",
      "Batch loss: 1.1145806312561035 batch: 43/840\n",
      "Batch loss: 1.2047219276428223 batch: 44/840\n",
      "Batch loss: 1.0403906106948853 batch: 45/840\n",
      "Batch loss: 1.0007107257843018 batch: 46/840\n",
      "Batch loss: 0.9474323391914368 batch: 47/840\n",
      "Batch loss: 1.1333937644958496 batch: 48/840\n",
      "Batch loss: 1.1486001014709473 batch: 49/840\n",
      "Batch loss: 1.1545413732528687 batch: 50/840\n",
      "Batch loss: 1.1436004638671875 batch: 51/840\n",
      "Batch loss: 1.1851648092269897 batch: 52/840\n",
      "Batch loss: 1.0394036769866943 batch: 53/840\n",
      "Batch loss: 1.0036412477493286 batch: 54/840\n",
      "Batch loss: 1.0303726196289062 batch: 55/840\n",
      "Batch loss: 1.1121147871017456 batch: 56/840\n",
      "Batch loss: 1.028930902481079 batch: 57/840\n",
      "Batch loss: 0.8809975981712341 batch: 58/840\n",
      "Batch loss: 1.0596389770507812 batch: 59/840\n",
      "Batch loss: 0.9840267300605774 batch: 60/840\n",
      "Batch loss: 1.080477237701416 batch: 61/840\n",
      "Batch loss: 0.9393677711486816 batch: 62/840\n",
      "Batch loss: 0.9547160863876343 batch: 63/840\n",
      "Batch loss: 1.0325440168380737 batch: 64/840\n",
      "Batch loss: 1.0178077220916748 batch: 65/840\n",
      "Batch loss: 1.0337761640548706 batch: 66/840\n",
      "Batch loss: 1.0157135725021362 batch: 67/840\n",
      "Batch loss: 0.893993079662323 batch: 68/840\n",
      "Batch loss: 1.0283994674682617 batch: 69/840\n",
      "Batch loss: 0.9264284372329712 batch: 70/840\n",
      "Batch loss: 1.1055896282196045 batch: 71/840\n",
      "Batch loss: 0.9738218784332275 batch: 72/840\n",
      "Batch loss: 0.9335829019546509 batch: 73/840\n",
      "Batch loss: 0.8083973526954651 batch: 74/840\n",
      "Batch loss: 1.0539231300354004 batch: 75/840\n",
      "Batch loss: 0.7630273699760437 batch: 76/840\n",
      "Batch loss: 0.9329349994659424 batch: 77/840\n",
      "Batch loss: 1.1043530702590942 batch: 78/840\n",
      "Batch loss: 0.992200493812561 batch: 79/840\n",
      "Batch loss: 0.9793035387992859 batch: 80/840\n",
      "Batch loss: 0.8959264159202576 batch: 81/840\n",
      "Batch loss: 1.0197243690490723 batch: 82/840\n",
      "Batch loss: 0.766976535320282 batch: 83/840\n",
      "Batch loss: 0.9879166483879089 batch: 84/840\n",
      "Batch loss: 0.9384203553199768 batch: 85/840\n",
      "Batch loss: 1.0815166234970093 batch: 86/840\n",
      "Batch loss: 0.8335462808609009 batch: 87/840\n",
      "Batch loss: 0.8019119501113892 batch: 88/840\n",
      "Batch loss: 0.7948311567306519 batch: 89/840\n",
      "Batch loss: 0.8508981466293335 batch: 90/840\n",
      "Batch loss: 0.8373168706893921 batch: 91/840\n",
      "Batch loss: 0.8426118493080139 batch: 92/840\n",
      "Batch loss: 0.9080469012260437 batch: 93/840\n",
      "Batch loss: 0.8164279460906982 batch: 94/840\n",
      "Batch loss: 0.8902287483215332 batch: 95/840\n",
      "Batch loss: 0.8466624617576599 batch: 96/840\n",
      "Batch loss: 0.8288511633872986 batch: 97/840\n",
      "Batch loss: 0.9889401197433472 batch: 98/840\n",
      "Batch loss: 0.8436118364334106 batch: 99/840\n",
      "Batch loss: 0.8645439743995667 batch: 100/840\n",
      "Batch loss: 0.7811578512191772 batch: 101/840\n",
      "Batch loss: 0.8405274152755737 batch: 102/840\n",
      "Batch loss: 0.9149134755134583 batch: 103/840\n",
      "Batch loss: 0.6584300994873047 batch: 104/840\n",
      "Batch loss: 0.8460894227027893 batch: 105/840\n",
      "Batch loss: 0.8819904327392578 batch: 106/840\n",
      "Batch loss: 0.782034158706665 batch: 107/840\n",
      "Batch loss: 1.0001685619354248 batch: 108/840\n",
      "Batch loss: 0.902615487575531 batch: 109/840\n",
      "Batch loss: 0.8200502991676331 batch: 110/840\n",
      "Batch loss: 0.752482533454895 batch: 111/840\n",
      "Batch loss: 0.9851238131523132 batch: 112/840\n",
      "Batch loss: 0.837787926197052 batch: 113/840\n",
      "Batch loss: 0.8182244896888733 batch: 114/840\n",
      "Batch loss: 0.7908470034599304 batch: 115/840\n",
      "Batch loss: 0.7464261651039124 batch: 116/840\n",
      "Batch loss: 0.7903379201889038 batch: 117/840\n",
      "Batch loss: 0.6690411567687988 batch: 118/840\n",
      "Batch loss: 0.7446272969245911 batch: 119/840\n",
      "Batch loss: 0.6441056132316589 batch: 120/840\n",
      "Batch loss: 0.9393509030342102 batch: 121/840\n",
      "Batch loss: 1.0170327425003052 batch: 122/840\n",
      "Batch loss: 0.6680334210395813 batch: 123/840\n",
      "Batch loss: 0.8095518350601196 batch: 124/840\n",
      "Batch loss: 0.8093446493148804 batch: 125/840\n",
      "Batch loss: 0.9660314917564392 batch: 126/840\n",
      "Batch loss: 0.9760226607322693 batch: 127/840\n",
      "Batch loss: 0.8753406405448914 batch: 128/840\n",
      "Batch loss: 0.9433822631835938 batch: 129/840\n",
      "Batch loss: 0.8210005760192871 batch: 130/840\n",
      "Batch loss: 0.8992106914520264 batch: 131/840\n",
      "Batch loss: 1.0911098718643188 batch: 132/840\n",
      "Batch loss: 0.8546934723854065 batch: 133/840\n",
      "Batch loss: 0.8074359893798828 batch: 134/840\n",
      "Batch loss: 0.7497779726982117 batch: 135/840\n",
      "Batch loss: 0.9733660817146301 batch: 136/840\n",
      "Batch loss: 0.7884703278541565 batch: 137/840\n",
      "Batch loss: 0.7221632599830627 batch: 138/840\n",
      "Batch loss: 0.7682856917381287 batch: 139/840\n",
      "Batch loss: 0.8646344542503357 batch: 140/840\n",
      "Batch loss: 0.6181581020355225 batch: 141/840\n",
      "Batch loss: 0.8521243929862976 batch: 142/840\n",
      "Batch loss: 0.7345171570777893 batch: 143/840\n",
      "Batch loss: 0.7914539575576782 batch: 144/840\n",
      "Batch loss: 0.9151392579078674 batch: 145/840\n",
      "Batch loss: 0.7904053330421448 batch: 146/840\n",
      "Batch loss: 0.6873084902763367 batch: 147/840\n",
      "Batch loss: 0.9534533023834229 batch: 148/840\n",
      "Batch loss: 0.7679104804992676 batch: 149/840\n",
      "Batch loss: 0.948434054851532 batch: 150/840\n",
      "Batch loss: 0.7833020687103271 batch: 151/840\n",
      "Batch loss: 0.7823042869567871 batch: 152/840\n",
      "Batch loss: 0.7167266607284546 batch: 153/840\n",
      "Batch loss: 0.835968017578125 batch: 154/840\n",
      "Batch loss: 0.7055265307426453 batch: 155/840\n",
      "Batch loss: 0.9171289801597595 batch: 156/840\n",
      "Batch loss: 0.8241891264915466 batch: 157/840\n",
      "Batch loss: 0.7217200994491577 batch: 158/840\n",
      "Batch loss: 0.7859199643135071 batch: 159/840\n",
      "Batch loss: 0.7243488430976868 batch: 160/840\n",
      "Batch loss: 0.8287220597267151 batch: 161/840\n",
      "Batch loss: 0.7503248453140259 batch: 162/840\n",
      "Batch loss: 0.9720410704612732 batch: 163/840\n",
      "Batch loss: 0.648097813129425 batch: 164/840\n",
      "Batch loss: 0.9456954002380371 batch: 165/840\n",
      "Batch loss: 0.6855098605155945 batch: 166/840\n",
      "Batch loss: 0.7917210459709167 batch: 167/840\n",
      "Batch loss: 0.8354200124740601 batch: 168/840\n",
      "Batch loss: 0.6926817893981934 batch: 169/840\n",
      "Batch loss: 0.9126407504081726 batch: 170/840\n",
      "Batch loss: 0.8226249814033508 batch: 171/840\n",
      "Batch loss: 1.0305819511413574 batch: 172/840\n",
      "Batch loss: 0.8312104940414429 batch: 173/840\n",
      "Batch loss: 0.7417733669281006 batch: 174/840\n",
      "Batch loss: 0.6876199841499329 batch: 175/840\n",
      "Batch loss: 0.869575560092926 batch: 176/840\n",
      "Batch loss: 0.8097705245018005 batch: 177/840\n",
      "Batch loss: 0.8241686820983887 batch: 178/840\n",
      "Batch loss: 0.9661252498626709 batch: 179/840\n",
      "Batch loss: 0.7413063049316406 batch: 180/840\n",
      "Batch loss: 0.794253408908844 batch: 181/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7951816320419312 batch: 182/840\n",
      "Batch loss: 0.8690511584281921 batch: 183/840\n",
      "Batch loss: 0.7038179039955139 batch: 184/840\n",
      "Batch loss: 0.6490274667739868 batch: 185/840\n",
      "Batch loss: 0.6118298172950745 batch: 186/840\n",
      "Batch loss: 0.899085283279419 batch: 187/840\n",
      "Batch loss: 0.6294940710067749 batch: 188/840\n",
      "Batch loss: 0.7716383934020996 batch: 189/840\n",
      "Batch loss: 0.8323737978935242 batch: 190/840\n",
      "Batch loss: 0.9039003252983093 batch: 191/840\n",
      "Batch loss: 0.6373999118804932 batch: 192/840\n",
      "Batch loss: 0.7699265480041504 batch: 193/840\n",
      "Batch loss: 0.6271510720252991 batch: 194/840\n",
      "Batch loss: 0.8850708603858948 batch: 195/840\n",
      "Batch loss: 0.8654938340187073 batch: 196/840\n",
      "Batch loss: 0.9021247029304504 batch: 197/840\n",
      "Batch loss: 0.6622282266616821 batch: 198/840\n",
      "Batch loss: 0.6796248555183411 batch: 199/840\n",
      "Batch loss: 1.0166761875152588 batch: 200/840\n",
      "Batch loss: 0.7078800797462463 batch: 201/840\n",
      "Batch loss: 0.7737282514572144 batch: 202/840\n",
      "Batch loss: 0.730421781539917 batch: 203/840\n",
      "Batch loss: 0.8351210951805115 batch: 204/840\n",
      "Batch loss: 0.8579612970352173 batch: 205/840\n",
      "Batch loss: 0.7078004479408264 batch: 206/840\n",
      "Batch loss: 0.7552841901779175 batch: 207/840\n",
      "Batch loss: 0.7476751804351807 batch: 208/840\n",
      "Batch loss: 0.7401540875434875 batch: 209/840\n",
      "Batch loss: 0.6846746802330017 batch: 210/840\n",
      "Batch loss: 0.6300572752952576 batch: 211/840\n",
      "Batch loss: 0.7512983679771423 batch: 212/840\n",
      "Batch loss: 0.9291923642158508 batch: 213/840\n",
      "Batch loss: 0.8425344824790955 batch: 214/840\n",
      "Batch loss: 0.7447025179862976 batch: 215/840\n",
      "Batch loss: 0.7951079607009888 batch: 216/840\n",
      "Batch loss: 0.7788694500923157 batch: 217/840\n",
      "Batch loss: 0.8157122135162354 batch: 218/840\n",
      "Batch loss: 0.8199535608291626 batch: 219/840\n",
      "Batch loss: 0.928852379322052 batch: 220/840\n",
      "Batch loss: 0.6830756664276123 batch: 221/840\n",
      "Batch loss: 0.9202058911323547 batch: 222/840\n",
      "Batch loss: 0.6869634985923767 batch: 223/840\n",
      "Batch loss: 0.9134249091148376 batch: 224/840\n",
      "Batch loss: 0.8379405736923218 batch: 225/840\n",
      "Batch loss: 0.9067552089691162 batch: 226/840\n",
      "Batch loss: 0.9043168425559998 batch: 227/840\n",
      "Batch loss: 0.6310155391693115 batch: 228/840\n",
      "Batch loss: 0.6600959300994873 batch: 229/840\n",
      "Batch loss: 0.7562967538833618 batch: 230/840\n",
      "Batch loss: 0.7302035689353943 batch: 231/840\n",
      "Batch loss: 0.7495886087417603 batch: 232/840\n",
      "Batch loss: 0.9023680090904236 batch: 233/840\n",
      "Batch loss: 0.769793689250946 batch: 234/840\n",
      "Batch loss: 0.7677983045578003 batch: 235/840\n",
      "Batch loss: 0.7672528028488159 batch: 236/840\n",
      "Batch loss: 0.6819242835044861 batch: 237/840\n",
      "Batch loss: 0.8334500193595886 batch: 238/840\n",
      "Batch loss: 0.8023523092269897 batch: 239/840\n",
      "Batch loss: 0.8257597088813782 batch: 240/840\n",
      "Batch loss: 0.8359930515289307 batch: 241/840\n",
      "Batch loss: 0.7063871622085571 batch: 242/840\n",
      "Batch loss: 0.7344509959220886 batch: 243/840\n",
      "Batch loss: 0.8470115065574646 batch: 244/840\n",
      "Batch loss: 0.6776733994483948 batch: 245/840\n",
      "Batch loss: 0.8247623443603516 batch: 246/840\n",
      "Batch loss: 0.9095033407211304 batch: 247/840\n",
      "Batch loss: 0.8381250500679016 batch: 248/840\n",
      "Batch loss: 1.0098897218704224 batch: 249/840\n",
      "Batch loss: 0.6897046566009521 batch: 250/840\n",
      "Batch loss: 0.7548316717147827 batch: 251/840\n",
      "Batch loss: 0.6726234555244446 batch: 252/840\n",
      "Batch loss: 0.822722315788269 batch: 253/840\n",
      "Batch loss: 0.8867841362953186 batch: 254/840\n",
      "Batch loss: 0.7637205719947815 batch: 255/840\n",
      "Batch loss: 0.7656963467597961 batch: 256/840\n",
      "Batch loss: 0.7107309699058533 batch: 257/840\n",
      "Batch loss: 0.7711632251739502 batch: 258/840\n",
      "Batch loss: 0.7474132776260376 batch: 259/840\n",
      "Batch loss: 0.6703073978424072 batch: 260/840\n",
      "Batch loss: 0.7098355889320374 batch: 261/840\n",
      "Batch loss: 0.5455536246299744 batch: 262/840\n",
      "Batch loss: 0.7786839008331299 batch: 263/840\n",
      "Batch loss: 0.8378664255142212 batch: 264/840\n",
      "Batch loss: 0.8840914368629456 batch: 265/840\n",
      "Batch loss: 0.7316944003105164 batch: 266/840\n",
      "Batch loss: 0.8067615628242493 batch: 267/840\n",
      "Batch loss: 0.6892997622489929 batch: 268/840\n",
      "Batch loss: 0.6266966462135315 batch: 269/840\n",
      "Batch loss: 0.6827536821365356 batch: 270/840\n",
      "Batch loss: 0.7031095027923584 batch: 271/840\n",
      "Batch loss: 0.8934045433998108 batch: 272/840\n",
      "Batch loss: 0.8773916363716125 batch: 273/840\n",
      "Batch loss: 0.7823954820632935 batch: 274/840\n",
      "Batch loss: 0.8576266765594482 batch: 275/840\n",
      "Batch loss: 0.6876860857009888 batch: 276/840\n",
      "Batch loss: 0.7551166415214539 batch: 277/840\n",
      "Batch loss: 0.9064822196960449 batch: 278/840\n",
      "Batch loss: 0.9102285504341125 batch: 279/840\n",
      "Batch loss: 0.8898351788520813 batch: 280/840\n",
      "Batch loss: 0.6829480528831482 batch: 281/840\n",
      "Batch loss: 0.6331601738929749 batch: 282/840\n",
      "Batch loss: 0.7179121375083923 batch: 283/840\n",
      "Batch loss: 0.5876362323760986 batch: 284/840\n",
      "Batch loss: 0.6184975504875183 batch: 285/840\n",
      "Batch loss: 0.7929881811141968 batch: 286/840\n",
      "Batch loss: 0.5581722259521484 batch: 287/840\n",
      "Batch loss: 0.6828656792640686 batch: 288/840\n",
      "Batch loss: 0.91942298412323 batch: 289/840\n",
      "Batch loss: 0.9191624522209167 batch: 290/840\n",
      "Batch loss: 0.8821653723716736 batch: 291/840\n",
      "Batch loss: 0.6980569958686829 batch: 292/840\n",
      "Batch loss: 0.91484534740448 batch: 293/840\n",
      "Batch loss: 0.7933852672576904 batch: 294/840\n",
      "Batch loss: 0.6103489995002747 batch: 295/840\n",
      "Batch loss: 0.7421553134918213 batch: 296/840\n",
      "Batch loss: 0.7656187415122986 batch: 297/840\n",
      "Batch loss: 0.7752965688705444 batch: 298/840\n",
      "Batch loss: 0.5974770784378052 batch: 299/840\n",
      "Batch loss: 0.8652617931365967 batch: 300/840\n",
      "Batch loss: 0.8167278170585632 batch: 301/840\n",
      "Batch loss: 0.8088144659996033 batch: 302/840\n",
      "Batch loss: 0.8551755547523499 batch: 303/840\n",
      "Batch loss: 0.6784457564353943 batch: 304/840\n",
      "Batch loss: 0.6364142894744873 batch: 305/840\n",
      "Batch loss: 0.7957452535629272 batch: 306/840\n",
      "Batch loss: 0.6444069147109985 batch: 307/840\n",
      "Batch loss: 0.8154283761978149 batch: 308/840\n",
      "Batch loss: 0.8087526559829712 batch: 309/840\n",
      "Batch loss: 0.9636982679367065 batch: 310/840\n",
      "Batch loss: 0.8403249382972717 batch: 311/840\n",
      "Batch loss: 0.8233022093772888 batch: 312/840\n",
      "Batch loss: 0.7931273579597473 batch: 313/840\n",
      "Batch loss: 0.7337809801101685 batch: 314/840\n",
      "Batch loss: 0.743028998374939 batch: 315/840\n",
      "Batch loss: 0.5364568829536438 batch: 316/840\n",
      "Batch loss: 0.7915827035903931 batch: 317/840\n",
      "Batch loss: 0.8115564584732056 batch: 318/840\n",
      "Batch loss: 0.7115915417671204 batch: 319/840\n",
      "Batch loss: 0.692980170249939 batch: 320/840\n",
      "Batch loss: 0.7651735544204712 batch: 321/840\n",
      "Batch loss: 0.8839263319969177 batch: 322/840\n",
      "Batch loss: 0.9303051829338074 batch: 323/840\n",
      "Batch loss: 0.7766202688217163 batch: 324/840\n",
      "Batch loss: 0.6430789828300476 batch: 325/840\n",
      "Batch loss: 0.7403461337089539 batch: 326/840\n",
      "Batch loss: 0.7041435241699219 batch: 327/840\n",
      "Batch loss: 0.894165575504303 batch: 328/840\n",
      "Batch loss: 0.8581542372703552 batch: 329/840\n",
      "Batch loss: 0.8530000448226929 batch: 330/840\n",
      "Batch loss: 0.8398312926292419 batch: 331/840\n",
      "Batch loss: 0.7468405961990356 batch: 332/840\n",
      "Batch loss: 0.7843902111053467 batch: 333/840\n",
      "Batch loss: 0.8454254865646362 batch: 334/840\n",
      "Batch loss: 0.6000325679779053 batch: 335/840\n",
      "Batch loss: 0.8304339647293091 batch: 336/840\n",
      "Batch loss: 0.9696974158287048 batch: 337/840\n",
      "Batch loss: 0.818204402923584 batch: 338/840\n",
      "Batch loss: 0.6511482000350952 batch: 339/840\n",
      "Batch loss: 0.9295607209205627 batch: 340/840\n",
      "Batch loss: 0.6462159156799316 batch: 341/840\n",
      "Batch loss: 0.6058388352394104 batch: 342/840\n",
      "Batch loss: 1.006306767463684 batch: 343/840\n",
      "Batch loss: 0.7100584506988525 batch: 344/840\n",
      "Batch loss: 0.5219612121582031 batch: 345/840\n",
      "Batch loss: 0.8315108418464661 batch: 346/840\n",
      "Batch loss: 0.7806875109672546 batch: 347/840\n",
      "Batch loss: 0.7622427344322205 batch: 348/840\n",
      "Batch loss: 0.8065559267997742 batch: 349/840\n",
      "Batch loss: 0.6146376729011536 batch: 350/840\n",
      "Batch loss: 0.7561618685722351 batch: 351/840\n",
      "Batch loss: 0.7577092051506042 batch: 352/840\n",
      "Batch loss: 0.7791333198547363 batch: 353/840\n",
      "Batch loss: 0.6618932485580444 batch: 354/840\n",
      "Batch loss: 0.725425660610199 batch: 355/840\n",
      "Batch loss: 0.7679660320281982 batch: 356/840\n",
      "Batch loss: 0.6617759466171265 batch: 357/840\n",
      "Batch loss: 0.8504395484924316 batch: 358/840\n",
      "Batch loss: 0.6721765995025635 batch: 359/840\n",
      "Batch loss: 0.9025173783302307 batch: 360/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8427690863609314 batch: 361/840\n",
      "Batch loss: 0.7094565033912659 batch: 362/840\n",
      "Batch loss: 0.6792579889297485 batch: 363/840\n",
      "Batch loss: 0.8090555667877197 batch: 364/840\n",
      "Batch loss: 0.6996357440948486 batch: 365/840\n",
      "Batch loss: 0.7451508045196533 batch: 366/840\n",
      "Batch loss: 0.6123248934745789 batch: 367/840\n",
      "Batch loss: 0.8089743256568909 batch: 368/840\n",
      "Batch loss: 0.7570825815200806 batch: 369/840\n",
      "Batch loss: 0.844102680683136 batch: 370/840\n",
      "Batch loss: 0.7829903364181519 batch: 371/840\n",
      "Batch loss: 0.5819993615150452 batch: 372/840\n",
      "Batch loss: 0.7638490200042725 batch: 373/840\n",
      "Batch loss: 0.7978590130805969 batch: 374/840\n",
      "Batch loss: 0.5898246169090271 batch: 375/840\n",
      "Batch loss: 0.6014736890792847 batch: 376/840\n",
      "Batch loss: 0.8016733527183533 batch: 377/840\n",
      "Batch loss: 0.6976556181907654 batch: 378/840\n",
      "Batch loss: 0.6751377582550049 batch: 379/840\n",
      "Batch loss: 0.8054524064064026 batch: 380/840\n",
      "Batch loss: 0.9623992443084717 batch: 381/840\n",
      "Batch loss: 0.9835478067398071 batch: 382/840\n",
      "Batch loss: 0.8008708953857422 batch: 383/840\n",
      "Batch loss: 0.802745521068573 batch: 384/840\n",
      "Batch loss: 0.8789119720458984 batch: 385/840\n",
      "Batch loss: 0.8579327464103699 batch: 386/840\n",
      "Batch loss: 0.6696092486381531 batch: 387/840\n",
      "Batch loss: 0.8905937075614929 batch: 388/840\n",
      "Batch loss: 0.6100744009017944 batch: 389/840\n",
      "Batch loss: 0.9699981808662415 batch: 390/840\n",
      "Batch loss: 0.7573015689849854 batch: 391/840\n",
      "Batch loss: 0.6381871700286865 batch: 392/840\n",
      "Batch loss: 0.5888119339942932 batch: 393/840\n",
      "Batch loss: 0.8065388202667236 batch: 394/840\n",
      "Batch loss: 0.6580434441566467 batch: 395/840\n",
      "Batch loss: 0.8163819909095764 batch: 396/840\n",
      "Batch loss: 0.6996429562568665 batch: 397/840\n",
      "Batch loss: 0.8862993121147156 batch: 398/840\n",
      "Batch loss: 0.5646684169769287 batch: 399/840\n",
      "Batch loss: 0.7099617719650269 batch: 400/840\n",
      "Batch loss: 0.7644769549369812 batch: 401/840\n",
      "Batch loss: 0.7106757164001465 batch: 402/840\n",
      "Batch loss: 0.6661720275878906 batch: 403/840\n",
      "Batch loss: 0.7129896283149719 batch: 404/840\n",
      "Batch loss: 0.7167194485664368 batch: 405/840\n",
      "Batch loss: 0.7419559955596924 batch: 406/840\n",
      "Batch loss: 0.6000840663909912 batch: 407/840\n",
      "Batch loss: 0.9253272414207458 batch: 408/840\n",
      "Batch loss: 0.9274099469184875 batch: 409/840\n",
      "Batch loss: 0.8172112107276917 batch: 410/840\n",
      "Batch loss: 0.7899494767189026 batch: 411/840\n",
      "Batch loss: 0.8234570026397705 batch: 412/840\n",
      "Batch loss: 0.7303366661071777 batch: 413/840\n",
      "Batch loss: 0.7387024760246277 batch: 414/840\n",
      "Batch loss: 1.0476547479629517 batch: 415/840\n",
      "Batch loss: 0.5351693630218506 batch: 416/840\n",
      "Batch loss: 0.6464765667915344 batch: 417/840\n",
      "Batch loss: 0.8594509959220886 batch: 418/840\n",
      "Batch loss: 0.7679800391197205 batch: 419/840\n",
      "Batch loss: 0.7371411323547363 batch: 420/840\n",
      "Batch loss: 0.6351109743118286 batch: 421/840\n",
      "Batch loss: 0.6767467260360718 batch: 422/840\n",
      "Batch loss: 0.7796214818954468 batch: 423/840\n",
      "Batch loss: 0.7081206440925598 batch: 424/840\n",
      "Batch loss: 0.9018512964248657 batch: 425/840\n",
      "Batch loss: 0.8348789215087891 batch: 426/840\n",
      "Batch loss: 0.6847171187400818 batch: 427/840\n",
      "Batch loss: 0.8917285799980164 batch: 428/840\n",
      "Batch loss: 0.6321458220481873 batch: 429/840\n",
      "Batch loss: 0.9598494172096252 batch: 430/840\n",
      "Batch loss: 0.7032824158668518 batch: 431/840\n",
      "Batch loss: 0.7439090013504028 batch: 432/840\n",
      "Batch loss: 0.5937683582305908 batch: 433/840\n",
      "Batch loss: 0.6212596297264099 batch: 434/840\n",
      "Batch loss: 0.8266329169273376 batch: 435/840\n",
      "Batch loss: 0.7301924228668213 batch: 436/840\n",
      "Batch loss: 0.7026227712631226 batch: 437/840\n",
      "Batch loss: 0.5260910987854004 batch: 438/840\n",
      "Batch loss: 0.711230993270874 batch: 439/840\n",
      "Batch loss: 0.8737883567810059 batch: 440/840\n",
      "Batch loss: 0.7415962815284729 batch: 441/840\n",
      "Batch loss: 0.7912445068359375 batch: 442/840\n",
      "Batch loss: 0.6550309658050537 batch: 443/840\n",
      "Batch loss: 0.7800386548042297 batch: 444/840\n",
      "Batch loss: 0.674582839012146 batch: 445/840\n",
      "Batch loss: 0.8590657711029053 batch: 446/840\n",
      "Batch loss: 0.8225042223930359 batch: 447/840\n",
      "Batch loss: 0.7985542416572571 batch: 448/840\n",
      "Batch loss: 0.5870850086212158 batch: 449/840\n",
      "Batch loss: 0.7464544773101807 batch: 450/840\n",
      "Batch loss: 0.863778293132782 batch: 451/840\n",
      "Batch loss: 0.8805585503578186 batch: 452/840\n",
      "Batch loss: 0.6894951462745667 batch: 453/840\n",
      "Batch loss: 0.7189857959747314 batch: 454/840\n",
      "Batch loss: 0.7771183848381042 batch: 455/840\n",
      "Batch loss: 0.6516904234886169 batch: 456/840\n",
      "Batch loss: 0.7003863453865051 batch: 457/840\n",
      "Batch loss: 0.7130957245826721 batch: 458/840\n",
      "Batch loss: 0.6404416561126709 batch: 459/840\n",
      "Batch loss: 0.6304706931114197 batch: 460/840\n",
      "Batch loss: 0.8277229070663452 batch: 461/840\n",
      "Batch loss: 0.6793665885925293 batch: 462/840\n",
      "Batch loss: 0.853218138217926 batch: 463/840\n",
      "Batch loss: 0.6483994126319885 batch: 464/840\n",
      "Batch loss: 0.9993390440940857 batch: 465/840\n",
      "Batch loss: 0.6589207649230957 batch: 466/840\n",
      "Batch loss: 0.9731259942054749 batch: 467/840\n",
      "Batch loss: 0.6768772602081299 batch: 468/840\n",
      "Batch loss: 0.6731615662574768 batch: 469/840\n",
      "Batch loss: 0.778638482093811 batch: 470/840\n",
      "Batch loss: 0.8209729790687561 batch: 471/840\n",
      "Batch loss: 0.7281450629234314 batch: 472/840\n",
      "Batch loss: 0.840819239616394 batch: 473/840\n",
      "Batch loss: 0.684548020362854 batch: 474/840\n",
      "Batch loss: 0.7686811089515686 batch: 475/840\n",
      "Batch loss: 0.7592004537582397 batch: 476/840\n",
      "Batch loss: 0.6373613476753235 batch: 477/840\n",
      "Batch loss: 0.6850400567054749 batch: 478/840\n",
      "Batch loss: 0.5830897092819214 batch: 479/840\n",
      "Batch loss: 0.6926044225692749 batch: 480/840\n",
      "Batch loss: 0.7804431319236755 batch: 481/840\n",
      "Batch loss: 0.9289750456809998 batch: 482/840\n",
      "Batch loss: 0.7155503034591675 batch: 483/840\n",
      "Batch loss: 0.6276050806045532 batch: 484/840\n",
      "Batch loss: 0.7821799516677856 batch: 485/840\n",
      "Batch loss: 0.7658100724220276 batch: 486/840\n",
      "Batch loss: 0.5502262711524963 batch: 487/840\n",
      "Batch loss: 0.7421863079071045 batch: 488/840\n",
      "Batch loss: 0.687233030796051 batch: 489/840\n",
      "Batch loss: 0.7978622913360596 batch: 490/840\n",
      "Batch loss: 0.4579855799674988 batch: 491/840\n",
      "Batch loss: 0.7931911945343018 batch: 492/840\n",
      "Batch loss: 0.8103206753730774 batch: 493/840\n",
      "Batch loss: 0.4989774227142334 batch: 494/840\n",
      "Batch loss: 0.8121504783630371 batch: 495/840\n",
      "Batch loss: 0.6312475800514221 batch: 496/840\n",
      "Batch loss: 0.8116934895515442 batch: 497/840\n",
      "Batch loss: 0.6197177171707153 batch: 498/840\n",
      "Batch loss: 0.667772114276886 batch: 499/840\n",
      "Batch loss: 0.7086105346679688 batch: 500/840\n",
      "Batch loss: 0.593315601348877 batch: 501/840\n",
      "Batch loss: 0.761813759803772 batch: 502/840\n",
      "Batch loss: 0.5688883066177368 batch: 503/840\n",
      "Batch loss: 0.8101077079772949 batch: 504/840\n",
      "Batch loss: 0.9146877527236938 batch: 505/840\n",
      "Batch loss: 0.6906892657279968 batch: 506/840\n",
      "Batch loss: 0.9223622679710388 batch: 507/840\n",
      "Batch loss: 0.6861299276351929 batch: 508/840\n",
      "Batch loss: 0.8068592548370361 batch: 509/840\n",
      "Batch loss: 0.726612389087677 batch: 510/840\n",
      "Batch loss: 0.638775110244751 batch: 511/840\n",
      "Batch loss: 0.6498658061027527 batch: 512/840\n",
      "Batch loss: 0.6222566962242126 batch: 513/840\n",
      "Batch loss: 0.8294315934181213 batch: 514/840\n",
      "Batch loss: 0.5554300546646118 batch: 515/840\n",
      "Batch loss: 0.839521050453186 batch: 516/840\n",
      "Batch loss: 0.7247058749198914 batch: 517/840\n",
      "Batch loss: 0.7002192735671997 batch: 518/840\n",
      "Batch loss: 0.8602188229560852 batch: 519/840\n",
      "Batch loss: 0.7972637414932251 batch: 520/840\n",
      "Batch loss: 0.823851466178894 batch: 521/840\n",
      "Batch loss: 0.5886939167976379 batch: 522/840\n",
      "Batch loss: 0.477186918258667 batch: 523/840\n",
      "Batch loss: 0.6180340647697449 batch: 524/840\n",
      "Batch loss: 0.756319522857666 batch: 525/840\n",
      "Batch loss: 0.8393875956535339 batch: 526/840\n",
      "Batch loss: 0.7814403772354126 batch: 527/840\n",
      "Batch loss: 0.6719967722892761 batch: 528/840\n",
      "Batch loss: 0.5802749395370483 batch: 529/840\n",
      "Batch loss: 0.6889181733131409 batch: 530/840\n",
      "Batch loss: 0.5997861623764038 batch: 531/840\n",
      "Batch loss: 0.5248462557792664 batch: 532/840\n",
      "Batch loss: 0.782046377658844 batch: 533/840\n",
      "Batch loss: 0.67730313539505 batch: 534/840\n",
      "Batch loss: 0.6123578548431396 batch: 535/840\n",
      "Batch loss: 0.7296411991119385 batch: 536/840\n",
      "Batch loss: 0.6950559020042419 batch: 537/840\n",
      "Batch loss: 0.559359610080719 batch: 538/840\n",
      "Batch loss: 0.6815558671951294 batch: 539/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.9284355044364929 batch: 540/840\n",
      "Batch loss: 0.6596453189849854 batch: 541/840\n",
      "Batch loss: 0.7162241339683533 batch: 542/840\n",
      "Batch loss: 0.4929408133029938 batch: 543/840\n",
      "Batch loss: 0.7537846565246582 batch: 544/840\n",
      "Batch loss: 0.6873822212219238 batch: 545/840\n",
      "Batch loss: 0.6991969347000122 batch: 546/840\n",
      "Batch loss: 0.6705578565597534 batch: 547/840\n",
      "Batch loss: 0.5631430149078369 batch: 548/840\n",
      "Batch loss: 0.6281132102012634 batch: 549/840\n",
      "Batch loss: 0.5886301398277283 batch: 550/840\n",
      "Batch loss: 0.7284570932388306 batch: 551/840\n",
      "Batch loss: 0.5658173561096191 batch: 552/840\n",
      "Batch loss: 0.7600362300872803 batch: 553/840\n",
      "Batch loss: 0.7598035335540771 batch: 554/840\n",
      "Batch loss: 0.757946789264679 batch: 555/840\n",
      "Batch loss: 0.7470917701721191 batch: 556/840\n",
      "Batch loss: 0.7330151200294495 batch: 557/840\n",
      "Batch loss: 0.6981569528579712 batch: 558/840\n",
      "Batch loss: 0.7330963611602783 batch: 559/840\n",
      "Batch loss: 0.7011208534240723 batch: 560/840\n",
      "Batch loss: 0.7255317568778992 batch: 561/840\n",
      "Batch loss: 0.6361351609230042 batch: 562/840\n",
      "Batch loss: 0.5959472060203552 batch: 563/840\n",
      "Batch loss: 0.7681716084480286 batch: 564/840\n",
      "Batch loss: 0.7729013562202454 batch: 565/840\n",
      "Batch loss: 0.6923785209655762 batch: 566/840\n",
      "Batch loss: 0.7072561383247375 batch: 567/840\n",
      "Batch loss: 0.7201734185218811 batch: 568/840\n",
      "Batch loss: 0.7848525047302246 batch: 569/840\n",
      "Batch loss: 0.48861420154571533 batch: 570/840\n",
      "Batch loss: 0.7693206667900085 batch: 571/840\n",
      "Batch loss: 0.750326931476593 batch: 572/840\n",
      "Batch loss: 0.6381860971450806 batch: 573/840\n",
      "Batch loss: 0.8343979716300964 batch: 574/840\n",
      "Batch loss: 0.661516547203064 batch: 575/840\n",
      "Batch loss: 0.6679654717445374 batch: 576/840\n",
      "Batch loss: 0.5427640676498413 batch: 577/840\n",
      "Batch loss: 0.829922616481781 batch: 578/840\n",
      "Batch loss: 0.7183812856674194 batch: 579/840\n",
      "Batch loss: 0.8241249918937683 batch: 580/840\n",
      "Batch loss: 0.7535507678985596 batch: 581/840\n",
      "Batch loss: 0.8674861192703247 batch: 582/840\n",
      "Batch loss: 0.7260913252830505 batch: 583/840\n",
      "Batch loss: 0.7922950983047485 batch: 584/840\n",
      "Batch loss: 0.7772717475891113 batch: 585/840\n",
      "Batch loss: 0.6777333617210388 batch: 586/840\n",
      "Batch loss: 0.7266984581947327 batch: 587/840\n",
      "Batch loss: 0.6502945423126221 batch: 588/840\n",
      "Batch loss: 0.8332443237304688 batch: 589/840\n",
      "Batch loss: 0.6223947405815125 batch: 590/840\n",
      "Batch loss: 0.519939124584198 batch: 591/840\n",
      "Batch loss: 0.5768404603004456 batch: 592/840\n",
      "Batch loss: 0.7978602647781372 batch: 593/840\n",
      "Batch loss: 0.7698492407798767 batch: 594/840\n",
      "Batch loss: 0.5294814109802246 batch: 595/840\n",
      "Batch loss: 0.6418055891990662 batch: 596/840\n",
      "Batch loss: 0.6147196292877197 batch: 597/840\n",
      "Batch loss: 0.5311290621757507 batch: 598/840\n",
      "Batch loss: 0.6504144072532654 batch: 599/840\n",
      "Batch loss: 0.8397696018218994 batch: 600/840\n",
      "Batch loss: 0.7854253649711609 batch: 601/840\n",
      "Batch loss: 0.5710785388946533 batch: 602/840\n",
      "Batch loss: 0.722515881061554 batch: 603/840\n",
      "Batch loss: 0.7211179137229919 batch: 604/840\n",
      "Batch loss: 0.9216301441192627 batch: 605/840\n",
      "Batch loss: 0.9172370433807373 batch: 606/840\n",
      "Batch loss: 0.6550869941711426 batch: 607/840\n",
      "Batch loss: 0.7144277095794678 batch: 608/840\n",
      "Batch loss: 0.5174921154975891 batch: 609/840\n",
      "Batch loss: 0.7457237243652344 batch: 610/840\n",
      "Batch loss: 0.7449977993965149 batch: 611/840\n",
      "Batch loss: 0.7843619585037231 batch: 612/840\n",
      "Batch loss: 0.7403607964515686 batch: 613/840\n",
      "Batch loss: 0.6714370846748352 batch: 614/840\n",
      "Batch loss: 0.6681466102600098 batch: 615/840\n",
      "Batch loss: 0.6252648830413818 batch: 616/840\n",
      "Batch loss: 0.6509347558021545 batch: 617/840\n",
      "Batch loss: 0.64743971824646 batch: 618/840\n",
      "Batch loss: 0.8931577205657959 batch: 619/840\n",
      "Batch loss: 0.6656956672668457 batch: 620/840\n",
      "Batch loss: 0.6361619830131531 batch: 621/840\n",
      "Batch loss: 0.694710373878479 batch: 622/840\n",
      "Batch loss: 0.6911092400550842 batch: 623/840\n",
      "Batch loss: 0.9067542552947998 batch: 624/840\n",
      "Batch loss: 0.7145543694496155 batch: 625/840\n",
      "Batch loss: 0.6132739782333374 batch: 626/840\n",
      "Batch loss: 0.5757741332054138 batch: 627/840\n",
      "Batch loss: 0.6453720331192017 batch: 628/840\n",
      "Batch loss: 0.5866825580596924 batch: 629/840\n",
      "Batch loss: 0.665867030620575 batch: 630/840\n",
      "Batch loss: 0.8237708210945129 batch: 631/840\n",
      "Batch loss: 0.7086240649223328 batch: 632/840\n",
      "Batch loss: 0.7136219143867493 batch: 633/840\n",
      "Batch loss: 0.8075804114341736 batch: 634/840\n",
      "Batch loss: 0.719127893447876 batch: 635/840\n",
      "Batch loss: 0.6280066967010498 batch: 636/840\n",
      "Batch loss: 0.49677956104278564 batch: 637/840\n",
      "Batch loss: 0.7294143438339233 batch: 638/840\n",
      "Batch loss: 0.7164978981018066 batch: 639/840\n",
      "Batch loss: 0.696796715259552 batch: 640/840\n",
      "Batch loss: 0.8510734438896179 batch: 641/840\n",
      "Batch loss: 0.630959153175354 batch: 642/840\n",
      "Batch loss: 1.0776050090789795 batch: 643/840\n",
      "Batch loss: 0.6909457445144653 batch: 644/840\n",
      "Batch loss: 0.6833533644676208 batch: 645/840\n",
      "Batch loss: 0.6508137583732605 batch: 646/840\n",
      "Batch loss: 0.8548347353935242 batch: 647/840\n",
      "Batch loss: 0.7209470272064209 batch: 648/840\n",
      "Batch loss: 0.6951816082000732 batch: 649/840\n",
      "Batch loss: 0.6777332425117493 batch: 650/840\n",
      "Batch loss: 0.6248751878738403 batch: 651/840\n",
      "Batch loss: 0.7959551215171814 batch: 652/840\n",
      "Batch loss: 0.5687386393547058 batch: 653/840\n",
      "Batch loss: 0.9362084269523621 batch: 654/840\n",
      "Batch loss: 0.6316233277320862 batch: 655/840\n",
      "Batch loss: 0.8072594404220581 batch: 656/840\n",
      "Batch loss: 0.6669940948486328 batch: 657/840\n",
      "Batch loss: 0.8012976050376892 batch: 658/840\n",
      "Batch loss: 0.7411496043205261 batch: 659/840\n",
      "Batch loss: 0.6514655351638794 batch: 660/840\n",
      "Batch loss: 0.7828424572944641 batch: 661/840\n",
      "Batch loss: 0.7602586150169373 batch: 662/840\n",
      "Batch loss: 0.6395235657691956 batch: 663/840\n",
      "Batch loss: 0.7779421210289001 batch: 664/840\n",
      "Batch loss: 0.808180570602417 batch: 665/840\n",
      "Batch loss: 0.7102540731430054 batch: 666/840\n",
      "Batch loss: 0.7743659019470215 batch: 667/840\n",
      "Batch loss: 0.7035906314849854 batch: 668/840\n",
      "Batch loss: 0.5863332152366638 batch: 669/840\n",
      "Batch loss: 0.7667574286460876 batch: 670/840\n",
      "Batch loss: 0.7014737725257874 batch: 671/840\n",
      "Batch loss: 0.687055230140686 batch: 672/840\n",
      "Batch loss: 0.8978049755096436 batch: 673/840\n",
      "Batch loss: 1.0405408143997192 batch: 674/840\n",
      "Batch loss: 0.641416072845459 batch: 675/840\n",
      "Batch loss: 0.5318828821182251 batch: 676/840\n",
      "Batch loss: 0.6577731370925903 batch: 677/840\n",
      "Batch loss: 0.7063036561012268 batch: 678/840\n",
      "Batch loss: 0.9688512682914734 batch: 679/840\n",
      "Batch loss: 0.7732287049293518 batch: 680/840\n",
      "Batch loss: 0.6734228730201721 batch: 681/840\n",
      "Batch loss: 0.7301920056343079 batch: 682/840\n",
      "Batch loss: 0.6689362525939941 batch: 683/840\n",
      "Batch loss: 0.9422650337219238 batch: 684/840\n",
      "Batch loss: 0.7557915449142456 batch: 685/840\n",
      "Batch loss: 0.8198146820068359 batch: 686/840\n",
      "Batch loss: 0.73878014087677 batch: 687/840\n",
      "Batch loss: 0.721604585647583 batch: 688/840\n",
      "Batch loss: 0.6695974469184875 batch: 689/840\n",
      "Batch loss: 0.6030853390693665 batch: 690/840\n",
      "Batch loss: 0.7373685240745544 batch: 691/840\n",
      "Batch loss: 0.7168192267417908 batch: 692/840\n",
      "Batch loss: 0.637640118598938 batch: 693/840\n",
      "Batch loss: 0.8585317730903625 batch: 694/840\n",
      "Batch loss: 0.7786118984222412 batch: 695/840\n",
      "Batch loss: 0.6331905126571655 batch: 696/840\n",
      "Batch loss: 0.6118327379226685 batch: 697/840\n",
      "Batch loss: 0.6612290740013123 batch: 698/840\n",
      "Batch loss: 0.8492124080657959 batch: 699/840\n",
      "Batch loss: 0.7310619354248047 batch: 700/840\n",
      "Batch loss: 0.8446578979492188 batch: 701/840\n",
      "Batch loss: 0.5923353433609009 batch: 702/840\n",
      "Batch loss: 0.7820711731910706 batch: 703/840\n",
      "Batch loss: 0.7974397540092468 batch: 704/840\n",
      "Batch loss: 0.7234027981758118 batch: 705/840\n",
      "Batch loss: 0.8072585463523865 batch: 706/840\n",
      "Batch loss: 0.7493538856506348 batch: 707/840\n",
      "Batch loss: 0.8111811876296997 batch: 708/840\n",
      "Batch loss: 0.8123468160629272 batch: 709/840\n",
      "Batch loss: 0.8226314783096313 batch: 710/840\n",
      "Batch loss: 0.5668047070503235 batch: 711/840\n",
      "Batch loss: 0.8185155987739563 batch: 712/840\n",
      "Batch loss: 0.7445831894874573 batch: 713/840\n",
      "Batch loss: 0.6860826015472412 batch: 714/840\n",
      "Batch loss: 0.8126358985900879 batch: 715/840\n",
      "Batch loss: 0.6833783984184265 batch: 716/840\n",
      "Batch loss: 0.7494419813156128 batch: 717/840\n",
      "Batch loss: 0.6846067905426025 batch: 718/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4618542790412903 batch: 719/840\n",
      "Batch loss: 0.6074709892272949 batch: 720/840\n",
      "Batch loss: 0.7665508985519409 batch: 721/840\n",
      "Batch loss: 0.8022251129150391 batch: 722/840\n",
      "Batch loss: 0.6108312010765076 batch: 723/840\n",
      "Batch loss: 0.8637728691101074 batch: 724/840\n",
      "Batch loss: 0.6542158722877502 batch: 725/840\n",
      "Batch loss: 0.6007346510887146 batch: 726/840\n",
      "Batch loss: 0.9786267876625061 batch: 727/840\n",
      "Batch loss: 0.7840850353240967 batch: 728/840\n",
      "Batch loss: 0.7231331467628479 batch: 729/840\n",
      "Batch loss: 0.6894960999488831 batch: 730/840\n",
      "Batch loss: 0.5432916879653931 batch: 731/840\n",
      "Batch loss: 0.8383905291557312 batch: 732/840\n",
      "Batch loss: 0.6988280415534973 batch: 733/840\n",
      "Batch loss: 0.6126189231872559 batch: 734/840\n",
      "Batch loss: 0.6437366604804993 batch: 735/840\n",
      "Batch loss: 0.7362656593322754 batch: 736/840\n",
      "Batch loss: 0.6232479810714722 batch: 737/840\n",
      "Batch loss: 0.5787289142608643 batch: 738/840\n",
      "Batch loss: 0.8120728135108948 batch: 739/840\n",
      "Batch loss: 0.7618032693862915 batch: 740/840\n",
      "Batch loss: 0.6149795651435852 batch: 741/840\n",
      "Batch loss: 0.6201191544532776 batch: 742/840\n",
      "Batch loss: 0.4743601679801941 batch: 743/840\n",
      "Batch loss: 0.6006105542182922 batch: 744/840\n",
      "Batch loss: 0.6872966289520264 batch: 745/840\n",
      "Batch loss: 0.6298424005508423 batch: 746/840\n",
      "Batch loss: 0.7725096940994263 batch: 747/840\n",
      "Batch loss: 0.7065796852111816 batch: 748/840\n",
      "Batch loss: 0.6082316040992737 batch: 749/840\n",
      "Batch loss: 0.5610285997390747 batch: 750/840\n",
      "Batch loss: 0.7396194338798523 batch: 751/840\n",
      "Batch loss: 0.9280748963356018 batch: 752/840\n",
      "Batch loss: 0.6060839295387268 batch: 753/840\n",
      "Batch loss: 0.697370707988739 batch: 754/840\n",
      "Batch loss: 0.6604989767074585 batch: 755/840\n",
      "Batch loss: 0.6075887680053711 batch: 756/840\n",
      "Batch loss: 0.82044517993927 batch: 757/840\n",
      "Batch loss: 0.6253809928894043 batch: 758/840\n",
      "Batch loss: 0.5744667053222656 batch: 759/840\n",
      "Batch loss: 0.6119848489761353 batch: 760/840\n",
      "Batch loss: 0.6053149700164795 batch: 761/840\n",
      "Batch loss: 0.8653160333633423 batch: 762/840\n",
      "Batch loss: 0.4610530138015747 batch: 763/840\n",
      "Batch loss: 0.7331504821777344 batch: 764/840\n",
      "Batch loss: 0.5596319437026978 batch: 765/840\n",
      "Batch loss: 0.5695728063583374 batch: 766/840\n",
      "Batch loss: 0.7556436061859131 batch: 767/840\n",
      "Batch loss: 0.8188984394073486 batch: 768/840\n",
      "Batch loss: 0.7871633768081665 batch: 769/840\n",
      "Batch loss: 0.6331767439842224 batch: 770/840\n",
      "Batch loss: 0.6877277493476868 batch: 771/840\n",
      "Batch loss: 0.6012520790100098 batch: 772/840\n",
      "Batch loss: 0.5667913556098938 batch: 773/840\n",
      "Batch loss: 0.5376211404800415 batch: 774/840\n",
      "Batch loss: 0.5628949999809265 batch: 775/840\n",
      "Batch loss: 0.5561257004737854 batch: 776/840\n",
      "Batch loss: 0.6621267795562744 batch: 777/840\n",
      "Batch loss: 0.5186286568641663 batch: 778/840\n",
      "Batch loss: 0.7493164539337158 batch: 779/840\n",
      "Batch loss: 0.6406619548797607 batch: 780/840\n",
      "Batch loss: 0.6348430514335632 batch: 781/840\n",
      "Batch loss: 0.7348397970199585 batch: 782/840\n",
      "Batch loss: 0.5613973140716553 batch: 783/840\n",
      "Batch loss: 0.7605627179145813 batch: 784/840\n",
      "Batch loss: 0.582872748374939 batch: 785/840\n",
      "Batch loss: 0.6921576857566833 batch: 786/840\n",
      "Batch loss: 0.634382426738739 batch: 787/840\n",
      "Batch loss: 0.7584924101829529 batch: 788/840\n",
      "Batch loss: 0.8135921359062195 batch: 789/840\n",
      "Batch loss: 0.7582466006278992 batch: 790/840\n",
      "Batch loss: 0.6496978998184204 batch: 791/840\n",
      "Batch loss: 0.47757527232170105 batch: 792/840\n",
      "Batch loss: 0.6901763081550598 batch: 793/840\n",
      "Batch loss: 0.7512154579162598 batch: 794/840\n",
      "Batch loss: 0.6287511587142944 batch: 795/840\n",
      "Batch loss: 0.7911645770072937 batch: 796/840\n",
      "Batch loss: 0.6129522919654846 batch: 797/840\n",
      "Batch loss: 0.6238332986831665 batch: 798/840\n",
      "Batch loss: 0.7167971134185791 batch: 799/840\n",
      "Batch loss: 0.5591557621955872 batch: 800/840\n",
      "Batch loss: 0.7191274762153625 batch: 801/840\n",
      "Batch loss: 0.5413810014724731 batch: 802/840\n",
      "Batch loss: 0.5869043469429016 batch: 803/840\n",
      "Batch loss: 0.8639692664146423 batch: 804/840\n",
      "Batch loss: 0.6430721879005432 batch: 805/840\n",
      "Batch loss: 0.718207597732544 batch: 806/840\n",
      "Batch loss: 0.6268273591995239 batch: 807/840\n",
      "Batch loss: 0.5928024053573608 batch: 808/840\n",
      "Batch loss: 0.6750867962837219 batch: 809/840\n",
      "Batch loss: 0.5447700619697571 batch: 810/840\n",
      "Batch loss: 0.5911977887153625 batch: 811/840\n",
      "Batch loss: 0.6828923225402832 batch: 812/840\n",
      "Batch loss: 0.5606848001480103 batch: 813/840\n",
      "Batch loss: 0.6604775190353394 batch: 814/840\n",
      "Batch loss: 0.7471532225608826 batch: 815/840\n",
      "Batch loss: 0.7211652398109436 batch: 816/840\n",
      "Batch loss: 0.7206898331642151 batch: 817/840\n",
      "Batch loss: 0.6672260761260986 batch: 818/840\n",
      "Batch loss: 0.5931358337402344 batch: 819/840\n",
      "Batch loss: 0.7356935143470764 batch: 820/840\n",
      "Batch loss: 0.6522672176361084 batch: 821/840\n",
      "Batch loss: 0.6370822787284851 batch: 822/840\n",
      "Batch loss: 0.7792375683784485 batch: 823/840\n",
      "Batch loss: 0.8695340752601624 batch: 824/840\n",
      "Batch loss: 0.770775318145752 batch: 825/840\n",
      "Batch loss: 0.5831409096717834 batch: 826/840\n",
      "Batch loss: 0.6043570041656494 batch: 827/840\n",
      "Batch loss: 0.7234512567520142 batch: 828/840\n",
      "Batch loss: 0.6360976696014404 batch: 829/840\n",
      "Batch loss: 0.772331953048706 batch: 830/840\n",
      "Batch loss: 0.5887317061424255 batch: 831/840\n",
      "Batch loss: 0.6855537295341492 batch: 832/840\n",
      "Batch loss: 0.7427964210510254 batch: 833/840\n",
      "Batch loss: 0.6813523173332214 batch: 834/840\n",
      "Batch loss: 0.5802333950996399 batch: 835/840\n",
      "Batch loss: 0.6799648404121399 batch: 836/840\n",
      "Batch loss: 0.7583465576171875 batch: 837/840\n",
      "Batch loss: 0.8439517021179199 batch: 838/840\n",
      "Batch loss: 0.5850529074668884 batch: 839/840\n",
      "Batch loss: 0.6961898803710938 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 1/15..  Training Loss: 0.008..  Test Loss: 0.006..  Test Accuracy: 0.790\n",
      "Running epoch 2/15\n",
      "Batch loss: 0.6029226183891296 batch: 1/840\n",
      "Batch loss: 0.9192140698432922 batch: 2/840\n",
      "Batch loss: 0.7789579033851624 batch: 3/840\n",
      "Batch loss: 0.804303765296936 batch: 4/840\n",
      "Batch loss: 0.599369466304779 batch: 5/840\n",
      "Batch loss: 0.5705364346504211 batch: 6/840\n",
      "Batch loss: 0.6164393424987793 batch: 7/840\n",
      "Batch loss: 0.7701637148857117 batch: 8/840\n",
      "Batch loss: 0.7463527917861938 batch: 9/840\n",
      "Batch loss: 0.7714696526527405 batch: 10/840\n",
      "Batch loss: 0.6816656589508057 batch: 11/840\n",
      "Batch loss: 0.7669410109519958 batch: 12/840\n",
      "Batch loss: 0.5824150443077087 batch: 13/840\n",
      "Batch loss: 0.6656674146652222 batch: 14/840\n",
      "Batch loss: 0.6997206807136536 batch: 15/840\n",
      "Batch loss: 0.6327930688858032 batch: 16/840\n",
      "Batch loss: 0.6221206784248352 batch: 17/840\n",
      "Batch loss: 0.7667720317840576 batch: 18/840\n",
      "Batch loss: 0.7559778094291687 batch: 19/840\n",
      "Batch loss: 0.8402844071388245 batch: 20/840\n",
      "Batch loss: 0.7649118304252625 batch: 21/840\n",
      "Batch loss: 0.6300752758979797 batch: 22/840\n",
      "Batch loss: 0.7544831037521362 batch: 23/840\n",
      "Batch loss: 0.5960456132888794 batch: 24/840\n",
      "Batch loss: 0.7264237999916077 batch: 25/840\n",
      "Batch loss: 0.7244921922683716 batch: 26/840\n",
      "Batch loss: 0.7266708612442017 batch: 27/840\n",
      "Batch loss: 0.7363619208335876 batch: 28/840\n",
      "Batch loss: 0.7444431185722351 batch: 29/840\n",
      "Batch loss: 0.6311648488044739 batch: 30/840\n",
      "Batch loss: 0.6457972526550293 batch: 31/840\n",
      "Batch loss: 0.5797836184501648 batch: 32/840\n",
      "Batch loss: 0.818697988986969 batch: 33/840\n",
      "Batch loss: 0.6421350240707397 batch: 34/840\n",
      "Batch loss: 0.7099639177322388 batch: 35/840\n",
      "Batch loss: 0.6022860407829285 batch: 36/840\n",
      "Batch loss: 0.7591323852539062 batch: 37/840\n",
      "Batch loss: 0.7299278378486633 batch: 38/840\n",
      "Batch loss: 0.8624811768531799 batch: 39/840\n",
      "Batch loss: 0.6424375176429749 batch: 40/840\n",
      "Batch loss: 0.809791624546051 batch: 41/840\n",
      "Batch loss: 0.7507405281066895 batch: 42/840\n",
      "Batch loss: 0.6741464734077454 batch: 43/840\n",
      "Batch loss: 0.8090240359306335 batch: 44/840\n",
      "Batch loss: 0.8429519534111023 batch: 45/840\n",
      "Batch loss: 0.5996167659759521 batch: 46/840\n",
      "Batch loss: 0.5588281154632568 batch: 47/840\n",
      "Batch loss: 0.7374796271324158 batch: 48/840\n",
      "Batch loss: 0.6612227559089661 batch: 49/840\n",
      "Batch loss: 0.6631848216056824 batch: 50/840\n",
      "Batch loss: 0.7401119470596313 batch: 51/840\n",
      "Batch loss: 0.877453625202179 batch: 52/840\n",
      "Batch loss: 0.7513034343719482 batch: 53/840\n",
      "Batch loss: 0.8251457214355469 batch: 54/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7629560232162476 batch: 55/840\n",
      "Batch loss: 0.7648380994796753 batch: 56/840\n",
      "Batch loss: 0.6573198437690735 batch: 57/840\n",
      "Batch loss: 0.6300498843193054 batch: 58/840\n",
      "Batch loss: 0.7975641489028931 batch: 59/840\n",
      "Batch loss: 0.6121431589126587 batch: 60/840\n",
      "Batch loss: 0.9078845977783203 batch: 61/840\n",
      "Batch loss: 0.8052216172218323 batch: 62/840\n",
      "Batch loss: 0.7341633439064026 batch: 63/840\n",
      "Batch loss: 0.8013801574707031 batch: 64/840\n",
      "Batch loss: 0.6917120218276978 batch: 65/840\n",
      "Batch loss: 0.7406668066978455 batch: 66/840\n",
      "Batch loss: 0.8201132416725159 batch: 67/840\n",
      "Batch loss: 0.6787261962890625 batch: 68/840\n",
      "Batch loss: 0.7875643968582153 batch: 69/840\n",
      "Batch loss: 0.7249207496643066 batch: 70/840\n",
      "Batch loss: 0.8109495639801025 batch: 71/840\n",
      "Batch loss: 0.7322428822517395 batch: 72/840\n",
      "Batch loss: 0.8322797417640686 batch: 73/840\n",
      "Batch loss: 0.7041040658950806 batch: 74/840\n",
      "Batch loss: 0.7945618629455566 batch: 75/840\n",
      "Batch loss: 0.4904438853263855 batch: 76/840\n",
      "Batch loss: 0.7310658097267151 batch: 77/840\n",
      "Batch loss: 0.830313503742218 batch: 78/840\n",
      "Batch loss: 0.6753180027008057 batch: 79/840\n",
      "Batch loss: 0.8005918264389038 batch: 80/840\n",
      "Batch loss: 0.7195922136306763 batch: 81/840\n",
      "Batch loss: 0.7205642461776733 batch: 82/840\n",
      "Batch loss: 0.580193281173706 batch: 83/840\n",
      "Batch loss: 0.8434167504310608 batch: 84/840\n",
      "Batch loss: 0.6783515810966492 batch: 85/840\n",
      "Batch loss: 1.0394231081008911 batch: 86/840\n",
      "Batch loss: 0.6402897834777832 batch: 87/840\n",
      "Batch loss: 0.5573580265045166 batch: 88/840\n",
      "Batch loss: 0.6441317200660706 batch: 89/840\n",
      "Batch loss: 0.5881546139717102 batch: 90/840\n",
      "Batch loss: 0.6825526356697083 batch: 91/840\n",
      "Batch loss: 0.6856787800788879 batch: 92/840\n",
      "Batch loss: 0.7719298005104065 batch: 93/840\n",
      "Batch loss: 0.6092115044593811 batch: 94/840\n",
      "Batch loss: 0.7052109241485596 batch: 95/840\n",
      "Batch loss: 0.7824404835700989 batch: 96/840\n",
      "Batch loss: 0.7564022541046143 batch: 97/840\n",
      "Batch loss: 0.9358994960784912 batch: 98/840\n",
      "Batch loss: 0.6666938662528992 batch: 99/840\n",
      "Batch loss: 0.8300975561141968 batch: 100/840\n",
      "Batch loss: 0.6213992834091187 batch: 101/840\n",
      "Batch loss: 0.5890413522720337 batch: 102/840\n",
      "Batch loss: 0.7354185581207275 batch: 103/840\n",
      "Batch loss: 0.5489211082458496 batch: 104/840\n",
      "Batch loss: 0.6681442260742188 batch: 105/840\n",
      "Batch loss: 0.7022404670715332 batch: 106/840\n",
      "Batch loss: 0.6490452289581299 batch: 107/840\n",
      "Batch loss: 0.8160800933837891 batch: 108/840\n",
      "Batch loss: 0.7219924926757812 batch: 109/840\n",
      "Batch loss: 0.6270152926445007 batch: 110/840\n",
      "Batch loss: 0.657133936882019 batch: 111/840\n",
      "Batch loss: 0.8271347880363464 batch: 112/840\n",
      "Batch loss: 0.8789941668510437 batch: 113/840\n",
      "Batch loss: 0.763576328754425 batch: 114/840\n",
      "Batch loss: 0.6846645474433899 batch: 115/840\n",
      "Batch loss: 0.5830299854278564 batch: 116/840\n",
      "Batch loss: 0.7453193068504333 batch: 117/840\n",
      "Batch loss: 0.5787047743797302 batch: 118/840\n",
      "Batch loss: 0.7903535962104797 batch: 119/840\n",
      "Batch loss: 0.6778914928436279 batch: 120/840\n",
      "Batch loss: 0.838039755821228 batch: 121/840\n",
      "Batch loss: 0.8939587473869324 batch: 122/840\n",
      "Batch loss: 0.5641469955444336 batch: 123/840\n",
      "Batch loss: 0.6405159831047058 batch: 124/840\n",
      "Batch loss: 0.651262104511261 batch: 125/840\n",
      "Batch loss: 0.7719004154205322 batch: 126/840\n",
      "Batch loss: 0.8706905841827393 batch: 127/840\n",
      "Batch loss: 0.798916757106781 batch: 128/840\n",
      "Batch loss: 0.8064119815826416 batch: 129/840\n",
      "Batch loss: 0.6094524264335632 batch: 130/840\n",
      "Batch loss: 0.8187436461448669 batch: 131/840\n",
      "Batch loss: 0.9905645847320557 batch: 132/840\n",
      "Batch loss: 0.7358172535896301 batch: 133/840\n",
      "Batch loss: 0.6356213092803955 batch: 134/840\n",
      "Batch loss: 0.6180508136749268 batch: 135/840\n",
      "Batch loss: 0.8259282112121582 batch: 136/840\n",
      "Batch loss: 0.7212246656417847 batch: 137/840\n",
      "Batch loss: 0.5429712533950806 batch: 138/840\n",
      "Batch loss: 0.6425367593765259 batch: 139/840\n",
      "Batch loss: 0.7118792533874512 batch: 140/840\n",
      "Batch loss: 0.5204927921295166 batch: 141/840\n",
      "Batch loss: 0.7049000263214111 batch: 142/840\n",
      "Batch loss: 0.673240065574646 batch: 143/840\n",
      "Batch loss: 0.7121874094009399 batch: 144/840\n",
      "Batch loss: 0.7931289672851562 batch: 145/840\n",
      "Batch loss: 0.9279465675354004 batch: 146/840\n",
      "Batch loss: 0.6206538081169128 batch: 147/840\n",
      "Batch loss: 0.8887471556663513 batch: 148/840\n",
      "Batch loss: 0.8246793150901794 batch: 149/840\n",
      "Batch loss: 0.8405137658119202 batch: 150/840\n",
      "Batch loss: 0.8087702393531799 batch: 151/840\n",
      "Batch loss: 0.6374130845069885 batch: 152/840\n",
      "Batch loss: 0.5996913313865662 batch: 153/840\n",
      "Batch loss: 0.7812432050704956 batch: 154/840\n",
      "Batch loss: 0.5832639932632446 batch: 155/840\n",
      "Batch loss: 0.7412902116775513 batch: 156/840\n",
      "Batch loss: 0.7963218688964844 batch: 157/840\n",
      "Batch loss: 0.580900251865387 batch: 158/840\n",
      "Batch loss: 0.5919039249420166 batch: 159/840\n",
      "Batch loss: 0.6645504236221313 batch: 160/840\n",
      "Batch loss: 0.7380496859550476 batch: 161/840\n",
      "Batch loss: 0.757265567779541 batch: 162/840\n",
      "Batch loss: 0.807160496711731 batch: 163/840\n",
      "Batch loss: 0.5377044081687927 batch: 164/840\n",
      "Batch loss: 0.8108199238777161 batch: 165/840\n",
      "Batch loss: 0.5470985174179077 batch: 166/840\n",
      "Batch loss: 0.859331488609314 batch: 167/840\n",
      "Batch loss: 0.6515962481498718 batch: 168/840\n",
      "Batch loss: 0.6391414403915405 batch: 169/840\n",
      "Batch loss: 0.8413419127464294 batch: 170/840\n",
      "Batch loss: 0.7821393013000488 batch: 171/840\n",
      "Batch loss: 0.8729594349861145 batch: 172/840\n",
      "Batch loss: 0.6686167120933533 batch: 173/840\n",
      "Batch loss: 0.6517744660377502 batch: 174/840\n",
      "Batch loss: 0.6491020917892456 batch: 175/840\n",
      "Batch loss: 0.7444759607315063 batch: 176/840\n",
      "Batch loss: 0.7562717199325562 batch: 177/840\n",
      "Batch loss: 0.6965910196304321 batch: 178/840\n",
      "Batch loss: 0.8233889937400818 batch: 179/840\n",
      "Batch loss: 0.6363637447357178 batch: 180/840\n",
      "Batch loss: 0.6734601855278015 batch: 181/840\n",
      "Batch loss: 0.6929802894592285 batch: 182/840\n",
      "Batch loss: 0.7423838973045349 batch: 183/840\n",
      "Batch loss: 0.6549036502838135 batch: 184/840\n",
      "Batch loss: 0.43263113498687744 batch: 185/840\n",
      "Batch loss: 0.5752149820327759 batch: 186/840\n",
      "Batch loss: 0.6807397603988647 batch: 187/840\n",
      "Batch loss: 0.6762242317199707 batch: 188/840\n",
      "Batch loss: 0.7153347730636597 batch: 189/840\n",
      "Batch loss: 0.7935307025909424 batch: 190/840\n",
      "Batch loss: 0.9103025794029236 batch: 191/840\n",
      "Batch loss: 0.5822007656097412 batch: 192/840\n",
      "Batch loss: 0.6245846748352051 batch: 193/840\n",
      "Batch loss: 0.5361424088478088 batch: 194/840\n",
      "Batch loss: 0.6748999953269958 batch: 195/840\n",
      "Batch loss: 0.9769794940948486 batch: 196/840\n",
      "Batch loss: 0.7660973072052002 batch: 197/840\n",
      "Batch loss: 0.5627879500389099 batch: 198/840\n",
      "Batch loss: 0.6892405152320862 batch: 199/840\n",
      "Batch loss: 0.8509958386421204 batch: 200/840\n",
      "Batch loss: 0.6453380584716797 batch: 201/840\n",
      "Batch loss: 0.7079281806945801 batch: 202/840\n",
      "Batch loss: 0.6765396595001221 batch: 203/840\n",
      "Batch loss: 0.8591858148574829 batch: 204/840\n",
      "Batch loss: 0.8065179586410522 batch: 205/840\n",
      "Batch loss: 0.6959232091903687 batch: 206/840\n",
      "Batch loss: 0.7453796863555908 batch: 207/840\n",
      "Batch loss: 0.7266271114349365 batch: 208/840\n",
      "Batch loss: 0.6658352017402649 batch: 209/840\n",
      "Batch loss: 0.6143521666526794 batch: 210/840\n",
      "Batch loss: 0.6245222687721252 batch: 211/840\n",
      "Batch loss: 0.7604873180389404 batch: 212/840\n",
      "Batch loss: 0.7345529794692993 batch: 213/840\n",
      "Batch loss: 0.8508728742599487 batch: 214/840\n",
      "Batch loss: 0.6949092745780945 batch: 215/840\n",
      "Batch loss: 0.7405847311019897 batch: 216/840\n",
      "Batch loss: 0.6803479194641113 batch: 217/840\n",
      "Batch loss: 0.7164949774742126 batch: 218/840\n",
      "Batch loss: 0.7295795679092407 batch: 219/840\n",
      "Batch loss: 1.065812587738037 batch: 220/840\n",
      "Batch loss: 0.7256700992584229 batch: 221/840\n",
      "Batch loss: 0.8232293725013733 batch: 222/840\n",
      "Batch loss: 0.6850855946540833 batch: 223/840\n",
      "Batch loss: 0.7672192454338074 batch: 224/840\n",
      "Batch loss: 0.7816148996353149 batch: 225/840\n",
      "Batch loss: 0.7311661243438721 batch: 226/840\n",
      "Batch loss: 0.7864633798599243 batch: 227/840\n",
      "Batch loss: 0.6147741079330444 batch: 228/840\n",
      "Batch loss: 0.6168493032455444 batch: 229/840\n",
      "Batch loss: 0.6906480193138123 batch: 230/840\n",
      "Batch loss: 0.7477226853370667 batch: 231/840\n",
      "Batch loss: 0.6750122308731079 batch: 232/840\n",
      "Batch loss: 0.7713662981987 batch: 233/840\n",
      "Batch loss: 0.6300598382949829 batch: 234/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7729421257972717 batch: 235/840\n",
      "Batch loss: 0.7577117681503296 batch: 236/840\n",
      "Batch loss: 0.5641129612922668 batch: 237/840\n",
      "Batch loss: 0.7865288257598877 batch: 238/840\n",
      "Batch loss: 0.7301586866378784 batch: 239/840\n",
      "Batch loss: 0.7609365582466125 batch: 240/840\n",
      "Batch loss: 0.8113994598388672 batch: 241/840\n",
      "Batch loss: 0.6313235759735107 batch: 242/840\n",
      "Batch loss: 0.5920937657356262 batch: 243/840\n",
      "Batch loss: 0.785841703414917 batch: 244/840\n",
      "Batch loss: 0.5799567699432373 batch: 245/840\n",
      "Batch loss: 0.7028455138206482 batch: 246/840\n",
      "Batch loss: 0.6982203722000122 batch: 247/840\n",
      "Batch loss: 0.7640137672424316 batch: 248/840\n",
      "Batch loss: 0.9223235249519348 batch: 249/840\n",
      "Batch loss: 0.6225365996360779 batch: 250/840\n",
      "Batch loss: 0.694206714630127 batch: 251/840\n",
      "Batch loss: 0.6365183591842651 batch: 252/840\n",
      "Batch loss: 0.7373077273368835 batch: 253/840\n",
      "Batch loss: 0.9442362785339355 batch: 254/840\n",
      "Batch loss: 0.7323944568634033 batch: 255/840\n",
      "Batch loss: 0.7443397641181946 batch: 256/840\n",
      "Batch loss: 0.744129478931427 batch: 257/840\n",
      "Batch loss: 0.7245039939880371 batch: 258/840\n",
      "Batch loss: 0.6682178378105164 batch: 259/840\n",
      "Batch loss: 0.5716337561607361 batch: 260/840\n",
      "Batch loss: 0.6033448576927185 batch: 261/840\n",
      "Batch loss: 0.519263744354248 batch: 262/840\n",
      "Batch loss: 0.674630880355835 batch: 263/840\n",
      "Batch loss: 0.728682279586792 batch: 264/840\n",
      "Batch loss: 0.7776702642440796 batch: 265/840\n",
      "Batch loss: 0.6382128000259399 batch: 266/840\n",
      "Batch loss: 0.6667643189430237 batch: 267/840\n",
      "Batch loss: 0.6626213192939758 batch: 268/840\n",
      "Batch loss: 0.5740553140640259 batch: 269/840\n",
      "Batch loss: 0.6218188405036926 batch: 270/840\n",
      "Batch loss: 0.6278319358825684 batch: 271/840\n",
      "Batch loss: 0.8201756477355957 batch: 272/840\n",
      "Batch loss: 0.8617464303970337 batch: 273/840\n",
      "Batch loss: 0.7268837690353394 batch: 274/840\n",
      "Batch loss: 0.7644572257995605 batch: 275/840\n",
      "Batch loss: 0.5813027024269104 batch: 276/840\n",
      "Batch loss: 0.6651880145072937 batch: 277/840\n",
      "Batch loss: 0.8115487098693848 batch: 278/840\n",
      "Batch loss: 0.8564385771751404 batch: 279/840\n",
      "Batch loss: 0.8319611549377441 batch: 280/840\n",
      "Batch loss: 0.676337480545044 batch: 281/840\n",
      "Batch loss: 0.6052840948104858 batch: 282/840\n",
      "Batch loss: 0.752261757850647 batch: 283/840\n",
      "Batch loss: 0.5331385135650635 batch: 284/840\n",
      "Batch loss: 0.6384496092796326 batch: 285/840\n",
      "Batch loss: 0.7907183170318604 batch: 286/840\n",
      "Batch loss: 0.5074187517166138 batch: 287/840\n",
      "Batch loss: 0.6603693962097168 batch: 288/840\n",
      "Batch loss: 0.9948161244392395 batch: 289/840\n",
      "Batch loss: 0.8213522434234619 batch: 290/840\n",
      "Batch loss: 0.7873676419258118 batch: 291/840\n",
      "Batch loss: 0.6730175614356995 batch: 292/840\n",
      "Batch loss: 0.9069156050682068 batch: 293/840\n",
      "Batch loss: 0.6687906384468079 batch: 294/840\n",
      "Batch loss: 0.6102657914161682 batch: 295/840\n",
      "Batch loss: 0.7852917313575745 batch: 296/840\n",
      "Batch loss: 0.7760459780693054 batch: 297/840\n",
      "Batch loss: 0.7043584585189819 batch: 298/840\n",
      "Batch loss: 0.6896561980247498 batch: 299/840\n",
      "Batch loss: 0.7736295461654663 batch: 300/840\n",
      "Batch loss: 0.7498190402984619 batch: 301/840\n",
      "Batch loss: 0.7053563594818115 batch: 302/840\n",
      "Batch loss: 0.8338521718978882 batch: 303/840\n",
      "Batch loss: 0.5910747647285461 batch: 304/840\n",
      "Batch loss: 0.6295678019523621 batch: 305/840\n",
      "Batch loss: 0.7029553055763245 batch: 306/840\n",
      "Batch loss: 0.5665872097015381 batch: 307/840\n",
      "Batch loss: 0.705403208732605 batch: 308/840\n",
      "Batch loss: 0.7400661706924438 batch: 309/840\n",
      "Batch loss: 0.9345749616622925 batch: 310/840\n",
      "Batch loss: 0.7216198444366455 batch: 311/840\n",
      "Batch loss: 0.7472123503684998 batch: 312/840\n",
      "Batch loss: 0.8462965488433838 batch: 313/840\n",
      "Batch loss: 0.6738066673278809 batch: 314/840\n",
      "Batch loss: 0.6638588905334473 batch: 315/840\n",
      "Batch loss: 0.556470513343811 batch: 316/840\n",
      "Batch loss: 0.7716571092605591 batch: 317/840\n",
      "Batch loss: 0.7675206065177917 batch: 318/840\n",
      "Batch loss: 0.7552623748779297 batch: 319/840\n",
      "Batch loss: 0.6480640172958374 batch: 320/840\n",
      "Batch loss: 0.6604530215263367 batch: 321/840\n",
      "Batch loss: 0.831468939781189 batch: 322/840\n",
      "Batch loss: 0.7876414656639099 batch: 323/840\n",
      "Batch loss: 0.8131046295166016 batch: 324/840\n",
      "Batch loss: 0.541635274887085 batch: 325/840\n",
      "Batch loss: 0.6895543932914734 batch: 326/840\n",
      "Batch loss: 0.5582233667373657 batch: 327/840\n",
      "Batch loss: 0.8961377143859863 batch: 328/840\n",
      "Batch loss: 0.7389905452728271 batch: 329/840\n",
      "Batch loss: 0.7811323404312134 batch: 330/840\n",
      "Batch loss: 0.750231146812439 batch: 331/840\n",
      "Batch loss: 0.7296022772789001 batch: 332/840\n",
      "Batch loss: 0.6579037308692932 batch: 333/840\n",
      "Batch loss: 0.6732548475265503 batch: 334/840\n",
      "Batch loss: 0.5951087474822998 batch: 335/840\n",
      "Batch loss: 0.8601725697517395 batch: 336/840\n",
      "Batch loss: 0.9185715317726135 batch: 337/840\n",
      "Batch loss: 0.7805890440940857 batch: 338/840\n",
      "Batch loss: 0.7165205478668213 batch: 339/840\n",
      "Batch loss: 0.9173948168754578 batch: 340/840\n",
      "Batch loss: 0.6037760376930237 batch: 341/840\n",
      "Batch loss: 0.6064789295196533 batch: 342/840\n",
      "Batch loss: 0.9014034867286682 batch: 343/840\n",
      "Batch loss: 0.7242260575294495 batch: 344/840\n",
      "Batch loss: 0.5219289660453796 batch: 345/840\n",
      "Batch loss: 0.6502332091331482 batch: 346/840\n",
      "Batch loss: 0.8185712695121765 batch: 347/840\n",
      "Batch loss: 0.7344782948493958 batch: 348/840\n",
      "Batch loss: 0.7459110021591187 batch: 349/840\n",
      "Batch loss: 0.611625075340271 batch: 350/840\n",
      "Batch loss: 0.7121859788894653 batch: 351/840\n",
      "Batch loss: 0.7245633602142334 batch: 352/840\n",
      "Batch loss: 0.787109911441803 batch: 353/840\n",
      "Batch loss: 0.6635225415229797 batch: 354/840\n",
      "Batch loss: 0.6364280581474304 batch: 355/840\n",
      "Batch loss: 0.7109991312026978 batch: 356/840\n",
      "Batch loss: 0.6298506259918213 batch: 357/840\n",
      "Batch loss: 0.7829416394233704 batch: 358/840\n",
      "Batch loss: 0.7169057726860046 batch: 359/840\n",
      "Batch loss: 0.9260469675064087 batch: 360/840\n",
      "Batch loss: 0.6842662692070007 batch: 361/840\n",
      "Batch loss: 0.6509982347488403 batch: 362/840\n",
      "Batch loss: 0.6454347372055054 batch: 363/840\n",
      "Batch loss: 0.7240166664123535 batch: 364/840\n",
      "Batch loss: 0.5729831457138062 batch: 365/840\n",
      "Batch loss: 0.6742371320724487 batch: 366/840\n",
      "Batch loss: 0.5696076154708862 batch: 367/840\n",
      "Batch loss: 0.8355214595794678 batch: 368/840\n",
      "Batch loss: 0.731550931930542 batch: 369/840\n",
      "Batch loss: 0.849934995174408 batch: 370/840\n",
      "Batch loss: 0.868886411190033 batch: 371/840\n",
      "Batch loss: 0.5202680230140686 batch: 372/840\n",
      "Batch loss: 0.8417753577232361 batch: 373/840\n",
      "Batch loss: 0.7975879907608032 batch: 374/840\n",
      "Batch loss: 0.5797039270401001 batch: 375/840\n",
      "Batch loss: 0.5599792003631592 batch: 376/840\n",
      "Batch loss: 0.7951077222824097 batch: 377/840\n",
      "Batch loss: 0.7017595767974854 batch: 378/840\n",
      "Batch loss: 0.5545739531517029 batch: 379/840\n",
      "Batch loss: 0.8953619599342346 batch: 380/840\n",
      "Batch loss: 0.9078771471977234 batch: 381/840\n",
      "Batch loss: 0.7835339903831482 batch: 382/840\n",
      "Batch loss: 0.725562334060669 batch: 383/840\n",
      "Batch loss: 0.6476001143455505 batch: 384/840\n",
      "Batch loss: 0.7560939788818359 batch: 385/840\n",
      "Batch loss: 0.7022886872291565 batch: 386/840\n",
      "Batch loss: 0.5722173452377319 batch: 387/840\n",
      "Batch loss: 0.7587332129478455 batch: 388/840\n",
      "Batch loss: 0.5638351440429688 batch: 389/840\n",
      "Batch loss: 0.8709649443626404 batch: 390/840\n",
      "Batch loss: 0.7538694143295288 batch: 391/840\n",
      "Batch loss: 0.5313372611999512 batch: 392/840\n",
      "Batch loss: 0.4656153917312622 batch: 393/840\n",
      "Batch loss: 0.8651426434516907 batch: 394/840\n",
      "Batch loss: 0.6792442798614502 batch: 395/840\n",
      "Batch loss: 0.7301221489906311 batch: 396/840\n",
      "Batch loss: 0.6166637539863586 batch: 397/840\n",
      "Batch loss: 0.78658127784729 batch: 398/840\n",
      "Batch loss: 0.54317307472229 batch: 399/840\n",
      "Batch loss: 0.6586026549339294 batch: 400/840\n",
      "Batch loss: 0.7143203020095825 batch: 401/840\n",
      "Batch loss: 0.6930955648422241 batch: 402/840\n",
      "Batch loss: 0.6663808226585388 batch: 403/840\n",
      "Batch loss: 0.8073700070381165 batch: 404/840\n",
      "Batch loss: 0.6213076114654541 batch: 405/840\n",
      "Batch loss: 0.6912751197814941 batch: 406/840\n",
      "Batch loss: 0.743553102016449 batch: 407/840\n",
      "Batch loss: 0.7400141954421997 batch: 408/840\n",
      "Batch loss: 0.7533331513404846 batch: 409/840\n",
      "Batch loss: 0.9567080736160278 batch: 410/840\n",
      "Batch loss: 0.8244326114654541 batch: 411/840\n",
      "Batch loss: 0.8815210461616516 batch: 412/840\n",
      "Batch loss: 0.7173139452934265 batch: 413/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.702406644821167 batch: 414/840\n",
      "Batch loss: 0.9646587371826172 batch: 415/840\n",
      "Batch loss: 0.5132680535316467 batch: 416/840\n",
      "Batch loss: 0.7814856767654419 batch: 417/840\n",
      "Batch loss: 0.8084311485290527 batch: 418/840\n",
      "Batch loss: 0.8190696239471436 batch: 419/840\n",
      "Batch loss: 0.710356593132019 batch: 420/840\n",
      "Batch loss: 0.6220932602882385 batch: 421/840\n",
      "Batch loss: 0.6323842406272888 batch: 422/840\n",
      "Batch loss: 0.8008817434310913 batch: 423/840\n",
      "Batch loss: 0.7588991522789001 batch: 424/840\n",
      "Batch loss: 0.7628356218338013 batch: 425/840\n",
      "Batch loss: 0.7265452742576599 batch: 426/840\n",
      "Batch loss: 0.717620313167572 batch: 427/840\n",
      "Batch loss: 0.8238585591316223 batch: 428/840\n",
      "Batch loss: 0.6396276354789734 batch: 429/840\n",
      "Batch loss: 0.9030716419219971 batch: 430/840\n",
      "Batch loss: 0.6654850244522095 batch: 431/840\n",
      "Batch loss: 0.7328319549560547 batch: 432/840\n",
      "Batch loss: 0.6005955338478088 batch: 433/840\n",
      "Batch loss: 0.6661014556884766 batch: 434/840\n",
      "Batch loss: 0.8167585134506226 batch: 435/840\n",
      "Batch loss: 0.5977407097816467 batch: 436/840\n",
      "Batch loss: 0.7420798540115356 batch: 437/840\n",
      "Batch loss: 0.4375597834587097 batch: 438/840\n",
      "Batch loss: 0.7357406616210938 batch: 439/840\n",
      "Batch loss: 0.8167614936828613 batch: 440/840\n",
      "Batch loss: 0.6695716977119446 batch: 441/840\n",
      "Batch loss: 0.7343645691871643 batch: 442/840\n",
      "Batch loss: 0.7100843191146851 batch: 443/840\n",
      "Batch loss: 0.8099563121795654 batch: 444/840\n",
      "Batch loss: 0.6240289211273193 batch: 445/840\n",
      "Batch loss: 0.7636380791664124 batch: 446/840\n",
      "Batch loss: 0.7400844097137451 batch: 447/840\n",
      "Batch loss: 0.742420494556427 batch: 448/840\n",
      "Batch loss: 0.6646629571914673 batch: 449/840\n",
      "Batch loss: 0.6889075636863708 batch: 450/840\n",
      "Batch loss: 0.7646487951278687 batch: 451/840\n",
      "Batch loss: 0.724306046962738 batch: 452/840\n",
      "Batch loss: 0.6956504583358765 batch: 453/840\n",
      "Batch loss: 0.5825599431991577 batch: 454/840\n",
      "Batch loss: 0.6960315108299255 batch: 455/840\n",
      "Batch loss: 0.6669864654541016 batch: 456/840\n",
      "Batch loss: 0.6236057877540588 batch: 457/840\n",
      "Batch loss: 0.7831437587738037 batch: 458/840\n",
      "Batch loss: 0.5717434883117676 batch: 459/840\n",
      "Batch loss: 0.5561047196388245 batch: 460/840\n",
      "Batch loss: 0.763244092464447 batch: 461/840\n",
      "Batch loss: 0.6604855060577393 batch: 462/840\n",
      "Batch loss: 0.8631288409233093 batch: 463/840\n",
      "Batch loss: 0.5500525832176208 batch: 464/840\n",
      "Batch loss: 0.9536370635032654 batch: 465/840\n",
      "Batch loss: 0.7422077655792236 batch: 466/840\n",
      "Batch loss: 0.9448556303977966 batch: 467/840\n",
      "Batch loss: 0.6937987804412842 batch: 468/840\n",
      "Batch loss: 0.6310485005378723 batch: 469/840\n",
      "Batch loss: 0.7560955882072449 batch: 470/840\n",
      "Batch loss: 0.6851664781570435 batch: 471/840\n",
      "Batch loss: 0.7878330945968628 batch: 472/840\n",
      "Batch loss: 0.7975071668624878 batch: 473/840\n",
      "Batch loss: 0.6574990749359131 batch: 474/840\n",
      "Batch loss: 0.6415067315101624 batch: 475/840\n",
      "Batch loss: 0.7289808392524719 batch: 476/840\n",
      "Batch loss: 0.6408835053443909 batch: 477/840\n",
      "Batch loss: 0.5883412957191467 batch: 478/840\n",
      "Batch loss: 0.5735530257225037 batch: 479/840\n",
      "Batch loss: 0.8292704224586487 batch: 480/840\n",
      "Batch loss: 0.7096595764160156 batch: 481/840\n",
      "Batch loss: 0.7645530104637146 batch: 482/840\n",
      "Batch loss: 0.8307143449783325 batch: 483/840\n",
      "Batch loss: 0.6542266011238098 batch: 484/840\n",
      "Batch loss: 0.7300626635551453 batch: 485/840\n",
      "Batch loss: 0.703372061252594 batch: 486/840\n",
      "Batch loss: 0.5590330362319946 batch: 487/840\n",
      "Batch loss: 0.7426040172576904 batch: 488/840\n",
      "Batch loss: 0.7632491588592529 batch: 489/840\n",
      "Batch loss: 0.7688781023025513 batch: 490/840\n",
      "Batch loss: 0.45476898550987244 batch: 491/840\n",
      "Batch loss: 0.7385368347167969 batch: 492/840\n",
      "Batch loss: 0.8375856280326843 batch: 493/840\n",
      "Batch loss: 0.4861849844455719 batch: 494/840\n",
      "Batch loss: 0.8048438429832458 batch: 495/840\n",
      "Batch loss: 0.7734599113464355 batch: 496/840\n",
      "Batch loss: 0.8219894170761108 batch: 497/840\n",
      "Batch loss: 0.5082494616508484 batch: 498/840\n",
      "Batch loss: 0.7810426354408264 batch: 499/840\n",
      "Batch loss: 0.6848259568214417 batch: 500/840\n",
      "Batch loss: 0.5889864563941956 batch: 501/840\n",
      "Batch loss: 0.7154238224029541 batch: 502/840\n",
      "Batch loss: 0.5614111423492432 batch: 503/840\n",
      "Batch loss: 0.6946288347244263 batch: 504/840\n",
      "Batch loss: 0.7235785722732544 batch: 505/840\n",
      "Batch loss: 0.6911023855209351 batch: 506/840\n",
      "Batch loss: 0.7246540784835815 batch: 507/840\n",
      "Batch loss: 0.6085751056671143 batch: 508/840\n",
      "Batch loss: 0.8077313899993896 batch: 509/840\n",
      "Batch loss: 0.6768929958343506 batch: 510/840\n",
      "Batch loss: 0.5703304409980774 batch: 511/840\n",
      "Batch loss: 0.6899781227111816 batch: 512/840\n",
      "Batch loss: 0.6116536259651184 batch: 513/840\n",
      "Batch loss: 0.7350046038627625 batch: 514/840\n",
      "Batch loss: 0.5861701965332031 batch: 515/840\n",
      "Batch loss: 0.7680050134658813 batch: 516/840\n",
      "Batch loss: 0.7302577495574951 batch: 517/840\n",
      "Batch loss: 0.6421898007392883 batch: 518/840\n",
      "Batch loss: 0.9873073101043701 batch: 519/840\n",
      "Batch loss: 0.710685133934021 batch: 520/840\n",
      "Batch loss: 0.8811385631561279 batch: 521/840\n",
      "Batch loss: 0.7017942070960999 batch: 522/840\n",
      "Batch loss: 0.4838366210460663 batch: 523/840\n",
      "Batch loss: 0.7790950536727905 batch: 524/840\n",
      "Batch loss: 0.7197664380073547 batch: 525/840\n",
      "Batch loss: 0.8641851544380188 batch: 526/840\n",
      "Batch loss: 0.8023478984832764 batch: 527/840\n",
      "Batch loss: 0.683576226234436 batch: 528/840\n",
      "Batch loss: 0.6271611452102661 batch: 529/840\n",
      "Batch loss: 0.7498647570610046 batch: 530/840\n",
      "Batch loss: 0.6117919087409973 batch: 531/840\n",
      "Batch loss: 0.5091969966888428 batch: 532/840\n",
      "Batch loss: 0.759019672870636 batch: 533/840\n",
      "Batch loss: 0.7812132239341736 batch: 534/840\n",
      "Batch loss: 0.6470082998275757 batch: 535/840\n",
      "Batch loss: 0.6707894206047058 batch: 536/840\n",
      "Batch loss: 0.8117685914039612 batch: 537/840\n",
      "Batch loss: 0.6344968676567078 batch: 538/840\n",
      "Batch loss: 0.6435467004776001 batch: 539/840\n",
      "Batch loss: 0.7912895083427429 batch: 540/840\n",
      "Batch loss: 0.5765401721000671 batch: 541/840\n",
      "Batch loss: 0.7578522562980652 batch: 542/840\n",
      "Batch loss: 0.5429040789604187 batch: 543/840\n",
      "Batch loss: 0.7135900855064392 batch: 544/840\n",
      "Batch loss: 0.7039528489112854 batch: 545/840\n",
      "Batch loss: 0.6729803681373596 batch: 546/840\n",
      "Batch loss: 0.7488980889320374 batch: 547/840\n",
      "Batch loss: 0.5348513126373291 batch: 548/840\n",
      "Batch loss: 0.6399645805358887 batch: 549/840\n",
      "Batch loss: 0.558493435382843 batch: 550/840\n",
      "Batch loss: 0.7193220257759094 batch: 551/840\n",
      "Batch loss: 0.6177694201469421 batch: 552/840\n",
      "Batch loss: 0.7414395809173584 batch: 553/840\n",
      "Batch loss: 0.7645357251167297 batch: 554/840\n",
      "Batch loss: 0.710432767868042 batch: 555/840\n",
      "Batch loss: 0.717771053314209 batch: 556/840\n",
      "Batch loss: 0.7014948129653931 batch: 557/840\n",
      "Batch loss: 0.6156557202339172 batch: 558/840\n",
      "Batch loss: 0.6645970940589905 batch: 559/840\n",
      "Batch loss: 0.7446514964103699 batch: 560/840\n",
      "Batch loss: 0.654458224773407 batch: 561/840\n",
      "Batch loss: 0.6876296401023865 batch: 562/840\n",
      "Batch loss: 0.5878861546516418 batch: 563/840\n",
      "Batch loss: 0.6492574214935303 batch: 564/840\n",
      "Batch loss: 0.890495777130127 batch: 565/840\n",
      "Batch loss: 0.6965442895889282 batch: 566/840\n",
      "Batch loss: 0.8126800060272217 batch: 567/840\n",
      "Batch loss: 0.7233667969703674 batch: 568/840\n",
      "Batch loss: 0.7012945413589478 batch: 569/840\n",
      "Batch loss: 0.5210492610931396 batch: 570/840\n",
      "Batch loss: 0.6847760677337646 batch: 571/840\n",
      "Batch loss: 0.777985155582428 batch: 572/840\n",
      "Batch loss: 0.6971357464790344 batch: 573/840\n",
      "Batch loss: 0.7101508975028992 batch: 574/840\n",
      "Batch loss: 0.6349354386329651 batch: 575/840\n",
      "Batch loss: 0.707944393157959 batch: 576/840\n",
      "Batch loss: 0.6460769176483154 batch: 577/840\n",
      "Batch loss: 0.6910712718963623 batch: 578/840\n",
      "Batch loss: 0.7053505778312683 batch: 579/840\n",
      "Batch loss: 0.8714719414710999 batch: 580/840\n",
      "Batch loss: 0.7353156805038452 batch: 581/840\n",
      "Batch loss: 0.9038500785827637 batch: 582/840\n",
      "Batch loss: 0.6592394113540649 batch: 583/840\n",
      "Batch loss: 0.893883228302002 batch: 584/840\n",
      "Batch loss: 0.6618192791938782 batch: 585/840\n",
      "Batch loss: 0.7400809526443481 batch: 586/840\n",
      "Batch loss: 0.678882360458374 batch: 587/840\n",
      "Batch loss: 0.6283236145973206 batch: 588/840\n",
      "Batch loss: 0.816448986530304 batch: 589/840\n",
      "Batch loss: 0.7303951382637024 batch: 590/840\n",
      "Batch loss: 0.5080763101577759 batch: 591/840\n",
      "Batch loss: 0.6122757196426392 batch: 592/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7742642760276794 batch: 593/840\n",
      "Batch loss: 0.6991603374481201 batch: 594/840\n",
      "Batch loss: 0.45113399624824524 batch: 595/840\n",
      "Batch loss: 0.591407835483551 batch: 596/840\n",
      "Batch loss: 0.7118915319442749 batch: 597/840\n",
      "Batch loss: 0.5601863265037537 batch: 598/840\n",
      "Batch loss: 0.5582578182220459 batch: 599/840\n",
      "Batch loss: 0.8223369717597961 batch: 600/840\n",
      "Batch loss: 0.7205594778060913 batch: 601/840\n",
      "Batch loss: 0.6125538349151611 batch: 602/840\n",
      "Batch loss: 0.7006803750991821 batch: 603/840\n",
      "Batch loss: 0.7223252058029175 batch: 604/840\n",
      "Batch loss: 0.9176857471466064 batch: 605/840\n",
      "Batch loss: 0.8329961895942688 batch: 606/840\n",
      "Batch loss: 0.7359417080879211 batch: 607/840\n",
      "Batch loss: 0.8659982085227966 batch: 608/840\n",
      "Batch loss: 0.6168786883354187 batch: 609/840\n",
      "Batch loss: 0.7695086002349854 batch: 610/840\n",
      "Batch loss: 0.7614015936851501 batch: 611/840\n",
      "Batch loss: 0.8142792582511902 batch: 612/840\n",
      "Batch loss: 0.7714126706123352 batch: 613/840\n",
      "Batch loss: 0.7432668805122375 batch: 614/840\n",
      "Batch loss: 0.6640731692314148 batch: 615/840\n",
      "Batch loss: 0.5826902985572815 batch: 616/840\n",
      "Batch loss: 0.6125352382659912 batch: 617/840\n",
      "Batch loss: 0.7527706027030945 batch: 618/840\n",
      "Batch loss: 0.895979106426239 batch: 619/840\n",
      "Batch loss: 0.6663525104522705 batch: 620/840\n",
      "Batch loss: 0.7341112494468689 batch: 621/840\n",
      "Batch loss: 0.6819728016853333 batch: 622/840\n",
      "Batch loss: 0.6287203431129456 batch: 623/840\n",
      "Batch loss: 0.8659553527832031 batch: 624/840\n",
      "Batch loss: 0.7590544819831848 batch: 625/840\n",
      "Batch loss: 0.6061800718307495 batch: 626/840\n",
      "Batch loss: 0.6616079807281494 batch: 627/840\n",
      "Batch loss: 0.6946694254875183 batch: 628/840\n",
      "Batch loss: 0.7375814318656921 batch: 629/840\n",
      "Batch loss: 0.7755905985832214 batch: 630/840\n",
      "Batch loss: 0.7827664017677307 batch: 631/840\n",
      "Batch loss: 0.628215491771698 batch: 632/840\n",
      "Batch loss: 0.5978583097457886 batch: 633/840\n",
      "Batch loss: 0.8589648604393005 batch: 634/840\n",
      "Batch loss: 0.5830287933349609 batch: 635/840\n",
      "Batch loss: 0.7007493376731873 batch: 636/840\n",
      "Batch loss: 0.5907618403434753 batch: 637/840\n",
      "Batch loss: 0.6956007480621338 batch: 638/840\n",
      "Batch loss: 0.6997547745704651 batch: 639/840\n",
      "Batch loss: 0.6059902310371399 batch: 640/840\n",
      "Batch loss: 0.9028146266937256 batch: 641/840\n",
      "Batch loss: 0.663917064666748 batch: 642/840\n",
      "Batch loss: 1.092801570892334 batch: 643/840\n",
      "Batch loss: 0.6191686391830444 batch: 644/840\n",
      "Batch loss: 0.5793887972831726 batch: 645/840\n",
      "Batch loss: 0.5713828802108765 batch: 646/840\n",
      "Batch loss: 0.846193253993988 batch: 647/840\n",
      "Batch loss: 0.7412552833557129 batch: 648/840\n",
      "Batch loss: 0.6929494738578796 batch: 649/840\n",
      "Batch loss: 0.6866304278373718 batch: 650/840\n",
      "Batch loss: 0.6820539832115173 batch: 651/840\n",
      "Batch loss: 0.7325344681739807 batch: 652/840\n",
      "Batch loss: 0.619225025177002 batch: 653/840\n",
      "Batch loss: 0.978009819984436 batch: 654/840\n",
      "Batch loss: 0.5857833027839661 batch: 655/840\n",
      "Batch loss: 0.7157042622566223 batch: 656/840\n",
      "Batch loss: 0.6587038636207581 batch: 657/840\n",
      "Batch loss: 0.8059656620025635 batch: 658/840\n",
      "Batch loss: 0.6832592487335205 batch: 659/840\n",
      "Batch loss: 0.5625000596046448 batch: 660/840\n",
      "Batch loss: 0.7763313055038452 batch: 661/840\n",
      "Batch loss: 0.7792249917984009 batch: 662/840\n",
      "Batch loss: 0.5703359842300415 batch: 663/840\n",
      "Batch loss: 0.696309506893158 batch: 664/840\n",
      "Batch loss: 0.895750105381012 batch: 665/840\n",
      "Batch loss: 0.6039090156555176 batch: 666/840\n",
      "Batch loss: 0.8476486206054688 batch: 667/840\n",
      "Batch loss: 0.7145172953605652 batch: 668/840\n",
      "Batch loss: 0.6364412307739258 batch: 669/840\n",
      "Batch loss: 0.8017176985740662 batch: 670/840\n",
      "Batch loss: 0.672763466835022 batch: 671/840\n",
      "Batch loss: 0.5514310002326965 batch: 672/840\n",
      "Batch loss: 0.886818528175354 batch: 673/840\n",
      "Batch loss: 0.8606014847755432 batch: 674/840\n",
      "Batch loss: 0.7637040019035339 batch: 675/840\n",
      "Batch loss: 0.5476066470146179 batch: 676/840\n",
      "Batch loss: 0.6354988217353821 batch: 677/840\n",
      "Batch loss: 0.7453188896179199 batch: 678/840\n",
      "Batch loss: 0.8154376745223999 batch: 679/840\n",
      "Batch loss: 0.7458692789077759 batch: 680/840\n",
      "Batch loss: 0.7246568202972412 batch: 681/840\n",
      "Batch loss: 0.6892200708389282 batch: 682/840\n",
      "Batch loss: 0.574097216129303 batch: 683/840\n",
      "Batch loss: 1.001095175743103 batch: 684/840\n",
      "Batch loss: 0.7412264943122864 batch: 685/840\n",
      "Batch loss: 0.7862299084663391 batch: 686/840\n",
      "Batch loss: 0.7922530174255371 batch: 687/840\n",
      "Batch loss: 0.6434167623519897 batch: 688/840\n",
      "Batch loss: 0.7113308906555176 batch: 689/840\n",
      "Batch loss: 0.6377032995223999 batch: 690/840\n",
      "Batch loss: 0.7484589219093323 batch: 691/840\n",
      "Batch loss: 0.6618409752845764 batch: 692/840\n",
      "Batch loss: 0.8190010190010071 batch: 693/840\n",
      "Batch loss: 0.9171802997589111 batch: 694/840\n",
      "Batch loss: 0.6786880493164062 batch: 695/840\n",
      "Batch loss: 0.683376669883728 batch: 696/840\n",
      "Batch loss: 0.576196014881134 batch: 697/840\n",
      "Batch loss: 0.7586234211921692 batch: 698/840\n",
      "Batch loss: 0.7628630995750427 batch: 699/840\n",
      "Batch loss: 0.8105953931808472 batch: 700/840\n",
      "Batch loss: 0.8580771088600159 batch: 701/840\n",
      "Batch loss: 0.7867946028709412 batch: 702/840\n",
      "Batch loss: 0.639206051826477 batch: 703/840\n",
      "Batch loss: 0.7644959092140198 batch: 704/840\n",
      "Batch loss: 0.6790940165519714 batch: 705/840\n",
      "Batch loss: 0.7552591562271118 batch: 706/840\n",
      "Batch loss: 0.7112879753112793 batch: 707/840\n",
      "Batch loss: 0.8037510514259338 batch: 708/840\n",
      "Batch loss: 0.7327543497085571 batch: 709/840\n",
      "Batch loss: 0.8296389579772949 batch: 710/840\n",
      "Batch loss: 0.5481941103935242 batch: 711/840\n",
      "Batch loss: 0.795703649520874 batch: 712/840\n",
      "Batch loss: 0.6069822311401367 batch: 713/840\n",
      "Batch loss: 0.7235825061798096 batch: 714/840\n",
      "Batch loss: 0.8470842242240906 batch: 715/840\n",
      "Batch loss: 0.7552571296691895 batch: 716/840\n",
      "Batch loss: 0.7734373211860657 batch: 717/840\n",
      "Batch loss: 0.6380036473274231 batch: 718/840\n",
      "Batch loss: 0.5867109894752502 batch: 719/840\n",
      "Batch loss: 0.6532554030418396 batch: 720/840\n",
      "Batch loss: 0.7343044877052307 batch: 721/840\n",
      "Batch loss: 0.8242468237876892 batch: 722/840\n",
      "Batch loss: 0.5664815902709961 batch: 723/840\n",
      "Batch loss: 0.8527726531028748 batch: 724/840\n",
      "Batch loss: 0.7298452258110046 batch: 725/840\n",
      "Batch loss: 0.640082597732544 batch: 726/840\n",
      "Batch loss: 0.8818922638893127 batch: 727/840\n",
      "Batch loss: 0.8203645944595337 batch: 728/840\n",
      "Batch loss: 0.8387424349784851 batch: 729/840\n",
      "Batch loss: 0.7050416469573975 batch: 730/840\n",
      "Batch loss: 0.5482653975486755 batch: 731/840\n",
      "Batch loss: 0.9335175156593323 batch: 732/840\n",
      "Batch loss: 0.7129432559013367 batch: 733/840\n",
      "Batch loss: 0.6376798152923584 batch: 734/840\n",
      "Batch loss: 0.6214609742164612 batch: 735/840\n",
      "Batch loss: 0.6912397742271423 batch: 736/840\n",
      "Batch loss: 0.7135916948318481 batch: 737/840\n",
      "Batch loss: 0.6082621216773987 batch: 738/840\n",
      "Batch loss: 0.7658849358558655 batch: 739/840\n",
      "Batch loss: 0.8786351680755615 batch: 740/840\n",
      "Batch loss: 0.5474098920822144 batch: 741/840\n",
      "Batch loss: 0.6506959795951843 batch: 742/840\n",
      "Batch loss: 0.48150426149368286 batch: 743/840\n",
      "Batch loss: 0.6574898362159729 batch: 744/840\n",
      "Batch loss: 0.5812066793441772 batch: 745/840\n",
      "Batch loss: 0.6173197627067566 batch: 746/840\n",
      "Batch loss: 0.7829426527023315 batch: 747/840\n",
      "Batch loss: 0.7999216318130493 batch: 748/840\n",
      "Batch loss: 0.5464867949485779 batch: 749/840\n",
      "Batch loss: 0.6149702668190002 batch: 750/840\n",
      "Batch loss: 0.7370191216468811 batch: 751/840\n",
      "Batch loss: 0.9299324750900269 batch: 752/840\n",
      "Batch loss: 0.6259210705757141 batch: 753/840\n",
      "Batch loss: 0.6891704797744751 batch: 754/840\n",
      "Batch loss: 0.7143238186836243 batch: 755/840\n",
      "Batch loss: 0.6613421440124512 batch: 756/840\n",
      "Batch loss: 0.8142955303192139 batch: 757/840\n",
      "Batch loss: 0.7244391441345215 batch: 758/840\n",
      "Batch loss: 0.6434761881828308 batch: 759/840\n",
      "Batch loss: 0.6497369408607483 batch: 760/840\n",
      "Batch loss: 0.6163647174835205 batch: 761/840\n",
      "Batch loss: 0.8074173927307129 batch: 762/840\n",
      "Batch loss: 0.4875809848308563 batch: 763/840\n",
      "Batch loss: 0.6783390641212463 batch: 764/840\n",
      "Batch loss: 0.5722348690032959 batch: 765/840\n",
      "Batch loss: 0.6222872138023376 batch: 766/840\n",
      "Batch loss: 0.8086626529693604 batch: 767/840\n",
      "Batch loss: 0.7660396099090576 batch: 768/840\n",
      "Batch loss: 0.683281660079956 batch: 769/840\n",
      "Batch loss: 0.6326858401298523 batch: 770/840\n",
      "Batch loss: 0.6503607034683228 batch: 771/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7136338949203491 batch: 772/840\n",
      "Batch loss: 0.5603766441345215 batch: 773/840\n",
      "Batch loss: 0.520460307598114 batch: 774/840\n",
      "Batch loss: 0.54917311668396 batch: 775/840\n",
      "Batch loss: 0.5805213451385498 batch: 776/840\n",
      "Batch loss: 0.6006617546081543 batch: 777/840\n",
      "Batch loss: 0.5212373733520508 batch: 778/840\n",
      "Batch loss: 0.8456566333770752 batch: 779/840\n",
      "Batch loss: 0.726939857006073 batch: 780/840\n",
      "Batch loss: 0.7831668257713318 batch: 781/840\n",
      "Batch loss: 0.6716859340667725 batch: 782/840\n",
      "Batch loss: 0.5151993632316589 batch: 783/840\n",
      "Batch loss: 0.7599252462387085 batch: 784/840\n",
      "Batch loss: 0.7033916711807251 batch: 785/840\n",
      "Batch loss: 0.7887646555900574 batch: 786/840\n",
      "Batch loss: 0.6017487049102783 batch: 787/840\n",
      "Batch loss: 0.7289230227470398 batch: 788/840\n",
      "Batch loss: 0.8354370594024658 batch: 789/840\n",
      "Batch loss: 0.7312440276145935 batch: 790/840\n",
      "Batch loss: 0.6571652889251709 batch: 791/840\n",
      "Batch loss: 0.5245264172554016 batch: 792/840\n",
      "Batch loss: 0.6717785000801086 batch: 793/840\n",
      "Batch loss: 0.6919209361076355 batch: 794/840\n",
      "Batch loss: 0.7017455101013184 batch: 795/840\n",
      "Batch loss: 0.7670915722846985 batch: 796/840\n",
      "Batch loss: 0.8048036694526672 batch: 797/840\n",
      "Batch loss: 0.8219286203384399 batch: 798/840\n",
      "Batch loss: 0.7080124020576477 batch: 799/840\n",
      "Batch loss: 0.5959576964378357 batch: 800/840\n",
      "Batch loss: 0.7900893688201904 batch: 801/840\n",
      "Batch loss: 0.6071262955665588 batch: 802/840\n",
      "Batch loss: 0.6315797567367554 batch: 803/840\n",
      "Batch loss: 0.8457129001617432 batch: 804/840\n",
      "Batch loss: 0.6365774273872375 batch: 805/840\n",
      "Batch loss: 0.6789788007736206 batch: 806/840\n",
      "Batch loss: 0.6178592443466187 batch: 807/840\n",
      "Batch loss: 0.6649655699729919 batch: 808/840\n",
      "Batch loss: 0.6788281202316284 batch: 809/840\n",
      "Batch loss: 0.5758408904075623 batch: 810/840\n",
      "Batch loss: 0.5763948559761047 batch: 811/840\n",
      "Batch loss: 0.6571162939071655 batch: 812/840\n",
      "Batch loss: 0.6320579051971436 batch: 813/840\n",
      "Batch loss: 0.6317616701126099 batch: 814/840\n",
      "Batch loss: 0.824118435382843 batch: 815/840\n",
      "Batch loss: 0.7167108058929443 batch: 816/840\n",
      "Batch loss: 0.7546290755271912 batch: 817/840\n",
      "Batch loss: 0.8387067914009094 batch: 818/840\n",
      "Batch loss: 0.6156520247459412 batch: 819/840\n",
      "Batch loss: 0.6365466117858887 batch: 820/840\n",
      "Batch loss: 0.669282853603363 batch: 821/840\n",
      "Batch loss: 0.714084804058075 batch: 822/840\n",
      "Batch loss: 0.7635595202445984 batch: 823/840\n",
      "Batch loss: 0.7860910296440125 batch: 824/840\n",
      "Batch loss: 0.7800630927085876 batch: 825/840\n",
      "Batch loss: 0.671176552772522 batch: 826/840\n",
      "Batch loss: 0.529365599155426 batch: 827/840\n",
      "Batch loss: 0.7464150786399841 batch: 828/840\n",
      "Batch loss: 0.6289170980453491 batch: 829/840\n",
      "Batch loss: 0.6961411237716675 batch: 830/840\n",
      "Batch loss: 0.6037839651107788 batch: 831/840\n",
      "Batch loss: 0.754848837852478 batch: 832/840\n",
      "Batch loss: 0.8561864495277405 batch: 833/840\n",
      "Batch loss: 0.6042765974998474 batch: 834/840\n",
      "Batch loss: 0.6488367319107056 batch: 835/840\n",
      "Batch loss: 0.7277008295059204 batch: 836/840\n",
      "Batch loss: 0.6650522351264954 batch: 837/840\n",
      "Batch loss: 0.8746609687805176 batch: 838/840\n",
      "Batch loss: 0.6077144742012024 batch: 839/840\n",
      "Batch loss: 0.6680744886398315 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 2/15..  Training Loss: 0.007..  Test Loss: 0.005..  Test Accuracy: 0.810\n",
      "Running epoch 3/15\n",
      "Batch loss: 0.5292207598686218 batch: 1/840\n",
      "Batch loss: 1.1986984014511108 batch: 2/840\n",
      "Batch loss: 0.6761066317558289 batch: 3/840\n",
      "Batch loss: 0.7186218500137329 batch: 4/840\n",
      "Batch loss: 0.6352089047431946 batch: 5/840\n",
      "Batch loss: 0.5574839115142822 batch: 6/840\n",
      "Batch loss: 0.6954131126403809 batch: 7/840\n",
      "Batch loss: 0.7303635478019714 batch: 8/840\n",
      "Batch loss: 0.6508526802062988 batch: 9/840\n",
      "Batch loss: 0.7037162780761719 batch: 10/840\n",
      "Batch loss: 0.6162607669830322 batch: 11/840\n",
      "Batch loss: 0.5849102139472961 batch: 12/840\n",
      "Batch loss: 0.574469804763794 batch: 13/840\n",
      "Batch loss: 0.7351755499839783 batch: 14/840\n",
      "Batch loss: 0.6822904944419861 batch: 15/840\n",
      "Batch loss: 0.5665834546089172 batch: 16/840\n",
      "Batch loss: 0.5626401305198669 batch: 17/840\n",
      "Batch loss: 0.7241134643554688 batch: 18/840\n",
      "Batch loss: 0.8139712810516357 batch: 19/840\n",
      "Batch loss: 0.8222740888595581 batch: 20/840\n",
      "Batch loss: 0.8378984928131104 batch: 21/840\n",
      "Batch loss: 0.6387631893157959 batch: 22/840\n",
      "Batch loss: 0.8066418170928955 batch: 23/840\n",
      "Batch loss: 0.6241397261619568 batch: 24/840\n",
      "Batch loss: 0.6128789782524109 batch: 25/840\n",
      "Batch loss: 0.7747552990913391 batch: 26/840\n",
      "Batch loss: 0.7204374670982361 batch: 27/840\n",
      "Batch loss: 0.738588273525238 batch: 28/840\n",
      "Batch loss: 0.6567575931549072 batch: 29/840\n",
      "Batch loss: 0.5358717441558838 batch: 30/840\n",
      "Batch loss: 0.6334027051925659 batch: 31/840\n",
      "Batch loss: 0.5238910913467407 batch: 32/840\n",
      "Batch loss: 0.683676540851593 batch: 33/840\n",
      "Batch loss: 0.5652094483375549 batch: 34/840\n",
      "Batch loss: 0.5891364812850952 batch: 35/840\n",
      "Batch loss: 0.554172158241272 batch: 36/840\n",
      "Batch loss: 0.742882251739502 batch: 37/840\n",
      "Batch loss: 0.6906801462173462 batch: 38/840\n",
      "Batch loss: 0.6924279928207397 batch: 39/840\n",
      "Batch loss: 0.6503846049308777 batch: 40/840\n",
      "Batch loss: 0.8234766125679016 batch: 41/840\n",
      "Batch loss: 0.7231494188308716 batch: 42/840\n",
      "Batch loss: 0.7425180077552795 batch: 43/840\n",
      "Batch loss: 0.7232996225357056 batch: 44/840\n",
      "Batch loss: 0.7576478719711304 batch: 45/840\n",
      "Batch loss: 0.5954294204711914 batch: 46/840\n",
      "Batch loss: 0.5560197234153748 batch: 47/840\n",
      "Batch loss: 0.7273024916648865 batch: 48/840\n",
      "Batch loss: 0.6476754546165466 batch: 49/840\n",
      "Batch loss: 0.6449614763259888 batch: 50/840\n",
      "Batch loss: 0.7566247582435608 batch: 51/840\n",
      "Batch loss: 0.8039968609809875 batch: 52/840\n",
      "Batch loss: 0.6154347062110901 batch: 53/840\n",
      "Batch loss: 0.713840126991272 batch: 54/840\n",
      "Batch loss: 0.642334520816803 batch: 55/840\n",
      "Batch loss: 0.6962252855300903 batch: 56/840\n",
      "Batch loss: 0.7950189709663391 batch: 57/840\n",
      "Batch loss: 0.5700063109397888 batch: 58/840\n",
      "Batch loss: 0.6815273761749268 batch: 59/840\n",
      "Batch loss: 0.5854414105415344 batch: 60/840\n",
      "Batch loss: 0.7995607256889343 batch: 61/840\n",
      "Batch loss: 0.7397872805595398 batch: 62/840\n",
      "Batch loss: 0.7534514665603638 batch: 63/840\n",
      "Batch loss: 0.6259179711341858 batch: 64/840\n",
      "Batch loss: 0.5985757112503052 batch: 65/840\n",
      "Batch loss: 0.6679425239562988 batch: 66/840\n",
      "Batch loss: 0.7322959899902344 batch: 67/840\n",
      "Batch loss: 0.7146134972572327 batch: 68/840\n",
      "Batch loss: 0.7339687943458557 batch: 69/840\n",
      "Batch loss: 0.6987648606300354 batch: 70/840\n",
      "Batch loss: 0.7616095542907715 batch: 71/840\n",
      "Batch loss: 0.8117280006408691 batch: 72/840\n",
      "Batch loss: 0.7067753076553345 batch: 73/840\n",
      "Batch loss: 0.5244948863983154 batch: 74/840\n",
      "Batch loss: 0.7970660924911499 batch: 75/840\n",
      "Batch loss: 0.4835788309574127 batch: 76/840\n",
      "Batch loss: 0.5858955383300781 batch: 77/840\n",
      "Batch loss: 0.874768853187561 batch: 78/840\n",
      "Batch loss: 0.7373043894767761 batch: 79/840\n",
      "Batch loss: 0.7142813801765442 batch: 80/840\n",
      "Batch loss: 0.6474249958992004 batch: 81/840\n",
      "Batch loss: 0.8030839562416077 batch: 82/840\n",
      "Batch loss: 0.6213977932929993 batch: 83/840\n",
      "Batch loss: 0.7868121862411499 batch: 84/840\n",
      "Batch loss: 0.6138266324996948 batch: 85/840\n",
      "Batch loss: 0.932350754737854 batch: 86/840\n",
      "Batch loss: 0.5684287548065186 batch: 87/840\n",
      "Batch loss: 0.48073700070381165 batch: 88/840\n",
      "Batch loss: 0.4704946279525757 batch: 89/840\n",
      "Batch loss: 0.657000720500946 batch: 90/840\n",
      "Batch loss: 0.5853283405303955 batch: 91/840\n",
      "Batch loss: 0.6301538348197937 batch: 92/840\n",
      "Batch loss: 0.78029465675354 batch: 93/840\n",
      "Batch loss: 0.6902642846107483 batch: 94/840\n",
      "Batch loss: 0.6314200758934021 batch: 95/840\n",
      "Batch loss: 0.7050585746765137 batch: 96/840\n",
      "Batch loss: 0.7228636741638184 batch: 97/840\n",
      "Batch loss: 0.7826513051986694 batch: 98/840\n",
      "Batch loss: 0.6391090154647827 batch: 99/840\n",
      "Batch loss: 0.6953184604644775 batch: 100/840\n",
      "Batch loss: 0.6104817390441895 batch: 101/840\n",
      "Batch loss: 0.5838564038276672 batch: 102/840\n",
      "Batch loss: 0.7240455746650696 batch: 103/840\n",
      "Batch loss: 0.5170993208885193 batch: 104/840\n",
      "Batch loss: 0.6524917483329773 batch: 105/840\n",
      "Batch loss: 0.7329418063163757 batch: 106/840\n",
      "Batch loss: 0.6061649918556213 batch: 107/840\n",
      "Batch loss: 0.8365018367767334 batch: 108/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6759828925132751 batch: 109/840\n",
      "Batch loss: 0.5473222136497498 batch: 110/840\n",
      "Batch loss: 0.5582352876663208 batch: 111/840\n",
      "Batch loss: 0.7974791526794434 batch: 112/840\n",
      "Batch loss: 0.7353047132492065 batch: 113/840\n",
      "Batch loss: 0.6366559863090515 batch: 114/840\n",
      "Batch loss: 0.6948081851005554 batch: 115/840\n",
      "Batch loss: 0.6145722270011902 batch: 116/840\n",
      "Batch loss: 0.6543692946434021 batch: 117/840\n",
      "Batch loss: 0.5518554449081421 batch: 118/840\n",
      "Batch loss: 0.6531992554664612 batch: 119/840\n",
      "Batch loss: 0.6035178899765015 batch: 120/840\n",
      "Batch loss: 0.7630844712257385 batch: 121/840\n",
      "Batch loss: 1.0080513954162598 batch: 122/840\n",
      "Batch loss: 0.5163282155990601 batch: 123/840\n",
      "Batch loss: 0.6124362349510193 batch: 124/840\n",
      "Batch loss: 0.7135162353515625 batch: 125/840\n",
      "Batch loss: 0.7433531880378723 batch: 126/840\n",
      "Batch loss: 0.879401683807373 batch: 127/840\n",
      "Batch loss: 0.7361404895782471 batch: 128/840\n",
      "Batch loss: 0.777320921421051 batch: 129/840\n",
      "Batch loss: 0.7002102732658386 batch: 130/840\n",
      "Batch loss: 0.7680094838142395 batch: 131/840\n",
      "Batch loss: 0.9450829029083252 batch: 132/840\n",
      "Batch loss: 0.8155010938644409 batch: 133/840\n",
      "Batch loss: 0.6288754940032959 batch: 134/840\n",
      "Batch loss: 0.5664722323417664 batch: 135/840\n",
      "Batch loss: 0.778412938117981 batch: 136/840\n",
      "Batch loss: 0.6148585677146912 batch: 137/840\n",
      "Batch loss: 0.4889746606349945 batch: 138/840\n",
      "Batch loss: 0.5972638726234436 batch: 139/840\n",
      "Batch loss: 0.7391817569732666 batch: 140/840\n",
      "Batch loss: 0.5256446599960327 batch: 141/840\n",
      "Batch loss: 0.6561333537101746 batch: 142/840\n",
      "Batch loss: 0.5814676880836487 batch: 143/840\n",
      "Batch loss: 0.6083439588546753 batch: 144/840\n",
      "Batch loss: 0.7821031808853149 batch: 145/840\n",
      "Batch loss: 0.8158199191093445 batch: 146/840\n",
      "Batch loss: 0.5236732959747314 batch: 147/840\n",
      "Batch loss: 0.7334246635437012 batch: 148/840\n",
      "Batch loss: 0.6162160634994507 batch: 149/840\n",
      "Batch loss: 0.820746898651123 batch: 150/840\n",
      "Batch loss: 0.6805431842803955 batch: 151/840\n",
      "Batch loss: 0.7007073163986206 batch: 152/840\n",
      "Batch loss: 0.5896347165107727 batch: 153/840\n",
      "Batch loss: 0.7722908854484558 batch: 154/840\n",
      "Batch loss: 0.5831841230392456 batch: 155/840\n",
      "Batch loss: 0.5941354036331177 batch: 156/840\n",
      "Batch loss: 0.7083683013916016 batch: 157/840\n",
      "Batch loss: 0.6186941862106323 batch: 158/840\n",
      "Batch loss: 0.5682351589202881 batch: 159/840\n",
      "Batch loss: 0.7470566034317017 batch: 160/840\n",
      "Batch loss: 0.6944715976715088 batch: 161/840\n",
      "Batch loss: 0.7580193877220154 batch: 162/840\n",
      "Batch loss: 0.7906889915466309 batch: 163/840\n",
      "Batch loss: 0.47355353832244873 batch: 164/840\n",
      "Batch loss: 0.7566112279891968 batch: 165/840\n",
      "Batch loss: 0.5876017212867737 batch: 166/840\n",
      "Batch loss: 0.7391262650489807 batch: 167/840\n",
      "Batch loss: 0.6823269724845886 batch: 168/840\n",
      "Batch loss: 0.6459876894950867 batch: 169/840\n",
      "Batch loss: 0.7545575499534607 batch: 170/840\n",
      "Batch loss: 0.7272630333900452 batch: 171/840\n",
      "Batch loss: 0.8801185488700867 batch: 172/840\n",
      "Batch loss: 0.6814892292022705 batch: 173/840\n",
      "Batch loss: 0.6487762331962585 batch: 174/840\n",
      "Batch loss: 0.5761613249778748 batch: 175/840\n",
      "Batch loss: 0.6622571349143982 batch: 176/840\n",
      "Batch loss: 0.6817266345024109 batch: 177/840\n",
      "Batch loss: 0.6972366571426392 batch: 178/840\n",
      "Batch loss: 0.8675450682640076 batch: 179/840\n",
      "Batch loss: 0.5873242616653442 batch: 180/840\n",
      "Batch loss: 0.6515727043151855 batch: 181/840\n",
      "Batch loss: 0.6569843292236328 batch: 182/840\n",
      "Batch loss: 0.6407313346862793 batch: 183/840\n",
      "Batch loss: 0.614936113357544 batch: 184/840\n",
      "Batch loss: 0.5850903987884521 batch: 185/840\n",
      "Batch loss: 0.5880479216575623 batch: 186/840\n",
      "Batch loss: 0.6788926124572754 batch: 187/840\n",
      "Batch loss: 0.5891046524047852 batch: 188/840\n",
      "Batch loss: 0.6280491352081299 batch: 189/840\n",
      "Batch loss: 0.7070273756980896 batch: 190/840\n",
      "Batch loss: 0.8302708268165588 batch: 191/840\n",
      "Batch loss: 0.46264979243278503 batch: 192/840\n",
      "Batch loss: 0.6020762920379639 batch: 193/840\n",
      "Batch loss: 0.5048969388008118 batch: 194/840\n",
      "Batch loss: 0.5906074643135071 batch: 195/840\n",
      "Batch loss: 0.7448968291282654 batch: 196/840\n",
      "Batch loss: 0.6873935461044312 batch: 197/840\n",
      "Batch loss: 0.597726047039032 batch: 198/840\n",
      "Batch loss: 0.6262585520744324 batch: 199/840\n",
      "Batch loss: 0.8702123761177063 batch: 200/840\n",
      "Batch loss: 0.6169610619544983 batch: 201/840\n",
      "Batch loss: 0.6554545760154724 batch: 202/840\n",
      "Batch loss: 0.6052875518798828 batch: 203/840\n",
      "Batch loss: 0.7332398891448975 batch: 204/840\n",
      "Batch loss: 0.7620304226875305 batch: 205/840\n",
      "Batch loss: 0.6752526164054871 batch: 206/840\n",
      "Batch loss: 0.7029845714569092 batch: 207/840\n",
      "Batch loss: 0.5991977453231812 batch: 208/840\n",
      "Batch loss: 0.7156296372413635 batch: 209/840\n",
      "Batch loss: 0.5676149725914001 batch: 210/840\n",
      "Batch loss: 0.5432453751564026 batch: 211/840\n",
      "Batch loss: 0.5930386781692505 batch: 212/840\n",
      "Batch loss: 0.8196274042129517 batch: 213/840\n",
      "Batch loss: 0.8203933238983154 batch: 214/840\n",
      "Batch loss: 0.7130584716796875 batch: 215/840\n",
      "Batch loss: 0.661228358745575 batch: 216/840\n",
      "Batch loss: 0.6874755024909973 batch: 217/840\n",
      "Batch loss: 0.7344672679901123 batch: 218/840\n",
      "Batch loss: 0.6410859823226929 batch: 219/840\n",
      "Batch loss: 0.9800155162811279 batch: 220/840\n",
      "Batch loss: 0.6587973833084106 batch: 221/840\n",
      "Batch loss: 0.8279618620872498 batch: 222/840\n",
      "Batch loss: 0.67839515209198 batch: 223/840\n",
      "Batch loss: 0.8442971110343933 batch: 224/840\n",
      "Batch loss: 0.8393027782440186 batch: 225/840\n",
      "Batch loss: 0.7502449154853821 batch: 226/840\n",
      "Batch loss: 0.7148061990737915 batch: 227/840\n",
      "Batch loss: 0.5006052851676941 batch: 228/840\n",
      "Batch loss: 0.547107994556427 batch: 229/840\n",
      "Batch loss: 0.6287885308265686 batch: 230/840\n",
      "Batch loss: 0.5333742499351501 batch: 231/840\n",
      "Batch loss: 0.6670415997505188 batch: 232/840\n",
      "Batch loss: 0.8235865235328674 batch: 233/840\n",
      "Batch loss: 0.5976186394691467 batch: 234/840\n",
      "Batch loss: 0.6674396991729736 batch: 235/840\n",
      "Batch loss: 0.6411418318748474 batch: 236/840\n",
      "Batch loss: 0.5893219709396362 batch: 237/840\n",
      "Batch loss: 0.7425962090492249 batch: 238/840\n",
      "Batch loss: 0.6890612840652466 batch: 239/840\n",
      "Batch loss: 0.7552255988121033 batch: 240/840\n",
      "Batch loss: 0.6784260272979736 batch: 241/840\n",
      "Batch loss: 0.6638046503067017 batch: 242/840\n",
      "Batch loss: 0.6215224862098694 batch: 243/840\n",
      "Batch loss: 0.7837616801261902 batch: 244/840\n",
      "Batch loss: 0.5373261570930481 batch: 245/840\n",
      "Batch loss: 0.6769661903381348 batch: 246/840\n",
      "Batch loss: 0.7817390561103821 batch: 247/840\n",
      "Batch loss: 0.7233042120933533 batch: 248/840\n",
      "Batch loss: 0.9018840193748474 batch: 249/840\n",
      "Batch loss: 0.5284417867660522 batch: 250/840\n",
      "Batch loss: 0.612044095993042 batch: 251/840\n",
      "Batch loss: 0.6324005722999573 batch: 252/840\n",
      "Batch loss: 0.7046444416046143 batch: 253/840\n",
      "Batch loss: 0.9489901065826416 batch: 254/840\n",
      "Batch loss: 0.6412734389305115 batch: 255/840\n",
      "Batch loss: 0.6348953247070312 batch: 256/840\n",
      "Batch loss: 0.6809638738632202 batch: 257/840\n",
      "Batch loss: 0.8491652011871338 batch: 258/840\n",
      "Batch loss: 0.5439004302024841 batch: 259/840\n",
      "Batch loss: 0.6350505948066711 batch: 260/840\n",
      "Batch loss: 0.5666463971138 batch: 261/840\n",
      "Batch loss: 0.44795623421669006 batch: 262/840\n",
      "Batch loss: 0.6520458459854126 batch: 263/840\n",
      "Batch loss: 0.6282167434692383 batch: 264/840\n",
      "Batch loss: 0.6813735365867615 batch: 265/840\n",
      "Batch loss: 0.6381561756134033 batch: 266/840\n",
      "Batch loss: 0.79027259349823 batch: 267/840\n",
      "Batch loss: 0.6198767423629761 batch: 268/840\n",
      "Batch loss: 0.5281496047973633 batch: 269/840\n",
      "Batch loss: 0.7161064147949219 batch: 270/840\n",
      "Batch loss: 0.5564363598823547 batch: 271/840\n",
      "Batch loss: 0.8222915530204773 batch: 272/840\n",
      "Batch loss: 0.8429687023162842 batch: 273/840\n",
      "Batch loss: 0.7167608737945557 batch: 274/840\n",
      "Batch loss: 0.8018202781677246 batch: 275/840\n",
      "Batch loss: 0.6126463413238525 batch: 276/840\n",
      "Batch loss: 0.6621963977813721 batch: 277/840\n",
      "Batch loss: 0.8906367421150208 batch: 278/840\n",
      "Batch loss: 0.7562804222106934 batch: 279/840\n",
      "Batch loss: 0.7988795638084412 batch: 280/840\n",
      "Batch loss: 0.6189867258071899 batch: 281/840\n",
      "Batch loss: 0.543018102645874 batch: 282/840\n",
      "Batch loss: 0.7002343535423279 batch: 283/840\n",
      "Batch loss: 0.5056979656219482 batch: 284/840\n",
      "Batch loss: 0.6551500558853149 batch: 285/840\n",
      "Batch loss: 0.7698352932929993 batch: 286/840\n",
      "Batch loss: 0.4890199899673462 batch: 287/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5904445052146912 batch: 288/840\n",
      "Batch loss: 0.8545376062393188 batch: 289/840\n",
      "Batch loss: 0.7372949719429016 batch: 290/840\n",
      "Batch loss: 0.7338487505912781 batch: 291/840\n",
      "Batch loss: 0.587815523147583 batch: 292/840\n",
      "Batch loss: 0.8504446148872375 batch: 293/840\n",
      "Batch loss: 0.644611120223999 batch: 294/840\n",
      "Batch loss: 0.4997488856315613 batch: 295/840\n",
      "Batch loss: 0.6419566869735718 batch: 296/840\n",
      "Batch loss: 0.7463201284408569 batch: 297/840\n",
      "Batch loss: 0.6318334341049194 batch: 298/840\n",
      "Batch loss: 0.6352393627166748 batch: 299/840\n",
      "Batch loss: 0.8721566200256348 batch: 300/840\n",
      "Batch loss: 0.7264401316642761 batch: 301/840\n",
      "Batch loss: 0.6743545532226562 batch: 302/840\n",
      "Batch loss: 0.7715914845466614 batch: 303/840\n",
      "Batch loss: 0.6067880988121033 batch: 304/840\n",
      "Batch loss: 0.6194544434547424 batch: 305/840\n",
      "Batch loss: 0.7523966431617737 batch: 306/840\n",
      "Batch loss: 0.599102258682251 batch: 307/840\n",
      "Batch loss: 0.8265693187713623 batch: 308/840\n",
      "Batch loss: 0.660824716091156 batch: 309/840\n",
      "Batch loss: 0.9150470495223999 batch: 310/840\n",
      "Batch loss: 0.8629376292228699 batch: 311/840\n",
      "Batch loss: 0.8302644491195679 batch: 312/840\n",
      "Batch loss: 0.7749472260475159 batch: 313/840\n",
      "Batch loss: 0.5419723391532898 batch: 314/840\n",
      "Batch loss: 0.7515854835510254 batch: 315/840\n",
      "Batch loss: 0.5131895542144775 batch: 316/840\n",
      "Batch loss: 0.7461003065109253 batch: 317/840\n",
      "Batch loss: 0.7572046518325806 batch: 318/840\n",
      "Batch loss: 0.7196505069732666 batch: 319/840\n",
      "Batch loss: 0.639992356300354 batch: 320/840\n",
      "Batch loss: 0.6267759799957275 batch: 321/840\n",
      "Batch loss: 0.7776464223861694 batch: 322/840\n",
      "Batch loss: 0.7618372440338135 batch: 323/840\n",
      "Batch loss: 0.6961045861244202 batch: 324/840\n",
      "Batch loss: 0.6090784668922424 batch: 325/840\n",
      "Batch loss: 0.643649697303772 batch: 326/840\n",
      "Batch loss: 0.5503444671630859 batch: 327/840\n",
      "Batch loss: 0.8593096137046814 batch: 328/840\n",
      "Batch loss: 0.6676755547523499 batch: 329/840\n",
      "Batch loss: 0.7035089135169983 batch: 330/840\n",
      "Batch loss: 0.7207019925117493 batch: 331/840\n",
      "Batch loss: 0.6973122358322144 batch: 332/840\n",
      "Batch loss: 0.5729824900627136 batch: 333/840\n",
      "Batch loss: 0.7677855491638184 batch: 334/840\n",
      "Batch loss: 0.6890727877616882 batch: 335/840\n",
      "Batch loss: 0.7020449042320251 batch: 336/840\n",
      "Batch loss: 0.8913402557373047 batch: 337/840\n",
      "Batch loss: 0.7355833649635315 batch: 338/840\n",
      "Batch loss: 0.6620103716850281 batch: 339/840\n",
      "Batch loss: 0.9778664112091064 batch: 340/840\n",
      "Batch loss: 0.5598682165145874 batch: 341/840\n",
      "Batch loss: 0.5662665963172913 batch: 342/840\n",
      "Batch loss: 0.9832368493080139 batch: 343/840\n",
      "Batch loss: 0.7752060890197754 batch: 344/840\n",
      "Batch loss: 0.47979941964149475 batch: 345/840\n",
      "Batch loss: 0.6048124432563782 batch: 346/840\n",
      "Batch loss: 0.6332291960716248 batch: 347/840\n",
      "Batch loss: 0.6737316250801086 batch: 348/840\n",
      "Batch loss: 0.7276186347007751 batch: 349/840\n",
      "Batch loss: 0.6286462545394897 batch: 350/840\n",
      "Batch loss: 0.7098740339279175 batch: 351/840\n",
      "Batch loss: 0.6303096413612366 batch: 352/840\n",
      "Batch loss: 0.6900155544281006 batch: 353/840\n",
      "Batch loss: 0.6036521792411804 batch: 354/840\n",
      "Batch loss: 0.5811572074890137 batch: 355/840\n",
      "Batch loss: 0.702908992767334 batch: 356/840\n",
      "Batch loss: 0.5464809536933899 batch: 357/840\n",
      "Batch loss: 0.9108471870422363 batch: 358/840\n",
      "Batch loss: 0.582111120223999 batch: 359/840\n",
      "Batch loss: 0.8233972787857056 batch: 360/840\n",
      "Batch loss: 0.8094844222068787 batch: 361/840\n",
      "Batch loss: 0.6437841057777405 batch: 362/840\n",
      "Batch loss: 0.5875324010848999 batch: 363/840\n",
      "Batch loss: 0.7418036460876465 batch: 364/840\n",
      "Batch loss: 0.6796518564224243 batch: 365/840\n",
      "Batch loss: 0.6995059847831726 batch: 366/840\n",
      "Batch loss: 0.4745640158653259 batch: 367/840\n",
      "Batch loss: 0.8555423021316528 batch: 368/840\n",
      "Batch loss: 0.7084084153175354 batch: 369/840\n",
      "Batch loss: 0.8184750080108643 batch: 370/840\n",
      "Batch loss: 0.6607599854469299 batch: 371/840\n",
      "Batch loss: 0.5264657735824585 batch: 372/840\n",
      "Batch loss: 0.7089617252349854 batch: 373/840\n",
      "Batch loss: 0.7412328124046326 batch: 374/840\n",
      "Batch loss: 0.625572919845581 batch: 375/840\n",
      "Batch loss: 0.541475236415863 batch: 376/840\n",
      "Batch loss: 0.690131664276123 batch: 377/840\n",
      "Batch loss: 0.6620076894760132 batch: 378/840\n",
      "Batch loss: 0.5737888813018799 batch: 379/840\n",
      "Batch loss: 0.9482927918434143 batch: 380/840\n",
      "Batch loss: 1.0053647756576538 batch: 381/840\n",
      "Batch loss: 0.7931101322174072 batch: 382/840\n",
      "Batch loss: 0.7719290256500244 batch: 383/840\n",
      "Batch loss: 0.6393359303474426 batch: 384/840\n",
      "Batch loss: 0.7291098237037659 batch: 385/840\n",
      "Batch loss: 0.6941267251968384 batch: 386/840\n",
      "Batch loss: 0.5857675075531006 batch: 387/840\n",
      "Batch loss: 0.7245252728462219 batch: 388/840\n",
      "Batch loss: 0.5602688789367676 batch: 389/840\n",
      "Batch loss: 0.8016747236251831 batch: 390/840\n",
      "Batch loss: 0.6444945335388184 batch: 391/840\n",
      "Batch loss: 0.6086071133613586 batch: 392/840\n",
      "Batch loss: 0.6348661780357361 batch: 393/840\n",
      "Batch loss: 0.7428893446922302 batch: 394/840\n",
      "Batch loss: 0.6571449041366577 batch: 395/840\n",
      "Batch loss: 0.672706663608551 batch: 396/840\n",
      "Batch loss: 0.5485537052154541 batch: 397/840\n",
      "Batch loss: 0.6721324920654297 batch: 398/840\n",
      "Batch loss: 0.4717581272125244 batch: 399/840\n",
      "Batch loss: 0.6154483556747437 batch: 400/840\n",
      "Batch loss: 0.6863588094711304 batch: 401/840\n",
      "Batch loss: 0.5632344484329224 batch: 402/840\n",
      "Batch loss: 0.6025034785270691 batch: 403/840\n",
      "Batch loss: 0.5916458368301392 batch: 404/840\n",
      "Batch loss: 0.6441299915313721 batch: 405/840\n",
      "Batch loss: 0.705671489238739 batch: 406/840\n",
      "Batch loss: 0.6266862750053406 batch: 407/840\n",
      "Batch loss: 0.8234997391700745 batch: 408/840\n",
      "Batch loss: 0.8286640644073486 batch: 409/840\n",
      "Batch loss: 0.8027642965316772 batch: 410/840\n",
      "Batch loss: 0.7851920127868652 batch: 411/840\n",
      "Batch loss: 0.7649288773536682 batch: 412/840\n",
      "Batch loss: 0.7627025842666626 batch: 413/840\n",
      "Batch loss: 0.641562283039093 batch: 414/840\n",
      "Batch loss: 0.8656739592552185 batch: 415/840\n",
      "Batch loss: 0.5269205570220947 batch: 416/840\n",
      "Batch loss: 0.7758761048316956 batch: 417/840\n",
      "Batch loss: 0.830413281917572 batch: 418/840\n",
      "Batch loss: 0.6438229084014893 batch: 419/840\n",
      "Batch loss: 0.7373525500297546 batch: 420/840\n",
      "Batch loss: 0.5737909078598022 batch: 421/840\n",
      "Batch loss: 0.6246374845504761 batch: 422/840\n",
      "Batch loss: 0.7294979691505432 batch: 423/840\n",
      "Batch loss: 0.7270993590354919 batch: 424/840\n",
      "Batch loss: 0.7620376348495483 batch: 425/840\n",
      "Batch loss: 0.6804660558700562 batch: 426/840\n",
      "Batch loss: 0.6379976868629456 batch: 427/840\n",
      "Batch loss: 0.7744386196136475 batch: 428/840\n",
      "Batch loss: 0.6585944294929504 batch: 429/840\n",
      "Batch loss: 0.8793081045150757 batch: 430/840\n",
      "Batch loss: 0.5939826369285583 batch: 431/840\n",
      "Batch loss: 0.6840217113494873 batch: 432/840\n",
      "Batch loss: 0.46089237928390503 batch: 433/840\n",
      "Batch loss: 0.6013216972351074 batch: 434/840\n",
      "Batch loss: 0.791527271270752 batch: 435/840\n",
      "Batch loss: 0.6470206379890442 batch: 436/840\n",
      "Batch loss: 0.8112750053405762 batch: 437/840\n",
      "Batch loss: 0.45820748805999756 batch: 438/840\n",
      "Batch loss: 0.6521183252334595 batch: 439/840\n",
      "Batch loss: 0.7629374861717224 batch: 440/840\n",
      "Batch loss: 0.7105911374092102 batch: 441/840\n",
      "Batch loss: 0.7835143804550171 batch: 442/840\n",
      "Batch loss: 0.6698861718177795 batch: 443/840\n",
      "Batch loss: 0.795269250869751 batch: 444/840\n",
      "Batch loss: 0.7070181965827942 batch: 445/840\n",
      "Batch loss: 0.7964733242988586 batch: 446/840\n",
      "Batch loss: 0.666452944278717 batch: 447/840\n",
      "Batch loss: 0.6693607568740845 batch: 448/840\n",
      "Batch loss: 0.5406451225280762 batch: 449/840\n",
      "Batch loss: 0.6825119853019714 batch: 450/840\n",
      "Batch loss: 0.6978782415390015 batch: 451/840\n",
      "Batch loss: 0.7754018902778625 batch: 452/840\n",
      "Batch loss: 0.7421532273292542 batch: 453/840\n",
      "Batch loss: 0.5887036323547363 batch: 454/840\n",
      "Batch loss: 0.8021400570869446 batch: 455/840\n",
      "Batch loss: 0.5683062076568604 batch: 456/840\n",
      "Batch loss: 0.6911169290542603 batch: 457/840\n",
      "Batch loss: 0.676493227481842 batch: 458/840\n",
      "Batch loss: 0.5187962651252747 batch: 459/840\n",
      "Batch loss: 0.5082948803901672 batch: 460/840\n",
      "Batch loss: 0.7981167435646057 batch: 461/840\n",
      "Batch loss: 0.6494214534759521 batch: 462/840\n",
      "Batch loss: 0.8252214193344116 batch: 463/840\n",
      "Batch loss: 0.5893120169639587 batch: 464/840\n",
      "Batch loss: 0.9655330181121826 batch: 465/840\n",
      "Batch loss: 0.7415111064910889 batch: 466/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.9584640264511108 batch: 467/840\n",
      "Batch loss: 0.6751030683517456 batch: 468/840\n",
      "Batch loss: 0.5632038116455078 batch: 469/840\n",
      "Batch loss: 0.6779122948646545 batch: 470/840\n",
      "Batch loss: 0.6806524395942688 batch: 471/840\n",
      "Batch loss: 0.7061455249786377 batch: 472/840\n",
      "Batch loss: 0.8148102760314941 batch: 473/840\n",
      "Batch loss: 0.6753948330879211 batch: 474/840\n",
      "Batch loss: 0.7666313648223877 batch: 475/840\n",
      "Batch loss: 0.6829323768615723 batch: 476/840\n",
      "Batch loss: 0.5460050702095032 batch: 477/840\n",
      "Batch loss: 0.5037806034088135 batch: 478/840\n",
      "Batch loss: 0.5709492564201355 batch: 479/840\n",
      "Batch loss: 0.6975857615470886 batch: 480/840\n",
      "Batch loss: 0.6135264039039612 batch: 481/840\n",
      "Batch loss: 0.7064226269721985 batch: 482/840\n",
      "Batch loss: 0.8125025033950806 batch: 483/840\n",
      "Batch loss: 0.6295955181121826 batch: 484/840\n",
      "Batch loss: 0.7056680917739868 batch: 485/840\n",
      "Batch loss: 0.65228271484375 batch: 486/840\n",
      "Batch loss: 0.5058897137641907 batch: 487/840\n",
      "Batch loss: 0.6829477548599243 batch: 488/840\n",
      "Batch loss: 0.6367775201797485 batch: 489/840\n",
      "Batch loss: 0.75360107421875 batch: 490/840\n",
      "Batch loss: 0.4096617102622986 batch: 491/840\n",
      "Batch loss: 0.6956496238708496 batch: 492/840\n",
      "Batch loss: 0.8177298903465271 batch: 493/840\n",
      "Batch loss: 0.44891321659088135 batch: 494/840\n",
      "Batch loss: 0.752946138381958 batch: 495/840\n",
      "Batch loss: 0.7009569406509399 batch: 496/840\n",
      "Batch loss: 0.7487439513206482 batch: 497/840\n",
      "Batch loss: 0.5231101512908936 batch: 498/840\n",
      "Batch loss: 0.6768803596496582 batch: 499/840\n",
      "Batch loss: 0.6220065951347351 batch: 500/840\n",
      "Batch loss: 0.5847339034080505 batch: 501/840\n",
      "Batch loss: 0.7798440456390381 batch: 502/840\n",
      "Batch loss: 0.5213668942451477 batch: 503/840\n",
      "Batch loss: 0.6811915040016174 batch: 504/840\n",
      "Batch loss: 0.7675828337669373 batch: 505/840\n",
      "Batch loss: 0.6482070088386536 batch: 506/840\n",
      "Batch loss: 0.7779091596603394 batch: 507/840\n",
      "Batch loss: 0.5793027877807617 batch: 508/840\n",
      "Batch loss: 0.6584389209747314 batch: 509/840\n",
      "Batch loss: 0.6792506575584412 batch: 510/840\n",
      "Batch loss: 0.5684774518013 batch: 511/840\n",
      "Batch loss: 0.660030722618103 batch: 512/840\n",
      "Batch loss: 0.5678296685218811 batch: 513/840\n",
      "Batch loss: 0.7858923077583313 batch: 514/840\n",
      "Batch loss: 0.45554423332214355 batch: 515/840\n",
      "Batch loss: 0.8053213357925415 batch: 516/840\n",
      "Batch loss: 0.6632695198059082 batch: 517/840\n",
      "Batch loss: 0.6903179883956909 batch: 518/840\n",
      "Batch loss: 0.8299355506896973 batch: 519/840\n",
      "Batch loss: 0.7512891888618469 batch: 520/840\n",
      "Batch loss: 0.8784910440444946 batch: 521/840\n",
      "Batch loss: 0.6754671335220337 batch: 522/840\n",
      "Batch loss: 0.5320752859115601 batch: 523/840\n",
      "Batch loss: 0.7175873517990112 batch: 524/840\n",
      "Batch loss: 0.6512175798416138 batch: 525/840\n",
      "Batch loss: 0.7313984036445618 batch: 526/840\n",
      "Batch loss: 0.6676557064056396 batch: 527/840\n",
      "Batch loss: 0.6464194655418396 batch: 528/840\n",
      "Batch loss: 0.6980313062667847 batch: 529/840\n",
      "Batch loss: 0.7326853275299072 batch: 530/840\n",
      "Batch loss: 0.6853725910186768 batch: 531/840\n",
      "Batch loss: 0.4578430652618408 batch: 532/840\n",
      "Batch loss: 0.645962119102478 batch: 533/840\n",
      "Batch loss: 0.6407473087310791 batch: 534/840\n",
      "Batch loss: 0.6277467012405396 batch: 535/840\n",
      "Batch loss: 0.6562480926513672 batch: 536/840\n",
      "Batch loss: 0.6141652464866638 batch: 537/840\n",
      "Batch loss: 0.6008147597312927 batch: 538/840\n",
      "Batch loss: 0.6748085021972656 batch: 539/840\n",
      "Batch loss: 0.825217068195343 batch: 540/840\n",
      "Batch loss: 0.6378690004348755 batch: 541/840\n",
      "Batch loss: 0.7282984256744385 batch: 542/840\n",
      "Batch loss: 0.4513382613658905 batch: 543/840\n",
      "Batch loss: 0.6461004018783569 batch: 544/840\n",
      "Batch loss: 0.7862672209739685 batch: 545/840\n",
      "Batch loss: 0.6193813681602478 batch: 546/840\n",
      "Batch loss: 0.7189417481422424 batch: 547/840\n",
      "Batch loss: 0.5139961838722229 batch: 548/840\n",
      "Batch loss: 0.6298730969429016 batch: 549/840\n",
      "Batch loss: 0.5331751704216003 batch: 550/840\n",
      "Batch loss: 0.8083446621894836 batch: 551/840\n",
      "Batch loss: 0.560539960861206 batch: 552/840\n",
      "Batch loss: 0.7165554761886597 batch: 553/840\n",
      "Batch loss: 0.7062100768089294 batch: 554/840\n",
      "Batch loss: 0.5826693177223206 batch: 555/840\n",
      "Batch loss: 0.6997038125991821 batch: 556/840\n",
      "Batch loss: 0.6986414194107056 batch: 557/840\n",
      "Batch loss: 0.7054933905601501 batch: 558/840\n",
      "Batch loss: 0.702338695526123 batch: 559/840\n",
      "Batch loss: 0.7031010389328003 batch: 560/840\n",
      "Batch loss: 0.6747907996177673 batch: 561/840\n",
      "Batch loss: 0.6387931108474731 batch: 562/840\n",
      "Batch loss: 0.5175350904464722 batch: 563/840\n",
      "Batch loss: 0.7304028272628784 batch: 564/840\n",
      "Batch loss: 0.7472683191299438 batch: 565/840\n",
      "Batch loss: 0.7583571672439575 batch: 566/840\n",
      "Batch loss: 0.674254834651947 batch: 567/840\n",
      "Batch loss: 0.642535924911499 batch: 568/840\n",
      "Batch loss: 0.6230875253677368 batch: 569/840\n",
      "Batch loss: 0.4600975811481476 batch: 570/840\n",
      "Batch loss: 0.7954020500183105 batch: 571/840\n",
      "Batch loss: 0.7777830362319946 batch: 572/840\n",
      "Batch loss: 0.7194681763648987 batch: 573/840\n",
      "Batch loss: 0.7835903763771057 batch: 574/840\n",
      "Batch loss: 0.4929087162017822 batch: 575/840\n",
      "Batch loss: 0.6718025803565979 batch: 576/840\n",
      "Batch loss: 0.5351783633232117 batch: 577/840\n",
      "Batch loss: 0.9020559787750244 batch: 578/840\n",
      "Batch loss: 0.719678521156311 batch: 579/840\n",
      "Batch loss: 0.8700887560844421 batch: 580/840\n",
      "Batch loss: 0.7624561190605164 batch: 581/840\n",
      "Batch loss: 0.921083927154541 batch: 582/840\n",
      "Batch loss: 0.6436145305633545 batch: 583/840\n",
      "Batch loss: 0.8267760276794434 batch: 584/840\n",
      "Batch loss: 0.5727982521057129 batch: 585/840\n",
      "Batch loss: 0.8010988831520081 batch: 586/840\n",
      "Batch loss: 0.6353273987770081 batch: 587/840\n",
      "Batch loss: 0.6832774877548218 batch: 588/840\n",
      "Batch loss: 0.8563888669013977 batch: 589/840\n",
      "Batch loss: 0.6391000747680664 batch: 590/840\n",
      "Batch loss: 0.5246423482894897 batch: 591/840\n",
      "Batch loss: 0.5344951748847961 batch: 592/840\n",
      "Batch loss: 0.8433985114097595 batch: 593/840\n",
      "Batch loss: 0.6938533782958984 batch: 594/840\n",
      "Batch loss: 0.4371657073497772 batch: 595/840\n",
      "Batch loss: 0.525175929069519 batch: 596/840\n",
      "Batch loss: 0.7126699686050415 batch: 597/840\n",
      "Batch loss: 0.5696234107017517 batch: 598/840\n",
      "Batch loss: 0.5849857926368713 batch: 599/840\n",
      "Batch loss: 0.7818862795829773 batch: 600/840\n",
      "Batch loss: 0.8254173398017883 batch: 601/840\n",
      "Batch loss: 0.6824988722801208 batch: 602/840\n",
      "Batch loss: 0.7506749033927917 batch: 603/840\n",
      "Batch loss: 0.6945915222167969 batch: 604/840\n",
      "Batch loss: 0.8049057722091675 batch: 605/840\n",
      "Batch loss: 0.8220254778862 batch: 606/840\n",
      "Batch loss: 0.7058334946632385 batch: 607/840\n",
      "Batch loss: 0.6531522274017334 batch: 608/840\n",
      "Batch loss: 0.554485559463501 batch: 609/840\n",
      "Batch loss: 0.795515239238739 batch: 610/840\n",
      "Batch loss: 0.7011276483535767 batch: 611/840\n",
      "Batch loss: 0.783488392829895 batch: 612/840\n",
      "Batch loss: 0.7498406767845154 batch: 613/840\n",
      "Batch loss: 0.6739577651023865 batch: 614/840\n",
      "Batch loss: 0.6436756253242493 batch: 615/840\n",
      "Batch loss: 0.5616018772125244 batch: 616/840\n",
      "Batch loss: 0.5462725162506104 batch: 617/840\n",
      "Batch loss: 0.7685183882713318 batch: 618/840\n",
      "Batch loss: 0.8348039388656616 batch: 619/840\n",
      "Batch loss: 0.6269638538360596 batch: 620/840\n",
      "Batch loss: 0.7014845013618469 batch: 621/840\n",
      "Batch loss: 0.7400104999542236 batch: 622/840\n",
      "Batch loss: 0.625187337398529 batch: 623/840\n",
      "Batch loss: 0.8074169158935547 batch: 624/840\n",
      "Batch loss: 0.7010225057601929 batch: 625/840\n",
      "Batch loss: 0.7309373617172241 batch: 626/840\n",
      "Batch loss: 0.6489055156707764 batch: 627/840\n",
      "Batch loss: 0.7446125745773315 batch: 628/840\n",
      "Batch loss: 0.6928624510765076 batch: 629/840\n",
      "Batch loss: 0.6657309532165527 batch: 630/840\n",
      "Batch loss: 0.7815794348716736 batch: 631/840\n",
      "Batch loss: 0.6903691291809082 batch: 632/840\n",
      "Batch loss: 0.5951265096664429 batch: 633/840\n",
      "Batch loss: 0.7602951526641846 batch: 634/840\n",
      "Batch loss: 0.605158805847168 batch: 635/840\n",
      "Batch loss: 0.7629391551017761 batch: 636/840\n",
      "Batch loss: 0.5684154033660889 batch: 637/840\n",
      "Batch loss: 0.7616546750068665 batch: 638/840\n",
      "Batch loss: 0.6860383749008179 batch: 639/840\n",
      "Batch loss: 0.6143381595611572 batch: 640/840\n",
      "Batch loss: 0.8384448289871216 batch: 641/840\n",
      "Batch loss: 0.6158287525177002 batch: 642/840\n",
      "Batch loss: 1.1253068447113037 batch: 643/840\n",
      "Batch loss: 0.7089605927467346 batch: 644/840\n",
      "Batch loss: 0.5701092481613159 batch: 645/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6532209515571594 batch: 646/840\n",
      "Batch loss: 0.789581298828125 batch: 647/840\n",
      "Batch loss: 0.6360241770744324 batch: 648/840\n",
      "Batch loss: 0.646595299243927 batch: 649/840\n",
      "Batch loss: 0.5973663926124573 batch: 650/840\n",
      "Batch loss: 0.712001621723175 batch: 651/840\n",
      "Batch loss: 0.7650576233863831 batch: 652/840\n",
      "Batch loss: 0.6351963877677917 batch: 653/840\n",
      "Batch loss: 0.878908634185791 batch: 654/840\n",
      "Batch loss: 0.6146227717399597 batch: 655/840\n",
      "Batch loss: 0.633087158203125 batch: 656/840\n",
      "Batch loss: 0.6378815174102783 batch: 657/840\n",
      "Batch loss: 0.7844903469085693 batch: 658/840\n",
      "Batch loss: 0.8119395971298218 batch: 659/840\n",
      "Batch loss: 0.49511516094207764 batch: 660/840\n",
      "Batch loss: 0.7709668874740601 batch: 661/840\n",
      "Batch loss: 0.7787835001945496 batch: 662/840\n",
      "Batch loss: 0.618738055229187 batch: 663/840\n",
      "Batch loss: 0.7101131677627563 batch: 664/840\n",
      "Batch loss: 0.8232844471931458 batch: 665/840\n",
      "Batch loss: 0.7267553210258484 batch: 666/840\n",
      "Batch loss: 0.7803328633308411 batch: 667/840\n",
      "Batch loss: 0.6141418218612671 batch: 668/840\n",
      "Batch loss: 0.7186586260795593 batch: 669/840\n",
      "Batch loss: 0.7901802659034729 batch: 670/840\n",
      "Batch loss: 0.7055351734161377 batch: 671/840\n",
      "Batch loss: 0.5935698747634888 batch: 672/840\n",
      "Batch loss: 0.7925467491149902 batch: 673/840\n",
      "Batch loss: 0.8145984411239624 batch: 674/840\n",
      "Batch loss: 0.7195038795471191 batch: 675/840\n",
      "Batch loss: 0.6430642604827881 batch: 676/840\n",
      "Batch loss: 0.669167697429657 batch: 677/840\n",
      "Batch loss: 0.7183033227920532 batch: 678/840\n",
      "Batch loss: 0.7923975586891174 batch: 679/840\n",
      "Batch loss: 0.6826640367507935 batch: 680/840\n",
      "Batch loss: 0.7219721078872681 batch: 681/840\n",
      "Batch loss: 0.6071663498878479 batch: 682/840\n",
      "Batch loss: 0.5542818307876587 batch: 683/840\n",
      "Batch loss: 0.8805963397026062 batch: 684/840\n",
      "Batch loss: 0.7343411445617676 batch: 685/840\n",
      "Batch loss: 0.7805081009864807 batch: 686/840\n",
      "Batch loss: 0.8436467051506042 batch: 687/840\n",
      "Batch loss: 0.6721456050872803 batch: 688/840\n",
      "Batch loss: 0.545055091381073 batch: 689/840\n",
      "Batch loss: 0.5779061913490295 batch: 690/840\n",
      "Batch loss: 0.7198444604873657 batch: 691/840\n",
      "Batch loss: 0.6548673510551453 batch: 692/840\n",
      "Batch loss: 0.6150044798851013 batch: 693/840\n",
      "Batch loss: 0.8867434859275818 batch: 694/840\n",
      "Batch loss: 0.766560971736908 batch: 695/840\n",
      "Batch loss: 0.6803909540176392 batch: 696/840\n",
      "Batch loss: 0.617820680141449 batch: 697/840\n",
      "Batch loss: 0.6619570255279541 batch: 698/840\n",
      "Batch loss: 0.6902887225151062 batch: 699/840\n",
      "Batch loss: 0.767862856388092 batch: 700/840\n",
      "Batch loss: 0.8173931241035461 batch: 701/840\n",
      "Batch loss: 0.6454843878746033 batch: 702/840\n",
      "Batch loss: 0.6969179511070251 batch: 703/840\n",
      "Batch loss: 0.7868345379829407 batch: 704/840\n",
      "Batch loss: 0.6581979393959045 batch: 705/840\n",
      "Batch loss: 0.6888158321380615 batch: 706/840\n",
      "Batch loss: 0.7417828440666199 batch: 707/840\n",
      "Batch loss: 0.7146010398864746 batch: 708/840\n",
      "Batch loss: 0.8363426327705383 batch: 709/840\n",
      "Batch loss: 0.8125569820404053 batch: 710/840\n",
      "Batch loss: 0.4634772539138794 batch: 711/840\n",
      "Batch loss: 0.7446786761283875 batch: 712/840\n",
      "Batch loss: 0.6335785984992981 batch: 713/840\n",
      "Batch loss: 0.6388753056526184 batch: 714/840\n",
      "Batch loss: 0.7643426656723022 batch: 715/840\n",
      "Batch loss: 0.7191261053085327 batch: 716/840\n",
      "Batch loss: 0.7020911574363708 batch: 717/840\n",
      "Batch loss: 0.6115820407867432 batch: 718/840\n",
      "Batch loss: 0.5581796765327454 batch: 719/840\n",
      "Batch loss: 0.5977510213851929 batch: 720/840\n",
      "Batch loss: 0.8213948011398315 batch: 721/840\n",
      "Batch loss: 0.7955420017242432 batch: 722/840\n",
      "Batch loss: 0.589901864528656 batch: 723/840\n",
      "Batch loss: 0.7304213047027588 batch: 724/840\n",
      "Batch loss: 0.6878179907798767 batch: 725/840\n",
      "Batch loss: 0.7057885527610779 batch: 726/840\n",
      "Batch loss: 0.78778475522995 batch: 727/840\n",
      "Batch loss: 0.8471944332122803 batch: 728/840\n",
      "Batch loss: 0.663964033126831 batch: 729/840\n",
      "Batch loss: 0.6210848093032837 batch: 730/840\n",
      "Batch loss: 0.6142894625663757 batch: 731/840\n",
      "Batch loss: 0.8397148251533508 batch: 732/840\n",
      "Batch loss: 0.6541348099708557 batch: 733/840\n",
      "Batch loss: 0.6300913691520691 batch: 734/840\n",
      "Batch loss: 0.7098459005355835 batch: 735/840\n",
      "Batch loss: 0.7155582904815674 batch: 736/840\n",
      "Batch loss: 0.7054893374443054 batch: 737/840\n",
      "Batch loss: 0.5427650213241577 batch: 738/840\n",
      "Batch loss: 0.7526225447654724 batch: 739/840\n",
      "Batch loss: 0.6948500871658325 batch: 740/840\n",
      "Batch loss: 0.5597883462905884 batch: 741/840\n",
      "Batch loss: 0.5738098621368408 batch: 742/840\n",
      "Batch loss: 0.48201456665992737 batch: 743/840\n",
      "Batch loss: 0.5195892453193665 batch: 744/840\n",
      "Batch loss: 0.5957646369934082 batch: 745/840\n",
      "Batch loss: 0.5575375556945801 batch: 746/840\n",
      "Batch loss: 0.7456523180007935 batch: 747/840\n",
      "Batch loss: 0.7320849895477295 batch: 748/840\n",
      "Batch loss: 0.533130407333374 batch: 749/840\n",
      "Batch loss: 0.5691654086112976 batch: 750/840\n",
      "Batch loss: 0.8970606327056885 batch: 751/840\n",
      "Batch loss: 0.8239104747772217 batch: 752/840\n",
      "Batch loss: 0.6469030976295471 batch: 753/840\n",
      "Batch loss: 0.7272319197654724 batch: 754/840\n",
      "Batch loss: 0.7103757262229919 batch: 755/840\n",
      "Batch loss: 0.6257691383361816 batch: 756/840\n",
      "Batch loss: 0.7446935772895813 batch: 757/840\n",
      "Batch loss: 0.6622567176818848 batch: 758/840\n",
      "Batch loss: 0.5843033194541931 batch: 759/840\n",
      "Batch loss: 0.5984998941421509 batch: 760/840\n",
      "Batch loss: 0.5821872353553772 batch: 761/840\n",
      "Batch loss: 0.7869553565979004 batch: 762/840\n",
      "Batch loss: 0.4388342797756195 batch: 763/840\n",
      "Batch loss: 0.6530168056488037 batch: 764/840\n",
      "Batch loss: 0.5021260976791382 batch: 765/840\n",
      "Batch loss: 0.6228669881820679 batch: 766/840\n",
      "Batch loss: 0.7333948612213135 batch: 767/840\n",
      "Batch loss: 0.8017477989196777 batch: 768/840\n",
      "Batch loss: 0.6686837673187256 batch: 769/840\n",
      "Batch loss: 0.6044396162033081 batch: 770/840\n",
      "Batch loss: 0.6780687570571899 batch: 771/840\n",
      "Batch loss: 0.6125106811523438 batch: 772/840\n",
      "Batch loss: 0.5096156597137451 batch: 773/840\n",
      "Batch loss: 0.4799736440181732 batch: 774/840\n",
      "Batch loss: 0.5806646943092346 batch: 775/840\n",
      "Batch loss: 0.5639837384223938 batch: 776/840\n",
      "Batch loss: 0.6042894124984741 batch: 777/840\n",
      "Batch loss: 0.46402865648269653 batch: 778/840\n",
      "Batch loss: 0.7100027203559875 batch: 779/840\n",
      "Batch loss: 0.5452606081962585 batch: 780/840\n",
      "Batch loss: 0.6777303218841553 batch: 781/840\n",
      "Batch loss: 0.6783962845802307 batch: 782/840\n",
      "Batch loss: 0.5760607719421387 batch: 783/840\n",
      "Batch loss: 0.7369829416275024 batch: 784/840\n",
      "Batch loss: 0.6006405353546143 batch: 785/840\n",
      "Batch loss: 0.71966552734375 batch: 786/840\n",
      "Batch loss: 0.5446161031723022 batch: 787/840\n",
      "Batch loss: 0.8335806131362915 batch: 788/840\n",
      "Batch loss: 0.7812325954437256 batch: 789/840\n",
      "Batch loss: 0.8055216073989868 batch: 790/840\n",
      "Batch loss: 0.6355195045471191 batch: 791/840\n",
      "Batch loss: 0.41982653737068176 batch: 792/840\n",
      "Batch loss: 0.6673406958580017 batch: 793/840\n",
      "Batch loss: 0.6365672945976257 batch: 794/840\n",
      "Batch loss: 0.6972249746322632 batch: 795/840\n",
      "Batch loss: 0.8153803944587708 batch: 796/840\n",
      "Batch loss: 0.8093570470809937 batch: 797/840\n",
      "Batch loss: 0.7239332795143127 batch: 798/840\n",
      "Batch loss: 0.6576796174049377 batch: 799/840\n",
      "Batch loss: 0.5656439065933228 batch: 800/840\n",
      "Batch loss: 0.8273488879203796 batch: 801/840\n",
      "Batch loss: 0.49882829189300537 batch: 802/840\n",
      "Batch loss: 0.546786904335022 batch: 803/840\n",
      "Batch loss: 0.7449720501899719 batch: 804/840\n",
      "Batch loss: 0.5465046763420105 batch: 805/840\n",
      "Batch loss: 0.6408942937850952 batch: 806/840\n",
      "Batch loss: 0.6785414814949036 batch: 807/840\n",
      "Batch loss: 0.6913799047470093 batch: 808/840\n",
      "Batch loss: 0.6287401914596558 batch: 809/840\n",
      "Batch loss: 0.6139675974845886 batch: 810/840\n",
      "Batch loss: 0.5884485244750977 batch: 811/840\n",
      "Batch loss: 0.6705777645111084 batch: 812/840\n",
      "Batch loss: 0.5483236312866211 batch: 813/840\n",
      "Batch loss: 0.5836105942726135 batch: 814/840\n",
      "Batch loss: 0.7182996869087219 batch: 815/840\n",
      "Batch loss: 0.7181693911552429 batch: 816/840\n",
      "Batch loss: 0.813944399356842 batch: 817/840\n",
      "Batch loss: 0.6490886211395264 batch: 818/840\n",
      "Batch loss: 0.6792826056480408 batch: 819/840\n",
      "Batch loss: 0.7148258686065674 batch: 820/840\n",
      "Batch loss: 0.5971429944038391 batch: 821/840\n",
      "Batch loss: 0.7128568887710571 batch: 822/840\n",
      "Batch loss: 0.7320404052734375 batch: 823/840\n",
      "Batch loss: 0.7934302687644958 batch: 824/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7179213166236877 batch: 825/840\n",
      "Batch loss: 0.6002063751220703 batch: 826/840\n",
      "Batch loss: 0.6432604789733887 batch: 827/840\n",
      "Batch loss: 0.8179863095283508 batch: 828/840\n",
      "Batch loss: 0.6113344430923462 batch: 829/840\n",
      "Batch loss: 0.7261989712715149 batch: 830/840\n",
      "Batch loss: 0.5813494324684143 batch: 831/840\n",
      "Batch loss: 0.7723399996757507 batch: 832/840\n",
      "Batch loss: 0.7425729632377625 batch: 833/840\n",
      "Batch loss: 0.7208622694015503 batch: 834/840\n",
      "Batch loss: 0.5306658744812012 batch: 835/840\n",
      "Batch loss: 0.6025689840316772 batch: 836/840\n",
      "Batch loss: 0.7683371901512146 batch: 837/840\n",
      "Batch loss: 0.7025558948516846 batch: 838/840\n",
      "Batch loss: 0.6042853593826294 batch: 839/840\n",
      "Batch loss: 0.7173480987548828 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 3/15..  Training Loss: 0.007..  Test Loss: 0.005..  Test Accuracy: 0.814\n",
      "Running epoch 4/15\n",
      "Batch loss: 0.5206408500671387 batch: 1/840\n",
      "Batch loss: 1.11375892162323 batch: 2/840\n",
      "Batch loss: 0.7229720950126648 batch: 3/840\n",
      "Batch loss: 0.6693472266197205 batch: 4/840\n",
      "Batch loss: 0.6335413455963135 batch: 5/840\n",
      "Batch loss: 0.4929981529712677 batch: 6/840\n",
      "Batch loss: 0.7075619697570801 batch: 7/840\n",
      "Batch loss: 0.6048402190208435 batch: 8/840\n",
      "Batch loss: 0.5736889839172363 batch: 9/840\n",
      "Batch loss: 0.6623403429985046 batch: 10/840\n",
      "Batch loss: 0.6530306935310364 batch: 11/840\n",
      "Batch loss: 0.6672430634498596 batch: 12/840\n",
      "Batch loss: 0.7083922624588013 batch: 13/840\n",
      "Batch loss: 0.7160241603851318 batch: 14/840\n",
      "Batch loss: 0.6263395547866821 batch: 15/840\n",
      "Batch loss: 0.5253153443336487 batch: 16/840\n",
      "Batch loss: 0.5486098527908325 batch: 17/840\n",
      "Batch loss: 0.7259644865989685 batch: 18/840\n",
      "Batch loss: 0.8387373089790344 batch: 19/840\n",
      "Batch loss: 0.8206789493560791 batch: 20/840\n",
      "Batch loss: 0.7721521854400635 batch: 21/840\n",
      "Batch loss: 0.6385712623596191 batch: 22/840\n",
      "Batch loss: 0.6569602489471436 batch: 23/840\n",
      "Batch loss: 0.568686842918396 batch: 24/840\n",
      "Batch loss: 0.6213878393173218 batch: 25/840\n",
      "Batch loss: 0.7740215063095093 batch: 26/840\n",
      "Batch loss: 0.6938268542289734 batch: 27/840\n",
      "Batch loss: 0.7019743323326111 batch: 28/840\n",
      "Batch loss: 0.5875966548919678 batch: 29/840\n",
      "Batch loss: 0.5387950539588928 batch: 30/840\n",
      "Batch loss: 0.6023010015487671 batch: 31/840\n",
      "Batch loss: 0.5847218632698059 batch: 32/840\n",
      "Batch loss: 0.6635276675224304 batch: 33/840\n",
      "Batch loss: 0.567918598651886 batch: 34/840\n",
      "Batch loss: 0.6072606444358826 batch: 35/840\n",
      "Batch loss: 0.5847804546356201 batch: 36/840\n",
      "Batch loss: 0.8173204064369202 batch: 37/840\n",
      "Batch loss: 0.7274540662765503 batch: 38/840\n",
      "Batch loss: 0.6967854499816895 batch: 39/840\n",
      "Batch loss: 0.5763232707977295 batch: 40/840\n",
      "Batch loss: 0.7586778998374939 batch: 41/840\n",
      "Batch loss: 0.7768921852111816 batch: 42/840\n",
      "Batch loss: 0.7197971343994141 batch: 43/840\n",
      "Batch loss: 0.6889934539794922 batch: 44/840\n",
      "Batch loss: 0.820767343044281 batch: 45/840\n",
      "Batch loss: 0.6296344995498657 batch: 46/840\n",
      "Batch loss: 0.5614907145500183 batch: 47/840\n",
      "Batch loss: 0.7069549560546875 batch: 48/840\n",
      "Batch loss: 0.6513507962226868 batch: 49/840\n",
      "Batch loss: 0.6906310319900513 batch: 50/840\n",
      "Batch loss: 0.7349098920822144 batch: 51/840\n",
      "Batch loss: 0.7978430390357971 batch: 52/840\n",
      "Batch loss: 0.6091529726982117 batch: 53/840\n",
      "Batch loss: 0.6641615033149719 batch: 54/840\n",
      "Batch loss: 0.6934677362442017 batch: 55/840\n",
      "Batch loss: 0.6823491454124451 batch: 56/840\n",
      "Batch loss: 0.7066705226898193 batch: 57/840\n",
      "Batch loss: 0.5718010067939758 batch: 58/840\n",
      "Batch loss: 0.5725207328796387 batch: 59/840\n",
      "Batch loss: 0.5746898651123047 batch: 60/840\n",
      "Batch loss: 0.7981194853782654 batch: 61/840\n",
      "Batch loss: 0.7348024845123291 batch: 62/840\n",
      "Batch loss: 0.6527225971221924 batch: 63/840\n",
      "Batch loss: 0.6650065779685974 batch: 64/840\n",
      "Batch loss: 0.6457895636558533 batch: 65/840\n",
      "Batch loss: 0.6629142165184021 batch: 66/840\n",
      "Batch loss: 0.7476174831390381 batch: 67/840\n",
      "Batch loss: 0.7167757153511047 batch: 68/840\n",
      "Batch loss: 0.7155864834785461 batch: 69/840\n",
      "Batch loss: 0.6099184155464172 batch: 70/840\n",
      "Batch loss: 0.7511225938796997 batch: 71/840\n",
      "Batch loss: 0.7569541931152344 batch: 72/840\n",
      "Batch loss: 0.6737692356109619 batch: 73/840\n",
      "Batch loss: 0.6281631588935852 batch: 74/840\n",
      "Batch loss: 0.7198997735977173 batch: 75/840\n",
      "Batch loss: 0.4590536952018738 batch: 76/840\n",
      "Batch loss: 0.720604658126831 batch: 77/840\n",
      "Batch loss: 0.8442906141281128 batch: 78/840\n",
      "Batch loss: 0.6806983351707458 batch: 79/840\n",
      "Batch loss: 0.6521221995353699 batch: 80/840\n",
      "Batch loss: 0.5743758082389832 batch: 81/840\n",
      "Batch loss: 0.7165048122406006 batch: 82/840\n",
      "Batch loss: 0.6693659424781799 batch: 83/840\n",
      "Batch loss: 0.7471780180931091 batch: 84/840\n",
      "Batch loss: 0.5937157273292542 batch: 85/840\n",
      "Batch loss: 0.9011015295982361 batch: 86/840\n",
      "Batch loss: 0.5617426633834839 batch: 87/840\n",
      "Batch loss: 0.542584240436554 batch: 88/840\n",
      "Batch loss: 0.4893237054347992 batch: 89/840\n",
      "Batch loss: 0.5591933727264404 batch: 90/840\n",
      "Batch loss: 0.6090479493141174 batch: 91/840\n",
      "Batch loss: 0.6979056596755981 batch: 92/840\n",
      "Batch loss: 0.786608099937439 batch: 93/840\n",
      "Batch loss: 0.6056075096130371 batch: 94/840\n",
      "Batch loss: 0.7184961438179016 batch: 95/840\n",
      "Batch loss: 0.6897013783454895 batch: 96/840\n",
      "Batch loss: 0.6598894596099854 batch: 97/840\n",
      "Batch loss: 0.7872051000595093 batch: 98/840\n",
      "Batch loss: 0.5638586282730103 batch: 99/840\n",
      "Batch loss: 0.7395327687263489 batch: 100/840\n",
      "Batch loss: 0.5894121527671814 batch: 101/840\n",
      "Batch loss: 0.5259002447128296 batch: 102/840\n",
      "Batch loss: 0.6663440465927124 batch: 103/840\n",
      "Batch loss: 0.577675998210907 batch: 104/840\n",
      "Batch loss: 0.6512499451637268 batch: 105/840\n",
      "Batch loss: 0.6330211162567139 batch: 106/840\n",
      "Batch loss: 0.5561537742614746 batch: 107/840\n",
      "Batch loss: 0.7778205871582031 batch: 108/840\n",
      "Batch loss: 0.6392990946769714 batch: 109/840\n",
      "Batch loss: 0.6056361198425293 batch: 110/840\n",
      "Batch loss: 0.5853554010391235 batch: 111/840\n",
      "Batch loss: 0.7150206565856934 batch: 112/840\n",
      "Batch loss: 0.7913064360618591 batch: 113/840\n",
      "Batch loss: 0.5776790380477905 batch: 114/840\n",
      "Batch loss: 0.6288166642189026 batch: 115/840\n",
      "Batch loss: 0.5881909728050232 batch: 116/840\n",
      "Batch loss: 0.6269080638885498 batch: 117/840\n",
      "Batch loss: 0.4825553596019745 batch: 118/840\n",
      "Batch loss: 0.8170546889305115 batch: 119/840\n",
      "Batch loss: 0.6240985989570618 batch: 120/840\n",
      "Batch loss: 0.772996723651886 batch: 121/840\n",
      "Batch loss: 0.7474091053009033 batch: 122/840\n",
      "Batch loss: 0.49376288056373596 batch: 123/840\n",
      "Batch loss: 0.522334635257721 batch: 124/840\n",
      "Batch loss: 0.7062118649482727 batch: 125/840\n",
      "Batch loss: 0.6191122531890869 batch: 126/840\n",
      "Batch loss: 0.7062781453132629 batch: 127/840\n",
      "Batch loss: 0.676433265209198 batch: 128/840\n",
      "Batch loss: 0.6958855986595154 batch: 129/840\n",
      "Batch loss: 0.5636293888092041 batch: 130/840\n",
      "Batch loss: 0.8024196028709412 batch: 131/840\n",
      "Batch loss: 1.0775421857833862 batch: 132/840\n",
      "Batch loss: 0.7838882207870483 batch: 133/840\n",
      "Batch loss: 0.5297893285751343 batch: 134/840\n",
      "Batch loss: 0.5680281519889832 batch: 135/840\n",
      "Batch loss: 0.7067002654075623 batch: 136/840\n",
      "Batch loss: 0.7289568185806274 batch: 137/840\n",
      "Batch loss: 0.5718027353286743 batch: 138/840\n",
      "Batch loss: 0.5871459245681763 batch: 139/840\n",
      "Batch loss: 0.7056137919425964 batch: 140/840\n",
      "Batch loss: 0.5638985633850098 batch: 141/840\n",
      "Batch loss: 0.7039416432380676 batch: 142/840\n",
      "Batch loss: 0.5578309297561646 batch: 143/840\n",
      "Batch loss: 0.6313391327857971 batch: 144/840\n",
      "Batch loss: 0.7791110277175903 batch: 145/840\n",
      "Batch loss: 0.7240590453147888 batch: 146/840\n",
      "Batch loss: 0.5452578663825989 batch: 147/840\n",
      "Batch loss: 0.655192494392395 batch: 148/840\n",
      "Batch loss: 0.7258913516998291 batch: 149/840\n",
      "Batch loss: 0.7656686902046204 batch: 150/840\n",
      "Batch loss: 0.6193640232086182 batch: 151/840\n",
      "Batch loss: 0.6787059903144836 batch: 152/840\n",
      "Batch loss: 0.5295735001564026 batch: 153/840\n",
      "Batch loss: 0.873755693435669 batch: 154/840\n",
      "Batch loss: 0.5923499464988708 batch: 155/840\n",
      "Batch loss: 0.5554658770561218 batch: 156/840\n",
      "Batch loss: 0.7097892165184021 batch: 157/840\n",
      "Batch loss: 0.49912700057029724 batch: 158/840\n",
      "Batch loss: 0.5745890140533447 batch: 159/840\n",
      "Batch loss: 0.560745120048523 batch: 160/840\n",
      "Batch loss: 0.6486131548881531 batch: 161/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8055455088615417 batch: 162/840\n",
      "Batch loss: 0.8526853322982788 batch: 163/840\n",
      "Batch loss: 0.4948110580444336 batch: 164/840\n",
      "Batch loss: 0.6111969351768494 batch: 165/840\n",
      "Batch loss: 0.6300873756408691 batch: 166/840\n",
      "Batch loss: 0.7273434996604919 batch: 167/840\n",
      "Batch loss: 0.592089831829071 batch: 168/840\n",
      "Batch loss: 0.5644845366477966 batch: 169/840\n",
      "Batch loss: 0.7071936726570129 batch: 170/840\n",
      "Batch loss: 0.6894271969795227 batch: 171/840\n",
      "Batch loss: 0.7669909596443176 batch: 172/840\n",
      "Batch loss: 0.563789427280426 batch: 173/840\n",
      "Batch loss: 0.6264147758483887 batch: 174/840\n",
      "Batch loss: 0.6515375375747681 batch: 175/840\n",
      "Batch loss: 0.7897191643714905 batch: 176/840\n",
      "Batch loss: 0.6838936805725098 batch: 177/840\n",
      "Batch loss: 0.6111046075820923 batch: 178/840\n",
      "Batch loss: 0.7797077298164368 batch: 179/840\n",
      "Batch loss: 0.5645722150802612 batch: 180/840\n",
      "Batch loss: 0.5818890333175659 batch: 181/840\n",
      "Batch loss: 0.6301219463348389 batch: 182/840\n",
      "Batch loss: 0.7318995594978333 batch: 183/840\n",
      "Batch loss: 0.5103947520256042 batch: 184/840\n",
      "Batch loss: 0.5313145518302917 batch: 185/840\n",
      "Batch loss: 0.5409291982650757 batch: 186/840\n",
      "Batch loss: 0.7304352521896362 batch: 187/840\n",
      "Batch loss: 0.6144669055938721 batch: 188/840\n",
      "Batch loss: 0.6969654560089111 batch: 189/840\n",
      "Batch loss: 0.6882447004318237 batch: 190/840\n",
      "Batch loss: 0.8382846117019653 batch: 191/840\n",
      "Batch loss: 0.5479690432548523 batch: 192/840\n",
      "Batch loss: 0.5401067733764648 batch: 193/840\n",
      "Batch loss: 0.4546408951282501 batch: 194/840\n",
      "Batch loss: 0.6508830785751343 batch: 195/840\n",
      "Batch loss: 0.7450392246246338 batch: 196/840\n",
      "Batch loss: 0.6798925995826721 batch: 197/840\n",
      "Batch loss: 0.5834998488426208 batch: 198/840\n",
      "Batch loss: 0.6420061588287354 batch: 199/840\n",
      "Batch loss: 0.808563232421875 batch: 200/840\n",
      "Batch loss: 0.5336841940879822 batch: 201/840\n",
      "Batch loss: 0.5998263359069824 batch: 202/840\n",
      "Batch loss: 0.6332675814628601 batch: 203/840\n",
      "Batch loss: 0.710997462272644 batch: 204/840\n",
      "Batch loss: 0.728432297706604 batch: 205/840\n",
      "Batch loss: 0.7532259225845337 batch: 206/840\n",
      "Batch loss: 0.6128024458885193 batch: 207/840\n",
      "Batch loss: 0.7228003144264221 batch: 208/840\n",
      "Batch loss: 0.6103166341781616 batch: 209/840\n",
      "Batch loss: 0.5985656380653381 batch: 210/840\n",
      "Batch loss: 0.5163418650627136 batch: 211/840\n",
      "Batch loss: 0.6315719485282898 batch: 212/840\n",
      "Batch loss: 0.6976892352104187 batch: 213/840\n",
      "Batch loss: 0.7311553359031677 batch: 214/840\n",
      "Batch loss: 0.7831034064292908 batch: 215/840\n",
      "Batch loss: 0.6690928936004639 batch: 216/840\n",
      "Batch loss: 0.6207399964332581 batch: 217/840\n",
      "Batch loss: 0.7288849353790283 batch: 218/840\n",
      "Batch loss: 0.6333940029144287 batch: 219/840\n",
      "Batch loss: 0.8470423221588135 batch: 220/840\n",
      "Batch loss: 0.622408926486969 batch: 221/840\n",
      "Batch loss: 0.7624351978302002 batch: 222/840\n",
      "Batch loss: 0.647672176361084 batch: 223/840\n",
      "Batch loss: 0.7119609117507935 batch: 224/840\n",
      "Batch loss: 0.722464382648468 batch: 225/840\n",
      "Batch loss: 0.7707647085189819 batch: 226/840\n",
      "Batch loss: 0.6948212385177612 batch: 227/840\n",
      "Batch loss: 0.568926990032196 batch: 228/840\n",
      "Batch loss: 0.5745319128036499 batch: 229/840\n",
      "Batch loss: 0.679155707359314 batch: 230/840\n",
      "Batch loss: 0.5733568072319031 batch: 231/840\n",
      "Batch loss: 0.6103163957595825 batch: 232/840\n",
      "Batch loss: 0.8540319800376892 batch: 233/840\n",
      "Batch loss: 0.5791156888008118 batch: 234/840\n",
      "Batch loss: 0.5896663665771484 batch: 235/840\n",
      "Batch loss: 0.7749969959259033 batch: 236/840\n",
      "Batch loss: 0.5689256191253662 batch: 237/840\n",
      "Batch loss: 0.8396617770195007 batch: 238/840\n",
      "Batch loss: 0.6484330892562866 batch: 239/840\n",
      "Batch loss: 0.6273399591445923 batch: 240/840\n",
      "Batch loss: 0.7082788348197937 batch: 241/840\n",
      "Batch loss: 0.6475258469581604 batch: 242/840\n",
      "Batch loss: 0.5821616649627686 batch: 243/840\n",
      "Batch loss: 0.7535585165023804 batch: 244/840\n",
      "Batch loss: 0.5039025545120239 batch: 245/840\n",
      "Batch loss: 0.663362443447113 batch: 246/840\n",
      "Batch loss: 0.8099774122238159 batch: 247/840\n",
      "Batch loss: 0.7784122228622437 batch: 248/840\n",
      "Batch loss: 0.992618203163147 batch: 249/840\n",
      "Batch loss: 0.5372372269630432 batch: 250/840\n",
      "Batch loss: 0.6136429905891418 batch: 251/840\n",
      "Batch loss: 0.6303190588951111 batch: 252/840\n",
      "Batch loss: 0.6787398457527161 batch: 253/840\n",
      "Batch loss: 0.7691569328308105 batch: 254/840\n",
      "Batch loss: 0.7042590379714966 batch: 255/840\n",
      "Batch loss: 0.7310745120048523 batch: 256/840\n",
      "Batch loss: 0.597533643245697 batch: 257/840\n",
      "Batch loss: 0.7620802521705627 batch: 258/840\n",
      "Batch loss: 0.6125026345252991 batch: 259/840\n",
      "Batch loss: 0.5649226307868958 batch: 260/840\n",
      "Batch loss: 0.563365638256073 batch: 261/840\n",
      "Batch loss: 0.49944618344306946 batch: 262/840\n",
      "Batch loss: 0.6443873047828674 batch: 263/840\n",
      "Batch loss: 0.6307184100151062 batch: 264/840\n",
      "Batch loss: 0.7425140142440796 batch: 265/840\n",
      "Batch loss: 0.6251440644264221 batch: 266/840\n",
      "Batch loss: 0.6934234499931335 batch: 267/840\n",
      "Batch loss: 0.7124192118644714 batch: 268/840\n",
      "Batch loss: 0.5686278343200684 batch: 269/840\n",
      "Batch loss: 0.637373149394989 batch: 270/840\n",
      "Batch loss: 0.6125648021697998 batch: 271/840\n",
      "Batch loss: 0.7476955652236938 batch: 272/840\n",
      "Batch loss: 0.8087285757064819 batch: 273/840\n",
      "Batch loss: 0.6843302249908447 batch: 274/840\n",
      "Batch loss: 0.7866818308830261 batch: 275/840\n",
      "Batch loss: 0.5998620390892029 batch: 276/840\n",
      "Batch loss: 0.6608609557151794 batch: 277/840\n",
      "Batch loss: 0.8399088382720947 batch: 278/840\n",
      "Batch loss: 0.7584282755851746 batch: 279/840\n",
      "Batch loss: 0.7910279631614685 batch: 280/840\n",
      "Batch loss: 0.640702486038208 batch: 281/840\n",
      "Batch loss: 0.539933979511261 batch: 282/840\n",
      "Batch loss: 0.61688631772995 batch: 283/840\n",
      "Batch loss: 0.5714269280433655 batch: 284/840\n",
      "Batch loss: 0.5237123966217041 batch: 285/840\n",
      "Batch loss: 0.6506938338279724 batch: 286/840\n",
      "Batch loss: 0.53575599193573 batch: 287/840\n",
      "Batch loss: 0.5757118463516235 batch: 288/840\n",
      "Batch loss: 0.8456674814224243 batch: 289/840\n",
      "Batch loss: 0.8018932938575745 batch: 290/840\n",
      "Batch loss: 0.7820891737937927 batch: 291/840\n",
      "Batch loss: 0.5629794001579285 batch: 292/840\n",
      "Batch loss: 0.6587616801261902 batch: 293/840\n",
      "Batch loss: 0.7731917500495911 batch: 294/840\n",
      "Batch loss: 0.5923469662666321 batch: 295/840\n",
      "Batch loss: 0.6595145463943481 batch: 296/840\n",
      "Batch loss: 0.6871139407157898 batch: 297/840\n",
      "Batch loss: 0.6782267689704895 batch: 298/840\n",
      "Batch loss: 0.6245079636573792 batch: 299/840\n",
      "Batch loss: 0.8013704419136047 batch: 300/840\n",
      "Batch loss: 0.7044625878334045 batch: 301/840\n",
      "Batch loss: 0.6407842040061951 batch: 302/840\n",
      "Batch loss: 0.7859790325164795 batch: 303/840\n",
      "Batch loss: 0.649467408657074 batch: 304/840\n",
      "Batch loss: 0.5352523326873779 batch: 305/840\n",
      "Batch loss: 0.7060136198997498 batch: 306/840\n",
      "Batch loss: 0.5991559624671936 batch: 307/840\n",
      "Batch loss: 0.8036422729492188 batch: 308/840\n",
      "Batch loss: 0.5777233839035034 batch: 309/840\n",
      "Batch loss: 0.8921323418617249 batch: 310/840\n",
      "Batch loss: 0.6512722969055176 batch: 311/840\n",
      "Batch loss: 0.7321298122406006 batch: 312/840\n",
      "Batch loss: 0.7064468860626221 batch: 313/840\n",
      "Batch loss: 0.5630528926849365 batch: 314/840\n",
      "Batch loss: 0.6953601837158203 batch: 315/840\n",
      "Batch loss: 0.6303985118865967 batch: 316/840\n",
      "Batch loss: 0.720857560634613 batch: 317/840\n",
      "Batch loss: 0.7590441107749939 batch: 318/840\n",
      "Batch loss: 0.7422415018081665 batch: 319/840\n",
      "Batch loss: 0.6318797469139099 batch: 320/840\n",
      "Batch loss: 0.6097332239151001 batch: 321/840\n",
      "Batch loss: 0.6258555054664612 batch: 322/840\n",
      "Batch loss: 0.7467612624168396 batch: 323/840\n",
      "Batch loss: 0.7373157739639282 batch: 324/840\n",
      "Batch loss: 0.561779260635376 batch: 325/840\n",
      "Batch loss: 0.6053300499916077 batch: 326/840\n",
      "Batch loss: 0.5063046813011169 batch: 327/840\n",
      "Batch loss: 0.8005856871604919 batch: 328/840\n",
      "Batch loss: 0.7502833008766174 batch: 329/840\n",
      "Batch loss: 0.6717022061347961 batch: 330/840\n",
      "Batch loss: 0.7744457721710205 batch: 331/840\n",
      "Batch loss: 0.6493514180183411 batch: 332/840\n",
      "Batch loss: 0.673545241355896 batch: 333/840\n",
      "Batch loss: 0.7331529259681702 batch: 334/840\n",
      "Batch loss: 0.6381990909576416 batch: 335/840\n",
      "Batch loss: 0.6708759069442749 batch: 336/840\n",
      "Batch loss: 0.895317554473877 batch: 337/840\n",
      "Batch loss: 0.7135608196258545 batch: 338/840\n",
      "Batch loss: 0.5805525183677673 batch: 339/840\n",
      "Batch loss: 0.7954812049865723 batch: 340/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5393564701080322 batch: 341/840\n",
      "Batch loss: 0.5791621208190918 batch: 342/840\n",
      "Batch loss: 0.8212456703186035 batch: 343/840\n",
      "Batch loss: 0.691765308380127 batch: 344/840\n",
      "Batch loss: 0.43591389060020447 batch: 345/840\n",
      "Batch loss: 0.6217524409294128 batch: 346/840\n",
      "Batch loss: 0.6984596252441406 batch: 347/840\n",
      "Batch loss: 0.6453769207000732 batch: 348/840\n",
      "Batch loss: 0.7100834846496582 batch: 349/840\n",
      "Batch loss: 0.5771466493606567 batch: 350/840\n",
      "Batch loss: 0.7074903249740601 batch: 351/840\n",
      "Batch loss: 0.7597488164901733 batch: 352/840\n",
      "Batch loss: 0.8080952167510986 batch: 353/840\n",
      "Batch loss: 0.6019771695137024 batch: 354/840\n",
      "Batch loss: 0.5836263298988342 batch: 355/840\n",
      "Batch loss: 0.6068894863128662 batch: 356/840\n",
      "Batch loss: 0.5619048476219177 batch: 357/840\n",
      "Batch loss: 0.9108259677886963 batch: 358/840\n",
      "Batch loss: 0.5716696977615356 batch: 359/840\n",
      "Batch loss: 0.9013494253158569 batch: 360/840\n",
      "Batch loss: 0.8102597594261169 batch: 361/840\n",
      "Batch loss: 0.5468874573707581 batch: 362/840\n",
      "Batch loss: 0.49778202176094055 batch: 363/840\n",
      "Batch loss: 0.7627338171005249 batch: 364/840\n",
      "Batch loss: 0.5456343293190002 batch: 365/840\n",
      "Batch loss: 0.7170369625091553 batch: 366/840\n",
      "Batch loss: 0.48602649569511414 batch: 367/840\n",
      "Batch loss: 0.8291847705841064 batch: 368/840\n",
      "Batch loss: 0.6309696435928345 batch: 369/840\n",
      "Batch loss: 0.8518025875091553 batch: 370/840\n",
      "Batch loss: 0.6774284839630127 batch: 371/840\n",
      "Batch loss: 0.5316429734230042 batch: 372/840\n",
      "Batch loss: 0.7661010026931763 batch: 373/840\n",
      "Batch loss: 0.7162462472915649 batch: 374/840\n",
      "Batch loss: 0.5234076380729675 batch: 375/840\n",
      "Batch loss: 0.5877513289451599 batch: 376/840\n",
      "Batch loss: 0.6966211199760437 batch: 377/840\n",
      "Batch loss: 0.7106175422668457 batch: 378/840\n",
      "Batch loss: 0.5754316449165344 batch: 379/840\n",
      "Batch loss: 0.9840682148933411 batch: 380/840\n",
      "Batch loss: 0.8701531291007996 batch: 381/840\n",
      "Batch loss: 0.8057776689529419 batch: 382/840\n",
      "Batch loss: 0.7215025424957275 batch: 383/840\n",
      "Batch loss: 0.6119314432144165 batch: 384/840\n",
      "Batch loss: 0.7380624413490295 batch: 385/840\n",
      "Batch loss: 0.7630196213722229 batch: 386/840\n",
      "Batch loss: 0.649884819984436 batch: 387/840\n",
      "Batch loss: 0.7901115417480469 batch: 388/840\n",
      "Batch loss: 0.599256694316864 batch: 389/840\n",
      "Batch loss: 0.8780872821807861 batch: 390/840\n",
      "Batch loss: 0.71985924243927 batch: 391/840\n",
      "Batch loss: 0.5343344211578369 batch: 392/840\n",
      "Batch loss: 0.5057488083839417 batch: 393/840\n",
      "Batch loss: 0.7452318668365479 batch: 394/840\n",
      "Batch loss: 0.6040912866592407 batch: 395/840\n",
      "Batch loss: 0.6574905514717102 batch: 396/840\n",
      "Batch loss: 0.6640942096710205 batch: 397/840\n",
      "Batch loss: 0.7499387264251709 batch: 398/840\n",
      "Batch loss: 0.45918065309524536 batch: 399/840\n",
      "Batch loss: 0.6056952476501465 batch: 400/840\n",
      "Batch loss: 0.6908972263336182 batch: 401/840\n",
      "Batch loss: 0.5732905864715576 batch: 402/840\n",
      "Batch loss: 0.6148589849472046 batch: 403/840\n",
      "Batch loss: 0.5641529560089111 batch: 404/840\n",
      "Batch loss: 0.7611299753189087 batch: 405/840\n",
      "Batch loss: 0.6582345366477966 batch: 406/840\n",
      "Batch loss: 0.5792036652565002 batch: 407/840\n",
      "Batch loss: 0.7406706809997559 batch: 408/840\n",
      "Batch loss: 0.7451518177986145 batch: 409/840\n",
      "Batch loss: 0.7225003838539124 batch: 410/840\n",
      "Batch loss: 0.6479372978210449 batch: 411/840\n",
      "Batch loss: 0.7348377704620361 batch: 412/840\n",
      "Batch loss: 0.7532176375389099 batch: 413/840\n",
      "Batch loss: 0.5759634375572205 batch: 414/840\n",
      "Batch loss: 0.7899202108383179 batch: 415/840\n",
      "Batch loss: 0.4454193413257599 batch: 416/840\n",
      "Batch loss: 0.706239640712738 batch: 417/840\n",
      "Batch loss: 0.8667216300964355 batch: 418/840\n",
      "Batch loss: 0.7518168687820435 batch: 419/840\n",
      "Batch loss: 0.7419865131378174 batch: 420/840\n",
      "Batch loss: 0.6040948629379272 batch: 421/840\n",
      "Batch loss: 0.6499083042144775 batch: 422/840\n",
      "Batch loss: 0.6141567826271057 batch: 423/840\n",
      "Batch loss: 0.7313035726547241 batch: 424/840\n",
      "Batch loss: 0.7897422313690186 batch: 425/840\n",
      "Batch loss: 0.6035574078559875 batch: 426/840\n",
      "Batch loss: 0.6091293692588806 batch: 427/840\n",
      "Batch loss: 0.700481116771698 batch: 428/840\n",
      "Batch loss: 0.6124027967453003 batch: 429/840\n",
      "Batch loss: 0.8211553692817688 batch: 430/840\n",
      "Batch loss: 0.556688666343689 batch: 431/840\n",
      "Batch loss: 0.6778416633605957 batch: 432/840\n",
      "Batch loss: 0.46514928340911865 batch: 433/840\n",
      "Batch loss: 0.6235085129737854 batch: 434/840\n",
      "Batch loss: 0.8503655791282654 batch: 435/840\n",
      "Batch loss: 0.6610072255134583 batch: 436/840\n",
      "Batch loss: 0.7922542095184326 batch: 437/840\n",
      "Batch loss: 0.4074805974960327 batch: 438/840\n",
      "Batch loss: 0.7066138982772827 batch: 439/840\n",
      "Batch loss: 0.723958432674408 batch: 440/840\n",
      "Batch loss: 0.7870781421661377 batch: 441/840\n",
      "Batch loss: 0.7100499868392944 batch: 442/840\n",
      "Batch loss: 0.6666779518127441 batch: 443/840\n",
      "Batch loss: 0.7036826610565186 batch: 444/840\n",
      "Batch loss: 0.5826128125190735 batch: 445/840\n",
      "Batch loss: 0.7398027181625366 batch: 446/840\n",
      "Batch loss: 0.7070688009262085 batch: 447/840\n",
      "Batch loss: 0.7428818345069885 batch: 448/840\n",
      "Batch loss: 0.577948272228241 batch: 449/840\n",
      "Batch loss: 0.6562036871910095 batch: 450/840\n",
      "Batch loss: 0.8185612559318542 batch: 451/840\n",
      "Batch loss: 0.7412586808204651 batch: 452/840\n",
      "Batch loss: 0.7101884484291077 batch: 453/840\n",
      "Batch loss: 0.6589282155036926 batch: 454/840\n",
      "Batch loss: 0.6945684552192688 batch: 455/840\n",
      "Batch loss: 0.611594557762146 batch: 456/840\n",
      "Batch loss: 0.6732745170593262 batch: 457/840\n",
      "Batch loss: 0.6450904607772827 batch: 458/840\n",
      "Batch loss: 0.5567727088928223 batch: 459/840\n",
      "Batch loss: 0.4453246295452118 batch: 460/840\n",
      "Batch loss: 0.7099666595458984 batch: 461/840\n",
      "Batch loss: 0.6946277022361755 batch: 462/840\n",
      "Batch loss: 0.8021658062934875 batch: 463/840\n",
      "Batch loss: 0.5224119424819946 batch: 464/840\n",
      "Batch loss: 0.984839677810669 batch: 465/840\n",
      "Batch loss: 0.6515871286392212 batch: 466/840\n",
      "Batch loss: 0.8253361582756042 batch: 467/840\n",
      "Batch loss: 0.6410252451896667 batch: 468/840\n",
      "Batch loss: 0.5419092178344727 batch: 469/840\n",
      "Batch loss: 0.6594933271408081 batch: 470/840\n",
      "Batch loss: 0.7413829565048218 batch: 471/840\n",
      "Batch loss: 0.7482587695121765 batch: 472/840\n",
      "Batch loss: 0.7975959181785583 batch: 473/840\n",
      "Batch loss: 0.5602424144744873 batch: 474/840\n",
      "Batch loss: 0.6452203989028931 batch: 475/840\n",
      "Batch loss: 0.6057410836219788 batch: 476/840\n",
      "Batch loss: 0.4989984929561615 batch: 477/840\n",
      "Batch loss: 0.6594129204750061 batch: 478/840\n",
      "Batch loss: 0.5690715312957764 batch: 479/840\n",
      "Batch loss: 0.6989727020263672 batch: 480/840\n",
      "Batch loss: 0.7060373425483704 batch: 481/840\n",
      "Batch loss: 0.648192286491394 batch: 482/840\n",
      "Batch loss: 0.7837880849838257 batch: 483/840\n",
      "Batch loss: 0.546903669834137 batch: 484/840\n",
      "Batch loss: 0.6560665369033813 batch: 485/840\n",
      "Batch loss: 0.7133265733718872 batch: 486/840\n",
      "Batch loss: 0.5102633833885193 batch: 487/840\n",
      "Batch loss: 0.6714050769805908 batch: 488/840\n",
      "Batch loss: 0.6696580648422241 batch: 489/840\n",
      "Batch loss: 0.7244837284088135 batch: 490/840\n",
      "Batch loss: 0.4987180233001709 batch: 491/840\n",
      "Batch loss: 0.7244848608970642 batch: 492/840\n",
      "Batch loss: 0.7847307324409485 batch: 493/840\n",
      "Batch loss: 0.4506928622722626 batch: 494/840\n",
      "Batch loss: 0.8073765635490417 batch: 495/840\n",
      "Batch loss: 0.6314924955368042 batch: 496/840\n",
      "Batch loss: 0.6916081309318542 batch: 497/840\n",
      "Batch loss: 0.523723840713501 batch: 498/840\n",
      "Batch loss: 0.7576763033866882 batch: 499/840\n",
      "Batch loss: 0.6414938569068909 batch: 500/840\n",
      "Batch loss: 0.4962000250816345 batch: 501/840\n",
      "Batch loss: 0.7209928035736084 batch: 502/840\n",
      "Batch loss: 0.5216266512870789 batch: 503/840\n",
      "Batch loss: 0.6695737242698669 batch: 504/840\n",
      "Batch loss: 0.7840448617935181 batch: 505/840\n",
      "Batch loss: 0.5335889458656311 batch: 506/840\n",
      "Batch loss: 0.7832791209220886 batch: 507/840\n",
      "Batch loss: 0.5560196042060852 batch: 508/840\n",
      "Batch loss: 0.7335250973701477 batch: 509/840\n",
      "Batch loss: 0.718132495880127 batch: 510/840\n",
      "Batch loss: 0.5983782410621643 batch: 511/840\n",
      "Batch loss: 0.6049350500106812 batch: 512/840\n",
      "Batch loss: 0.5953409075737 batch: 513/840\n",
      "Batch loss: 0.731273353099823 batch: 514/840\n",
      "Batch loss: 0.525320291519165 batch: 515/840\n",
      "Batch loss: 0.9086540341377258 batch: 516/840\n",
      "Batch loss: 0.6791911125183105 batch: 517/840\n",
      "Batch loss: 0.6779746413230896 batch: 518/840\n",
      "Batch loss: 0.8064096570014954 batch: 519/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7167176604270935 batch: 520/840\n",
      "Batch loss: 0.7690946459770203 batch: 521/840\n",
      "Batch loss: 0.6057003736495972 batch: 522/840\n",
      "Batch loss: 0.5384803414344788 batch: 523/840\n",
      "Batch loss: 0.6711328029632568 batch: 524/840\n",
      "Batch loss: 0.6166236996650696 batch: 525/840\n",
      "Batch loss: 0.7826942205429077 batch: 526/840\n",
      "Batch loss: 0.6991108655929565 batch: 527/840\n",
      "Batch loss: 0.592963457107544 batch: 528/840\n",
      "Batch loss: 0.5513065457344055 batch: 529/840\n",
      "Batch loss: 0.5628508925437927 batch: 530/840\n",
      "Batch loss: 0.6286256909370422 batch: 531/840\n",
      "Batch loss: 0.4899005591869354 batch: 532/840\n",
      "Batch loss: 0.6869046688079834 batch: 533/840\n",
      "Batch loss: 0.6446962952613831 batch: 534/840\n",
      "Batch loss: 0.7142298817634583 batch: 535/840\n",
      "Batch loss: 0.7137712240219116 batch: 536/840\n",
      "Batch loss: 0.7373092174530029 batch: 537/840\n",
      "Batch loss: 0.6155681014060974 batch: 538/840\n",
      "Batch loss: 0.617030143737793 batch: 539/840\n",
      "Batch loss: 0.7862457036972046 batch: 540/840\n",
      "Batch loss: 0.6259289979934692 batch: 541/840\n",
      "Batch loss: 0.6706520915031433 batch: 542/840\n",
      "Batch loss: 0.45789188146591187 batch: 543/840\n",
      "Batch loss: 0.7369605302810669 batch: 544/840\n",
      "Batch loss: 0.6159692406654358 batch: 545/840\n",
      "Batch loss: 0.7011238932609558 batch: 546/840\n",
      "Batch loss: 0.6542739272117615 batch: 547/840\n",
      "Batch loss: 0.47659024596214294 batch: 548/840\n",
      "Batch loss: 0.48789745569229126 batch: 549/840\n",
      "Batch loss: 0.5141378045082092 batch: 550/840\n",
      "Batch loss: 0.6261225938796997 batch: 551/840\n",
      "Batch loss: 0.6313766837120056 batch: 552/840\n",
      "Batch loss: 0.6508033871650696 batch: 553/840\n",
      "Batch loss: 0.6925060153007507 batch: 554/840\n",
      "Batch loss: 0.7043871283531189 batch: 555/840\n",
      "Batch loss: 0.6661534905433655 batch: 556/840\n",
      "Batch loss: 0.6379587054252625 batch: 557/840\n",
      "Batch loss: 0.6979573965072632 batch: 558/840\n",
      "Batch loss: 0.6057749390602112 batch: 559/840\n",
      "Batch loss: 0.6344029903411865 batch: 560/840\n",
      "Batch loss: 0.655392050743103 batch: 561/840\n",
      "Batch loss: 0.5445970296859741 batch: 562/840\n",
      "Batch loss: 0.4889090061187744 batch: 563/840\n",
      "Batch loss: 0.6523417830467224 batch: 564/840\n",
      "Batch loss: 0.7520204782485962 batch: 565/840\n",
      "Batch loss: 0.7033494710922241 batch: 566/840\n",
      "Batch loss: 0.7145190238952637 batch: 567/840\n",
      "Batch loss: 0.6043339371681213 batch: 568/840\n",
      "Batch loss: 0.6791520118713379 batch: 569/840\n",
      "Batch loss: 0.46748027205467224 batch: 570/840\n",
      "Batch loss: 0.6683049201965332 batch: 571/840\n",
      "Batch loss: 0.7654505372047424 batch: 572/840\n",
      "Batch loss: 0.6518812775611877 batch: 573/840\n",
      "Batch loss: 0.7580068111419678 batch: 574/840\n",
      "Batch loss: 0.5259227156639099 batch: 575/840\n",
      "Batch loss: 0.7326400279998779 batch: 576/840\n",
      "Batch loss: 0.6265782713890076 batch: 577/840\n",
      "Batch loss: 0.7457125186920166 batch: 578/840\n",
      "Batch loss: 0.5893328189849854 batch: 579/840\n",
      "Batch loss: 0.7322673201560974 batch: 580/840\n",
      "Batch loss: 0.6677314043045044 batch: 581/840\n",
      "Batch loss: 0.7604107856750488 batch: 582/840\n",
      "Batch loss: 0.5488142967224121 batch: 583/840\n",
      "Batch loss: 0.8021907210350037 batch: 584/840\n",
      "Batch loss: 0.6851984262466431 batch: 585/840\n",
      "Batch loss: 0.6737388372421265 batch: 586/840\n",
      "Batch loss: 0.6876089572906494 batch: 587/840\n",
      "Batch loss: 0.5596317052841187 batch: 588/840\n",
      "Batch loss: 0.8281107544898987 batch: 589/840\n",
      "Batch loss: 0.5675554871559143 batch: 590/840\n",
      "Batch loss: 0.4707554876804352 batch: 591/840\n",
      "Batch loss: 0.572927713394165 batch: 592/840\n",
      "Batch loss: 0.7482057213783264 batch: 593/840\n",
      "Batch loss: 0.7275305986404419 batch: 594/840\n",
      "Batch loss: 0.46029385924339294 batch: 595/840\n",
      "Batch loss: 0.5008952021598816 batch: 596/840\n",
      "Batch loss: 0.572573184967041 batch: 597/840\n",
      "Batch loss: 0.5303345918655396 batch: 598/840\n",
      "Batch loss: 0.6726426482200623 batch: 599/840\n",
      "Batch loss: 0.7917066812515259 batch: 600/840\n",
      "Batch loss: 0.7138418555259705 batch: 601/840\n",
      "Batch loss: 0.5714566707611084 batch: 602/840\n",
      "Batch loss: 0.6534757018089294 batch: 603/840\n",
      "Batch loss: 0.7839952111244202 batch: 604/840\n",
      "Batch loss: 0.8310518860816956 batch: 605/840\n",
      "Batch loss: 0.8638436198234558 batch: 606/840\n",
      "Batch loss: 0.662477433681488 batch: 607/840\n",
      "Batch loss: 0.633350133895874 batch: 608/840\n",
      "Batch loss: 0.45028847455978394 batch: 609/840\n",
      "Batch loss: 0.723051130771637 batch: 610/840\n",
      "Batch loss: 0.6737548112869263 batch: 611/840\n",
      "Batch loss: 0.806816816329956 batch: 612/840\n",
      "Batch loss: 0.7078296542167664 batch: 613/840\n",
      "Batch loss: 0.7440127730369568 batch: 614/840\n",
      "Batch loss: 0.6254810094833374 batch: 615/840\n",
      "Batch loss: 0.5479253530502319 batch: 616/840\n",
      "Batch loss: 0.6079022288322449 batch: 617/840\n",
      "Batch loss: 0.6926430463790894 batch: 618/840\n",
      "Batch loss: 0.8174885511398315 batch: 619/840\n",
      "Batch loss: 0.6740795373916626 batch: 620/840\n",
      "Batch loss: 0.7092229723930359 batch: 621/840\n",
      "Batch loss: 0.7457681894302368 batch: 622/840\n",
      "Batch loss: 0.6931520104408264 batch: 623/840\n",
      "Batch loss: 0.7843824625015259 batch: 624/840\n",
      "Batch loss: 0.6938657164573669 batch: 625/840\n",
      "Batch loss: 0.6628496050834656 batch: 626/840\n",
      "Batch loss: 0.6035131216049194 batch: 627/840\n",
      "Batch loss: 0.7015402913093567 batch: 628/840\n",
      "Batch loss: 0.662036657333374 batch: 629/840\n",
      "Batch loss: 0.6848746538162231 batch: 630/840\n",
      "Batch loss: 0.7389309406280518 batch: 631/840\n",
      "Batch loss: 0.6265669465065002 batch: 632/840\n",
      "Batch loss: 0.5806835889816284 batch: 633/840\n",
      "Batch loss: 0.7776480913162231 batch: 634/840\n",
      "Batch loss: 0.6046913862228394 batch: 635/840\n",
      "Batch loss: 0.6081206798553467 batch: 636/840\n",
      "Batch loss: 0.568398118019104 batch: 637/840\n",
      "Batch loss: 0.6644163727760315 batch: 638/840\n",
      "Batch loss: 0.6677922010421753 batch: 639/840\n",
      "Batch loss: 0.7620920538902283 batch: 640/840\n",
      "Batch loss: 0.8330228328704834 batch: 641/840\n",
      "Batch loss: 0.6124322414398193 batch: 642/840\n",
      "Batch loss: 1.0041042566299438 batch: 643/840\n",
      "Batch loss: 0.6397931575775146 batch: 644/840\n",
      "Batch loss: 0.5407095551490784 batch: 645/840\n",
      "Batch loss: 0.612989068031311 batch: 646/840\n",
      "Batch loss: 0.7055810689926147 batch: 647/840\n",
      "Batch loss: 0.6750120520591736 batch: 648/840\n",
      "Batch loss: 0.5862414836883545 batch: 649/840\n",
      "Batch loss: 0.5280277132987976 batch: 650/840\n",
      "Batch loss: 0.6880375742912292 batch: 651/840\n",
      "Batch loss: 0.7250780463218689 batch: 652/840\n",
      "Batch loss: 0.6215495467185974 batch: 653/840\n",
      "Batch loss: 0.827726423740387 batch: 654/840\n",
      "Batch loss: 0.5642338395118713 batch: 655/840\n",
      "Batch loss: 0.6819469928741455 batch: 656/840\n",
      "Batch loss: 0.5459190607070923 batch: 657/840\n",
      "Batch loss: 0.6950334906578064 batch: 658/840\n",
      "Batch loss: 0.7037609815597534 batch: 659/840\n",
      "Batch loss: 0.5448965430259705 batch: 660/840\n",
      "Batch loss: 0.7386176586151123 batch: 661/840\n",
      "Batch loss: 0.6078054904937744 batch: 662/840\n",
      "Batch loss: 0.6255228519439697 batch: 663/840\n",
      "Batch loss: 0.5919354557991028 batch: 664/840\n",
      "Batch loss: 0.7688488960266113 batch: 665/840\n",
      "Batch loss: 0.6335257887840271 batch: 666/840\n",
      "Batch loss: 0.8373423218727112 batch: 667/840\n",
      "Batch loss: 0.627683162689209 batch: 668/840\n",
      "Batch loss: 0.5270505547523499 batch: 669/840\n",
      "Batch loss: 0.6652369499206543 batch: 670/840\n",
      "Batch loss: 0.6323064565658569 batch: 671/840\n",
      "Batch loss: 0.7581493258476257 batch: 672/840\n",
      "Batch loss: 0.7815131545066833 batch: 673/840\n",
      "Batch loss: 0.9454973340034485 batch: 674/840\n",
      "Batch loss: 0.6966735124588013 batch: 675/840\n",
      "Batch loss: 0.5533939003944397 batch: 676/840\n",
      "Batch loss: 0.697333812713623 batch: 677/840\n",
      "Batch loss: 0.6656415462493896 batch: 678/840\n",
      "Batch loss: 0.8980386257171631 batch: 679/840\n",
      "Batch loss: 0.6960514187812805 batch: 680/840\n",
      "Batch loss: 0.8005968332290649 batch: 681/840\n",
      "Batch loss: 0.5882728695869446 batch: 682/840\n",
      "Batch loss: 0.6101349592208862 batch: 683/840\n",
      "Batch loss: 0.8223640322685242 batch: 684/840\n",
      "Batch loss: 0.6907307505607605 batch: 685/840\n",
      "Batch loss: 0.7665377259254456 batch: 686/840\n",
      "Batch loss: 0.7911548614501953 batch: 687/840\n",
      "Batch loss: 0.6055740714073181 batch: 688/840\n",
      "Batch loss: 0.6675022840499878 batch: 689/840\n",
      "Batch loss: 0.5757557153701782 batch: 690/840\n",
      "Batch loss: 0.7295191287994385 batch: 691/840\n",
      "Batch loss: 0.6915239691734314 batch: 692/840\n",
      "Batch loss: 0.6722008585929871 batch: 693/840\n",
      "Batch loss: 0.8716204762458801 batch: 694/840\n",
      "Batch loss: 0.7735043168067932 batch: 695/840\n",
      "Batch loss: 0.6455048322677612 batch: 696/840\n",
      "Batch loss: 0.6789077520370483 batch: 697/840\n",
      "Batch loss: 0.538398265838623 batch: 698/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7416943311691284 batch: 699/840\n",
      "Batch loss: 0.7269849181175232 batch: 700/840\n",
      "Batch loss: 0.8738850355148315 batch: 701/840\n",
      "Batch loss: 0.5809977054595947 batch: 702/840\n",
      "Batch loss: 0.5549842715263367 batch: 703/840\n",
      "Batch loss: 0.7913877964019775 batch: 704/840\n",
      "Batch loss: 0.6765732765197754 batch: 705/840\n",
      "Batch loss: 0.6729186177253723 batch: 706/840\n",
      "Batch loss: 0.6869041323661804 batch: 707/840\n",
      "Batch loss: 0.7430425882339478 batch: 708/840\n",
      "Batch loss: 0.7930179834365845 batch: 709/840\n",
      "Batch loss: 0.7631410360336304 batch: 710/840\n",
      "Batch loss: 0.49731120467185974 batch: 711/840\n",
      "Batch loss: 0.7682493329048157 batch: 712/840\n",
      "Batch loss: 0.631742000579834 batch: 713/840\n",
      "Batch loss: 0.6991836428642273 batch: 714/840\n",
      "Batch loss: 0.7876458168029785 batch: 715/840\n",
      "Batch loss: 0.7220051288604736 batch: 716/840\n",
      "Batch loss: 0.6779417991638184 batch: 717/840\n",
      "Batch loss: 0.6698852777481079 batch: 718/840\n",
      "Batch loss: 0.4665464460849762 batch: 719/840\n",
      "Batch loss: 0.5797619223594666 batch: 720/840\n",
      "Batch loss: 0.686551034450531 batch: 721/840\n",
      "Batch loss: 0.7253842353820801 batch: 722/840\n",
      "Batch loss: 0.4747060537338257 batch: 723/840\n",
      "Batch loss: 0.7137206792831421 batch: 724/840\n",
      "Batch loss: 0.6132251620292664 batch: 725/840\n",
      "Batch loss: 0.6037855744361877 batch: 726/840\n",
      "Batch loss: 0.7580405473709106 batch: 727/840\n",
      "Batch loss: 0.8051555156707764 batch: 728/840\n",
      "Batch loss: 0.6820836663246155 batch: 729/840\n",
      "Batch loss: 0.6699376702308655 batch: 730/840\n",
      "Batch loss: 0.49274981021881104 batch: 731/840\n",
      "Batch loss: 0.8201907873153687 batch: 732/840\n",
      "Batch loss: 0.6508485674858093 batch: 733/840\n",
      "Batch loss: 0.5950610041618347 batch: 734/840\n",
      "Batch loss: 0.6086148619651794 batch: 735/840\n",
      "Batch loss: 0.6237545609474182 batch: 736/840\n",
      "Batch loss: 0.7091502547264099 batch: 737/840\n",
      "Batch loss: 0.5579733848571777 batch: 738/840\n",
      "Batch loss: 0.7899843454360962 batch: 739/840\n",
      "Batch loss: 0.7430877089500427 batch: 740/840\n",
      "Batch loss: 0.5079784989356995 batch: 741/840\n",
      "Batch loss: 0.6320860385894775 batch: 742/840\n",
      "Batch loss: 0.5094702243804932 batch: 743/840\n",
      "Batch loss: 0.5973995327949524 batch: 744/840\n",
      "Batch loss: 0.6292659640312195 batch: 745/840\n",
      "Batch loss: 0.6283223032951355 batch: 746/840\n",
      "Batch loss: 0.7742183804512024 batch: 747/840\n",
      "Batch loss: 0.6718264818191528 batch: 748/840\n",
      "Batch loss: 0.6176334023475647 batch: 749/840\n",
      "Batch loss: 0.5105289816856384 batch: 750/840\n",
      "Batch loss: 0.7013963460922241 batch: 751/840\n",
      "Batch loss: 0.8588721752166748 batch: 752/840\n",
      "Batch loss: 0.6192128658294678 batch: 753/840\n",
      "Batch loss: 0.7119999527931213 batch: 754/840\n",
      "Batch loss: 0.6216949820518494 batch: 755/840\n",
      "Batch loss: 0.5210278630256653 batch: 756/840\n",
      "Batch loss: 0.8171290755271912 batch: 757/840\n",
      "Batch loss: 0.6592044830322266 batch: 758/840\n",
      "Batch loss: 0.6146625876426697 batch: 759/840\n",
      "Batch loss: 0.6084794402122498 batch: 760/840\n",
      "Batch loss: 0.587904155254364 batch: 761/840\n",
      "Batch loss: 0.7698978185653687 batch: 762/840\n",
      "Batch loss: 0.580777645111084 batch: 763/840\n",
      "Batch loss: 0.6972757577896118 batch: 764/840\n",
      "Batch loss: 0.48633238673210144 batch: 765/840\n",
      "Batch loss: 0.5135074853897095 batch: 766/840\n",
      "Batch loss: 0.7486589550971985 batch: 767/840\n",
      "Batch loss: 0.7837965488433838 batch: 768/840\n",
      "Batch loss: 0.7194850444793701 batch: 769/840\n",
      "Batch loss: 0.5945953726768494 batch: 770/840\n",
      "Batch loss: 0.6608965992927551 batch: 771/840\n",
      "Batch loss: 0.6659670472145081 batch: 772/840\n",
      "Batch loss: 0.5642642974853516 batch: 773/840\n",
      "Batch loss: 0.46213406324386597 batch: 774/840\n",
      "Batch loss: 0.5913911461830139 batch: 775/840\n",
      "Batch loss: 0.5541769862174988 batch: 776/840\n",
      "Batch loss: 0.5395271182060242 batch: 777/840\n",
      "Batch loss: 0.4791565239429474 batch: 778/840\n",
      "Batch loss: 0.7441255450248718 batch: 779/840\n",
      "Batch loss: 0.5659030675888062 batch: 780/840\n",
      "Batch loss: 0.770453691482544 batch: 781/840\n",
      "Batch loss: 0.6772854328155518 batch: 782/840\n",
      "Batch loss: 0.5207470059394836 batch: 783/840\n",
      "Batch loss: 0.7712628245353699 batch: 784/840\n",
      "Batch loss: 0.5758941173553467 batch: 785/840\n",
      "Batch loss: 0.7924472689628601 batch: 786/840\n",
      "Batch loss: 0.6358855962753296 batch: 787/840\n",
      "Batch loss: 0.7637870907783508 batch: 788/840\n",
      "Batch loss: 0.7705594897270203 batch: 789/840\n",
      "Batch loss: 0.7949985265731812 batch: 790/840\n",
      "Batch loss: 0.6804578304290771 batch: 791/840\n",
      "Batch loss: 0.4569440186023712 batch: 792/840\n",
      "Batch loss: 0.5852242708206177 batch: 793/840\n",
      "Batch loss: 0.6818675398826599 batch: 794/840\n",
      "Batch loss: 0.7391211986541748 batch: 795/840\n",
      "Batch loss: 0.6922296285629272 batch: 796/840\n",
      "Batch loss: 0.7755659222602844 batch: 797/840\n",
      "Batch loss: 0.6570724248886108 batch: 798/840\n",
      "Batch loss: 0.6466385722160339 batch: 799/840\n",
      "Batch loss: 0.5862010717391968 batch: 800/840\n",
      "Batch loss: 0.8760162591934204 batch: 801/840\n",
      "Batch loss: 0.48676013946533203 batch: 802/840\n",
      "Batch loss: 0.5511940121650696 batch: 803/840\n",
      "Batch loss: 0.6898212432861328 batch: 804/840\n",
      "Batch loss: 0.5580154657363892 batch: 805/840\n",
      "Batch loss: 0.6340872049331665 batch: 806/840\n",
      "Batch loss: 0.595282793045044 batch: 807/840\n",
      "Batch loss: 0.6412134766578674 batch: 808/840\n",
      "Batch loss: 0.6656831502914429 batch: 809/840\n",
      "Batch loss: 0.5509278774261475 batch: 810/840\n",
      "Batch loss: 0.5662599802017212 batch: 811/840\n",
      "Batch loss: 0.6546202301979065 batch: 812/840\n",
      "Batch loss: 0.5368027687072754 batch: 813/840\n",
      "Batch loss: 0.6969314813613892 batch: 814/840\n",
      "Batch loss: 0.6679428219795227 batch: 815/840\n",
      "Batch loss: 0.6834139823913574 batch: 816/840\n",
      "Batch loss: 0.6953280568122864 batch: 817/840\n",
      "Batch loss: 0.6403626799583435 batch: 818/840\n",
      "Batch loss: 0.5460317730903625 batch: 819/840\n",
      "Batch loss: 0.6920501589775085 batch: 820/840\n",
      "Batch loss: 0.6386290788650513 batch: 821/840\n",
      "Batch loss: 0.6356033086776733 batch: 822/840\n",
      "Batch loss: 0.8008231520652771 batch: 823/840\n",
      "Batch loss: 0.7812426686286926 batch: 824/840\n",
      "Batch loss: 0.6338187456130981 batch: 825/840\n",
      "Batch loss: 0.5532727241516113 batch: 826/840\n",
      "Batch loss: 0.5776223540306091 batch: 827/840\n",
      "Batch loss: 0.7155188918113708 batch: 828/840\n",
      "Batch loss: 0.6463446021080017 batch: 829/840\n",
      "Batch loss: 0.6782060861587524 batch: 830/840\n",
      "Batch loss: 0.6075801253318787 batch: 831/840\n",
      "Batch loss: 0.7342447638511658 batch: 832/840\n",
      "Batch loss: 0.710483193397522 batch: 833/840\n",
      "Batch loss: 0.5782282948493958 batch: 834/840\n",
      "Batch loss: 0.6080584526062012 batch: 835/840\n",
      "Batch loss: 0.6462249159812927 batch: 836/840\n",
      "Batch loss: 0.6766570210456848 batch: 837/840\n",
      "Batch loss: 0.7251611351966858 batch: 838/840\n",
      "Batch loss: 0.6520530581474304 batch: 839/840\n",
      "Batch loss: 0.6374151706695557 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 4/15..  Training Loss: 0.007..  Test Loss: 0.005..  Test Accuracy: 0.808\n",
      "Running epoch 5/15\n",
      "Batch loss: 0.5576475858688354 batch: 1/840\n",
      "Batch loss: 1.074262261390686 batch: 2/840\n",
      "Batch loss: 0.6981188058853149 batch: 3/840\n",
      "Batch loss: 0.6922661662101746 batch: 4/840\n",
      "Batch loss: 0.6539745330810547 batch: 5/840\n",
      "Batch loss: 0.546034038066864 batch: 6/840\n",
      "Batch loss: 0.5279750823974609 batch: 7/840\n",
      "Batch loss: 0.7277286648750305 batch: 8/840\n",
      "Batch loss: 0.6246364712715149 batch: 9/840\n",
      "Batch loss: 0.5952658653259277 batch: 10/840\n",
      "Batch loss: 0.6103188395500183 batch: 11/840\n",
      "Batch loss: 0.6054365038871765 batch: 12/840\n",
      "Batch loss: 0.5704494714736938 batch: 13/840\n",
      "Batch loss: 0.6569434404373169 batch: 14/840\n",
      "Batch loss: 0.5896061658859253 batch: 15/840\n",
      "Batch loss: 0.5325726270675659 batch: 16/840\n",
      "Batch loss: 0.5576989650726318 batch: 17/840\n",
      "Batch loss: 0.7444626092910767 batch: 18/840\n",
      "Batch loss: 0.7163824439048767 batch: 19/840\n",
      "Batch loss: 0.7107425928115845 batch: 20/840\n",
      "Batch loss: 0.7581619024276733 batch: 21/840\n",
      "Batch loss: 0.5811477899551392 batch: 22/840\n",
      "Batch loss: 0.6124337315559387 batch: 23/840\n",
      "Batch loss: 0.5542964935302734 batch: 24/840\n",
      "Batch loss: 0.5197584629058838 batch: 25/840\n",
      "Batch loss: 0.7116819024085999 batch: 26/840\n",
      "Batch loss: 0.7311483025550842 batch: 27/840\n",
      "Batch loss: 0.6755580306053162 batch: 28/840\n",
      "Batch loss: 0.6410043239593506 batch: 29/840\n",
      "Batch loss: 0.6132904291152954 batch: 30/840\n",
      "Batch loss: 0.5892517566680908 batch: 31/840\n",
      "Batch loss: 0.637982964515686 batch: 32/840\n",
      "Batch loss: 0.7332155704498291 batch: 33/840\n",
      "Batch loss: 0.5467442870140076 batch: 34/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5553137063980103 batch: 35/840\n",
      "Batch loss: 0.547267735004425 batch: 36/840\n",
      "Batch loss: 0.6446153521537781 batch: 37/840\n",
      "Batch loss: 0.7532804608345032 batch: 38/840\n",
      "Batch loss: 0.6299211382865906 batch: 39/840\n",
      "Batch loss: 0.5711548328399658 batch: 40/840\n",
      "Batch loss: 0.7376341819763184 batch: 41/840\n",
      "Batch loss: 0.7272316813468933 batch: 42/840\n",
      "Batch loss: 0.5959916114807129 batch: 43/840\n",
      "Batch loss: 0.6881881952285767 batch: 44/840\n",
      "Batch loss: 0.6869748830795288 batch: 45/840\n",
      "Batch loss: 0.5083778500556946 batch: 46/840\n",
      "Batch loss: 0.549892783164978 batch: 47/840\n",
      "Batch loss: 0.6762640476226807 batch: 48/840\n",
      "Batch loss: 0.6384792923927307 batch: 49/840\n",
      "Batch loss: 0.684328019618988 batch: 50/840\n",
      "Batch loss: 0.7534106969833374 batch: 51/840\n",
      "Batch loss: 0.7040141820907593 batch: 52/840\n",
      "Batch loss: 0.5750580430030823 batch: 53/840\n",
      "Batch loss: 0.592047929763794 batch: 54/840\n",
      "Batch loss: 0.594737708568573 batch: 55/840\n",
      "Batch loss: 0.5907946228981018 batch: 56/840\n",
      "Batch loss: 0.7689990401268005 batch: 57/840\n",
      "Batch loss: 0.5694923996925354 batch: 58/840\n",
      "Batch loss: 0.6231485605239868 batch: 59/840\n",
      "Batch loss: 0.5746103525161743 batch: 60/840\n",
      "Batch loss: 0.9177908301353455 batch: 61/840\n",
      "Batch loss: 0.7709454298019409 batch: 62/840\n",
      "Batch loss: 0.6278478503227234 batch: 63/840\n",
      "Batch loss: 0.6551820635795593 batch: 64/840\n",
      "Batch loss: 0.5979943871498108 batch: 65/840\n",
      "Batch loss: 0.6494338512420654 batch: 66/840\n",
      "Batch loss: 0.7106356620788574 batch: 67/840\n",
      "Batch loss: 0.7021384239196777 batch: 68/840\n",
      "Batch loss: 0.7588775157928467 batch: 69/840\n",
      "Batch loss: 0.6165828704833984 batch: 70/840\n",
      "Batch loss: 0.743045449256897 batch: 71/840\n",
      "Batch loss: 0.8794558048248291 batch: 72/840\n",
      "Batch loss: 0.8066565990447998 batch: 73/840\n",
      "Batch loss: 0.6123185753822327 batch: 74/840\n",
      "Batch loss: 0.689664900302887 batch: 75/840\n",
      "Batch loss: 0.44176480174064636 batch: 76/840\n",
      "Batch loss: 0.6079925298690796 batch: 77/840\n",
      "Batch loss: 0.8811651468276978 batch: 78/840\n",
      "Batch loss: 0.6253269910812378 batch: 79/840\n",
      "Batch loss: 0.7569841146469116 batch: 80/840\n",
      "Batch loss: 0.5802474021911621 batch: 81/840\n",
      "Batch loss: 0.7786105871200562 batch: 82/840\n",
      "Batch loss: 0.509421706199646 batch: 83/840\n",
      "Batch loss: 0.7087010145187378 batch: 84/840\n",
      "Batch loss: 0.6845076084136963 batch: 85/840\n",
      "Batch loss: 1.073310136795044 batch: 86/840\n",
      "Batch loss: 0.5010038614273071 batch: 87/840\n",
      "Batch loss: 0.515251636505127 batch: 88/840\n",
      "Batch loss: 0.5228665471076965 batch: 89/840\n",
      "Batch loss: 0.559683620929718 batch: 90/840\n",
      "Batch loss: 0.6129360198974609 batch: 91/840\n",
      "Batch loss: 0.6156232953071594 batch: 92/840\n",
      "Batch loss: 0.7305769324302673 batch: 93/840\n",
      "Batch loss: 0.5734337568283081 batch: 94/840\n",
      "Batch loss: 0.6511157155036926 batch: 95/840\n",
      "Batch loss: 0.6177282929420471 batch: 96/840\n",
      "Batch loss: 0.6712436676025391 batch: 97/840\n",
      "Batch loss: 0.7941197752952576 batch: 98/840\n",
      "Batch loss: 0.6259621381759644 batch: 99/840\n",
      "Batch loss: 0.6717895269393921 batch: 100/840\n",
      "Batch loss: 0.5275867581367493 batch: 101/840\n",
      "Batch loss: 0.4691292941570282 batch: 102/840\n",
      "Batch loss: 0.6344815492630005 batch: 103/840\n",
      "Batch loss: 0.5425907373428345 batch: 104/840\n",
      "Batch loss: 0.5735757946968079 batch: 105/840\n",
      "Batch loss: 0.713043749332428 batch: 106/840\n",
      "Batch loss: 0.5170965790748596 batch: 107/840\n",
      "Batch loss: 0.7519557476043701 batch: 108/840\n",
      "Batch loss: 0.6864544153213501 batch: 109/840\n",
      "Batch loss: 0.5048671960830688 batch: 110/840\n",
      "Batch loss: 0.4831468164920807 batch: 111/840\n",
      "Batch loss: 0.7485452890396118 batch: 112/840\n",
      "Batch loss: 0.723965048789978 batch: 113/840\n",
      "Batch loss: 0.6297357082366943 batch: 114/840\n",
      "Batch loss: 0.7012079358100891 batch: 115/840\n",
      "Batch loss: 0.6029764413833618 batch: 116/840\n",
      "Batch loss: 0.6577638983726501 batch: 117/840\n",
      "Batch loss: 0.45536187291145325 batch: 118/840\n",
      "Batch loss: 0.6930286884307861 batch: 119/840\n",
      "Batch loss: 0.5414382219314575 batch: 120/840\n",
      "Batch loss: 0.7517291903495789 batch: 121/840\n",
      "Batch loss: 0.7465444207191467 batch: 122/840\n",
      "Batch loss: 0.5217483043670654 batch: 123/840\n",
      "Batch loss: 0.6004552841186523 batch: 124/840\n",
      "Batch loss: 0.529574990272522 batch: 125/840\n",
      "Batch loss: 0.7196484208106995 batch: 126/840\n",
      "Batch loss: 0.7110944986343384 batch: 127/840\n",
      "Batch loss: 0.7073361277580261 batch: 128/840\n",
      "Batch loss: 0.7430258393287659 batch: 129/840\n",
      "Batch loss: 0.6065340042114258 batch: 130/840\n",
      "Batch loss: 0.7489368915557861 batch: 131/840\n",
      "Batch loss: 1.2400082349777222 batch: 132/840\n",
      "Batch loss: 0.7948396801948547 batch: 133/840\n",
      "Batch loss: 0.6250574588775635 batch: 134/840\n",
      "Batch loss: 0.6024020314216614 batch: 135/840\n",
      "Batch loss: 0.7394249439239502 batch: 136/840\n",
      "Batch loss: 0.6317024827003479 batch: 137/840\n",
      "Batch loss: 0.616268515586853 batch: 138/840\n",
      "Batch loss: 0.561343789100647 batch: 139/840\n",
      "Batch loss: 0.6672735810279846 batch: 140/840\n",
      "Batch loss: 0.5652211904525757 batch: 141/840\n",
      "Batch loss: 0.580367386341095 batch: 142/840\n",
      "Batch loss: 0.6511909365653992 batch: 143/840\n",
      "Batch loss: 0.5841090679168701 batch: 144/840\n",
      "Batch loss: 0.8576616048812866 batch: 145/840\n",
      "Batch loss: 0.7964657545089722 batch: 146/840\n",
      "Batch loss: 0.5824859738349915 batch: 147/840\n",
      "Batch loss: 0.8177032470703125 batch: 148/840\n",
      "Batch loss: 0.6693203449249268 batch: 149/840\n",
      "Batch loss: 0.6434007883071899 batch: 150/840\n",
      "Batch loss: 0.589569628238678 batch: 151/840\n",
      "Batch loss: 0.666158139705658 batch: 152/840\n",
      "Batch loss: 0.5803529024124146 batch: 153/840\n",
      "Batch loss: 0.6771237850189209 batch: 154/840\n",
      "Batch loss: 0.6030092239379883 batch: 155/840\n",
      "Batch loss: 0.7055261135101318 batch: 156/840\n",
      "Batch loss: 0.6163502335548401 batch: 157/840\n",
      "Batch loss: 0.548058032989502 batch: 158/840\n",
      "Batch loss: 0.5561482906341553 batch: 159/840\n",
      "Batch loss: 0.6358674168586731 batch: 160/840\n",
      "Batch loss: 0.6942718625068665 batch: 161/840\n",
      "Batch loss: 0.6772263050079346 batch: 162/840\n",
      "Batch loss: 0.631008505821228 batch: 163/840\n",
      "Batch loss: 0.47859272360801697 batch: 164/840\n",
      "Batch loss: 0.6941531896591187 batch: 165/840\n",
      "Batch loss: 0.5255507230758667 batch: 166/840\n",
      "Batch loss: 0.6758455038070679 batch: 167/840\n",
      "Batch loss: 0.6516655087471008 batch: 168/840\n",
      "Batch loss: 0.5743560791015625 batch: 169/840\n",
      "Batch loss: 0.7944241166114807 batch: 170/840\n",
      "Batch loss: 0.6507737636566162 batch: 171/840\n",
      "Batch loss: 0.8338829278945923 batch: 172/840\n",
      "Batch loss: 0.6452392339706421 batch: 173/840\n",
      "Batch loss: 0.5892031788825989 batch: 174/840\n",
      "Batch loss: 0.6079529523849487 batch: 175/840\n",
      "Batch loss: 0.820544421672821 batch: 176/840\n",
      "Batch loss: 0.6911842823028564 batch: 177/840\n",
      "Batch loss: 0.7651296854019165 batch: 178/840\n",
      "Batch loss: 0.6293900012969971 batch: 179/840\n",
      "Batch loss: 0.5783495903015137 batch: 180/840\n",
      "Batch loss: 0.5527352690696716 batch: 181/840\n",
      "Batch loss: 0.664518415927887 batch: 182/840\n",
      "Batch loss: 0.5770166516304016 batch: 183/840\n",
      "Batch loss: 0.5790047645568848 batch: 184/840\n",
      "Batch loss: 0.45561301708221436 batch: 185/840\n",
      "Batch loss: 0.6255784034729004 batch: 186/840\n",
      "Batch loss: 0.6515230536460876 batch: 187/840\n",
      "Batch loss: 0.6455799341201782 batch: 188/840\n",
      "Batch loss: 0.5752015709877014 batch: 189/840\n",
      "Batch loss: 0.6854360699653625 batch: 190/840\n",
      "Batch loss: 0.7691418528556824 batch: 191/840\n",
      "Batch loss: 0.6120997071266174 batch: 192/840\n",
      "Batch loss: 0.5123757719993591 batch: 193/840\n",
      "Batch loss: 0.358018696308136 batch: 194/840\n",
      "Batch loss: 0.6162603497505188 batch: 195/840\n",
      "Batch loss: 0.6912705898284912 batch: 196/840\n",
      "Batch loss: 0.6346877813339233 batch: 197/840\n",
      "Batch loss: 0.5439764261245728 batch: 198/840\n",
      "Batch loss: 0.6377611756324768 batch: 199/840\n",
      "Batch loss: 0.8281884789466858 batch: 200/840\n",
      "Batch loss: 0.6046531200408936 batch: 201/840\n",
      "Batch loss: 0.6428617835044861 batch: 202/840\n",
      "Batch loss: 0.5386370420455933 batch: 203/840\n",
      "Batch loss: 0.7215172052383423 batch: 204/840\n",
      "Batch loss: 0.7303135395050049 batch: 205/840\n",
      "Batch loss: 0.6996253728866577 batch: 206/840\n",
      "Batch loss: 0.6546691060066223 batch: 207/840\n",
      "Batch loss: 0.623607873916626 batch: 208/840\n",
      "Batch loss: 0.6184679865837097 batch: 209/840\n",
      "Batch loss: 0.6253526210784912 batch: 210/840\n",
      "Batch loss: 0.4598645865917206 batch: 211/840\n",
      "Batch loss: 0.5733277797698975 batch: 212/840\n",
      "Batch loss: 0.750885009765625 batch: 213/840\n",
      "Batch loss: 0.7845690250396729 batch: 214/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6859768629074097 batch: 215/840\n",
      "Batch loss: 0.6991797089576721 batch: 216/840\n",
      "Batch loss: 0.6782758235931396 batch: 217/840\n",
      "Batch loss: 0.6809844374656677 batch: 218/840\n",
      "Batch loss: 0.6858030557632446 batch: 219/840\n",
      "Batch loss: 0.9294419288635254 batch: 220/840\n",
      "Batch loss: 0.6737663745880127 batch: 221/840\n",
      "Batch loss: 0.6714615821838379 batch: 222/840\n",
      "Batch loss: 0.5512919425964355 batch: 223/840\n",
      "Batch loss: 0.8526037335395813 batch: 224/840\n",
      "Batch loss: 0.6670998334884644 batch: 225/840\n",
      "Batch loss: 0.7660945057868958 batch: 226/840\n",
      "Batch loss: 0.8043345808982849 batch: 227/840\n",
      "Batch loss: 0.5440222024917603 batch: 228/840\n",
      "Batch loss: 0.5156587362289429 batch: 229/840\n",
      "Batch loss: 0.6406474113464355 batch: 230/840\n",
      "Batch loss: 0.5344092845916748 batch: 231/840\n",
      "Batch loss: 0.6147727966308594 batch: 232/840\n",
      "Batch loss: 0.6722660660743713 batch: 233/840\n",
      "Batch loss: 0.7668278217315674 batch: 234/840\n",
      "Batch loss: 0.6226369738578796 batch: 235/840\n",
      "Batch loss: 0.7421488165855408 batch: 236/840\n",
      "Batch loss: 0.5057778358459473 batch: 237/840\n",
      "Batch loss: 0.6895812749862671 batch: 238/840\n",
      "Batch loss: 0.7362169623374939 batch: 239/840\n",
      "Batch loss: 0.7110412120819092 batch: 240/840\n",
      "Batch loss: 0.7355815172195435 batch: 241/840\n",
      "Batch loss: 0.6617482900619507 batch: 242/840\n",
      "Batch loss: 0.6881502270698547 batch: 243/840\n",
      "Batch loss: 0.8242912888526917 batch: 244/840\n",
      "Batch loss: 0.48784512281417847 batch: 245/840\n",
      "Batch loss: 0.5821460485458374 batch: 246/840\n",
      "Batch loss: 0.7768516540527344 batch: 247/840\n",
      "Batch loss: 0.7160273790359497 batch: 248/840\n",
      "Batch loss: 0.8285505175590515 batch: 249/840\n",
      "Batch loss: 0.5355020761489868 batch: 250/840\n",
      "Batch loss: 0.6471255421638489 batch: 251/840\n",
      "Batch loss: 0.5714888572692871 batch: 252/840\n",
      "Batch loss: 0.6793000102043152 batch: 253/840\n",
      "Batch loss: 0.7373387813568115 batch: 254/840\n",
      "Batch loss: 0.6297116875648499 batch: 255/840\n",
      "Batch loss: 0.7117129564285278 batch: 256/840\n",
      "Batch loss: 0.5785542130470276 batch: 257/840\n",
      "Batch loss: 0.7700614929199219 batch: 258/840\n",
      "Batch loss: 0.6329227685928345 batch: 259/840\n",
      "Batch loss: 0.5555452108383179 batch: 260/840\n",
      "Batch loss: 0.5994978547096252 batch: 261/840\n",
      "Batch loss: 0.49735501408576965 batch: 262/840\n",
      "Batch loss: 0.5722049474716187 batch: 263/840\n",
      "Batch loss: 0.5470601916313171 batch: 264/840\n",
      "Batch loss: 0.7142077684402466 batch: 265/840\n",
      "Batch loss: 0.5464870929718018 batch: 266/840\n",
      "Batch loss: 0.6878505945205688 batch: 267/840\n",
      "Batch loss: 0.6323967576026917 batch: 268/840\n",
      "Batch loss: 0.5273112654685974 batch: 269/840\n",
      "Batch loss: 0.6090486645698547 batch: 270/840\n",
      "Batch loss: 0.5991947650909424 batch: 271/840\n",
      "Batch loss: 0.76247239112854 batch: 272/840\n",
      "Batch loss: 0.714867115020752 batch: 273/840\n",
      "Batch loss: 0.6649771332740784 batch: 274/840\n",
      "Batch loss: 0.7666789293289185 batch: 275/840\n",
      "Batch loss: 0.504416286945343 batch: 276/840\n",
      "Batch loss: 0.5585353374481201 batch: 277/840\n",
      "Batch loss: 0.7741764187812805 batch: 278/840\n",
      "Batch loss: 0.8201912045478821 batch: 279/840\n",
      "Batch loss: 0.8722261786460876 batch: 280/840\n",
      "Batch loss: 0.5630881786346436 batch: 281/840\n",
      "Batch loss: 0.6519677639007568 batch: 282/840\n",
      "Batch loss: 0.7500597238540649 batch: 283/840\n",
      "Batch loss: 0.44188567996025085 batch: 284/840\n",
      "Batch loss: 0.5449387431144714 batch: 285/840\n",
      "Batch loss: 0.5959506034851074 batch: 286/840\n",
      "Batch loss: 0.4638402462005615 batch: 287/840\n",
      "Batch loss: 0.5986448526382446 batch: 288/840\n",
      "Batch loss: 0.8791549801826477 batch: 289/840\n",
      "Batch loss: 0.857506275177002 batch: 290/840\n",
      "Batch loss: 0.8062984347343445 batch: 291/840\n",
      "Batch loss: 0.6024148464202881 batch: 292/840\n",
      "Batch loss: 0.7464478015899658 batch: 293/840\n",
      "Batch loss: 0.5940485000610352 batch: 294/840\n",
      "Batch loss: 0.5630688071250916 batch: 295/840\n",
      "Batch loss: 0.7467426061630249 batch: 296/840\n",
      "Batch loss: 0.6360602378845215 batch: 297/840\n",
      "Batch loss: 0.6847087144851685 batch: 298/840\n",
      "Batch loss: 0.601772665977478 batch: 299/840\n",
      "Batch loss: 0.8472728729248047 batch: 300/840\n",
      "Batch loss: 0.7013434767723083 batch: 301/840\n",
      "Batch loss: 0.6760920882225037 batch: 302/840\n",
      "Batch loss: 0.7635756134986877 batch: 303/840\n",
      "Batch loss: 0.6069025993347168 batch: 304/840\n",
      "Batch loss: 0.5682925581932068 batch: 305/840\n",
      "Batch loss: 0.7371788024902344 batch: 306/840\n",
      "Batch loss: 0.48096588253974915 batch: 307/840\n",
      "Batch loss: 0.6815482378005981 batch: 308/840\n",
      "Batch loss: 0.6174023747444153 batch: 309/840\n",
      "Batch loss: 0.9007840752601624 batch: 310/840\n",
      "Batch loss: 0.7003787755966187 batch: 311/840\n",
      "Batch loss: 0.7009822130203247 batch: 312/840\n",
      "Batch loss: 0.7701127529144287 batch: 313/840\n",
      "Batch loss: 0.5385069251060486 batch: 314/840\n",
      "Batch loss: 0.661048412322998 batch: 315/840\n",
      "Batch loss: 0.4934290945529938 batch: 316/840\n",
      "Batch loss: 0.7934315204620361 batch: 317/840\n",
      "Batch loss: 0.6352955102920532 batch: 318/840\n",
      "Batch loss: 0.7604313492774963 batch: 319/840\n",
      "Batch loss: 0.6594257950782776 batch: 320/840\n",
      "Batch loss: 0.6543107032775879 batch: 321/840\n",
      "Batch loss: 0.762830376625061 batch: 322/840\n",
      "Batch loss: 0.7330037951469421 batch: 323/840\n",
      "Batch loss: 0.754243791103363 batch: 324/840\n",
      "Batch loss: 0.5646869540214539 batch: 325/840\n",
      "Batch loss: 0.630785346031189 batch: 326/840\n",
      "Batch loss: 0.5567302107810974 batch: 327/840\n",
      "Batch loss: 0.8167403340339661 batch: 328/840\n",
      "Batch loss: 0.7240191698074341 batch: 329/840\n",
      "Batch loss: 0.6609154343605042 batch: 330/840\n",
      "Batch loss: 0.7052819728851318 batch: 331/840\n",
      "Batch loss: 0.6556170582771301 batch: 332/840\n",
      "Batch loss: 0.5739328861236572 batch: 333/840\n",
      "Batch loss: 0.7367759943008423 batch: 334/840\n",
      "Batch loss: 0.6022569537162781 batch: 335/840\n",
      "Batch loss: 0.7285261750221252 batch: 336/840\n",
      "Batch loss: 0.7997615933418274 batch: 337/840\n",
      "Batch loss: 0.7348971366882324 batch: 338/840\n",
      "Batch loss: 0.6575078368186951 batch: 339/840\n",
      "Batch loss: 0.8188313841819763 batch: 340/840\n",
      "Batch loss: 0.6368504166603088 batch: 341/840\n",
      "Batch loss: 0.5135520696640015 batch: 342/840\n",
      "Batch loss: 0.9031776189804077 batch: 343/840\n",
      "Batch loss: 0.6263133883476257 batch: 344/840\n",
      "Batch loss: 0.4622958302497864 batch: 345/840\n",
      "Batch loss: 0.6503222584724426 batch: 346/840\n",
      "Batch loss: 0.7769143581390381 batch: 347/840\n",
      "Batch loss: 0.635028600692749 batch: 348/840\n",
      "Batch loss: 0.5713695287704468 batch: 349/840\n",
      "Batch loss: 0.5348128080368042 batch: 350/840\n",
      "Batch loss: 0.6759195923805237 batch: 351/840\n",
      "Batch loss: 0.6662173271179199 batch: 352/840\n",
      "Batch loss: 0.6864722371101379 batch: 353/840\n",
      "Batch loss: 0.6685733199119568 batch: 354/840\n",
      "Batch loss: 0.586560845375061 batch: 355/840\n",
      "Batch loss: 0.5979289412498474 batch: 356/840\n",
      "Batch loss: 0.5707971453666687 batch: 357/840\n",
      "Batch loss: 0.897739589214325 batch: 358/840\n",
      "Batch loss: 0.5092233419418335 batch: 359/840\n",
      "Batch loss: 0.8856388926506042 batch: 360/840\n",
      "Batch loss: 0.6526398658752441 batch: 361/840\n",
      "Batch loss: 0.5912660360336304 batch: 362/840\n",
      "Batch loss: 0.5864055752754211 batch: 363/840\n",
      "Batch loss: 0.758846640586853 batch: 364/840\n",
      "Batch loss: 0.592460036277771 batch: 365/840\n",
      "Batch loss: 0.7225911617279053 batch: 366/840\n",
      "Batch loss: 0.39174267649650574 batch: 367/840\n",
      "Batch loss: 0.862601637840271 batch: 368/840\n",
      "Batch loss: 0.6040643453598022 batch: 369/840\n",
      "Batch loss: 0.7196823358535767 batch: 370/840\n",
      "Batch loss: 0.6135584712028503 batch: 371/840\n",
      "Batch loss: 0.4689926207065582 batch: 372/840\n",
      "Batch loss: 0.7290188670158386 batch: 373/840\n",
      "Batch loss: 0.6928699016571045 batch: 374/840\n",
      "Batch loss: 0.6088442206382751 batch: 375/840\n",
      "Batch loss: 0.4690110385417938 batch: 376/840\n",
      "Batch loss: 0.6695973873138428 batch: 377/840\n",
      "Batch loss: 0.6590074300765991 batch: 378/840\n",
      "Batch loss: 0.5517866015434265 batch: 379/840\n",
      "Batch loss: 0.8439856767654419 batch: 380/840\n",
      "Batch loss: 0.9578734040260315 batch: 381/840\n",
      "Batch loss: 0.7931050062179565 batch: 382/840\n",
      "Batch loss: 0.7174583673477173 batch: 383/840\n",
      "Batch loss: 0.6219635605812073 batch: 384/840\n",
      "Batch loss: 0.7189337015151978 batch: 385/840\n",
      "Batch loss: 0.6761101484298706 batch: 386/840\n",
      "Batch loss: 0.6056203842163086 batch: 387/840\n",
      "Batch loss: 0.7709102630615234 batch: 388/840\n",
      "Batch loss: 0.5904181003570557 batch: 389/840\n",
      "Batch loss: 0.8718942999839783 batch: 390/840\n",
      "Batch loss: 0.6477551460266113 batch: 391/840\n",
      "Batch loss: 0.6224398612976074 batch: 392/840\n",
      "Batch loss: 0.438108891248703 batch: 393/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7345939874649048 batch: 394/840\n",
      "Batch loss: 0.5910273194313049 batch: 395/840\n",
      "Batch loss: 0.6479747891426086 batch: 396/840\n",
      "Batch loss: 0.603110671043396 batch: 397/840\n",
      "Batch loss: 0.7764988541603088 batch: 398/840\n",
      "Batch loss: 0.4449108839035034 batch: 399/840\n",
      "Batch loss: 0.6228424906730652 batch: 400/840\n",
      "Batch loss: 0.652009129524231 batch: 401/840\n",
      "Batch loss: 0.5601206421852112 batch: 402/840\n",
      "Batch loss: 0.644626259803772 batch: 403/840\n",
      "Batch loss: 0.6018016338348389 batch: 404/840\n",
      "Batch loss: 0.537771999835968 batch: 405/840\n",
      "Batch loss: 0.6725877523422241 batch: 406/840\n",
      "Batch loss: 0.5965656638145447 batch: 407/840\n",
      "Batch loss: 0.7520586252212524 batch: 408/840\n",
      "Batch loss: 0.6969004273414612 batch: 409/840\n",
      "Batch loss: 0.7321940064430237 batch: 410/840\n",
      "Batch loss: 0.7318657040596008 batch: 411/840\n",
      "Batch loss: 0.8395887017250061 batch: 412/840\n",
      "Batch loss: 0.7033280730247498 batch: 413/840\n",
      "Batch loss: 0.6510676741600037 batch: 414/840\n",
      "Batch loss: 0.9329430460929871 batch: 415/840\n",
      "Batch loss: 0.5061418414115906 batch: 416/840\n",
      "Batch loss: 0.6612134575843811 batch: 417/840\n",
      "Batch loss: 0.846304714679718 batch: 418/840\n",
      "Batch loss: 0.6679781079292297 batch: 419/840\n",
      "Batch loss: 0.6951597332954407 batch: 420/840\n",
      "Batch loss: 0.49440306425094604 batch: 421/840\n",
      "Batch loss: 0.6148686408996582 batch: 422/840\n",
      "Batch loss: 0.6375332474708557 batch: 423/840\n",
      "Batch loss: 0.6952133774757385 batch: 424/840\n",
      "Batch loss: 0.7508761882781982 batch: 425/840\n",
      "Batch loss: 0.6256619691848755 batch: 426/840\n",
      "Batch loss: 0.7383155822753906 batch: 427/840\n",
      "Batch loss: 0.7602963447570801 batch: 428/840\n",
      "Batch loss: 0.5665620565414429 batch: 429/840\n",
      "Batch loss: 0.8263814449310303 batch: 430/840\n",
      "Batch loss: 0.6407334804534912 batch: 431/840\n",
      "Batch loss: 0.6774618029594421 batch: 432/840\n",
      "Batch loss: 0.45680728554725647 batch: 433/840\n",
      "Batch loss: 0.6031068563461304 batch: 434/840\n",
      "Batch loss: 0.7566065192222595 batch: 435/840\n",
      "Batch loss: 0.6200162768363953 batch: 436/840\n",
      "Batch loss: 0.7079839110374451 batch: 437/840\n",
      "Batch loss: 0.3768993318080902 batch: 438/840\n",
      "Batch loss: 0.6766345500946045 batch: 439/840\n",
      "Batch loss: 0.7866652607917786 batch: 440/840\n",
      "Batch loss: 0.6179479956626892 batch: 441/840\n",
      "Batch loss: 0.7539970278739929 batch: 442/840\n",
      "Batch loss: 0.5933376550674438 batch: 443/840\n",
      "Batch loss: 0.677663266658783 batch: 444/840\n",
      "Batch loss: 0.5861918330192566 batch: 445/840\n",
      "Batch loss: 0.7473076581954956 batch: 446/840\n",
      "Batch loss: 0.7076768279075623 batch: 447/840\n",
      "Batch loss: 0.7721246480941772 batch: 448/840\n",
      "Batch loss: 0.5052064657211304 batch: 449/840\n",
      "Batch loss: 0.6550821661949158 batch: 450/840\n",
      "Batch loss: 0.7781834602355957 batch: 451/840\n",
      "Batch loss: 0.7661043405532837 batch: 452/840\n",
      "Batch loss: 0.6458780169487 batch: 453/840\n",
      "Batch loss: 0.659739077091217 batch: 454/840\n",
      "Batch loss: 0.6450521349906921 batch: 455/840\n",
      "Batch loss: 0.732283353805542 batch: 456/840\n",
      "Batch loss: 0.6584355235099792 batch: 457/840\n",
      "Batch loss: 0.6029708385467529 batch: 458/840\n",
      "Batch loss: 0.5894356966018677 batch: 459/840\n",
      "Batch loss: 0.4937534034252167 batch: 460/840\n",
      "Batch loss: 0.7201312780380249 batch: 461/840\n",
      "Batch loss: 0.6917359232902527 batch: 462/840\n",
      "Batch loss: 0.8034558296203613 batch: 463/840\n",
      "Batch loss: 0.5298892855644226 batch: 464/840\n",
      "Batch loss: 0.9105015397071838 batch: 465/840\n",
      "Batch loss: 0.6682237982749939 batch: 466/840\n",
      "Batch loss: 0.9936187863349915 batch: 467/840\n",
      "Batch loss: 0.5602394342422485 batch: 468/840\n",
      "Batch loss: 0.5810866951942444 batch: 469/840\n",
      "Batch loss: 0.6117379665374756 batch: 470/840\n",
      "Batch loss: 0.6299469470977783 batch: 471/840\n",
      "Batch loss: 0.7776394486427307 batch: 472/840\n",
      "Batch loss: 0.7176315784454346 batch: 473/840\n",
      "Batch loss: 0.5882217288017273 batch: 474/840\n",
      "Batch loss: 0.6279010772705078 batch: 475/840\n",
      "Batch loss: 0.6661180853843689 batch: 476/840\n",
      "Batch loss: 0.5459529757499695 batch: 477/840\n",
      "Batch loss: 0.5988069772720337 batch: 478/840\n",
      "Batch loss: 0.5918056964874268 batch: 479/840\n",
      "Batch loss: 0.7102453708648682 batch: 480/840\n",
      "Batch loss: 0.6611930727958679 batch: 481/840\n",
      "Batch loss: 0.6722288727760315 batch: 482/840\n",
      "Batch loss: 0.8515562415122986 batch: 483/840\n",
      "Batch loss: 0.5675894618034363 batch: 484/840\n",
      "Batch loss: 0.6471593976020813 batch: 485/840\n",
      "Batch loss: 0.6298452615737915 batch: 486/840\n",
      "Batch loss: 0.4034357964992523 batch: 487/840\n",
      "Batch loss: 0.6890853643417358 batch: 488/840\n",
      "Batch loss: 0.6274408102035522 batch: 489/840\n",
      "Batch loss: 0.6845178008079529 batch: 490/840\n",
      "Batch loss: 0.3959035873413086 batch: 491/840\n",
      "Batch loss: 0.8939723968505859 batch: 492/840\n",
      "Batch loss: 0.6868401169776917 batch: 493/840\n",
      "Batch loss: 0.41647475957870483 batch: 494/840\n",
      "Batch loss: 0.7548953890800476 batch: 495/840\n",
      "Batch loss: 0.6442213654518127 batch: 496/840\n",
      "Batch loss: 0.7600091695785522 batch: 497/840\n",
      "Batch loss: 0.5527200102806091 batch: 498/840\n",
      "Batch loss: 0.7675630450248718 batch: 499/840\n",
      "Batch loss: 0.6371462345123291 batch: 500/840\n",
      "Batch loss: 0.585002601146698 batch: 501/840\n",
      "Batch loss: 0.7552800178527832 batch: 502/840\n",
      "Batch loss: 0.5067822933197021 batch: 503/840\n",
      "Batch loss: 0.6874606609344482 batch: 504/840\n",
      "Batch loss: 0.7343412041664124 batch: 505/840\n",
      "Batch loss: 0.6691672801971436 batch: 506/840\n",
      "Batch loss: 0.8315305113792419 batch: 507/840\n",
      "Batch loss: 0.5528198480606079 batch: 508/840\n",
      "Batch loss: 0.6629657745361328 batch: 509/840\n",
      "Batch loss: 0.5889033079147339 batch: 510/840\n",
      "Batch loss: 0.5562590956687927 batch: 511/840\n",
      "Batch loss: 0.7903072237968445 batch: 512/840\n",
      "Batch loss: 0.6199682354927063 batch: 513/840\n",
      "Batch loss: 0.7248103618621826 batch: 514/840\n",
      "Batch loss: 0.5277380347251892 batch: 515/840\n",
      "Batch loss: 0.6829717755317688 batch: 516/840\n",
      "Batch loss: 0.6263736486434937 batch: 517/840\n",
      "Batch loss: 0.6667910218238831 batch: 518/840\n",
      "Batch loss: 0.8587108850479126 batch: 519/840\n",
      "Batch loss: 0.5782562494277954 batch: 520/840\n",
      "Batch loss: 0.6134236454963684 batch: 521/840\n",
      "Batch loss: 0.5868345499038696 batch: 522/840\n",
      "Batch loss: 0.4692569077014923 batch: 523/840\n",
      "Batch loss: 0.6511329412460327 batch: 524/840\n",
      "Batch loss: 0.6977231502532959 batch: 525/840\n",
      "Batch loss: 0.7630549073219299 batch: 526/840\n",
      "Batch loss: 0.7798963189125061 batch: 527/840\n",
      "Batch loss: 0.6963056325912476 batch: 528/840\n",
      "Batch loss: 0.5534203052520752 batch: 529/840\n",
      "Batch loss: 0.6859226226806641 batch: 530/840\n",
      "Batch loss: 0.5872577428817749 batch: 531/840\n",
      "Batch loss: 0.4508224129676819 batch: 532/840\n",
      "Batch loss: 0.7086886763572693 batch: 533/840\n",
      "Batch loss: 0.7194183468818665 batch: 534/840\n",
      "Batch loss: 0.6019004583358765 batch: 535/840\n",
      "Batch loss: 0.8949230313301086 batch: 536/840\n",
      "Batch loss: 0.6627991199493408 batch: 537/840\n",
      "Batch loss: 0.602260410785675 batch: 538/840\n",
      "Batch loss: 0.6231849193572998 batch: 539/840\n",
      "Batch loss: 0.778102695941925 batch: 540/840\n",
      "Batch loss: 0.5737347602844238 batch: 541/840\n",
      "Batch loss: 0.6906365156173706 batch: 542/840\n",
      "Batch loss: 0.4841456711292267 batch: 543/840\n",
      "Batch loss: 0.6812137365341187 batch: 544/840\n",
      "Batch loss: 0.7449018955230713 batch: 545/840\n",
      "Batch loss: 0.6208360195159912 batch: 546/840\n",
      "Batch loss: 0.6515896320343018 batch: 547/840\n",
      "Batch loss: 0.5071609616279602 batch: 548/840\n",
      "Batch loss: 0.5755045413970947 batch: 549/840\n",
      "Batch loss: 0.6409092545509338 batch: 550/840\n",
      "Batch loss: 0.5932726263999939 batch: 551/840\n",
      "Batch loss: 0.5277904272079468 batch: 552/840\n",
      "Batch loss: 0.7610070705413818 batch: 553/840\n",
      "Batch loss: 0.7690660357475281 batch: 554/840\n",
      "Batch loss: 0.610140323638916 batch: 555/840\n",
      "Batch loss: 0.6498798131942749 batch: 556/840\n",
      "Batch loss: 0.6651431322097778 batch: 557/840\n",
      "Batch loss: 0.5804977416992188 batch: 558/840\n",
      "Batch loss: 0.7979828119277954 batch: 559/840\n",
      "Batch loss: 0.678391695022583 batch: 560/840\n",
      "Batch loss: 0.6360915303230286 batch: 561/840\n",
      "Batch loss: 0.6203647255897522 batch: 562/840\n",
      "Batch loss: 0.46124371886253357 batch: 563/840\n",
      "Batch loss: 0.6718169450759888 batch: 564/840\n",
      "Batch loss: 0.8187614679336548 batch: 565/840\n",
      "Batch loss: 0.7001640200614929 batch: 566/840\n",
      "Batch loss: 0.6701110601425171 batch: 567/840\n",
      "Batch loss: 0.5541591048240662 batch: 568/840\n",
      "Batch loss: 0.6958848834037781 batch: 569/840\n",
      "Batch loss: 0.4051813781261444 batch: 570/840\n",
      "Batch loss: 0.7611845135688782 batch: 571/840\n",
      "Batch loss: 0.6757606267929077 batch: 572/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.656150221824646 batch: 573/840\n",
      "Batch loss: 0.7795054912567139 batch: 574/840\n",
      "Batch loss: 0.5167266726493835 batch: 575/840\n",
      "Batch loss: 0.7259087562561035 batch: 576/840\n",
      "Batch loss: 0.6271551251411438 batch: 577/840\n",
      "Batch loss: 0.750383734703064 batch: 578/840\n",
      "Batch loss: 0.6894859075546265 batch: 579/840\n",
      "Batch loss: 0.8437957167625427 batch: 580/840\n",
      "Batch loss: 0.7548210024833679 batch: 581/840\n",
      "Batch loss: 0.8586009740829468 batch: 582/840\n",
      "Batch loss: 0.6151660084724426 batch: 583/840\n",
      "Batch loss: 0.7065261006355286 batch: 584/840\n",
      "Batch loss: 0.6819703578948975 batch: 585/840\n",
      "Batch loss: 0.6843703985214233 batch: 586/840\n",
      "Batch loss: 0.6434952020645142 batch: 587/840\n",
      "Batch loss: 0.49952012300491333 batch: 588/840\n",
      "Batch loss: 0.7644034028053284 batch: 589/840\n",
      "Batch loss: 0.554949164390564 batch: 590/840\n",
      "Batch loss: 0.4679270088672638 batch: 591/840\n",
      "Batch loss: 0.5980646014213562 batch: 592/840\n",
      "Batch loss: 0.7976739406585693 batch: 593/840\n",
      "Batch loss: 0.6787362694740295 batch: 594/840\n",
      "Batch loss: 0.45024722814559937 batch: 595/840\n",
      "Batch loss: 0.5602824091911316 batch: 596/840\n",
      "Batch loss: 0.5499177575111389 batch: 597/840\n",
      "Batch loss: 0.5093665719032288 batch: 598/840\n",
      "Batch loss: 0.5466963052749634 batch: 599/840\n",
      "Batch loss: 0.6769907474517822 batch: 600/840\n",
      "Batch loss: 0.7551906704902649 batch: 601/840\n",
      "Batch loss: 0.5599700808525085 batch: 602/840\n",
      "Batch loss: 0.6197702288627625 batch: 603/840\n",
      "Batch loss: 0.7780841588973999 batch: 604/840\n",
      "Batch loss: 0.8523745536804199 batch: 605/840\n",
      "Batch loss: 0.8355969190597534 batch: 606/840\n",
      "Batch loss: 0.6582448482513428 batch: 607/840\n",
      "Batch loss: 0.7235580682754517 batch: 608/840\n",
      "Batch loss: 0.5501939654350281 batch: 609/840\n",
      "Batch loss: 0.7389907240867615 batch: 610/840\n",
      "Batch loss: 0.6788640022277832 batch: 611/840\n",
      "Batch loss: 0.6995313763618469 batch: 612/840\n",
      "Batch loss: 0.6886137127876282 batch: 613/840\n",
      "Batch loss: 0.698517918586731 batch: 614/840\n",
      "Batch loss: 0.6162468194961548 batch: 615/840\n",
      "Batch loss: 0.584744393825531 batch: 616/840\n",
      "Batch loss: 0.5048888921737671 batch: 617/840\n",
      "Batch loss: 0.6728412508964539 batch: 618/840\n",
      "Batch loss: 0.7358516454696655 batch: 619/840\n",
      "Batch loss: 0.6069002151489258 batch: 620/840\n",
      "Batch loss: 0.7127435207366943 batch: 621/840\n",
      "Batch loss: 0.6675792932510376 batch: 622/840\n",
      "Batch loss: 0.5412858724594116 batch: 623/840\n",
      "Batch loss: 0.8919780850410461 batch: 624/840\n",
      "Batch loss: 0.6576987504959106 batch: 625/840\n",
      "Batch loss: 0.5505960583686829 batch: 626/840\n",
      "Batch loss: 0.6475474834442139 batch: 627/840\n",
      "Batch loss: 0.6849014759063721 batch: 628/840\n",
      "Batch loss: 0.6546329259872437 batch: 629/840\n",
      "Batch loss: 0.7185606360435486 batch: 630/840\n",
      "Batch loss: 0.7159990668296814 batch: 631/840\n",
      "Batch loss: 0.6928489208221436 batch: 632/840\n",
      "Batch loss: 0.597139298915863 batch: 633/840\n",
      "Batch loss: 0.7457689046859741 batch: 634/840\n",
      "Batch loss: 0.6493092179298401 batch: 635/840\n",
      "Batch loss: 0.5910604596138 batch: 636/840\n",
      "Batch loss: 0.5186030864715576 batch: 637/840\n",
      "Batch loss: 0.6889356970787048 batch: 638/840\n",
      "Batch loss: 0.6531295776367188 batch: 639/840\n",
      "Batch loss: 0.5969009399414062 batch: 640/840\n",
      "Batch loss: 0.8046010732650757 batch: 641/840\n",
      "Batch loss: 0.6137198805809021 batch: 642/840\n",
      "Batch loss: 1.0404598712921143 batch: 643/840\n",
      "Batch loss: 0.5534742474555969 batch: 644/840\n",
      "Batch loss: 0.6039261817932129 batch: 645/840\n",
      "Batch loss: 0.5458519458770752 batch: 646/840\n",
      "Batch loss: 0.7357521653175354 batch: 647/840\n",
      "Batch loss: 0.6507548689842224 batch: 648/840\n",
      "Batch loss: 0.7234146595001221 batch: 649/840\n",
      "Batch loss: 0.5901943445205688 batch: 650/840\n",
      "Batch loss: 0.5794776678085327 batch: 651/840\n",
      "Batch loss: 0.8062310218811035 batch: 652/840\n",
      "Batch loss: 0.48448440432548523 batch: 653/840\n",
      "Batch loss: 0.8676561117172241 batch: 654/840\n",
      "Batch loss: 0.5928875803947449 batch: 655/840\n",
      "Batch loss: 0.8191848993301392 batch: 656/840\n",
      "Batch loss: 0.6271962523460388 batch: 657/840\n",
      "Batch loss: 0.7485415935516357 batch: 658/840\n",
      "Batch loss: 0.6278812289237976 batch: 659/840\n",
      "Batch loss: 0.5694518685340881 batch: 660/840\n",
      "Batch loss: 0.7335023283958435 batch: 661/840\n",
      "Batch loss: 0.6652538180351257 batch: 662/840\n",
      "Batch loss: 0.5643132925033569 batch: 663/840\n",
      "Batch loss: 0.7219548225402832 batch: 664/840\n",
      "Batch loss: 0.7300395965576172 batch: 665/840\n",
      "Batch loss: 0.7555928826332092 batch: 666/840\n",
      "Batch loss: 0.9152669310569763 batch: 667/840\n",
      "Batch loss: 0.5970762968063354 batch: 668/840\n",
      "Batch loss: 0.6291519999504089 batch: 669/840\n",
      "Batch loss: 0.7807609438896179 batch: 670/840\n",
      "Batch loss: 0.6162122488021851 batch: 671/840\n",
      "Batch loss: 0.6791744232177734 batch: 672/840\n",
      "Batch loss: 0.8894177079200745 batch: 673/840\n",
      "Batch loss: 0.759936511516571 batch: 674/840\n",
      "Batch loss: 0.6364536285400391 batch: 675/840\n",
      "Batch loss: 0.5824753642082214 batch: 676/840\n",
      "Batch loss: 0.5643472075462341 batch: 677/840\n",
      "Batch loss: 0.6553198099136353 batch: 678/840\n",
      "Batch loss: 0.7929583787918091 batch: 679/840\n",
      "Batch loss: 0.6185489892959595 batch: 680/840\n",
      "Batch loss: 0.7370302081108093 batch: 681/840\n",
      "Batch loss: 0.59312504529953 batch: 682/840\n",
      "Batch loss: 0.5107610821723938 batch: 683/840\n",
      "Batch loss: 0.8055129051208496 batch: 684/840\n",
      "Batch loss: 0.7130784392356873 batch: 685/840\n",
      "Batch loss: 0.8200277090072632 batch: 686/840\n",
      "Batch loss: 0.7514030933380127 batch: 687/840\n",
      "Batch loss: 0.6975242495536804 batch: 688/840\n",
      "Batch loss: 0.5749652981758118 batch: 689/840\n",
      "Batch loss: 0.5541152358055115 batch: 690/840\n",
      "Batch loss: 0.6215600371360779 batch: 691/840\n",
      "Batch loss: 0.6357659697532654 batch: 692/840\n",
      "Batch loss: 0.665438175201416 batch: 693/840\n",
      "Batch loss: 0.7427319288253784 batch: 694/840\n",
      "Batch loss: 0.6630600094795227 batch: 695/840\n",
      "Batch loss: 0.7152851819992065 batch: 696/840\n",
      "Batch loss: 0.6234151721000671 batch: 697/840\n",
      "Batch loss: 0.6751649379730225 batch: 698/840\n",
      "Batch loss: 0.7334191799163818 batch: 699/840\n",
      "Batch loss: 0.6711111664772034 batch: 700/840\n",
      "Batch loss: 0.7339352369308472 batch: 701/840\n",
      "Batch loss: 0.6430001258850098 batch: 702/840\n",
      "Batch loss: 0.5330235362052917 batch: 703/840\n",
      "Batch loss: 0.7647234201431274 batch: 704/840\n",
      "Batch loss: 0.5601646900177002 batch: 705/840\n",
      "Batch loss: 0.7192075848579407 batch: 706/840\n",
      "Batch loss: 0.6404776573181152 batch: 707/840\n",
      "Batch loss: 0.7400529384613037 batch: 708/840\n",
      "Batch loss: 0.7634978294372559 batch: 709/840\n",
      "Batch loss: 0.7935099601745605 batch: 710/840\n",
      "Batch loss: 0.47584763169288635 batch: 711/840\n",
      "Batch loss: 0.7150646448135376 batch: 712/840\n",
      "Batch loss: 0.5506978631019592 batch: 713/840\n",
      "Batch loss: 0.7298853993415833 batch: 714/840\n",
      "Batch loss: 0.8092837333679199 batch: 715/840\n",
      "Batch loss: 0.7103534936904907 batch: 716/840\n",
      "Batch loss: 0.5478464365005493 batch: 717/840\n",
      "Batch loss: 0.6369516253471375 batch: 718/840\n",
      "Batch loss: 0.45942923426628113 batch: 719/840\n",
      "Batch loss: 0.5454267263412476 batch: 720/840\n",
      "Batch loss: 0.7543792724609375 batch: 721/840\n",
      "Batch loss: 0.725563108921051 batch: 722/840\n",
      "Batch loss: 0.5638654828071594 batch: 723/840\n",
      "Batch loss: 0.7528751492500305 batch: 724/840\n",
      "Batch loss: 0.6247961521148682 batch: 725/840\n",
      "Batch loss: 0.6489140391349792 batch: 726/840\n",
      "Batch loss: 0.8465183973312378 batch: 727/840\n",
      "Batch loss: 0.7398495674133301 batch: 728/840\n",
      "Batch loss: 0.6860585808753967 batch: 729/840\n",
      "Batch loss: 0.6519638895988464 batch: 730/840\n",
      "Batch loss: 0.5511876940727234 batch: 731/840\n",
      "Batch loss: 0.9399339556694031 batch: 732/840\n",
      "Batch loss: 0.6930890083312988 batch: 733/840\n",
      "Batch loss: 0.6681844592094421 batch: 734/840\n",
      "Batch loss: 0.6048929691314697 batch: 735/840\n",
      "Batch loss: 0.7104897499084473 batch: 736/840\n",
      "Batch loss: 0.6392602920532227 batch: 737/840\n",
      "Batch loss: 0.5195401906967163 batch: 738/840\n",
      "Batch loss: 0.7040551900863647 batch: 739/840\n",
      "Batch loss: 0.8172253966331482 batch: 740/840\n",
      "Batch loss: 0.637834370136261 batch: 741/840\n",
      "Batch loss: 0.553602933883667 batch: 742/840\n",
      "Batch loss: 0.46747273206710815 batch: 743/840\n",
      "Batch loss: 0.5557677745819092 batch: 744/840\n",
      "Batch loss: 0.6301500797271729 batch: 745/840\n",
      "Batch loss: 0.6183401346206665 batch: 746/840\n",
      "Batch loss: 0.6391815543174744 batch: 747/840\n",
      "Batch loss: 0.7899317145347595 batch: 748/840\n",
      "Batch loss: 0.5213009119033813 batch: 749/840\n",
      "Batch loss: 0.5552691221237183 batch: 750/840\n",
      "Batch loss: 0.71518874168396 batch: 751/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8145378828048706 batch: 752/840\n",
      "Batch loss: 0.5652704834938049 batch: 753/840\n",
      "Batch loss: 0.6685783267021179 batch: 754/840\n",
      "Batch loss: 0.6702832579612732 batch: 755/840\n",
      "Batch loss: 0.5927320718765259 batch: 756/840\n",
      "Batch loss: 0.6473495960235596 batch: 757/840\n",
      "Batch loss: 0.6631720662117004 batch: 758/840\n",
      "Batch loss: 0.5276720523834229 batch: 759/840\n",
      "Batch loss: 0.6075963973999023 batch: 760/840\n",
      "Batch loss: 0.5651214122772217 batch: 761/840\n",
      "Batch loss: 0.8148454427719116 batch: 762/840\n",
      "Batch loss: 0.38564586639404297 batch: 763/840\n",
      "Batch loss: 0.6904526352882385 batch: 764/840\n",
      "Batch loss: 0.5302625894546509 batch: 765/840\n",
      "Batch loss: 0.5483038425445557 batch: 766/840\n",
      "Batch loss: 0.6895551085472107 batch: 767/840\n",
      "Batch loss: 0.6501303315162659 batch: 768/840\n",
      "Batch loss: 0.8633359670639038 batch: 769/840\n",
      "Batch loss: 0.5449306964874268 batch: 770/840\n",
      "Batch loss: 0.6466947793960571 batch: 771/840\n",
      "Batch loss: 0.5897641181945801 batch: 772/840\n",
      "Batch loss: 0.5487260818481445 batch: 773/840\n",
      "Batch loss: 0.5072406530380249 batch: 774/840\n",
      "Batch loss: 0.5498449206352234 batch: 775/840\n",
      "Batch loss: 0.573380172252655 batch: 776/840\n",
      "Batch loss: 0.5728092193603516 batch: 777/840\n",
      "Batch loss: 0.49387219548225403 batch: 778/840\n",
      "Batch loss: 0.7360808849334717 batch: 779/840\n",
      "Batch loss: 0.5937699675559998 batch: 780/840\n",
      "Batch loss: 0.6860910058021545 batch: 781/840\n",
      "Batch loss: 0.6672508120536804 batch: 782/840\n",
      "Batch loss: 0.549121081829071 batch: 783/840\n",
      "Batch loss: 0.7521575689315796 batch: 784/840\n",
      "Batch loss: 0.5735012888908386 batch: 785/840\n",
      "Batch loss: 0.7176650166511536 batch: 786/840\n",
      "Batch loss: 0.622219443321228 batch: 787/840\n",
      "Batch loss: 0.6977083086967468 batch: 788/840\n",
      "Batch loss: 0.6568061709403992 batch: 789/840\n",
      "Batch loss: 0.6667852997779846 batch: 790/840\n",
      "Batch loss: 0.6807142496109009 batch: 791/840\n",
      "Batch loss: 0.4586069881916046 batch: 792/840\n",
      "Batch loss: 0.6124240159988403 batch: 793/840\n",
      "Batch loss: 0.7291174530982971 batch: 794/840\n",
      "Batch loss: 0.6967217922210693 batch: 795/840\n",
      "Batch loss: 0.7657825350761414 batch: 796/840\n",
      "Batch loss: 0.6789363622665405 batch: 797/840\n",
      "Batch loss: 0.588072657585144 batch: 798/840\n",
      "Batch loss: 0.7695910930633545 batch: 799/840\n",
      "Batch loss: 0.6282835602760315 batch: 800/840\n",
      "Batch loss: 0.7373631000518799 batch: 801/840\n",
      "Batch loss: 0.5949417948722839 batch: 802/840\n",
      "Batch loss: 0.524946391582489 batch: 803/840\n",
      "Batch loss: 0.64277184009552 batch: 804/840\n",
      "Batch loss: 0.5473802089691162 batch: 805/840\n",
      "Batch loss: 0.615090012550354 batch: 806/840\n",
      "Batch loss: 0.614031970500946 batch: 807/840\n",
      "Batch loss: 0.6136566996574402 batch: 808/840\n",
      "Batch loss: 0.6557481288909912 batch: 809/840\n",
      "Batch loss: 0.614166259765625 batch: 810/840\n",
      "Batch loss: 0.5903039574623108 batch: 811/840\n",
      "Batch loss: 0.7312210202217102 batch: 812/840\n",
      "Batch loss: 0.5623680949211121 batch: 813/840\n",
      "Batch loss: 0.6391364932060242 batch: 814/840\n",
      "Batch loss: 0.6961365342140198 batch: 815/840\n",
      "Batch loss: 0.5988809466362 batch: 816/840\n",
      "Batch loss: 0.740019679069519 batch: 817/840\n",
      "Batch loss: 0.6793495416641235 batch: 818/840\n",
      "Batch loss: 0.552200436592102 batch: 819/840\n",
      "Batch loss: 0.7056140899658203 batch: 820/840\n",
      "Batch loss: 0.649421215057373 batch: 821/840\n",
      "Batch loss: 0.6934136748313904 batch: 822/840\n",
      "Batch loss: 0.7296501994132996 batch: 823/840\n",
      "Batch loss: 0.7515964508056641 batch: 824/840\n",
      "Batch loss: 0.7601261138916016 batch: 825/840\n",
      "Batch loss: 0.5486146211624146 batch: 826/840\n",
      "Batch loss: 0.6156942248344421 batch: 827/840\n",
      "Batch loss: 0.7425209283828735 batch: 828/840\n",
      "Batch loss: 0.5559021234512329 batch: 829/840\n",
      "Batch loss: 0.6660346984863281 batch: 830/840\n",
      "Batch loss: 0.5693060755729675 batch: 831/840\n",
      "Batch loss: 0.7216464877128601 batch: 832/840\n",
      "Batch loss: 0.7770317196846008 batch: 833/840\n",
      "Batch loss: 0.6745837330818176 batch: 834/840\n",
      "Batch loss: 0.4504276514053345 batch: 835/840\n",
      "Batch loss: 0.5683481693267822 batch: 836/840\n",
      "Batch loss: 0.6281469464302063 batch: 837/840\n",
      "Batch loss: 0.7732208967208862 batch: 838/840\n",
      "Batch loss: 0.5309184789657593 batch: 839/840\n",
      "Batch loss: 0.6494854688644409 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 5/15..  Training Loss: 0.007..  Test Loss: 0.005..  Test Accuracy: 0.822\n",
      "Running epoch 6/15\n",
      "Batch loss: 0.4427791237831116 batch: 1/840\n",
      "Batch loss: 1.0577385425567627 batch: 2/840\n",
      "Batch loss: 0.64284348487854 batch: 3/840\n",
      "Batch loss: 0.6610840559005737 batch: 4/840\n",
      "Batch loss: 0.6248111128807068 batch: 5/840\n",
      "Batch loss: 0.4566439390182495 batch: 6/840\n",
      "Batch loss: 0.6284620761871338 batch: 7/840\n",
      "Batch loss: 0.5164722204208374 batch: 8/840\n",
      "Batch loss: 0.5493101477622986 batch: 9/840\n",
      "Batch loss: 0.6524710655212402 batch: 10/840\n",
      "Batch loss: 0.6023821830749512 batch: 11/840\n",
      "Batch loss: 0.5896358489990234 batch: 12/840\n",
      "Batch loss: 0.5389100313186646 batch: 13/840\n",
      "Batch loss: 0.6428219079971313 batch: 14/840\n",
      "Batch loss: 0.5765491724014282 batch: 15/840\n",
      "Batch loss: 0.5305447578430176 batch: 16/840\n",
      "Batch loss: 0.48257124423980713 batch: 17/840\n",
      "Batch loss: 0.6573668122291565 batch: 18/840\n",
      "Batch loss: 0.7196542620658875 batch: 19/840\n",
      "Batch loss: 0.7289421558380127 batch: 20/840\n",
      "Batch loss: 0.7260549664497375 batch: 21/840\n",
      "Batch loss: 0.6496372818946838 batch: 22/840\n",
      "Batch loss: 0.6478361487388611 batch: 23/840\n",
      "Batch loss: 0.5231642127037048 batch: 24/840\n",
      "Batch loss: 0.6202481389045715 batch: 25/840\n",
      "Batch loss: 0.7341275215148926 batch: 26/840\n",
      "Batch loss: 0.7190302014350891 batch: 27/840\n",
      "Batch loss: 0.705524206161499 batch: 28/840\n",
      "Batch loss: 0.6501732468605042 batch: 29/840\n",
      "Batch loss: 0.5468518733978271 batch: 30/840\n",
      "Batch loss: 0.6027087569236755 batch: 31/840\n",
      "Batch loss: 0.5432612895965576 batch: 32/840\n",
      "Batch loss: 0.6552445888519287 batch: 33/840\n",
      "Batch loss: 0.5386765003204346 batch: 34/840\n",
      "Batch loss: 0.6682060360908508 batch: 35/840\n",
      "Batch loss: 0.6673434376716614 batch: 36/840\n",
      "Batch loss: 0.7163692712783813 batch: 37/840\n",
      "Batch loss: 0.6846441626548767 batch: 38/840\n",
      "Batch loss: 0.7087934613227844 batch: 39/840\n",
      "Batch loss: 0.5083931088447571 batch: 40/840\n",
      "Batch loss: 0.6734166741371155 batch: 41/840\n",
      "Batch loss: 0.6581780314445496 batch: 42/840\n",
      "Batch loss: 0.6979359984397888 batch: 43/840\n",
      "Batch loss: 0.6649924516677856 batch: 44/840\n",
      "Batch loss: 0.7050987482070923 batch: 45/840\n",
      "Batch loss: 0.5520228147506714 batch: 46/840\n",
      "Batch loss: 0.5231970548629761 batch: 47/840\n",
      "Batch loss: 0.6102287173271179 batch: 48/840\n",
      "Batch loss: 0.631365954875946 batch: 49/840\n",
      "Batch loss: 0.7041054368019104 batch: 50/840\n",
      "Batch loss: 0.802341103553772 batch: 51/840\n",
      "Batch loss: 0.6531246900558472 batch: 52/840\n",
      "Batch loss: 0.5714917182922363 batch: 53/840\n",
      "Batch loss: 0.6508570313453674 batch: 54/840\n",
      "Batch loss: 0.6319523453712463 batch: 55/840\n",
      "Batch loss: 0.7244227528572083 batch: 56/840\n",
      "Batch loss: 0.6868084669113159 batch: 57/840\n",
      "Batch loss: 0.4796222448348999 batch: 58/840\n",
      "Batch loss: 0.6307342648506165 batch: 59/840\n",
      "Batch loss: 0.5392623543739319 batch: 60/840\n",
      "Batch loss: 0.8089649677276611 batch: 61/840\n",
      "Batch loss: 0.6916823387145996 batch: 62/840\n",
      "Batch loss: 0.5783832669258118 batch: 63/840\n",
      "Batch loss: 0.7847622632980347 batch: 64/840\n",
      "Batch loss: 0.5509134531021118 batch: 65/840\n",
      "Batch loss: 0.6789810061454773 batch: 66/840\n",
      "Batch loss: 0.722990095615387 batch: 67/840\n",
      "Batch loss: 0.6783484816551208 batch: 68/840\n",
      "Batch loss: 0.6824244856834412 batch: 69/840\n",
      "Batch loss: 0.6246034502983093 batch: 70/840\n",
      "Batch loss: 0.7205767631530762 batch: 71/840\n",
      "Batch loss: 0.779075026512146 batch: 72/840\n",
      "Batch loss: 0.6390345096588135 batch: 73/840\n",
      "Batch loss: 0.6631450057029724 batch: 74/840\n",
      "Batch loss: 0.667595386505127 batch: 75/840\n",
      "Batch loss: 0.43769779801368713 batch: 76/840\n",
      "Batch loss: 0.704552412033081 batch: 77/840\n",
      "Batch loss: 0.7800388336181641 batch: 78/840\n",
      "Batch loss: 0.6109054684638977 batch: 79/840\n",
      "Batch loss: 0.6861116290092468 batch: 80/840\n",
      "Batch loss: 0.5936882495880127 batch: 81/840\n",
      "Batch loss: 0.6285815834999084 batch: 82/840\n",
      "Batch loss: 0.48747575283050537 batch: 83/840\n",
      "Batch loss: 0.708152711391449 batch: 84/840\n",
      "Batch loss: 0.6220013499259949 batch: 85/840\n",
      "Batch loss: 0.8152748942375183 batch: 86/840\n",
      "Batch loss: 0.5453541278839111 batch: 87/840\n",
      "Batch loss: 0.48351025581359863 batch: 88/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4273456931114197 batch: 89/840\n",
      "Batch loss: 0.49756741523742676 batch: 90/840\n",
      "Batch loss: 0.6277928352355957 batch: 91/840\n",
      "Batch loss: 0.6013364195823669 batch: 92/840\n",
      "Batch loss: 0.621572732925415 batch: 93/840\n",
      "Batch loss: 0.5959766507148743 batch: 94/840\n",
      "Batch loss: 0.4871208071708679 batch: 95/840\n",
      "Batch loss: 0.6136190891265869 batch: 96/840\n",
      "Batch loss: 0.6936027407646179 batch: 97/840\n",
      "Batch loss: 0.8742967844009399 batch: 98/840\n",
      "Batch loss: 0.529708981513977 batch: 99/840\n",
      "Batch loss: 0.6611769199371338 batch: 100/840\n",
      "Batch loss: 0.5891065001487732 batch: 101/840\n",
      "Batch loss: 0.48619404435157776 batch: 102/840\n",
      "Batch loss: 0.6648203134536743 batch: 103/840\n",
      "Batch loss: 0.4931459426879883 batch: 104/840\n",
      "Batch loss: 0.5454641580581665 batch: 105/840\n",
      "Batch loss: 0.6422783732414246 batch: 106/840\n",
      "Batch loss: 0.6101815700531006 batch: 107/840\n",
      "Batch loss: 0.80327969789505 batch: 108/840\n",
      "Batch loss: 0.599432110786438 batch: 109/840\n",
      "Batch loss: 0.5376288294792175 batch: 110/840\n",
      "Batch loss: 0.5453454256057739 batch: 111/840\n",
      "Batch loss: 0.6925486922264099 batch: 112/840\n",
      "Batch loss: 0.7447683215141296 batch: 113/840\n",
      "Batch loss: 0.5852773189544678 batch: 114/840\n",
      "Batch loss: 0.5892994403839111 batch: 115/840\n",
      "Batch loss: 0.5674786567687988 batch: 116/840\n",
      "Batch loss: 0.6326403617858887 batch: 117/840\n",
      "Batch loss: 0.47676360607147217 batch: 118/840\n",
      "Batch loss: 0.8171194195747375 batch: 119/840\n",
      "Batch loss: 0.5992203950881958 batch: 120/840\n",
      "Batch loss: 0.7004770040512085 batch: 121/840\n",
      "Batch loss: 0.7750688195228577 batch: 122/840\n",
      "Batch loss: 0.5592843890190125 batch: 123/840\n",
      "Batch loss: 0.6683327555656433 batch: 124/840\n",
      "Batch loss: 0.6216224431991577 batch: 125/840\n",
      "Batch loss: 0.5890533924102783 batch: 126/840\n",
      "Batch loss: 0.7206562757492065 batch: 127/840\n",
      "Batch loss: 0.7064852118492126 batch: 128/840\n",
      "Batch loss: 0.8078311085700989 batch: 129/840\n",
      "Batch loss: 0.6370584964752197 batch: 130/840\n",
      "Batch loss: 0.7997585535049438 batch: 131/840\n",
      "Batch loss: 1.011234164237976 batch: 132/840\n",
      "Batch loss: 0.8103521466255188 batch: 133/840\n",
      "Batch loss: 0.5671385526657104 batch: 134/840\n",
      "Batch loss: 0.5471674203872681 batch: 135/840\n",
      "Batch loss: 0.7229723930358887 batch: 136/840\n",
      "Batch loss: 0.6432016491889954 batch: 137/840\n",
      "Batch loss: 0.5628909468650818 batch: 138/840\n",
      "Batch loss: 0.5945526361465454 batch: 139/840\n",
      "Batch loss: 0.6731825470924377 batch: 140/840\n",
      "Batch loss: 0.5132843255996704 batch: 141/840\n",
      "Batch loss: 0.6067240834236145 batch: 142/840\n",
      "Batch loss: 0.5854052901268005 batch: 143/840\n",
      "Batch loss: 0.5641764402389526 batch: 144/840\n",
      "Batch loss: 0.7389586567878723 batch: 145/840\n",
      "Batch loss: 0.703859806060791 batch: 146/840\n",
      "Batch loss: 0.5247370004653931 batch: 147/840\n",
      "Batch loss: 0.7443997859954834 batch: 148/840\n",
      "Batch loss: 0.7154947519302368 batch: 149/840\n",
      "Batch loss: 0.642770528793335 batch: 150/840\n",
      "Batch loss: 0.7061426639556885 batch: 151/840\n",
      "Batch loss: 0.5842702984809875 batch: 152/840\n",
      "Batch loss: 0.49079930782318115 batch: 153/840\n",
      "Batch loss: 0.7176451086997986 batch: 154/840\n",
      "Batch loss: 0.5478666424751282 batch: 155/840\n",
      "Batch loss: 0.6453020572662354 batch: 156/840\n",
      "Batch loss: 0.6705985069274902 batch: 157/840\n",
      "Batch loss: 0.5694016814231873 batch: 158/840\n",
      "Batch loss: 0.5399797558784485 batch: 159/840\n",
      "Batch loss: 0.6057393550872803 batch: 160/840\n",
      "Batch loss: 0.7136102318763733 batch: 161/840\n",
      "Batch loss: 0.7493037581443787 batch: 162/840\n",
      "Batch loss: 0.7916649580001831 batch: 163/840\n",
      "Batch loss: 0.46947625279426575 batch: 164/840\n",
      "Batch loss: 0.7449190020561218 batch: 165/840\n",
      "Batch loss: 0.5791305899620056 batch: 166/840\n",
      "Batch loss: 0.7552075982093811 batch: 167/840\n",
      "Batch loss: 0.6921785473823547 batch: 168/840\n",
      "Batch loss: 0.5660135746002197 batch: 169/840\n",
      "Batch loss: 0.7861835956573486 batch: 170/840\n",
      "Batch loss: 0.7037910223007202 batch: 171/840\n",
      "Batch loss: 0.7482841610908508 batch: 172/840\n",
      "Batch loss: 0.6748127937316895 batch: 173/840\n",
      "Batch loss: 0.6534422039985657 batch: 174/840\n",
      "Batch loss: 0.5795272588729858 batch: 175/840\n",
      "Batch loss: 0.7264260649681091 batch: 176/840\n",
      "Batch loss: 0.6512619853019714 batch: 177/840\n",
      "Batch loss: 0.7820056676864624 batch: 178/840\n",
      "Batch loss: 0.6504443883895874 batch: 179/840\n",
      "Batch loss: 0.5440006852149963 batch: 180/840\n",
      "Batch loss: 0.5920050740242004 batch: 181/840\n",
      "Batch loss: 0.5410888195037842 batch: 182/840\n",
      "Batch loss: 0.6638121604919434 batch: 183/840\n",
      "Batch loss: 0.47903117537498474 batch: 184/840\n",
      "Batch loss: 0.4965284466743469 batch: 185/840\n",
      "Batch loss: 0.5158738493919373 batch: 186/840\n",
      "Batch loss: 0.6973487138748169 batch: 187/840\n",
      "Batch loss: 0.6447361707687378 batch: 188/840\n",
      "Batch loss: 0.7395555973052979 batch: 189/840\n",
      "Batch loss: 0.6908037662506104 batch: 190/840\n",
      "Batch loss: 0.8582199215888977 batch: 191/840\n",
      "Batch loss: 0.4733089804649353 batch: 192/840\n",
      "Batch loss: 0.5262660384178162 batch: 193/840\n",
      "Batch loss: 0.48133063316345215 batch: 194/840\n",
      "Batch loss: 0.6298543214797974 batch: 195/840\n",
      "Batch loss: 0.7835521101951599 batch: 196/840\n",
      "Batch loss: 0.5327398180961609 batch: 197/840\n",
      "Batch loss: 0.501495897769928 batch: 198/840\n",
      "Batch loss: 0.5920141339302063 batch: 199/840\n",
      "Batch loss: 0.7837663888931274 batch: 200/840\n",
      "Batch loss: 0.669575035572052 batch: 201/840\n",
      "Batch loss: 0.6334240436553955 batch: 202/840\n",
      "Batch loss: 0.5876715183258057 batch: 203/840\n",
      "Batch loss: 0.7997834086418152 batch: 204/840\n",
      "Batch loss: 0.7485499382019043 batch: 205/840\n",
      "Batch loss: 0.6490949392318726 batch: 206/840\n",
      "Batch loss: 0.5743077397346497 batch: 207/840\n",
      "Batch loss: 0.6047419905662537 batch: 208/840\n",
      "Batch loss: 0.6303954124450684 batch: 209/840\n",
      "Batch loss: 0.5811023116111755 batch: 210/840\n",
      "Batch loss: 0.5406386256217957 batch: 211/840\n",
      "Batch loss: 0.6299581527709961 batch: 212/840\n",
      "Batch loss: 0.7884486317634583 batch: 213/840\n",
      "Batch loss: 0.825346052646637 batch: 214/840\n",
      "Batch loss: 0.6356912851333618 batch: 215/840\n",
      "Batch loss: 0.650533139705658 batch: 216/840\n",
      "Batch loss: 0.5757441520690918 batch: 217/840\n",
      "Batch loss: 0.7106042504310608 batch: 218/840\n",
      "Batch loss: 0.6706544756889343 batch: 219/840\n",
      "Batch loss: 0.9983170032501221 batch: 220/840\n",
      "Batch loss: 0.6520171165466309 batch: 221/840\n",
      "Batch loss: 0.6658908724784851 batch: 222/840\n",
      "Batch loss: 0.622029185295105 batch: 223/840\n",
      "Batch loss: 0.7657694220542908 batch: 224/840\n",
      "Batch loss: 0.658933699131012 batch: 225/840\n",
      "Batch loss: 0.6911741495132446 batch: 226/840\n",
      "Batch loss: 0.7351888418197632 batch: 227/840\n",
      "Batch loss: 0.47562935948371887 batch: 228/840\n",
      "Batch loss: 0.4709925949573517 batch: 229/840\n",
      "Batch loss: 0.5957607626914978 batch: 230/840\n",
      "Batch loss: 0.6049935221672058 batch: 231/840\n",
      "Batch loss: 0.6214052438735962 batch: 232/840\n",
      "Batch loss: 0.7230568528175354 batch: 233/840\n",
      "Batch loss: 0.6347805857658386 batch: 234/840\n",
      "Batch loss: 0.6579006314277649 batch: 235/840\n",
      "Batch loss: 0.6338129639625549 batch: 236/840\n",
      "Batch loss: 0.586527943611145 batch: 237/840\n",
      "Batch loss: 0.7888216376304626 batch: 238/840\n",
      "Batch loss: 0.6100289821624756 batch: 239/840\n",
      "Batch loss: 0.634688138961792 batch: 240/840\n",
      "Batch loss: 0.7265248894691467 batch: 241/840\n",
      "Batch loss: 0.6475470066070557 batch: 242/840\n",
      "Batch loss: 0.6107156276702881 batch: 243/840\n",
      "Batch loss: 0.7060156464576721 batch: 244/840\n",
      "Batch loss: 0.4931323230266571 batch: 245/840\n",
      "Batch loss: 0.6896995306015015 batch: 246/840\n",
      "Batch loss: 0.751095175743103 batch: 247/840\n",
      "Batch loss: 0.7331769466400146 batch: 248/840\n",
      "Batch loss: 0.8148584961891174 batch: 249/840\n",
      "Batch loss: 0.5485062003135681 batch: 250/840\n",
      "Batch loss: 0.6183608770370483 batch: 251/840\n",
      "Batch loss: 0.6728734374046326 batch: 252/840\n",
      "Batch loss: 0.6347609162330627 batch: 253/840\n",
      "Batch loss: 0.7360260486602783 batch: 254/840\n",
      "Batch loss: 0.6186694502830505 batch: 255/840\n",
      "Batch loss: 0.6302557587623596 batch: 256/840\n",
      "Batch loss: 0.5855318307876587 batch: 257/840\n",
      "Batch loss: 0.7383038401603699 batch: 258/840\n",
      "Batch loss: 0.5595707893371582 batch: 259/840\n",
      "Batch loss: 0.5650416016578674 batch: 260/840\n",
      "Batch loss: 0.5253629088401794 batch: 261/840\n",
      "Batch loss: 0.4533044397830963 batch: 262/840\n",
      "Batch loss: 0.6862708926200867 batch: 263/840\n",
      "Batch loss: 0.5427812337875366 batch: 264/840\n",
      "Batch loss: 0.6353601217269897 batch: 265/840\n",
      "Batch loss: 0.4929039478302002 batch: 266/840\n",
      "Batch loss: 0.7243418097496033 batch: 267/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5435997843742371 batch: 268/840\n",
      "Batch loss: 0.5709262490272522 batch: 269/840\n",
      "Batch loss: 0.6061674952507019 batch: 270/840\n",
      "Batch loss: 0.5108600854873657 batch: 271/840\n",
      "Batch loss: 0.8558752536773682 batch: 272/840\n",
      "Batch loss: 0.6923969388008118 batch: 273/840\n",
      "Batch loss: 0.5989536643028259 batch: 274/840\n",
      "Batch loss: 0.7667465209960938 batch: 275/840\n",
      "Batch loss: 0.6629545092582703 batch: 276/840\n",
      "Batch loss: 0.5671050548553467 batch: 277/840\n",
      "Batch loss: 0.8004733324050903 batch: 278/840\n",
      "Batch loss: 0.6689884066581726 batch: 279/840\n",
      "Batch loss: 0.7161937952041626 batch: 280/840\n",
      "Batch loss: 0.6179115772247314 batch: 281/840\n",
      "Batch loss: 0.5336389541625977 batch: 282/840\n",
      "Batch loss: 0.771929919719696 batch: 283/840\n",
      "Batch loss: 0.4995283782482147 batch: 284/840\n",
      "Batch loss: 0.4961470067501068 batch: 285/840\n",
      "Batch loss: 0.6402315497398376 batch: 286/840\n",
      "Batch loss: 0.4910798668861389 batch: 287/840\n",
      "Batch loss: 0.642548143863678 batch: 288/840\n",
      "Batch loss: 0.8573712110519409 batch: 289/840\n",
      "Batch loss: 0.773802638053894 batch: 290/840\n",
      "Batch loss: 0.8708131313323975 batch: 291/840\n",
      "Batch loss: 0.5743513107299805 batch: 292/840\n",
      "Batch loss: 0.713755190372467 batch: 293/840\n",
      "Batch loss: 0.5708158612251282 batch: 294/840\n",
      "Batch loss: 0.5651620030403137 batch: 295/840\n",
      "Batch loss: 0.6037203073501587 batch: 296/840\n",
      "Batch loss: 0.7191534638404846 batch: 297/840\n",
      "Batch loss: 0.6857086420059204 batch: 298/840\n",
      "Batch loss: 0.6810958385467529 batch: 299/840\n",
      "Batch loss: 0.7767140865325928 batch: 300/840\n",
      "Batch loss: 0.8230865597724915 batch: 301/840\n",
      "Batch loss: 0.6252658367156982 batch: 302/840\n",
      "Batch loss: 0.7581771016120911 batch: 303/840\n",
      "Batch loss: 0.5827934741973877 batch: 304/840\n",
      "Batch loss: 0.5418897867202759 batch: 305/840\n",
      "Batch loss: 0.7125977873802185 batch: 306/840\n",
      "Batch loss: 0.5533897280693054 batch: 307/840\n",
      "Batch loss: 0.7355562448501587 batch: 308/840\n",
      "Batch loss: 0.682129979133606 batch: 309/840\n",
      "Batch loss: 0.8905746936798096 batch: 310/840\n",
      "Batch loss: 0.6567731499671936 batch: 311/840\n",
      "Batch loss: 0.7226932048797607 batch: 312/840\n",
      "Batch loss: 0.6755117774009705 batch: 313/840\n",
      "Batch loss: 0.5373058319091797 batch: 314/840\n",
      "Batch loss: 0.5616914629936218 batch: 315/840\n",
      "Batch loss: 0.4297991096973419 batch: 316/840\n",
      "Batch loss: 0.7285537719726562 batch: 317/840\n",
      "Batch loss: 0.6234760880470276 batch: 318/840\n",
      "Batch loss: 0.7005738615989685 batch: 319/840\n",
      "Batch loss: 0.522662341594696 batch: 320/840\n",
      "Batch loss: 0.548228919506073 batch: 321/840\n",
      "Batch loss: 0.7312858700752258 batch: 322/840\n",
      "Batch loss: 0.6443438529968262 batch: 323/840\n",
      "Batch loss: 0.7073708176612854 batch: 324/840\n",
      "Batch loss: 0.5187333822250366 batch: 325/840\n",
      "Batch loss: 0.7692974209785461 batch: 326/840\n",
      "Batch loss: 0.49512404203414917 batch: 327/840\n",
      "Batch loss: 0.7918998003005981 batch: 328/840\n",
      "Batch loss: 0.7735955119132996 batch: 329/840\n",
      "Batch loss: 0.6315653324127197 batch: 330/840\n",
      "Batch loss: 0.7738349437713623 batch: 331/840\n",
      "Batch loss: 0.6319593191146851 batch: 332/840\n",
      "Batch loss: 0.5452044010162354 batch: 333/840\n",
      "Batch loss: 0.7366662621498108 batch: 334/840\n",
      "Batch loss: 0.5686017870903015 batch: 335/840\n",
      "Batch loss: 0.6362873315811157 batch: 336/840\n",
      "Batch loss: 0.8532363772392273 batch: 337/840\n",
      "Batch loss: 0.7813506126403809 batch: 338/840\n",
      "Batch loss: 0.6060869693756104 batch: 339/840\n",
      "Batch loss: 0.8512327075004578 batch: 340/840\n",
      "Batch loss: 0.5118380784988403 batch: 341/840\n",
      "Batch loss: 0.609093427658081 batch: 342/840\n",
      "Batch loss: 0.7757450342178345 batch: 343/840\n",
      "Batch loss: 0.6197965741157532 batch: 344/840\n",
      "Batch loss: 0.4061219096183777 batch: 345/840\n",
      "Batch loss: 0.6359948515892029 batch: 346/840\n",
      "Batch loss: 0.6804583668708801 batch: 347/840\n",
      "Batch loss: 0.632725715637207 batch: 348/840\n",
      "Batch loss: 0.6728441715240479 batch: 349/840\n",
      "Batch loss: 0.48966923356056213 batch: 350/840\n",
      "Batch loss: 0.6312282085418701 batch: 351/840\n",
      "Batch loss: 0.7527974843978882 batch: 352/840\n",
      "Batch loss: 0.7393817901611328 batch: 353/840\n",
      "Batch loss: 0.5872254371643066 batch: 354/840\n",
      "Batch loss: 0.6202728748321533 batch: 355/840\n",
      "Batch loss: 0.5910630822181702 batch: 356/840\n",
      "Batch loss: 0.5177690386772156 batch: 357/840\n",
      "Batch loss: 0.9234930276870728 batch: 358/840\n",
      "Batch loss: 0.5600301623344421 batch: 359/840\n",
      "Batch loss: 0.6969939470291138 batch: 360/840\n",
      "Batch loss: 0.7608994245529175 batch: 361/840\n",
      "Batch loss: 0.5275318026542664 batch: 362/840\n",
      "Batch loss: 0.6469342708587646 batch: 363/840\n",
      "Batch loss: 0.7561547756195068 batch: 364/840\n",
      "Batch loss: 0.5477359890937805 batch: 365/840\n",
      "Batch loss: 0.6208765506744385 batch: 366/840\n",
      "Batch loss: 0.5258787870407104 batch: 367/840\n",
      "Batch loss: 0.7833674550056458 batch: 368/840\n",
      "Batch loss: 0.6726873517036438 batch: 369/840\n",
      "Batch loss: 0.7916128039360046 batch: 370/840\n",
      "Batch loss: 0.6822961568832397 batch: 371/840\n",
      "Batch loss: 0.47547826170921326 batch: 372/840\n",
      "Batch loss: 0.6080743074417114 batch: 373/840\n",
      "Batch loss: 0.6931324601173401 batch: 374/840\n",
      "Batch loss: 0.5454888343811035 batch: 375/840\n",
      "Batch loss: 0.44436702132225037 batch: 376/840\n",
      "Batch loss: 0.7600715160369873 batch: 377/840\n",
      "Batch loss: 0.6522443890571594 batch: 378/840\n",
      "Batch loss: 0.5170013308525085 batch: 379/840\n",
      "Batch loss: 0.7951309680938721 batch: 380/840\n",
      "Batch loss: 0.9612484574317932 batch: 381/840\n",
      "Batch loss: 0.6780532598495483 batch: 382/840\n",
      "Batch loss: 0.7030230164527893 batch: 383/840\n",
      "Batch loss: 0.6324485540390015 batch: 384/840\n",
      "Batch loss: 0.6437693238258362 batch: 385/840\n",
      "Batch loss: 0.7066855430603027 batch: 386/840\n",
      "Batch loss: 0.5752226114273071 batch: 387/840\n",
      "Batch loss: 0.6892290711402893 batch: 388/840\n",
      "Batch loss: 0.5539736747741699 batch: 389/840\n",
      "Batch loss: 0.8326975107192993 batch: 390/840\n",
      "Batch loss: 0.6070355772972107 batch: 391/840\n",
      "Batch loss: 0.5680962204933167 batch: 392/840\n",
      "Batch loss: 0.48093074560165405 batch: 393/840\n",
      "Batch loss: 0.7619759440422058 batch: 394/840\n",
      "Batch loss: 0.6246589422225952 batch: 395/840\n",
      "Batch loss: 0.6647646427154541 batch: 396/840\n",
      "Batch loss: 0.5899198651313782 batch: 397/840\n",
      "Batch loss: 0.7780020833015442 batch: 398/840\n",
      "Batch loss: 0.46448612213134766 batch: 399/840\n",
      "Batch loss: 0.5718801617622375 batch: 400/840\n",
      "Batch loss: 0.6877569556236267 batch: 401/840\n",
      "Batch loss: 0.5282734036445618 batch: 402/840\n",
      "Batch loss: 0.6592990756034851 batch: 403/840\n",
      "Batch loss: 0.5808974504470825 batch: 404/840\n",
      "Batch loss: 0.629147469997406 batch: 405/840\n",
      "Batch loss: 0.6156587600708008 batch: 406/840\n",
      "Batch loss: 0.5445139408111572 batch: 407/840\n",
      "Batch loss: 0.7985191345214844 batch: 408/840\n",
      "Batch loss: 0.6870334148406982 batch: 409/840\n",
      "Batch loss: 0.7507280707359314 batch: 410/840\n",
      "Batch loss: 0.6658082008361816 batch: 411/840\n",
      "Batch loss: 0.779462993144989 batch: 412/840\n",
      "Batch loss: 0.735136866569519 batch: 413/840\n",
      "Batch loss: 0.6330938339233398 batch: 414/840\n",
      "Batch loss: 0.8322679996490479 batch: 415/840\n",
      "Batch loss: 0.5441551804542542 batch: 416/840\n",
      "Batch loss: 0.7692745327949524 batch: 417/840\n",
      "Batch loss: 0.837364912033081 batch: 418/840\n",
      "Batch loss: 0.7158857583999634 batch: 419/840\n",
      "Batch loss: 0.7195382714271545 batch: 420/840\n",
      "Batch loss: 0.5528310537338257 batch: 421/840\n",
      "Batch loss: 0.5640355348587036 batch: 422/840\n",
      "Batch loss: 0.6887861490249634 batch: 423/840\n",
      "Batch loss: 0.7135075926780701 batch: 424/840\n",
      "Batch loss: 0.7388296723365784 batch: 425/840\n",
      "Batch loss: 0.6722004413604736 batch: 426/840\n",
      "Batch loss: 0.6257297992706299 batch: 427/840\n",
      "Batch loss: 0.7208126783370972 batch: 428/840\n",
      "Batch loss: 0.5398879051208496 batch: 429/840\n",
      "Batch loss: 0.7952010631561279 batch: 430/840\n",
      "Batch loss: 0.6741282939910889 batch: 431/840\n",
      "Batch loss: 0.5960460901260376 batch: 432/840\n",
      "Batch loss: 0.5357022881507874 batch: 433/840\n",
      "Batch loss: 0.5545645356178284 batch: 434/840\n",
      "Batch loss: 0.781685471534729 batch: 435/840\n",
      "Batch loss: 0.6975224018096924 batch: 436/840\n",
      "Batch loss: 0.7556021213531494 batch: 437/840\n",
      "Batch loss: 0.4734402000904083 batch: 438/840\n",
      "Batch loss: 0.5791715383529663 batch: 439/840\n",
      "Batch loss: 0.7362424731254578 batch: 440/840\n",
      "Batch loss: 0.6414436101913452 batch: 441/840\n",
      "Batch loss: 0.7648074626922607 batch: 442/840\n",
      "Batch loss: 0.651685357093811 batch: 443/840\n",
      "Batch loss: 0.676707923412323 batch: 444/840\n",
      "Batch loss: 0.6303725242614746 batch: 445/840\n",
      "Batch loss: 0.7929897904396057 batch: 446/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.636339545249939 batch: 447/840\n",
      "Batch loss: 0.7086731195449829 batch: 448/840\n",
      "Batch loss: 0.5532137751579285 batch: 449/840\n",
      "Batch loss: 0.6442967057228088 batch: 450/840\n",
      "Batch loss: 0.7028977274894714 batch: 451/840\n",
      "Batch loss: 0.7765887379646301 batch: 452/840\n",
      "Batch loss: 0.6512401700019836 batch: 453/840\n",
      "Batch loss: 0.5835393667221069 batch: 454/840\n",
      "Batch loss: 0.6583204865455627 batch: 455/840\n",
      "Batch loss: 0.6039807796478271 batch: 456/840\n",
      "Batch loss: 0.543761670589447 batch: 457/840\n",
      "Batch loss: 0.6709083318710327 batch: 458/840\n",
      "Batch loss: 0.672484815120697 batch: 459/840\n",
      "Batch loss: 0.5074782371520996 batch: 460/840\n",
      "Batch loss: 0.7323715090751648 batch: 461/840\n",
      "Batch loss: 0.6828070878982544 batch: 462/840\n",
      "Batch loss: 0.8199707269668579 batch: 463/840\n",
      "Batch loss: 0.45494842529296875 batch: 464/840\n",
      "Batch loss: 0.904721200466156 batch: 465/840\n",
      "Batch loss: 0.6809939742088318 batch: 466/840\n",
      "Batch loss: 0.8221043348312378 batch: 467/840\n",
      "Batch loss: 0.6063264012336731 batch: 468/840\n",
      "Batch loss: 0.6292956471443176 batch: 469/840\n",
      "Batch loss: 0.6614074110984802 batch: 470/840\n",
      "Batch loss: 0.7490280270576477 batch: 471/840\n",
      "Batch loss: 0.6895444393157959 batch: 472/840\n",
      "Batch loss: 0.6742106080055237 batch: 473/840\n",
      "Batch loss: 0.6011437773704529 batch: 474/840\n",
      "Batch loss: 0.6236380338668823 batch: 475/840\n",
      "Batch loss: 0.6710294485092163 batch: 476/840\n",
      "Batch loss: 0.5413771867752075 batch: 477/840\n",
      "Batch loss: 0.5596311092376709 batch: 478/840\n",
      "Batch loss: 0.511132538318634 batch: 479/840\n",
      "Batch loss: 0.6039208769798279 batch: 480/840\n",
      "Batch loss: 0.6602520942687988 batch: 481/840\n",
      "Batch loss: 0.7004439830780029 batch: 482/840\n",
      "Batch loss: 0.7378423810005188 batch: 483/840\n",
      "Batch loss: 0.6098974347114563 batch: 484/840\n",
      "Batch loss: 0.5791708827018738 batch: 485/840\n",
      "Batch loss: 0.6624462008476257 batch: 486/840\n",
      "Batch loss: 0.5192240476608276 batch: 487/840\n",
      "Batch loss: 0.6198405623435974 batch: 488/840\n",
      "Batch loss: 0.591937243938446 batch: 489/840\n",
      "Batch loss: 0.7375438213348389 batch: 490/840\n",
      "Batch loss: 0.3965136706829071 batch: 491/840\n",
      "Batch loss: 0.7623504400253296 batch: 492/840\n",
      "Batch loss: 0.7084364891052246 batch: 493/840\n",
      "Batch loss: 0.4315682351589203 batch: 494/840\n",
      "Batch loss: 0.7036900520324707 batch: 495/840\n",
      "Batch loss: 0.6878423094749451 batch: 496/840\n",
      "Batch loss: 0.6804498434066772 batch: 497/840\n",
      "Batch loss: 0.489009827375412 batch: 498/840\n",
      "Batch loss: 0.5769636631011963 batch: 499/840\n",
      "Batch loss: 0.5927760601043701 batch: 500/840\n",
      "Batch loss: 0.546172022819519 batch: 501/840\n",
      "Batch loss: 0.6935961842536926 batch: 502/840\n",
      "Batch loss: 0.5107674598693848 batch: 503/840\n",
      "Batch loss: 0.5878217220306396 batch: 504/840\n",
      "Batch loss: 0.7806567549705505 batch: 505/840\n",
      "Batch loss: 0.5370041131973267 batch: 506/840\n",
      "Batch loss: 0.8041580319404602 batch: 507/840\n",
      "Batch loss: 0.620310366153717 batch: 508/840\n",
      "Batch loss: 0.7751017212867737 batch: 509/840\n",
      "Batch loss: 0.6451423764228821 batch: 510/840\n",
      "Batch loss: 0.5889772176742554 batch: 511/840\n",
      "Batch loss: 0.7299354076385498 batch: 512/840\n",
      "Batch loss: 0.586275041103363 batch: 513/840\n",
      "Batch loss: 0.6604963541030884 batch: 514/840\n",
      "Batch loss: 0.4536445140838623 batch: 515/840\n",
      "Batch loss: 0.6669771075248718 batch: 516/840\n",
      "Batch loss: 0.654039740562439 batch: 517/840\n",
      "Batch loss: 0.585945725440979 batch: 518/840\n",
      "Batch loss: 0.7112500071525574 batch: 519/840\n",
      "Batch loss: 0.605008065700531 batch: 520/840\n",
      "Batch loss: 0.7903016805648804 batch: 521/840\n",
      "Batch loss: 0.510465145111084 batch: 522/840\n",
      "Batch loss: 0.5489230751991272 batch: 523/840\n",
      "Batch loss: 0.6487815380096436 batch: 524/840\n",
      "Batch loss: 0.6648113131523132 batch: 525/840\n",
      "Batch loss: 0.7470545768737793 batch: 526/840\n",
      "Batch loss: 0.6413627862930298 batch: 527/840\n",
      "Batch loss: 0.6617627143859863 batch: 528/840\n",
      "Batch loss: 0.5560178756713867 batch: 529/840\n",
      "Batch loss: 0.5972720980644226 batch: 530/840\n",
      "Batch loss: 0.5813930034637451 batch: 531/840\n",
      "Batch loss: 0.44834980368614197 batch: 532/840\n",
      "Batch loss: 0.7520504593849182 batch: 533/840\n",
      "Batch loss: 0.5736697912216187 batch: 534/840\n",
      "Batch loss: 0.6112828850746155 batch: 535/840\n",
      "Batch loss: 0.6730669140815735 batch: 536/840\n",
      "Batch loss: 0.6492143273353577 batch: 537/840\n",
      "Batch loss: 0.47650882601737976 batch: 538/840\n",
      "Batch loss: 0.5686668157577515 batch: 539/840\n",
      "Batch loss: 0.8911743760108948 batch: 540/840\n",
      "Batch loss: 0.6451303362846375 batch: 541/840\n",
      "Batch loss: 0.6784842610359192 batch: 542/840\n",
      "Batch loss: 0.4692623019218445 batch: 543/840\n",
      "Batch loss: 0.6511328220367432 batch: 544/840\n",
      "Batch loss: 0.6972582340240479 batch: 545/840\n",
      "Batch loss: 0.6243957281112671 batch: 546/840\n",
      "Batch loss: 0.7212463617324829 batch: 547/840\n",
      "Batch loss: 0.5514122843742371 batch: 548/840\n",
      "Batch loss: 0.6051757335662842 batch: 549/840\n",
      "Batch loss: 0.5654012560844421 batch: 550/840\n",
      "Batch loss: 0.6570783853530884 batch: 551/840\n",
      "Batch loss: 0.5251586437225342 batch: 552/840\n",
      "Batch loss: 0.7291300296783447 batch: 553/840\n",
      "Batch loss: 0.7305286526679993 batch: 554/840\n",
      "Batch loss: 0.5884832143783569 batch: 555/840\n",
      "Batch loss: 0.6728386878967285 batch: 556/840\n",
      "Batch loss: 0.673999547958374 batch: 557/840\n",
      "Batch loss: 0.6729382276535034 batch: 558/840\n",
      "Batch loss: 0.6387395262718201 batch: 559/840\n",
      "Batch loss: 0.6414124369621277 batch: 560/840\n",
      "Batch loss: 0.61347895860672 batch: 561/840\n",
      "Batch loss: 0.6074053645133972 batch: 562/840\n",
      "Batch loss: 0.47602084279060364 batch: 563/840\n",
      "Batch loss: 0.6949722170829773 batch: 564/840\n",
      "Batch loss: 0.756334125995636 batch: 565/840\n",
      "Batch loss: 0.6493483781814575 batch: 566/840\n",
      "Batch loss: 0.7786383032798767 batch: 567/840\n",
      "Batch loss: 0.6022438406944275 batch: 568/840\n",
      "Batch loss: 0.7196279168128967 batch: 569/840\n",
      "Batch loss: 0.46318501234054565 batch: 570/840\n",
      "Batch loss: 0.6868436932563782 batch: 571/840\n",
      "Batch loss: 0.6890386343002319 batch: 572/840\n",
      "Batch loss: 0.6202488541603088 batch: 573/840\n",
      "Batch loss: 0.6902375221252441 batch: 574/840\n",
      "Batch loss: 0.5774185061454773 batch: 575/840\n",
      "Batch loss: 0.5797051787376404 batch: 576/840\n",
      "Batch loss: 0.6177721619606018 batch: 577/840\n",
      "Batch loss: 0.8178910613059998 batch: 578/840\n",
      "Batch loss: 0.7021980285644531 batch: 579/840\n",
      "Batch loss: 0.8290143609046936 batch: 580/840\n",
      "Batch loss: 0.6990094780921936 batch: 581/840\n",
      "Batch loss: 0.8143937587738037 batch: 582/840\n",
      "Batch loss: 0.5986464023590088 batch: 583/840\n",
      "Batch loss: 0.7892124652862549 batch: 584/840\n",
      "Batch loss: 0.6397903561592102 batch: 585/840\n",
      "Batch loss: 0.616716742515564 batch: 586/840\n",
      "Batch loss: 0.7122653722763062 batch: 587/840\n",
      "Batch loss: 0.5908159017562866 batch: 588/840\n",
      "Batch loss: 0.8461025953292847 batch: 589/840\n",
      "Batch loss: 0.6098546385765076 batch: 590/840\n",
      "Batch loss: 0.5011546611785889 batch: 591/840\n",
      "Batch loss: 0.44578105211257935 batch: 592/840\n",
      "Batch loss: 0.6834196448326111 batch: 593/840\n",
      "Batch loss: 0.7060520052909851 batch: 594/840\n",
      "Batch loss: 0.34010207653045654 batch: 595/840\n",
      "Batch loss: 0.45629599690437317 batch: 596/840\n",
      "Batch loss: 0.5443628430366516 batch: 597/840\n",
      "Batch loss: 0.40094590187072754 batch: 598/840\n",
      "Batch loss: 0.6432642340660095 batch: 599/840\n",
      "Batch loss: 0.6826204061508179 batch: 600/840\n",
      "Batch loss: 0.7980536818504333 batch: 601/840\n",
      "Batch loss: 0.5105248093605042 batch: 602/840\n",
      "Batch loss: 0.6359168887138367 batch: 603/840\n",
      "Batch loss: 0.6085730791091919 batch: 604/840\n",
      "Batch loss: 0.8401634693145752 batch: 605/840\n",
      "Batch loss: 0.8221222758293152 batch: 606/840\n",
      "Batch loss: 0.715262234210968 batch: 607/840\n",
      "Batch loss: 0.6655756235122681 batch: 608/840\n",
      "Batch loss: 0.5602356791496277 batch: 609/840\n",
      "Batch loss: 0.8414802551269531 batch: 610/840\n",
      "Batch loss: 0.6462727189064026 batch: 611/840\n",
      "Batch loss: 0.7560555338859558 batch: 612/840\n",
      "Batch loss: 0.8358043432235718 batch: 613/840\n",
      "Batch loss: 0.695248007774353 batch: 614/840\n",
      "Batch loss: 0.5789650678634644 batch: 615/840\n",
      "Batch loss: 0.6083530187606812 batch: 616/840\n",
      "Batch loss: 0.4192081689834595 batch: 617/840\n",
      "Batch loss: 0.568108081817627 batch: 618/840\n",
      "Batch loss: 0.7855862975120544 batch: 619/840\n",
      "Batch loss: 0.626331627368927 batch: 620/840\n",
      "Batch loss: 0.7733395099639893 batch: 621/840\n",
      "Batch loss: 0.7392498850822449 batch: 622/840\n",
      "Batch loss: 0.5865130424499512 batch: 623/840\n",
      "Batch loss: 0.8188086748123169 batch: 624/840\n",
      "Batch loss: 0.7162477374076843 batch: 625/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5647445321083069 batch: 626/840\n",
      "Batch loss: 0.576361894607544 batch: 627/840\n",
      "Batch loss: 0.6583991050720215 batch: 628/840\n",
      "Batch loss: 0.5965865850448608 batch: 629/840\n",
      "Batch loss: 0.6124563813209534 batch: 630/840\n",
      "Batch loss: 0.6473554968833923 batch: 631/840\n",
      "Batch loss: 0.7086635828018188 batch: 632/840\n",
      "Batch loss: 0.5127256512641907 batch: 633/840\n",
      "Batch loss: 0.6960954070091248 batch: 634/840\n",
      "Batch loss: 0.5632712841033936 batch: 635/840\n",
      "Batch loss: 0.5921258330345154 batch: 636/840\n",
      "Batch loss: 0.5026082992553711 batch: 637/840\n",
      "Batch loss: 0.683290421962738 batch: 638/840\n",
      "Batch loss: 0.6440385580062866 batch: 639/840\n",
      "Batch loss: 0.6322287321090698 batch: 640/840\n",
      "Batch loss: 0.7800689935684204 batch: 641/840\n",
      "Batch loss: 0.573424756526947 batch: 642/840\n",
      "Batch loss: 1.0185548067092896 batch: 643/840\n",
      "Batch loss: 0.6422721743583679 batch: 644/840\n",
      "Batch loss: 0.5797909498214722 batch: 645/840\n",
      "Batch loss: 0.6838799118995667 batch: 646/840\n",
      "Batch loss: 0.7534171342849731 batch: 647/840\n",
      "Batch loss: 0.667603611946106 batch: 648/840\n",
      "Batch loss: 0.6193262338638306 batch: 649/840\n",
      "Batch loss: 0.5594245791435242 batch: 650/840\n",
      "Batch loss: 0.5699360370635986 batch: 651/840\n",
      "Batch loss: 0.6532782912254333 batch: 652/840\n",
      "Batch loss: 0.5660365223884583 batch: 653/840\n",
      "Batch loss: 0.9384874701499939 batch: 654/840\n",
      "Batch loss: 0.5719814300537109 batch: 655/840\n",
      "Batch loss: 0.6650614738464355 batch: 656/840\n",
      "Batch loss: 0.6430889368057251 batch: 657/840\n",
      "Batch loss: 0.6886972188949585 batch: 658/840\n",
      "Batch loss: 0.6633937954902649 batch: 659/840\n",
      "Batch loss: 0.726774275302887 batch: 660/840\n",
      "Batch loss: 0.7184497714042664 batch: 661/840\n",
      "Batch loss: 0.6352951526641846 batch: 662/840\n",
      "Batch loss: 0.553671658039093 batch: 663/840\n",
      "Batch loss: 0.6575353741645813 batch: 664/840\n",
      "Batch loss: 0.6869665384292603 batch: 665/840\n",
      "Batch loss: 0.6207258105278015 batch: 666/840\n",
      "Batch loss: 0.7381486296653748 batch: 667/840\n",
      "Batch loss: 0.536876380443573 batch: 668/840\n",
      "Batch loss: 0.5969077348709106 batch: 669/840\n",
      "Batch loss: 0.6358956694602966 batch: 670/840\n",
      "Batch loss: 0.632819414138794 batch: 671/840\n",
      "Batch loss: 0.6038539409637451 batch: 672/840\n",
      "Batch loss: 0.8144927024841309 batch: 673/840\n",
      "Batch loss: 0.822446346282959 batch: 674/840\n",
      "Batch loss: 0.6284465193748474 batch: 675/840\n",
      "Batch loss: 0.49777859449386597 batch: 676/840\n",
      "Batch loss: 0.6953803300857544 batch: 677/840\n",
      "Batch loss: 0.7604956030845642 batch: 678/840\n",
      "Batch loss: 0.8340761065483093 batch: 679/840\n",
      "Batch loss: 0.6868633031845093 batch: 680/840\n",
      "Batch loss: 0.724358081817627 batch: 681/840\n",
      "Batch loss: 0.6906272172927856 batch: 682/840\n",
      "Batch loss: 0.5784551501274109 batch: 683/840\n",
      "Batch loss: 0.825766384601593 batch: 684/840\n",
      "Batch loss: 0.6972206234931946 batch: 685/840\n",
      "Batch loss: 0.8240220546722412 batch: 686/840\n",
      "Batch loss: 0.7557879686355591 batch: 687/840\n",
      "Batch loss: 0.650726854801178 batch: 688/840\n",
      "Batch loss: 0.5562285780906677 batch: 689/840\n",
      "Batch loss: 0.5416309833526611 batch: 690/840\n",
      "Batch loss: 0.6622003316879272 batch: 691/840\n",
      "Batch loss: 0.6548200845718384 batch: 692/840\n",
      "Batch loss: 0.6729276180267334 batch: 693/840\n",
      "Batch loss: 0.7211247086524963 batch: 694/840\n",
      "Batch loss: 0.7516255974769592 batch: 695/840\n",
      "Batch loss: 0.5998575687408447 batch: 696/840\n",
      "Batch loss: 0.6805558800697327 batch: 697/840\n",
      "Batch loss: 0.615719199180603 batch: 698/840\n",
      "Batch loss: 0.7064415812492371 batch: 699/840\n",
      "Batch loss: 0.7391304969787598 batch: 700/840\n",
      "Batch loss: 0.7634431719779968 batch: 701/840\n",
      "Batch loss: 0.6384137272834778 batch: 702/840\n",
      "Batch loss: 0.6196831464767456 batch: 703/840\n",
      "Batch loss: 0.8862274289131165 batch: 704/840\n",
      "Batch loss: 0.6265158653259277 batch: 705/840\n",
      "Batch loss: 0.6265519261360168 batch: 706/840\n",
      "Batch loss: 0.7379148006439209 batch: 707/840\n",
      "Batch loss: 0.7537234425544739 batch: 708/840\n",
      "Batch loss: 0.7417618036270142 batch: 709/840\n",
      "Batch loss: 0.7070399522781372 batch: 710/840\n",
      "Batch loss: 0.45755621790885925 batch: 711/840\n",
      "Batch loss: 0.6993865966796875 batch: 712/840\n",
      "Batch loss: 0.6459407210350037 batch: 713/840\n",
      "Batch loss: 0.6163696050643921 batch: 714/840\n",
      "Batch loss: 0.8591667413711548 batch: 715/840\n",
      "Batch loss: 0.6489431858062744 batch: 716/840\n",
      "Batch loss: 0.6321430206298828 batch: 717/840\n",
      "Batch loss: 0.63008052110672 batch: 718/840\n",
      "Batch loss: 0.47273489832878113 batch: 719/840\n",
      "Batch loss: 0.5453336238861084 batch: 720/840\n",
      "Batch loss: 0.8056730628013611 batch: 721/840\n",
      "Batch loss: 0.8743040561676025 batch: 722/840\n",
      "Batch loss: 0.5730441808700562 batch: 723/840\n",
      "Batch loss: 0.7383199334144592 batch: 724/840\n",
      "Batch loss: 0.6598154306411743 batch: 725/840\n",
      "Batch loss: 0.6194438338279724 batch: 726/840\n",
      "Batch loss: 0.7717258930206299 batch: 727/840\n",
      "Batch loss: 0.8214147686958313 batch: 728/840\n",
      "Batch loss: 0.5648561120033264 batch: 729/840\n",
      "Batch loss: 0.6120448112487793 batch: 730/840\n",
      "Batch loss: 0.5518209934234619 batch: 731/840\n",
      "Batch loss: 0.7980049252510071 batch: 732/840\n",
      "Batch loss: 0.615145206451416 batch: 733/840\n",
      "Batch loss: 0.6063286662101746 batch: 734/840\n",
      "Batch loss: 0.6027598977088928 batch: 735/840\n",
      "Batch loss: 0.7250328063964844 batch: 736/840\n",
      "Batch loss: 0.7284079790115356 batch: 737/840\n",
      "Batch loss: 0.4884064197540283 batch: 738/840\n",
      "Batch loss: 0.6786816120147705 batch: 739/840\n",
      "Batch loss: 0.7617895603179932 batch: 740/840\n",
      "Batch loss: 0.5254208445549011 batch: 741/840\n",
      "Batch loss: 0.5292760729789734 batch: 742/840\n",
      "Batch loss: 0.39118102192878723 batch: 743/840\n",
      "Batch loss: 0.5783783793449402 batch: 744/840\n",
      "Batch loss: 0.6028093099594116 batch: 745/840\n",
      "Batch loss: 0.6064770221710205 batch: 746/840\n",
      "Batch loss: 0.6231358051300049 batch: 747/840\n",
      "Batch loss: 0.7423005700111389 batch: 748/840\n",
      "Batch loss: 0.6272719502449036 batch: 749/840\n",
      "Batch loss: 0.5226281881332397 batch: 750/840\n",
      "Batch loss: 0.6810934543609619 batch: 751/840\n",
      "Batch loss: 0.8778481483459473 batch: 752/840\n",
      "Batch loss: 0.525437593460083 batch: 753/840\n",
      "Batch loss: 0.7034143805503845 batch: 754/840\n",
      "Batch loss: 0.6821646690368652 batch: 755/840\n",
      "Batch loss: 0.5704973936080933 batch: 756/840\n",
      "Batch loss: 0.6546870470046997 batch: 757/840\n",
      "Batch loss: 0.5885108709335327 batch: 758/840\n",
      "Batch loss: 0.5215510725975037 batch: 759/840\n",
      "Batch loss: 0.5782721042633057 batch: 760/840\n",
      "Batch loss: 0.5578876733779907 batch: 761/840\n",
      "Batch loss: 0.8428516387939453 batch: 762/840\n",
      "Batch loss: 0.5214850902557373 batch: 763/840\n",
      "Batch loss: 0.7869952917098999 batch: 764/840\n",
      "Batch loss: 0.5379506349563599 batch: 765/840\n",
      "Batch loss: 0.5776470303535461 batch: 766/840\n",
      "Batch loss: 0.7299731373786926 batch: 767/840\n",
      "Batch loss: 0.72312331199646 batch: 768/840\n",
      "Batch loss: 0.604495644569397 batch: 769/840\n",
      "Batch loss: 0.5439364314079285 batch: 770/840\n",
      "Batch loss: 0.5366885662078857 batch: 771/840\n",
      "Batch loss: 0.6275190114974976 batch: 772/840\n",
      "Batch loss: 0.5742002725601196 batch: 773/840\n",
      "Batch loss: 0.49926283955574036 batch: 774/840\n",
      "Batch loss: 0.5675792098045349 batch: 775/840\n",
      "Batch loss: 0.5494810342788696 batch: 776/840\n",
      "Batch loss: 0.560888946056366 batch: 777/840\n",
      "Batch loss: 0.4455196261405945 batch: 778/840\n",
      "Batch loss: 0.7139601111412048 batch: 779/840\n",
      "Batch loss: 0.5974530577659607 batch: 780/840\n",
      "Batch loss: 0.6499314308166504 batch: 781/840\n",
      "Batch loss: 0.6057044863700867 batch: 782/840\n",
      "Batch loss: 0.5154740810394287 batch: 783/840\n",
      "Batch loss: 0.6690409183502197 batch: 784/840\n",
      "Batch loss: 0.566133439540863 batch: 785/840\n",
      "Batch loss: 0.691982090473175 batch: 786/840\n",
      "Batch loss: 0.5679447054862976 batch: 787/840\n",
      "Batch loss: 0.7891698479652405 batch: 788/840\n",
      "Batch loss: 0.7294090390205383 batch: 789/840\n",
      "Batch loss: 0.7722811698913574 batch: 790/840\n",
      "Batch loss: 0.5661606192588806 batch: 791/840\n",
      "Batch loss: 0.4103761911392212 batch: 792/840\n",
      "Batch loss: 0.6368871331214905 batch: 793/840\n",
      "Batch loss: 0.6524325609207153 batch: 794/840\n",
      "Batch loss: 0.6393938064575195 batch: 795/840\n",
      "Batch loss: 0.7487133145332336 batch: 796/840\n",
      "Batch loss: 0.8312733173370361 batch: 797/840\n",
      "Batch loss: 0.707491397857666 batch: 798/840\n",
      "Batch loss: 0.6709442138671875 batch: 799/840\n",
      "Batch loss: 0.5929656028747559 batch: 800/840\n",
      "Batch loss: 0.6971408724784851 batch: 801/840\n",
      "Batch loss: 0.39194586873054504 batch: 802/840\n",
      "Batch loss: 0.5076488256454468 batch: 803/840\n",
      "Batch loss: 0.6691423058509827 batch: 804/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5908075571060181 batch: 805/840\n",
      "Batch loss: 0.6861249804496765 batch: 806/840\n",
      "Batch loss: 0.6320688724517822 batch: 807/840\n",
      "Batch loss: 0.6443777680397034 batch: 808/840\n",
      "Batch loss: 0.5998581051826477 batch: 809/840\n",
      "Batch loss: 0.6267279982566833 batch: 810/840\n",
      "Batch loss: 0.5462456345558167 batch: 811/840\n",
      "Batch loss: 0.6458823680877686 batch: 812/840\n",
      "Batch loss: 0.4515721797943115 batch: 813/840\n",
      "Batch loss: 0.6888258457183838 batch: 814/840\n",
      "Batch loss: 0.6393541693687439 batch: 815/840\n",
      "Batch loss: 0.6975163817405701 batch: 816/840\n",
      "Batch loss: 0.6042117476463318 batch: 817/840\n",
      "Batch loss: 0.6168645024299622 batch: 818/840\n",
      "Batch loss: 0.529721200466156 batch: 819/840\n",
      "Batch loss: 0.6453955769538879 batch: 820/840\n",
      "Batch loss: 0.683186948299408 batch: 821/840\n",
      "Batch loss: 0.6761329174041748 batch: 822/840\n",
      "Batch loss: 0.6958857774734497 batch: 823/840\n",
      "Batch loss: 0.7436498403549194 batch: 824/840\n",
      "Batch loss: 0.6622024774551392 batch: 825/840\n",
      "Batch loss: 0.5878279805183411 batch: 826/840\n",
      "Batch loss: 0.5623992681503296 batch: 827/840\n",
      "Batch loss: 0.8069545030593872 batch: 828/840\n",
      "Batch loss: 0.6174829602241516 batch: 829/840\n",
      "Batch loss: 0.7824080586433411 batch: 830/840\n",
      "Batch loss: 0.6434537768363953 batch: 831/840\n",
      "Batch loss: 0.77278733253479 batch: 832/840\n",
      "Batch loss: 0.7115546464920044 batch: 833/840\n",
      "Batch loss: 0.6346166133880615 batch: 834/840\n",
      "Batch loss: 0.5272562503814697 batch: 835/840\n",
      "Batch loss: 0.5337109565734863 batch: 836/840\n",
      "Batch loss: 0.7192419171333313 batch: 837/840\n",
      "Batch loss: 0.8019983768463135 batch: 838/840\n",
      "Batch loss: 0.5635089874267578 batch: 839/840\n",
      "Batch loss: 0.7120628356933594 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 6/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.826\n",
      "Running epoch 7/15\n",
      "Batch loss: 0.5709648132324219 batch: 1/840\n",
      "Batch loss: 1.1030778884887695 batch: 2/840\n",
      "Batch loss: 0.6089138984680176 batch: 3/840\n",
      "Batch loss: 0.5786057710647583 batch: 4/840\n",
      "Batch loss: 0.6374487280845642 batch: 5/840\n",
      "Batch loss: 0.42577987909317017 batch: 6/840\n",
      "Batch loss: 0.5250431895256042 batch: 7/840\n",
      "Batch loss: 0.6497412919998169 batch: 8/840\n",
      "Batch loss: 0.550657331943512 batch: 9/840\n",
      "Batch loss: 0.6289719343185425 batch: 10/840\n",
      "Batch loss: 0.5493304133415222 batch: 11/840\n",
      "Batch loss: 0.5218162536621094 batch: 12/840\n",
      "Batch loss: 0.545586884021759 batch: 13/840\n",
      "Batch loss: 0.6334134340286255 batch: 14/840\n",
      "Batch loss: 0.5996279120445251 batch: 15/840\n",
      "Batch loss: 0.5236691236495972 batch: 16/840\n",
      "Batch loss: 0.5430002808570862 batch: 17/840\n",
      "Batch loss: 0.7320389747619629 batch: 18/840\n",
      "Batch loss: 0.6276296973228455 batch: 19/840\n",
      "Batch loss: 0.8326326012611389 batch: 20/840\n",
      "Batch loss: 0.6765657663345337 batch: 21/840\n",
      "Batch loss: 0.5481697916984558 batch: 22/840\n",
      "Batch loss: 0.6750845313072205 batch: 23/840\n",
      "Batch loss: 0.5419452786445618 batch: 24/840\n",
      "Batch loss: 0.6366824507713318 batch: 25/840\n",
      "Batch loss: 0.7574360370635986 batch: 26/840\n",
      "Batch loss: 0.6115626692771912 batch: 27/840\n",
      "Batch loss: 0.7246123552322388 batch: 28/840\n",
      "Batch loss: 0.8061103224754333 batch: 29/840\n",
      "Batch loss: 0.5983794927597046 batch: 30/840\n",
      "Batch loss: 0.6463040709495544 batch: 31/840\n",
      "Batch loss: 0.5967994332313538 batch: 32/840\n",
      "Batch loss: 0.692426860332489 batch: 33/840\n",
      "Batch loss: 0.5772900581359863 batch: 34/840\n",
      "Batch loss: 0.6560205817222595 batch: 35/840\n",
      "Batch loss: 0.5551370978355408 batch: 36/840\n",
      "Batch loss: 0.6974178552627563 batch: 37/840\n",
      "Batch loss: 0.6273386478424072 batch: 38/840\n",
      "Batch loss: 0.6678430438041687 batch: 39/840\n",
      "Batch loss: 0.6021298766136169 batch: 40/840\n",
      "Batch loss: 0.7519933581352234 batch: 41/840\n",
      "Batch loss: 0.6552875638008118 batch: 42/840\n",
      "Batch loss: 0.6431053280830383 batch: 43/840\n",
      "Batch loss: 0.6723336577415466 batch: 44/840\n",
      "Batch loss: 0.6939937472343445 batch: 45/840\n",
      "Batch loss: 0.538140058517456 batch: 46/840\n",
      "Batch loss: 0.4991813600063324 batch: 47/840\n",
      "Batch loss: 0.6798657774925232 batch: 48/840\n",
      "Batch loss: 0.6546993851661682 batch: 49/840\n",
      "Batch loss: 0.6509755849838257 batch: 50/840\n",
      "Batch loss: 0.6866243481636047 batch: 51/840\n",
      "Batch loss: 0.7651522159576416 batch: 52/840\n",
      "Batch loss: 0.5973700881004333 batch: 53/840\n",
      "Batch loss: 0.7065171599388123 batch: 54/840\n",
      "Batch loss: 0.6591717600822449 batch: 55/840\n",
      "Batch loss: 0.5859135389328003 batch: 56/840\n",
      "Batch loss: 0.6672688126564026 batch: 57/840\n",
      "Batch loss: 0.5670709609985352 batch: 58/840\n",
      "Batch loss: 0.5596886277198792 batch: 59/840\n",
      "Batch loss: 0.5404140949249268 batch: 60/840\n",
      "Batch loss: 0.8673567175865173 batch: 61/840\n",
      "Batch loss: 0.6405292749404907 batch: 62/840\n",
      "Batch loss: 0.5658876895904541 batch: 63/840\n",
      "Batch loss: 0.6584418416023254 batch: 64/840\n",
      "Batch loss: 0.5221930742263794 batch: 65/840\n",
      "Batch loss: 0.6908062696456909 batch: 66/840\n",
      "Batch loss: 0.634023129940033 batch: 67/840\n",
      "Batch loss: 0.6048822999000549 batch: 68/840\n",
      "Batch loss: 0.711967945098877 batch: 69/840\n",
      "Batch loss: 0.6512483358383179 batch: 70/840\n",
      "Batch loss: 0.7938758134841919 batch: 71/840\n",
      "Batch loss: 0.7674853801727295 batch: 72/840\n",
      "Batch loss: 0.6502633094787598 batch: 73/840\n",
      "Batch loss: 0.5675289034843445 batch: 74/840\n",
      "Batch loss: 0.6910108923912048 batch: 75/840\n",
      "Batch loss: 0.4403237998485565 batch: 76/840\n",
      "Batch loss: 0.6436483263969421 batch: 77/840\n",
      "Batch loss: 0.7287301421165466 batch: 78/840\n",
      "Batch loss: 0.5999414324760437 batch: 79/840\n",
      "Batch loss: 0.6305249333381653 batch: 80/840\n",
      "Batch loss: 0.4475507438182831 batch: 81/840\n",
      "Batch loss: 0.6553889513015747 batch: 82/840\n",
      "Batch loss: 0.5449706315994263 batch: 83/840\n",
      "Batch loss: 0.7368735671043396 batch: 84/840\n",
      "Batch loss: 0.6220842003822327 batch: 85/840\n",
      "Batch loss: 1.2030482292175293 batch: 86/840\n",
      "Batch loss: 0.5417267084121704 batch: 87/840\n",
      "Batch loss: 0.5352670550346375 batch: 88/840\n",
      "Batch loss: 0.4907957911491394 batch: 89/840\n",
      "Batch loss: 0.5455800890922546 batch: 90/840\n",
      "Batch loss: 0.5743022561073303 batch: 91/840\n",
      "Batch loss: 0.560980498790741 batch: 92/840\n",
      "Batch loss: 0.6645829081535339 batch: 93/840\n",
      "Batch loss: 0.6464946866035461 batch: 94/840\n",
      "Batch loss: 0.64168381690979 batch: 95/840\n",
      "Batch loss: 0.6211162805557251 batch: 96/840\n",
      "Batch loss: 0.6703132390975952 batch: 97/840\n",
      "Batch loss: 0.7237786650657654 batch: 98/840\n",
      "Batch loss: 0.5943559408187866 batch: 99/840\n",
      "Batch loss: 0.6438949704170227 batch: 100/840\n",
      "Batch loss: 0.5886241793632507 batch: 101/840\n",
      "Batch loss: 0.4758157730102539 batch: 102/840\n",
      "Batch loss: 0.6738863587379456 batch: 103/840\n",
      "Batch loss: 0.5037522315979004 batch: 104/840\n",
      "Batch loss: 0.6836622357368469 batch: 105/840\n",
      "Batch loss: 0.6912097930908203 batch: 106/840\n",
      "Batch loss: 0.5438364148139954 batch: 107/840\n",
      "Batch loss: 0.7802836894989014 batch: 108/840\n",
      "Batch loss: 0.656058669090271 batch: 109/840\n",
      "Batch loss: 0.5124607086181641 batch: 110/840\n",
      "Batch loss: 0.5064674615859985 batch: 111/840\n",
      "Batch loss: 0.6585208773612976 batch: 112/840\n",
      "Batch loss: 0.7331754565238953 batch: 113/840\n",
      "Batch loss: 0.5248286724090576 batch: 114/840\n",
      "Batch loss: 0.651109516620636 batch: 115/840\n",
      "Batch loss: 0.6234719157218933 batch: 116/840\n",
      "Batch loss: 0.6846804618835449 batch: 117/840\n",
      "Batch loss: 0.46248528361320496 batch: 118/840\n",
      "Batch loss: 0.726345419883728 batch: 119/840\n",
      "Batch loss: 0.5850338935852051 batch: 120/840\n",
      "Batch loss: 0.7067732214927673 batch: 121/840\n",
      "Batch loss: 0.8610847592353821 batch: 122/840\n",
      "Batch loss: 0.4936790466308594 batch: 123/840\n",
      "Batch loss: 0.5562310218811035 batch: 124/840\n",
      "Batch loss: 0.6349481344223022 batch: 125/840\n",
      "Batch loss: 0.7475749850273132 batch: 126/840\n",
      "Batch loss: 0.6277144551277161 batch: 127/840\n",
      "Batch loss: 0.623802125453949 batch: 128/840\n",
      "Batch loss: 0.7530741691589355 batch: 129/840\n",
      "Batch loss: 0.526330828666687 batch: 130/840\n",
      "Batch loss: 0.6671264171600342 batch: 131/840\n",
      "Batch loss: 1.1428219079971313 batch: 132/840\n",
      "Batch loss: 0.7483886480331421 batch: 133/840\n",
      "Batch loss: 0.5774515271186829 batch: 134/840\n",
      "Batch loss: 0.5526972413063049 batch: 135/840\n",
      "Batch loss: 0.69753098487854 batch: 136/840\n",
      "Batch loss: 0.6434727311134338 batch: 137/840\n",
      "Batch loss: 0.5649803280830383 batch: 138/840\n",
      "Batch loss: 0.6066953539848328 batch: 139/840\n",
      "Batch loss: 0.6286898255348206 batch: 140/840\n",
      "Batch loss: 0.5427082777023315 batch: 141/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.617235004901886 batch: 142/840\n",
      "Batch loss: 0.6275566220283508 batch: 143/840\n",
      "Batch loss: 0.5123825669288635 batch: 144/840\n",
      "Batch loss: 0.7076588273048401 batch: 145/840\n",
      "Batch loss: 0.7337955236434937 batch: 146/840\n",
      "Batch loss: 0.5931075215339661 batch: 147/840\n",
      "Batch loss: 0.6785932183265686 batch: 148/840\n",
      "Batch loss: 0.6694176197052002 batch: 149/840\n",
      "Batch loss: 0.6968677043914795 batch: 150/840\n",
      "Batch loss: 0.6138027310371399 batch: 151/840\n",
      "Batch loss: 0.584904670715332 batch: 152/840\n",
      "Batch loss: 0.5637467503547668 batch: 153/840\n",
      "Batch loss: 0.7191519141197205 batch: 154/840\n",
      "Batch loss: 0.5358077883720398 batch: 155/840\n",
      "Batch loss: 0.5569371581077576 batch: 156/840\n",
      "Batch loss: 0.6332311034202576 batch: 157/840\n",
      "Batch loss: 0.591666579246521 batch: 158/840\n",
      "Batch loss: 0.5717006921768188 batch: 159/840\n",
      "Batch loss: 0.5851190090179443 batch: 160/840\n",
      "Batch loss: 0.6754816174507141 batch: 161/840\n",
      "Batch loss: 0.6878563165664673 batch: 162/840\n",
      "Batch loss: 0.6785523295402527 batch: 163/840\n",
      "Batch loss: 0.4649832248687744 batch: 164/840\n",
      "Batch loss: 0.5790196061134338 batch: 165/840\n",
      "Batch loss: 0.6299583315849304 batch: 166/840\n",
      "Batch loss: 0.710025429725647 batch: 167/840\n",
      "Batch loss: 0.611667811870575 batch: 168/840\n",
      "Batch loss: 0.6079668998718262 batch: 169/840\n",
      "Batch loss: 0.6402617692947388 batch: 170/840\n",
      "Batch loss: 0.619454026222229 batch: 171/840\n",
      "Batch loss: 0.8016031384468079 batch: 172/840\n",
      "Batch loss: 0.6264302730560303 batch: 173/840\n",
      "Batch loss: 0.6922266483306885 batch: 174/840\n",
      "Batch loss: 0.5735363364219666 batch: 175/840\n",
      "Batch loss: 0.7054312825202942 batch: 176/840\n",
      "Batch loss: 0.6332536935806274 batch: 177/840\n",
      "Batch loss: 0.681161105632782 batch: 178/840\n",
      "Batch loss: 0.6863117218017578 batch: 179/840\n",
      "Batch loss: 0.6314709186553955 batch: 180/840\n",
      "Batch loss: 0.6303591728210449 batch: 181/840\n",
      "Batch loss: 0.6351163983345032 batch: 182/840\n",
      "Batch loss: 0.6341617703437805 batch: 183/840\n",
      "Batch loss: 0.5662977695465088 batch: 184/840\n",
      "Batch loss: 0.5577966570854187 batch: 185/840\n",
      "Batch loss: 0.5232274532318115 batch: 186/840\n",
      "Batch loss: 0.610721230506897 batch: 187/840\n",
      "Batch loss: 0.5684091448783875 batch: 188/840\n",
      "Batch loss: 0.650777280330658 batch: 189/840\n",
      "Batch loss: 0.7988709211349487 batch: 190/840\n",
      "Batch loss: 0.7011765837669373 batch: 191/840\n",
      "Batch loss: 0.4842875301837921 batch: 192/840\n",
      "Batch loss: 0.6546790599822998 batch: 193/840\n",
      "Batch loss: 0.4349280297756195 batch: 194/840\n",
      "Batch loss: 0.6529418230056763 batch: 195/840\n",
      "Batch loss: 0.7706522345542908 batch: 196/840\n",
      "Batch loss: 0.6659131646156311 batch: 197/840\n",
      "Batch loss: 0.5250908732414246 batch: 198/840\n",
      "Batch loss: 0.6679684519767761 batch: 199/840\n",
      "Batch loss: 0.8161227703094482 batch: 200/840\n",
      "Batch loss: 0.5435023307800293 batch: 201/840\n",
      "Batch loss: 0.6364103555679321 batch: 202/840\n",
      "Batch loss: 0.5392619371414185 batch: 203/840\n",
      "Batch loss: 0.6765679121017456 batch: 204/840\n",
      "Batch loss: 0.7779157757759094 batch: 205/840\n",
      "Batch loss: 0.6517800092697144 batch: 206/840\n",
      "Batch loss: 0.5958288311958313 batch: 207/840\n",
      "Batch loss: 0.6020084619522095 batch: 208/840\n",
      "Batch loss: 0.6032416224479675 batch: 209/840\n",
      "Batch loss: 0.6522551774978638 batch: 210/840\n",
      "Batch loss: 0.5992787480354309 batch: 211/840\n",
      "Batch loss: 0.6169359683990479 batch: 212/840\n",
      "Batch loss: 0.6969335675239563 batch: 213/840\n",
      "Batch loss: 0.8692418932914734 batch: 214/840\n",
      "Batch loss: 0.6973483562469482 batch: 215/840\n",
      "Batch loss: 0.5763964056968689 batch: 216/840\n",
      "Batch loss: 0.5403406023979187 batch: 217/840\n",
      "Batch loss: 0.6468042731285095 batch: 218/840\n",
      "Batch loss: 0.6083358526229858 batch: 219/840\n",
      "Batch loss: 0.8605177402496338 batch: 220/840\n",
      "Batch loss: 0.5622709393501282 batch: 221/840\n",
      "Batch loss: 0.7178025841712952 batch: 222/840\n",
      "Batch loss: 0.583894670009613 batch: 223/840\n",
      "Batch loss: 0.7239288091659546 batch: 224/840\n",
      "Batch loss: 0.7315001487731934 batch: 225/840\n",
      "Batch loss: 0.664671003818512 batch: 226/840\n",
      "Batch loss: 0.813810408115387 batch: 227/840\n",
      "Batch loss: 0.4579671621322632 batch: 228/840\n",
      "Batch loss: 0.5448158979415894 batch: 229/840\n",
      "Batch loss: 0.699013352394104 batch: 230/840\n",
      "Batch loss: 0.46274957060813904 batch: 231/840\n",
      "Batch loss: 0.6804159283638 batch: 232/840\n",
      "Batch loss: 0.6873989105224609 batch: 233/840\n",
      "Batch loss: 0.5892499685287476 batch: 234/840\n",
      "Batch loss: 0.7044241428375244 batch: 235/840\n",
      "Batch loss: 0.7184139490127563 batch: 236/840\n",
      "Batch loss: 0.5567940473556519 batch: 237/840\n",
      "Batch loss: 0.8149288296699524 batch: 238/840\n",
      "Batch loss: 0.7082035541534424 batch: 239/840\n",
      "Batch loss: 0.6897788047790527 batch: 240/840\n",
      "Batch loss: 0.7112328410148621 batch: 241/840\n",
      "Batch loss: 0.6207559108734131 batch: 242/840\n",
      "Batch loss: 0.677238941192627 batch: 243/840\n",
      "Batch loss: 0.7937489151954651 batch: 244/840\n",
      "Batch loss: 0.42530471086502075 batch: 245/840\n",
      "Batch loss: 0.6653264760971069 batch: 246/840\n",
      "Batch loss: 0.6642965078353882 batch: 247/840\n",
      "Batch loss: 0.6907752752304077 batch: 248/840\n",
      "Batch loss: 0.8183778524398804 batch: 249/840\n",
      "Batch loss: 0.5694994926452637 batch: 250/840\n",
      "Batch loss: 0.5834177136421204 batch: 251/840\n",
      "Batch loss: 0.5280511975288391 batch: 252/840\n",
      "Batch loss: 0.6864110827445984 batch: 253/840\n",
      "Batch loss: 0.7416823506355286 batch: 254/840\n",
      "Batch loss: 0.6013436913490295 batch: 255/840\n",
      "Batch loss: 0.6292271614074707 batch: 256/840\n",
      "Batch loss: 0.5954334139823914 batch: 257/840\n",
      "Batch loss: 0.7568342089653015 batch: 258/840\n",
      "Batch loss: 0.4816952645778656 batch: 259/840\n",
      "Batch loss: 0.5602381229400635 batch: 260/840\n",
      "Batch loss: 0.6423175930976868 batch: 261/840\n",
      "Batch loss: 0.4626861810684204 batch: 262/840\n",
      "Batch loss: 0.6606723666191101 batch: 263/840\n",
      "Batch loss: 0.6464968323707581 batch: 264/840\n",
      "Batch loss: 0.6906046867370605 batch: 265/840\n",
      "Batch loss: 0.538600742816925 batch: 266/840\n",
      "Batch loss: 0.7358994483947754 batch: 267/840\n",
      "Batch loss: 0.5424865484237671 batch: 268/840\n",
      "Batch loss: 0.5364407896995544 batch: 269/840\n",
      "Batch loss: 0.5676164031028748 batch: 270/840\n",
      "Batch loss: 0.5459401607513428 batch: 271/840\n",
      "Batch loss: 0.7737840414047241 batch: 272/840\n",
      "Batch loss: 0.885267972946167 batch: 273/840\n",
      "Batch loss: 0.6122360229492188 batch: 274/840\n",
      "Batch loss: 0.7429586052894592 batch: 275/840\n",
      "Batch loss: 0.6002721190452576 batch: 276/840\n",
      "Batch loss: 0.5300956964492798 batch: 277/840\n",
      "Batch loss: 0.8114519715309143 batch: 278/840\n",
      "Batch loss: 0.7839530110359192 batch: 279/840\n",
      "Batch loss: 0.7268072366714478 batch: 280/840\n",
      "Batch loss: 0.5830326676368713 batch: 281/840\n",
      "Batch loss: 0.6167712807655334 batch: 282/840\n",
      "Batch loss: 0.6806455850601196 batch: 283/840\n",
      "Batch loss: 0.5122262835502625 batch: 284/840\n",
      "Batch loss: 0.5460365414619446 batch: 285/840\n",
      "Batch loss: 0.7184427380561829 batch: 286/840\n",
      "Batch loss: 0.3997747302055359 batch: 287/840\n",
      "Batch loss: 0.6355633735656738 batch: 288/840\n",
      "Batch loss: 0.774721622467041 batch: 289/840\n",
      "Batch loss: 0.7269738912582397 batch: 290/840\n",
      "Batch loss: 0.7219573855400085 batch: 291/840\n",
      "Batch loss: 0.6193224191665649 batch: 292/840\n",
      "Batch loss: 0.7604551911354065 batch: 293/840\n",
      "Batch loss: 0.6257626414299011 batch: 294/840\n",
      "Batch loss: 0.5383694767951965 batch: 295/840\n",
      "Batch loss: 0.7664808034896851 batch: 296/840\n",
      "Batch loss: 0.7630065679550171 batch: 297/840\n",
      "Batch loss: 0.7520781755447388 batch: 298/840\n",
      "Batch loss: 0.5561292171478271 batch: 299/840\n",
      "Batch loss: 0.8723035454750061 batch: 300/840\n",
      "Batch loss: 0.7494620680809021 batch: 301/840\n",
      "Batch loss: 0.6492491364479065 batch: 302/840\n",
      "Batch loss: 0.7919862866401672 batch: 303/840\n",
      "Batch loss: 0.5279520153999329 batch: 304/840\n",
      "Batch loss: 0.6096018552780151 batch: 305/840\n",
      "Batch loss: 0.6749257445335388 batch: 306/840\n",
      "Batch loss: 0.5630313158035278 batch: 307/840\n",
      "Batch loss: 0.8349484205245972 batch: 308/840\n",
      "Batch loss: 0.5963736772537231 batch: 309/840\n",
      "Batch loss: 0.7870162725448608 batch: 310/840\n",
      "Batch loss: 0.6009953618049622 batch: 311/840\n",
      "Batch loss: 0.7664231657981873 batch: 312/840\n",
      "Batch loss: 0.76102614402771 batch: 313/840\n",
      "Batch loss: 0.5854833722114563 batch: 314/840\n",
      "Batch loss: 0.5775927305221558 batch: 315/840\n",
      "Batch loss: 0.5259283185005188 batch: 316/840\n",
      "Batch loss: 0.7563386559486389 batch: 317/840\n",
      "Batch loss: 0.6869723796844482 batch: 318/840\n",
      "Batch loss: 0.6978259086608887 batch: 319/840\n",
      "Batch loss: 0.531298041343689 batch: 320/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5292178392410278 batch: 321/840\n",
      "Batch loss: 0.7555029988288879 batch: 322/840\n",
      "Batch loss: 0.8196893334388733 batch: 323/840\n",
      "Batch loss: 0.6750500202178955 batch: 324/840\n",
      "Batch loss: 0.5271837115287781 batch: 325/840\n",
      "Batch loss: 0.5799279808998108 batch: 326/840\n",
      "Batch loss: 0.5284938216209412 batch: 327/840\n",
      "Batch loss: 0.8976287841796875 batch: 328/840\n",
      "Batch loss: 0.7130608558654785 batch: 329/840\n",
      "Batch loss: 0.6223104596138 batch: 330/840\n",
      "Batch loss: 0.7257717847824097 batch: 331/840\n",
      "Batch loss: 0.6819815635681152 batch: 332/840\n",
      "Batch loss: 0.6146687269210815 batch: 333/840\n",
      "Batch loss: 0.7319801449775696 batch: 334/840\n",
      "Batch loss: 0.5656142830848694 batch: 335/840\n",
      "Batch loss: 0.815721333026886 batch: 336/840\n",
      "Batch loss: 0.8373140692710876 batch: 337/840\n",
      "Batch loss: 0.6743718981742859 batch: 338/840\n",
      "Batch loss: 0.5682758092880249 batch: 339/840\n",
      "Batch loss: 0.7881045341491699 batch: 340/840\n",
      "Batch loss: 0.5675820112228394 batch: 341/840\n",
      "Batch loss: 0.48436182737350464 batch: 342/840\n",
      "Batch loss: 0.8499706387519836 batch: 343/840\n",
      "Batch loss: 0.5966794490814209 batch: 344/840\n",
      "Batch loss: 0.5064555406570435 batch: 345/840\n",
      "Batch loss: 0.5821129083633423 batch: 346/840\n",
      "Batch loss: 0.7116487622261047 batch: 347/840\n",
      "Batch loss: 0.5555675625801086 batch: 348/840\n",
      "Batch loss: 0.6614831686019897 batch: 349/840\n",
      "Batch loss: 0.5861234068870544 batch: 350/840\n",
      "Batch loss: 0.6182787418365479 batch: 351/840\n",
      "Batch loss: 0.640482485294342 batch: 352/840\n",
      "Batch loss: 0.7514700293540955 batch: 353/840\n",
      "Batch loss: 0.6186257600784302 batch: 354/840\n",
      "Batch loss: 0.5316941142082214 batch: 355/840\n",
      "Batch loss: 0.667888343334198 batch: 356/840\n",
      "Batch loss: 0.542559802532196 batch: 357/840\n",
      "Batch loss: 0.9224643111228943 batch: 358/840\n",
      "Batch loss: 0.6226885318756104 batch: 359/840\n",
      "Batch loss: 0.7964179515838623 batch: 360/840\n",
      "Batch loss: 0.7036740183830261 batch: 361/840\n",
      "Batch loss: 0.5156801342964172 batch: 362/840\n",
      "Batch loss: 0.5411044955253601 batch: 363/840\n",
      "Batch loss: 0.6582807898521423 batch: 364/840\n",
      "Batch loss: 0.5481505393981934 batch: 365/840\n",
      "Batch loss: 0.5722653865814209 batch: 366/840\n",
      "Batch loss: 0.48075780272483826 batch: 367/840\n",
      "Batch loss: 0.8118645548820496 batch: 368/840\n",
      "Batch loss: 0.6479403972625732 batch: 369/840\n",
      "Batch loss: 0.6795668601989746 batch: 370/840\n",
      "Batch loss: 0.6897702813148499 batch: 371/840\n",
      "Batch loss: 0.5143030881881714 batch: 372/840\n",
      "Batch loss: 0.6386053562164307 batch: 373/840\n",
      "Batch loss: 0.6829317212104797 batch: 374/840\n",
      "Batch loss: 0.5991946458816528 batch: 375/840\n",
      "Batch loss: 0.603059709072113 batch: 376/840\n",
      "Batch loss: 0.7516549825668335 batch: 377/840\n",
      "Batch loss: 0.5072377920150757 batch: 378/840\n",
      "Batch loss: 0.5606902241706848 batch: 379/840\n",
      "Batch loss: 0.8080239295959473 batch: 380/840\n",
      "Batch loss: 0.8941977620124817 batch: 381/840\n",
      "Batch loss: 0.7502934336662292 batch: 382/840\n",
      "Batch loss: 0.6826096177101135 batch: 383/840\n",
      "Batch loss: 0.5839645862579346 batch: 384/840\n",
      "Batch loss: 0.7403245568275452 batch: 385/840\n",
      "Batch loss: 0.8152364492416382 batch: 386/840\n",
      "Batch loss: 0.626849889755249 batch: 387/840\n",
      "Batch loss: 0.7797127366065979 batch: 388/840\n",
      "Batch loss: 0.5933694243431091 batch: 389/840\n",
      "Batch loss: 0.8937498331069946 batch: 390/840\n",
      "Batch loss: 0.7754369378089905 batch: 391/840\n",
      "Batch loss: 0.6048688292503357 batch: 392/840\n",
      "Batch loss: 0.5053113698959351 batch: 393/840\n",
      "Batch loss: 0.6858460903167725 batch: 394/840\n",
      "Batch loss: 0.6181448698043823 batch: 395/840\n",
      "Batch loss: 0.6559234857559204 batch: 396/840\n",
      "Batch loss: 0.5992431044578552 batch: 397/840\n",
      "Batch loss: 0.6610549688339233 batch: 398/840\n",
      "Batch loss: 0.46904799342155457 batch: 399/840\n",
      "Batch loss: 0.5837483406066895 batch: 400/840\n",
      "Batch loss: 0.8170451521873474 batch: 401/840\n",
      "Batch loss: 0.5359739661216736 batch: 402/840\n",
      "Batch loss: 0.6570939421653748 batch: 403/840\n",
      "Batch loss: 0.5770530104637146 batch: 404/840\n",
      "Batch loss: 0.57376629114151 batch: 405/840\n",
      "Batch loss: 0.6706852912902832 batch: 406/840\n",
      "Batch loss: 0.643984317779541 batch: 407/840\n",
      "Batch loss: 0.5959054231643677 batch: 408/840\n",
      "Batch loss: 0.7585865259170532 batch: 409/840\n",
      "Batch loss: 0.7683122158050537 batch: 410/840\n",
      "Batch loss: 0.5960739254951477 batch: 411/840\n",
      "Batch loss: 0.7757831811904907 batch: 412/840\n",
      "Batch loss: 0.7038189768791199 batch: 413/840\n",
      "Batch loss: 0.5786711573600769 batch: 414/840\n",
      "Batch loss: 0.9196168780326843 batch: 415/840\n",
      "Batch loss: 0.5308797359466553 batch: 416/840\n",
      "Batch loss: 0.6007914543151855 batch: 417/840\n",
      "Batch loss: 0.7376317381858826 batch: 418/840\n",
      "Batch loss: 0.7246309518814087 batch: 419/840\n",
      "Batch loss: 0.7282619476318359 batch: 420/840\n",
      "Batch loss: 0.5378038883209229 batch: 421/840\n",
      "Batch loss: 0.6222115755081177 batch: 422/840\n",
      "Batch loss: 0.6118895411491394 batch: 423/840\n",
      "Batch loss: 0.8123460412025452 batch: 424/840\n",
      "Batch loss: 0.7253207564353943 batch: 425/840\n",
      "Batch loss: 0.6017680168151855 batch: 426/840\n",
      "Batch loss: 0.5973221063613892 batch: 427/840\n",
      "Batch loss: 0.7269734740257263 batch: 428/840\n",
      "Batch loss: 0.557272732257843 batch: 429/840\n",
      "Batch loss: 0.7518962621688843 batch: 430/840\n",
      "Batch loss: 0.6837667226791382 batch: 431/840\n",
      "Batch loss: 0.6263042092323303 batch: 432/840\n",
      "Batch loss: 0.5259963870048523 batch: 433/840\n",
      "Batch loss: 0.5737271308898926 batch: 434/840\n",
      "Batch loss: 0.7782942056655884 batch: 435/840\n",
      "Batch loss: 0.5721579194068909 batch: 436/840\n",
      "Batch loss: 0.7150410413742065 batch: 437/840\n",
      "Batch loss: 0.43987685441970825 batch: 438/840\n",
      "Batch loss: 0.5828715562820435 batch: 439/840\n",
      "Batch loss: 0.7165267467498779 batch: 440/840\n",
      "Batch loss: 0.5935258269309998 batch: 441/840\n",
      "Batch loss: 0.6733680963516235 batch: 442/840\n",
      "Batch loss: 0.6127563714981079 batch: 443/840\n",
      "Batch loss: 0.6998282670974731 batch: 444/840\n",
      "Batch loss: 0.5941201448440552 batch: 445/840\n",
      "Batch loss: 0.7883711457252502 batch: 446/840\n",
      "Batch loss: 0.7860987782478333 batch: 447/840\n",
      "Batch loss: 0.7906144857406616 batch: 448/840\n",
      "Batch loss: 0.5540926456451416 batch: 449/840\n",
      "Batch loss: 0.6808439493179321 batch: 450/840\n",
      "Batch loss: 0.6371021866798401 batch: 451/840\n",
      "Batch loss: 0.686668872833252 batch: 452/840\n",
      "Batch loss: 0.6044151782989502 batch: 453/840\n",
      "Batch loss: 0.6107905507087708 batch: 454/840\n",
      "Batch loss: 0.6549685001373291 batch: 455/840\n",
      "Batch loss: 0.5304587483406067 batch: 456/840\n",
      "Batch loss: 0.5909923911094666 batch: 457/840\n",
      "Batch loss: 0.6451926231384277 batch: 458/840\n",
      "Batch loss: 0.5037850737571716 batch: 459/840\n",
      "Batch loss: 0.48150208592414856 batch: 460/840\n",
      "Batch loss: 0.6906142234802246 batch: 461/840\n",
      "Batch loss: 0.653928279876709 batch: 462/840\n",
      "Batch loss: 0.8487632870674133 batch: 463/840\n",
      "Batch loss: 0.45002949237823486 batch: 464/840\n",
      "Batch loss: 0.9069033265113831 batch: 465/840\n",
      "Batch loss: 0.6471363306045532 batch: 466/840\n",
      "Batch loss: 0.880522608757019 batch: 467/840\n",
      "Batch loss: 0.5974186062812805 batch: 468/840\n",
      "Batch loss: 0.5206358432769775 batch: 469/840\n",
      "Batch loss: 0.601131796836853 batch: 470/840\n",
      "Batch loss: 0.6804284453392029 batch: 471/840\n",
      "Batch loss: 0.6910699605941772 batch: 472/840\n",
      "Batch loss: 0.7179622650146484 batch: 473/840\n",
      "Batch loss: 0.5941975116729736 batch: 474/840\n",
      "Batch loss: 0.6574150323867798 batch: 475/840\n",
      "Batch loss: 0.6802299618721008 batch: 476/840\n",
      "Batch loss: 0.5208990573883057 batch: 477/840\n",
      "Batch loss: 0.4618496298789978 batch: 478/840\n",
      "Batch loss: 0.4466748833656311 batch: 479/840\n",
      "Batch loss: 0.6212098598480225 batch: 480/840\n",
      "Batch loss: 0.667600154876709 batch: 481/840\n",
      "Batch loss: 0.6416140794754028 batch: 482/840\n",
      "Batch loss: 0.6648815274238586 batch: 483/840\n",
      "Batch loss: 0.5691714286804199 batch: 484/840\n",
      "Batch loss: 0.5960108637809753 batch: 485/840\n",
      "Batch loss: 0.6446300745010376 batch: 486/840\n",
      "Batch loss: 0.4199869632720947 batch: 487/840\n",
      "Batch loss: 0.6427903771400452 batch: 488/840\n",
      "Batch loss: 0.6855975389480591 batch: 489/840\n",
      "Batch loss: 0.7422719597816467 batch: 490/840\n",
      "Batch loss: 0.4747777283191681 batch: 491/840\n",
      "Batch loss: 0.7633628845214844 batch: 492/840\n",
      "Batch loss: 0.6189452409744263 batch: 493/840\n",
      "Batch loss: 0.4174266755580902 batch: 494/840\n",
      "Batch loss: 0.7281481027603149 batch: 495/840\n",
      "Batch loss: 0.6631322503089905 batch: 496/840\n",
      "Batch loss: 0.7435570359230042 batch: 497/840\n",
      "Batch loss: 0.4619179666042328 batch: 498/840\n",
      "Batch loss: 0.7895164489746094 batch: 499/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6276779770851135 batch: 500/840\n",
      "Batch loss: 0.5198490023612976 batch: 501/840\n",
      "Batch loss: 0.7441518902778625 batch: 502/840\n",
      "Batch loss: 0.48566871881484985 batch: 503/840\n",
      "Batch loss: 0.6064630746841431 batch: 504/840\n",
      "Batch loss: 0.7661152482032776 batch: 505/840\n",
      "Batch loss: 0.5896593332290649 batch: 506/840\n",
      "Batch loss: 0.7413894534111023 batch: 507/840\n",
      "Batch loss: 0.5950033068656921 batch: 508/840\n",
      "Batch loss: 0.655758798122406 batch: 509/840\n",
      "Batch loss: 0.7134829163551331 batch: 510/840\n",
      "Batch loss: 0.4748131036758423 batch: 511/840\n",
      "Batch loss: 0.6362482309341431 batch: 512/840\n",
      "Batch loss: 0.5462290048599243 batch: 513/840\n",
      "Batch loss: 0.7183212041854858 batch: 514/840\n",
      "Batch loss: 0.5413353443145752 batch: 515/840\n",
      "Batch loss: 0.6891316175460815 batch: 516/840\n",
      "Batch loss: 0.5990265011787415 batch: 517/840\n",
      "Batch loss: 0.6191167831420898 batch: 518/840\n",
      "Batch loss: 0.7804262638092041 batch: 519/840\n",
      "Batch loss: 0.5558063387870789 batch: 520/840\n",
      "Batch loss: 0.6955779790878296 batch: 521/840\n",
      "Batch loss: 0.7268189787864685 batch: 522/840\n",
      "Batch loss: 0.49369126558303833 batch: 523/840\n",
      "Batch loss: 0.6884227991104126 batch: 524/840\n",
      "Batch loss: 0.5957916975021362 batch: 525/840\n",
      "Batch loss: 0.8110994100570679 batch: 526/840\n",
      "Batch loss: 0.7427738904953003 batch: 527/840\n",
      "Batch loss: 0.6095733046531677 batch: 528/840\n",
      "Batch loss: 0.5093529224395752 batch: 529/840\n",
      "Batch loss: 0.6681008338928223 batch: 530/840\n",
      "Batch loss: 0.5548457503318787 batch: 531/840\n",
      "Batch loss: 0.4854000210762024 batch: 532/840\n",
      "Batch loss: 0.6638998985290527 batch: 533/840\n",
      "Batch loss: 0.6497337222099304 batch: 534/840\n",
      "Batch loss: 0.5962560176849365 batch: 535/840\n",
      "Batch loss: 0.7738337516784668 batch: 536/840\n",
      "Batch loss: 0.5915484428405762 batch: 537/840\n",
      "Batch loss: 0.5732245445251465 batch: 538/840\n",
      "Batch loss: 0.544075071811676 batch: 539/840\n",
      "Batch loss: 0.7367690801620483 batch: 540/840\n",
      "Batch loss: 0.6921236515045166 batch: 541/840\n",
      "Batch loss: 0.7977545857429504 batch: 542/840\n",
      "Batch loss: 0.46114039421081543 batch: 543/840\n",
      "Batch loss: 0.6652393937110901 batch: 544/840\n",
      "Batch loss: 0.6169988512992859 batch: 545/840\n",
      "Batch loss: 0.5938172936439514 batch: 546/840\n",
      "Batch loss: 0.5908908843994141 batch: 547/840\n",
      "Batch loss: 0.47782671451568604 batch: 548/840\n",
      "Batch loss: 0.6372015476226807 batch: 549/840\n",
      "Batch loss: 0.5360086560249329 batch: 550/840\n",
      "Batch loss: 0.6102691888809204 batch: 551/840\n",
      "Batch loss: 0.4952876567840576 batch: 552/840\n",
      "Batch loss: 0.713803768157959 batch: 553/840\n",
      "Batch loss: 0.5766637325286865 batch: 554/840\n",
      "Batch loss: 0.6234041452407837 batch: 555/840\n",
      "Batch loss: 0.5666022300720215 batch: 556/840\n",
      "Batch loss: 0.5871455073356628 batch: 557/840\n",
      "Batch loss: 0.6937547922134399 batch: 558/840\n",
      "Batch loss: 0.608640730381012 batch: 559/840\n",
      "Batch loss: 0.6651586294174194 batch: 560/840\n",
      "Batch loss: 0.5537728667259216 batch: 561/840\n",
      "Batch loss: 0.6569433808326721 batch: 562/840\n",
      "Batch loss: 0.4788658022880554 batch: 563/840\n",
      "Batch loss: 0.6968801021575928 batch: 564/840\n",
      "Batch loss: 0.781059980392456 batch: 565/840\n",
      "Batch loss: 0.6978955268859863 batch: 566/840\n",
      "Batch loss: 0.8664994835853577 batch: 567/840\n",
      "Batch loss: 0.5964851379394531 batch: 568/840\n",
      "Batch loss: 0.6084027290344238 batch: 569/840\n",
      "Batch loss: 0.4445919096469879 batch: 570/840\n",
      "Batch loss: 0.6294340491294861 batch: 571/840\n",
      "Batch loss: 0.6792845129966736 batch: 572/840\n",
      "Batch loss: 0.6574609875679016 batch: 573/840\n",
      "Batch loss: 0.7721561193466187 batch: 574/840\n",
      "Batch loss: 0.5692801475524902 batch: 575/840\n",
      "Batch loss: 0.6161585450172424 batch: 576/840\n",
      "Batch loss: 0.5418342351913452 batch: 577/840\n",
      "Batch loss: 0.7180194854736328 batch: 578/840\n",
      "Batch loss: 0.591215193271637 batch: 579/840\n",
      "Batch loss: 0.8728615045547485 batch: 580/840\n",
      "Batch loss: 0.6727045178413391 batch: 581/840\n",
      "Batch loss: 0.8005272150039673 batch: 582/840\n",
      "Batch loss: 0.4954127371311188 batch: 583/840\n",
      "Batch loss: 0.7497552633285522 batch: 584/840\n",
      "Batch loss: 0.5899814367294312 batch: 585/840\n",
      "Batch loss: 0.6174929141998291 batch: 586/840\n",
      "Batch loss: 0.588767945766449 batch: 587/840\n",
      "Batch loss: 0.6275469064712524 batch: 588/840\n",
      "Batch loss: 0.7973417043685913 batch: 589/840\n",
      "Batch loss: 0.553413450717926 batch: 590/840\n",
      "Batch loss: 0.45655399560928345 batch: 591/840\n",
      "Batch loss: 0.5906376838684082 batch: 592/840\n",
      "Batch loss: 0.7400618195533752 batch: 593/840\n",
      "Batch loss: 0.5014061331748962 batch: 594/840\n",
      "Batch loss: 0.4421583414077759 batch: 595/840\n",
      "Batch loss: 0.47416383028030396 batch: 596/840\n",
      "Batch loss: 0.5811131596565247 batch: 597/840\n",
      "Batch loss: 0.438755065202713 batch: 598/840\n",
      "Batch loss: 0.5656000375747681 batch: 599/840\n",
      "Batch loss: 0.7111135125160217 batch: 600/840\n",
      "Batch loss: 0.7686660289764404 batch: 601/840\n",
      "Batch loss: 0.5568882822990417 batch: 602/840\n",
      "Batch loss: 0.5894761085510254 batch: 603/840\n",
      "Batch loss: 0.6511878967285156 batch: 604/840\n",
      "Batch loss: 0.8399825096130371 batch: 605/840\n",
      "Batch loss: 0.7587845325469971 batch: 606/840\n",
      "Batch loss: 0.7308220863342285 batch: 607/840\n",
      "Batch loss: 0.7649915218353271 batch: 608/840\n",
      "Batch loss: 0.530440092086792 batch: 609/840\n",
      "Batch loss: 0.7121700048446655 batch: 610/840\n",
      "Batch loss: 0.8052239418029785 batch: 611/840\n",
      "Batch loss: 0.7126628160476685 batch: 612/840\n",
      "Batch loss: 0.6506633162498474 batch: 613/840\n",
      "Batch loss: 0.6215384602546692 batch: 614/840\n",
      "Batch loss: 0.6293721795082092 batch: 615/840\n",
      "Batch loss: 0.516669511795044 batch: 616/840\n",
      "Batch loss: 0.5263872742652893 batch: 617/840\n",
      "Batch loss: 0.589577317237854 batch: 618/840\n",
      "Batch loss: 0.8536900281906128 batch: 619/840\n",
      "Batch loss: 0.606524646282196 batch: 620/840\n",
      "Batch loss: 0.651582658290863 batch: 621/840\n",
      "Batch loss: 0.6545377969741821 batch: 622/840\n",
      "Batch loss: 0.636722207069397 batch: 623/840\n",
      "Batch loss: 0.834757387638092 batch: 624/840\n",
      "Batch loss: 0.606955349445343 batch: 625/840\n",
      "Batch loss: 0.6253261566162109 batch: 626/840\n",
      "Batch loss: 0.5204188823699951 batch: 627/840\n",
      "Batch loss: 0.6221547722816467 batch: 628/840\n",
      "Batch loss: 0.713726818561554 batch: 629/840\n",
      "Batch loss: 0.613476037979126 batch: 630/840\n",
      "Batch loss: 0.702456533908844 batch: 631/840\n",
      "Batch loss: 0.6939460039138794 batch: 632/840\n",
      "Batch loss: 0.6494579315185547 batch: 633/840\n",
      "Batch loss: 0.6580744385719299 batch: 634/840\n",
      "Batch loss: 0.580546498298645 batch: 635/840\n",
      "Batch loss: 0.5490623712539673 batch: 636/840\n",
      "Batch loss: 0.49038514494895935 batch: 637/840\n",
      "Batch loss: 0.7535250186920166 batch: 638/840\n",
      "Batch loss: 0.761810302734375 batch: 639/840\n",
      "Batch loss: 0.5804468989372253 batch: 640/840\n",
      "Batch loss: 0.75177401304245 batch: 641/840\n",
      "Batch loss: 0.5078227519989014 batch: 642/840\n",
      "Batch loss: 0.9741442203521729 batch: 643/840\n",
      "Batch loss: 0.6283223628997803 batch: 644/840\n",
      "Batch loss: 0.524117648601532 batch: 645/840\n",
      "Batch loss: 0.5751835107803345 batch: 646/840\n",
      "Batch loss: 0.7481020092964172 batch: 647/840\n",
      "Batch loss: 0.7041773796081543 batch: 648/840\n",
      "Batch loss: 0.6209824085235596 batch: 649/840\n",
      "Batch loss: 0.5256218910217285 batch: 650/840\n",
      "Batch loss: 0.6370797753334045 batch: 651/840\n",
      "Batch loss: 0.6085742115974426 batch: 652/840\n",
      "Batch loss: 0.4958714246749878 batch: 653/840\n",
      "Batch loss: 0.8578867316246033 batch: 654/840\n",
      "Batch loss: 0.557511031627655 batch: 655/840\n",
      "Batch loss: 0.6488236784934998 batch: 656/840\n",
      "Batch loss: 0.5456349849700928 batch: 657/840\n",
      "Batch loss: 0.6757360696792603 batch: 658/840\n",
      "Batch loss: 0.7244516015052795 batch: 659/840\n",
      "Batch loss: 0.5907646417617798 batch: 660/840\n",
      "Batch loss: 0.7314589023590088 batch: 661/840\n",
      "Batch loss: 0.6468281745910645 batch: 662/840\n",
      "Batch loss: 0.5201566815376282 batch: 663/840\n",
      "Batch loss: 0.6699867248535156 batch: 664/840\n",
      "Batch loss: 0.6549898386001587 batch: 665/840\n",
      "Batch loss: 0.697869062423706 batch: 666/840\n",
      "Batch loss: 0.7873736619949341 batch: 667/840\n",
      "Batch loss: 0.6783437132835388 batch: 668/840\n",
      "Batch loss: 0.5397817492485046 batch: 669/840\n",
      "Batch loss: 0.6795867085456848 batch: 670/840\n",
      "Batch loss: 0.6287242770195007 batch: 671/840\n",
      "Batch loss: 0.5103703141212463 batch: 672/840\n",
      "Batch loss: 0.7558572888374329 batch: 673/840\n",
      "Batch loss: 0.750302255153656 batch: 674/840\n",
      "Batch loss: 0.6447227597236633 batch: 675/840\n",
      "Batch loss: 0.58421790599823 batch: 676/840\n",
      "Batch loss: 0.6750376224517822 batch: 677/840\n",
      "Batch loss: 0.6612523794174194 batch: 678/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8069524168968201 batch: 679/840\n",
      "Batch loss: 0.6602084636688232 batch: 680/840\n",
      "Batch loss: 0.647300124168396 batch: 681/840\n",
      "Batch loss: 0.6055466532707214 batch: 682/840\n",
      "Batch loss: 0.5354856252670288 batch: 683/840\n",
      "Batch loss: 0.8109566569328308 batch: 684/840\n",
      "Batch loss: 0.6712053418159485 batch: 685/840\n",
      "Batch loss: 0.6825747489929199 batch: 686/840\n",
      "Batch loss: 0.7098442912101746 batch: 687/840\n",
      "Batch loss: 0.6371276378631592 batch: 688/840\n",
      "Batch loss: 0.5666394233703613 batch: 689/840\n",
      "Batch loss: 0.5731914639472961 batch: 690/840\n",
      "Batch loss: 0.6444064974784851 batch: 691/840\n",
      "Batch loss: 0.5869895219802856 batch: 692/840\n",
      "Batch loss: 0.6465593576431274 batch: 693/840\n",
      "Batch loss: 0.7953531742095947 batch: 694/840\n",
      "Batch loss: 0.8226582407951355 batch: 695/840\n",
      "Batch loss: 0.6007908582687378 batch: 696/840\n",
      "Batch loss: 0.6721212863922119 batch: 697/840\n",
      "Batch loss: 0.6237316727638245 batch: 698/840\n",
      "Batch loss: 0.7440046072006226 batch: 699/840\n",
      "Batch loss: 0.7729341387748718 batch: 700/840\n",
      "Batch loss: 0.857451856136322 batch: 701/840\n",
      "Batch loss: 0.6321194171905518 batch: 702/840\n",
      "Batch loss: 0.47479483485221863 batch: 703/840\n",
      "Batch loss: 0.7029556035995483 batch: 704/840\n",
      "Batch loss: 0.5990222692489624 batch: 705/840\n",
      "Batch loss: 0.5936660766601562 batch: 706/840\n",
      "Batch loss: 0.6184207797050476 batch: 707/840\n",
      "Batch loss: 0.6140437722206116 batch: 708/840\n",
      "Batch loss: 0.6552755832672119 batch: 709/840\n",
      "Batch loss: 0.8328386545181274 batch: 710/840\n",
      "Batch loss: 0.4501416087150574 batch: 711/840\n",
      "Batch loss: 0.701049268245697 batch: 712/840\n",
      "Batch loss: 0.5883428454399109 batch: 713/840\n",
      "Batch loss: 0.6703964471817017 batch: 714/840\n",
      "Batch loss: 0.8127490282058716 batch: 715/840\n",
      "Batch loss: 0.6212634444236755 batch: 716/840\n",
      "Batch loss: 0.7207769751548767 batch: 717/840\n",
      "Batch loss: 0.5844826698303223 batch: 718/840\n",
      "Batch loss: 0.4838086664676666 batch: 719/840\n",
      "Batch loss: 0.5432713627815247 batch: 720/840\n",
      "Batch loss: 0.7130946516990662 batch: 721/840\n",
      "Batch loss: 0.7633966207504272 batch: 722/840\n",
      "Batch loss: 0.5293710827827454 batch: 723/840\n",
      "Batch loss: 0.7036890983581543 batch: 724/840\n",
      "Batch loss: 0.6854230761528015 batch: 725/840\n",
      "Batch loss: 0.6812272071838379 batch: 726/840\n",
      "Batch loss: 0.8903337717056274 batch: 727/840\n",
      "Batch loss: 0.7498289346694946 batch: 728/840\n",
      "Batch loss: 0.6686390042304993 batch: 729/840\n",
      "Batch loss: 0.7065771222114563 batch: 730/840\n",
      "Batch loss: 0.5219370126724243 batch: 731/840\n",
      "Batch loss: 0.7908115983009338 batch: 732/840\n",
      "Batch loss: 0.6271607875823975 batch: 733/840\n",
      "Batch loss: 0.6539736986160278 batch: 734/840\n",
      "Batch loss: 0.6600738763809204 batch: 735/840\n",
      "Batch loss: 0.7174165844917297 batch: 736/840\n",
      "Batch loss: 0.6832330226898193 batch: 737/840\n",
      "Batch loss: 0.5600235462188721 batch: 738/840\n",
      "Batch loss: 0.6882997155189514 batch: 739/840\n",
      "Batch loss: 0.7024974226951599 batch: 740/840\n",
      "Batch loss: 0.5201850533485413 batch: 741/840\n",
      "Batch loss: 0.5079082250595093 batch: 742/840\n",
      "Batch loss: 0.5069512724876404 batch: 743/840\n",
      "Batch loss: 0.5185856223106384 batch: 744/840\n",
      "Batch loss: 0.5974175930023193 batch: 745/840\n",
      "Batch loss: 0.7045333981513977 batch: 746/840\n",
      "Batch loss: 0.7278239727020264 batch: 747/840\n",
      "Batch loss: 0.6763189435005188 batch: 748/840\n",
      "Batch loss: 0.5428524613380432 batch: 749/840\n",
      "Batch loss: 0.5373184084892273 batch: 750/840\n",
      "Batch loss: 0.7232205271720886 batch: 751/840\n",
      "Batch loss: 0.8685796856880188 batch: 752/840\n",
      "Batch loss: 0.49410805106163025 batch: 753/840\n",
      "Batch loss: 0.673022985458374 batch: 754/840\n",
      "Batch loss: 0.5962781310081482 batch: 755/840\n",
      "Batch loss: 0.47774845361709595 batch: 756/840\n",
      "Batch loss: 0.7603409290313721 batch: 757/840\n",
      "Batch loss: 0.713762640953064 batch: 758/840\n",
      "Batch loss: 0.5775997638702393 batch: 759/840\n",
      "Batch loss: 0.5028247237205505 batch: 760/840\n",
      "Batch loss: 0.561984658241272 batch: 761/840\n",
      "Batch loss: 0.8045998215675354 batch: 762/840\n",
      "Batch loss: 0.4339669942855835 batch: 763/840\n",
      "Batch loss: 0.6196963787078857 batch: 764/840\n",
      "Batch loss: 0.4815136790275574 batch: 765/840\n",
      "Batch loss: 0.6295186877250671 batch: 766/840\n",
      "Batch loss: 0.8008703589439392 batch: 767/840\n",
      "Batch loss: 0.6971662044525146 batch: 768/840\n",
      "Batch loss: 0.6200082302093506 batch: 769/840\n",
      "Batch loss: 0.571692705154419 batch: 770/840\n",
      "Batch loss: 0.6643333435058594 batch: 771/840\n",
      "Batch loss: 0.557532787322998 batch: 772/840\n",
      "Batch loss: 0.5643251538276672 batch: 773/840\n",
      "Batch loss: 0.4644261300563812 batch: 774/840\n",
      "Batch loss: 0.6150056719779968 batch: 775/840\n",
      "Batch loss: 0.5518075823783875 batch: 776/840\n",
      "Batch loss: 0.5701661109924316 batch: 777/840\n",
      "Batch loss: 0.38466253876686096 batch: 778/840\n",
      "Batch loss: 0.7224602699279785 batch: 779/840\n",
      "Batch loss: 0.5760841965675354 batch: 780/840\n",
      "Batch loss: 0.6087930202484131 batch: 781/840\n",
      "Batch loss: 0.5509400963783264 batch: 782/840\n",
      "Batch loss: 0.57013338804245 batch: 783/840\n",
      "Batch loss: 0.6918087601661682 batch: 784/840\n",
      "Batch loss: 0.6163525581359863 batch: 785/840\n",
      "Batch loss: 0.6268275380134583 batch: 786/840\n",
      "Batch loss: 0.5304977297782898 batch: 787/840\n",
      "Batch loss: 0.6480234265327454 batch: 788/840\n",
      "Batch loss: 0.6966550946235657 batch: 789/840\n",
      "Batch loss: 0.6382980942726135 batch: 790/840\n",
      "Batch loss: 0.7336750626564026 batch: 791/840\n",
      "Batch loss: 0.4182857275009155 batch: 792/840\n",
      "Batch loss: 0.6686455011367798 batch: 793/840\n",
      "Batch loss: 0.6712117791175842 batch: 794/840\n",
      "Batch loss: 0.6371572017669678 batch: 795/840\n",
      "Batch loss: 0.6963874101638794 batch: 796/840\n",
      "Batch loss: 0.7334774732589722 batch: 797/840\n",
      "Batch loss: 0.6193171143531799 batch: 798/840\n",
      "Batch loss: 0.6963441371917725 batch: 799/840\n",
      "Batch loss: 0.5029644966125488 batch: 800/840\n",
      "Batch loss: 0.6806167364120483 batch: 801/840\n",
      "Batch loss: 0.5220948457717896 batch: 802/840\n",
      "Batch loss: 0.45471617579460144 batch: 803/840\n",
      "Batch loss: 0.7740231156349182 batch: 804/840\n",
      "Batch loss: 0.5421090722084045 batch: 805/840\n",
      "Batch loss: 0.6465129852294922 batch: 806/840\n",
      "Batch loss: 0.6355805397033691 batch: 807/840\n",
      "Batch loss: 0.6995715498924255 batch: 808/840\n",
      "Batch loss: 0.6284462213516235 batch: 809/840\n",
      "Batch loss: 0.622922420501709 batch: 810/840\n",
      "Batch loss: 0.5786053538322449 batch: 811/840\n",
      "Batch loss: 0.6976893544197083 batch: 812/840\n",
      "Batch loss: 0.5618670582771301 batch: 813/840\n",
      "Batch loss: 0.520894467830658 batch: 814/840\n",
      "Batch loss: 0.7840369939804077 batch: 815/840\n",
      "Batch loss: 0.6549965739250183 batch: 816/840\n",
      "Batch loss: 0.7116983532905579 batch: 817/840\n",
      "Batch loss: 0.6845954060554504 batch: 818/840\n",
      "Batch loss: 0.5744466185569763 batch: 819/840\n",
      "Batch loss: 0.7415162920951843 batch: 820/840\n",
      "Batch loss: 0.7505612373352051 batch: 821/840\n",
      "Batch loss: 0.6298381686210632 batch: 822/840\n",
      "Batch loss: 0.793806254863739 batch: 823/840\n",
      "Batch loss: 0.7997392416000366 batch: 824/840\n",
      "Batch loss: 0.7963678240776062 batch: 825/840\n",
      "Batch loss: 0.5583323836326599 batch: 826/840\n",
      "Batch loss: 0.5694411993026733 batch: 827/840\n",
      "Batch loss: 0.6815072894096375 batch: 828/840\n",
      "Batch loss: 0.5711588859558105 batch: 829/840\n",
      "Batch loss: 0.6839062571525574 batch: 830/840\n",
      "Batch loss: 0.6595587730407715 batch: 831/840\n",
      "Batch loss: 0.8114175200462341 batch: 832/840\n",
      "Batch loss: 0.7219017148017883 batch: 833/840\n",
      "Batch loss: 0.6075285077095032 batch: 834/840\n",
      "Batch loss: 0.5059248208999634 batch: 835/840\n",
      "Batch loss: 0.5777762532234192 batch: 836/840\n",
      "Batch loss: 0.652372419834137 batch: 837/840\n",
      "Batch loss: 0.6731007099151611 batch: 838/840\n",
      "Batch loss: 0.6388776898384094 batch: 839/840\n",
      "Batch loss: 0.6443740725517273 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 7/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.812\n",
      "Running epoch 8/15\n",
      "Batch loss: 0.5128111243247986 batch: 1/840\n",
      "Batch loss: 0.9548673033714294 batch: 2/840\n",
      "Batch loss: 0.6011151671409607 batch: 3/840\n",
      "Batch loss: 0.6650046706199646 batch: 4/840\n",
      "Batch loss: 0.5870201587677002 batch: 5/840\n",
      "Batch loss: 0.5618301033973694 batch: 6/840\n",
      "Batch loss: 0.5422947406768799 batch: 7/840\n",
      "Batch loss: 0.5696503520011902 batch: 8/840\n",
      "Batch loss: 0.5286530256271362 batch: 9/840\n",
      "Batch loss: 0.651623547077179 batch: 10/840\n",
      "Batch loss: 0.6232947707176208 batch: 11/840\n",
      "Batch loss: 0.6350938081741333 batch: 12/840\n",
      "Batch loss: 0.5176449418067932 batch: 13/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6568712592124939 batch: 14/840\n",
      "Batch loss: 0.6296234130859375 batch: 15/840\n",
      "Batch loss: 0.4546438157558441 batch: 16/840\n",
      "Batch loss: 0.48572295904159546 batch: 17/840\n",
      "Batch loss: 0.7581895589828491 batch: 18/840\n",
      "Batch loss: 0.6506277322769165 batch: 19/840\n",
      "Batch loss: 0.8572757244110107 batch: 20/840\n",
      "Batch loss: 0.7043890953063965 batch: 21/840\n",
      "Batch loss: 0.5363996624946594 batch: 22/840\n",
      "Batch loss: 0.612775981426239 batch: 23/840\n",
      "Batch loss: 0.5060285329818726 batch: 24/840\n",
      "Batch loss: 0.5380575060844421 batch: 25/840\n",
      "Batch loss: 0.6935567259788513 batch: 26/840\n",
      "Batch loss: 0.7029228210449219 batch: 27/840\n",
      "Batch loss: 0.7013592720031738 batch: 28/840\n",
      "Batch loss: 0.6477252244949341 batch: 29/840\n",
      "Batch loss: 0.4914020299911499 batch: 30/840\n",
      "Batch loss: 0.5624921321868896 batch: 31/840\n",
      "Batch loss: 0.6138068437576294 batch: 32/840\n",
      "Batch loss: 0.6311284303665161 batch: 33/840\n",
      "Batch loss: 0.5980865955352783 batch: 34/840\n",
      "Batch loss: 0.5652652978897095 batch: 35/840\n",
      "Batch loss: 0.4944818913936615 batch: 36/840\n",
      "Batch loss: 0.7387317419052124 batch: 37/840\n",
      "Batch loss: 0.6478815674781799 batch: 38/840\n",
      "Batch loss: 0.7202050089836121 batch: 39/840\n",
      "Batch loss: 0.5193273425102234 batch: 40/840\n",
      "Batch loss: 0.7539154887199402 batch: 41/840\n",
      "Batch loss: 0.5978882312774658 batch: 42/840\n",
      "Batch loss: 0.5821096301078796 batch: 43/840\n",
      "Batch loss: 0.6894686222076416 batch: 44/840\n",
      "Batch loss: 0.619533360004425 batch: 45/840\n",
      "Batch loss: 0.5432897806167603 batch: 46/840\n",
      "Batch loss: 0.5557215809822083 batch: 47/840\n",
      "Batch loss: 0.6501308679580688 batch: 48/840\n",
      "Batch loss: 0.6592495441436768 batch: 49/840\n",
      "Batch loss: 0.6228220462799072 batch: 50/840\n",
      "Batch loss: 0.7533465027809143 batch: 51/840\n",
      "Batch loss: 0.7214202284812927 batch: 52/840\n",
      "Batch loss: 0.5328546762466431 batch: 53/840\n",
      "Batch loss: 0.6200778484344482 batch: 54/840\n",
      "Batch loss: 0.6841332912445068 batch: 55/840\n",
      "Batch loss: 0.6250196695327759 batch: 56/840\n",
      "Batch loss: 0.6879875063896179 batch: 57/840\n",
      "Batch loss: 0.594144880771637 batch: 58/840\n",
      "Batch loss: 0.5745400190353394 batch: 59/840\n",
      "Batch loss: 0.5070312023162842 batch: 60/840\n",
      "Batch loss: 0.7853067517280579 batch: 61/840\n",
      "Batch loss: 0.6228654384613037 batch: 62/840\n",
      "Batch loss: 0.5512235760688782 batch: 63/840\n",
      "Batch loss: 0.6837093830108643 batch: 64/840\n",
      "Batch loss: 0.5736846327781677 batch: 65/840\n",
      "Batch loss: 0.6721099615097046 batch: 66/840\n",
      "Batch loss: 0.7080584168434143 batch: 67/840\n",
      "Batch loss: 0.6124699115753174 batch: 68/840\n",
      "Batch loss: 0.7873956561088562 batch: 69/840\n",
      "Batch loss: 0.6061660051345825 batch: 70/840\n",
      "Batch loss: 0.7953594326972961 batch: 71/840\n",
      "Batch loss: 0.8291816115379333 batch: 72/840\n",
      "Batch loss: 0.6851860880851746 batch: 73/840\n",
      "Batch loss: 0.5450896620750427 batch: 74/840\n",
      "Batch loss: 0.7028921246528625 batch: 75/840\n",
      "Batch loss: 0.4601759612560272 batch: 76/840\n",
      "Batch loss: 0.6512637138366699 batch: 77/840\n",
      "Batch loss: 0.8332604169845581 batch: 78/840\n",
      "Batch loss: 0.5980370044708252 batch: 79/840\n",
      "Batch loss: 0.7022064328193665 batch: 80/840\n",
      "Batch loss: 0.5835548639297485 batch: 81/840\n",
      "Batch loss: 0.6848793029785156 batch: 82/840\n",
      "Batch loss: 0.5279572010040283 batch: 83/840\n",
      "Batch loss: 0.6559535264968872 batch: 84/840\n",
      "Batch loss: 0.6630269885063171 batch: 85/840\n",
      "Batch loss: 0.9872961640357971 batch: 86/840\n",
      "Batch loss: 0.5566626191139221 batch: 87/840\n",
      "Batch loss: 0.5108455419540405 batch: 88/840\n",
      "Batch loss: 0.4829652011394501 batch: 89/840\n",
      "Batch loss: 0.514866292476654 batch: 90/840\n",
      "Batch loss: 0.6013532876968384 batch: 91/840\n",
      "Batch loss: 0.6488902568817139 batch: 92/840\n",
      "Batch loss: 0.6283096075057983 batch: 93/840\n",
      "Batch loss: 0.5962232351303101 batch: 94/840\n",
      "Batch loss: 0.7217710614204407 batch: 95/840\n",
      "Batch loss: 0.5430904626846313 batch: 96/840\n",
      "Batch loss: 0.6716234087944031 batch: 97/840\n",
      "Batch loss: 0.758811891078949 batch: 98/840\n",
      "Batch loss: 0.5400448441505432 batch: 99/840\n",
      "Batch loss: 0.6644431948661804 batch: 100/840\n",
      "Batch loss: 0.5885051488876343 batch: 101/840\n",
      "Batch loss: 0.589226245880127 batch: 102/840\n",
      "Batch loss: 0.6171228885650635 batch: 103/840\n",
      "Batch loss: 0.47750771045684814 batch: 104/840\n",
      "Batch loss: 0.580852210521698 batch: 105/840\n",
      "Batch loss: 0.6462547779083252 batch: 106/840\n",
      "Batch loss: 0.6789847612380981 batch: 107/840\n",
      "Batch loss: 0.7670633792877197 batch: 108/840\n",
      "Batch loss: 0.6271665692329407 batch: 109/840\n",
      "Batch loss: 0.5104203224182129 batch: 110/840\n",
      "Batch loss: 0.5267784595489502 batch: 111/840\n",
      "Batch loss: 0.7449449896812439 batch: 112/840\n",
      "Batch loss: 0.7858293652534485 batch: 113/840\n",
      "Batch loss: 0.5781019330024719 batch: 114/840\n",
      "Batch loss: 0.5813971161842346 batch: 115/840\n",
      "Batch loss: 0.6116781234741211 batch: 116/840\n",
      "Batch loss: 0.6271927952766418 batch: 117/840\n",
      "Batch loss: 0.4613778293132782 batch: 118/840\n",
      "Batch loss: 0.6783550381660461 batch: 119/840\n",
      "Batch loss: 0.5000003576278687 batch: 120/840\n",
      "Batch loss: 0.7083615660667419 batch: 121/840\n",
      "Batch loss: 0.8633304834365845 batch: 122/840\n",
      "Batch loss: 0.5172955393791199 batch: 123/840\n",
      "Batch loss: 0.5498222708702087 batch: 124/840\n",
      "Batch loss: 0.5698021650314331 batch: 125/840\n",
      "Batch loss: 0.6761744618415833 batch: 126/840\n",
      "Batch loss: 0.6750956177711487 batch: 127/840\n",
      "Batch loss: 0.6940518021583557 batch: 128/840\n",
      "Batch loss: 0.6891047954559326 batch: 129/840\n",
      "Batch loss: 0.5219240784645081 batch: 130/840\n",
      "Batch loss: 0.6825932860374451 batch: 131/840\n",
      "Batch loss: 1.0042521953582764 batch: 132/840\n",
      "Batch loss: 0.7299321889877319 batch: 133/840\n",
      "Batch loss: 0.6260238885879517 batch: 134/840\n",
      "Batch loss: 0.5162122845649719 batch: 135/840\n",
      "Batch loss: 0.774965226650238 batch: 136/840\n",
      "Batch loss: 0.6088712811470032 batch: 137/840\n",
      "Batch loss: 0.5250747799873352 batch: 138/840\n",
      "Batch loss: 0.5528392195701599 batch: 139/840\n",
      "Batch loss: 0.5810466408729553 batch: 140/840\n",
      "Batch loss: 0.4642254114151001 batch: 141/840\n",
      "Batch loss: 0.6289286613464355 batch: 142/840\n",
      "Batch loss: 0.5096028447151184 batch: 143/840\n",
      "Batch loss: 0.5455350279808044 batch: 144/840\n",
      "Batch loss: 0.7744268178939819 batch: 145/840\n",
      "Batch loss: 0.5905451774597168 batch: 146/840\n",
      "Batch loss: 0.5638038516044617 batch: 147/840\n",
      "Batch loss: 0.7884527444839478 batch: 148/840\n",
      "Batch loss: 0.620334267616272 batch: 149/840\n",
      "Batch loss: 0.6666066646575928 batch: 150/840\n",
      "Batch loss: 0.6164736151695251 batch: 151/840\n",
      "Batch loss: 0.6853533387184143 batch: 152/840\n",
      "Batch loss: 0.6115180253982544 batch: 153/840\n",
      "Batch loss: 0.687507688999176 batch: 154/840\n",
      "Batch loss: 0.613734781742096 batch: 155/840\n",
      "Batch loss: 0.6308721899986267 batch: 156/840\n",
      "Batch loss: 0.7458432912826538 batch: 157/840\n",
      "Batch loss: 0.5393956899642944 batch: 158/840\n",
      "Batch loss: 0.5556967258453369 batch: 159/840\n",
      "Batch loss: 0.5520035028457642 batch: 160/840\n",
      "Batch loss: 0.7148211598396301 batch: 161/840\n",
      "Batch loss: 0.7513863444328308 batch: 162/840\n",
      "Batch loss: 0.8407770395278931 batch: 163/840\n",
      "Batch loss: 0.41386932134628296 batch: 164/840\n",
      "Batch loss: 0.6991456747055054 batch: 165/840\n",
      "Batch loss: 0.58739173412323 batch: 166/840\n",
      "Batch loss: 0.6870185732841492 batch: 167/840\n",
      "Batch loss: 0.6418299078941345 batch: 168/840\n",
      "Batch loss: 0.592741072177887 batch: 169/840\n",
      "Batch loss: 0.7982550859451294 batch: 170/840\n",
      "Batch loss: 0.6753292679786682 batch: 171/840\n",
      "Batch loss: 0.7114159464836121 batch: 172/840\n",
      "Batch loss: 0.6697240471839905 batch: 173/840\n",
      "Batch loss: 0.5247454047203064 batch: 174/840\n",
      "Batch loss: 0.5993502736091614 batch: 175/840\n",
      "Batch loss: 0.742666482925415 batch: 176/840\n",
      "Batch loss: 0.6964554786682129 batch: 177/840\n",
      "Batch loss: 0.793745756149292 batch: 178/840\n",
      "Batch loss: 0.6836678385734558 batch: 179/840\n",
      "Batch loss: 0.5766265392303467 batch: 180/840\n",
      "Batch loss: 0.4970545470714569 batch: 181/840\n",
      "Batch loss: 0.5974594950675964 batch: 182/840\n",
      "Batch loss: 0.6308817863464355 batch: 183/840\n",
      "Batch loss: 0.5471920967102051 batch: 184/840\n",
      "Batch loss: 0.482420951128006 batch: 185/840\n",
      "Batch loss: 0.44370418787002563 batch: 186/840\n",
      "Batch loss: 0.6758627891540527 batch: 187/840\n",
      "Batch loss: 0.5240183472633362 batch: 188/840\n",
      "Batch loss: 0.609661340713501 batch: 189/840\n",
      "Batch loss: 0.5423316359519958 batch: 190/840\n",
      "Batch loss: 0.7638024091720581 batch: 191/840\n",
      "Batch loss: 0.6079984903335571 batch: 192/840\n",
      "Batch loss: 0.5728359818458557 batch: 193/840\n",
      "Batch loss: 0.5366203784942627 batch: 194/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6100456118583679 batch: 195/840\n",
      "Batch loss: 0.756122887134552 batch: 196/840\n",
      "Batch loss: 0.6547669768333435 batch: 197/840\n",
      "Batch loss: 0.4722850024700165 batch: 198/840\n",
      "Batch loss: 0.655456006526947 batch: 199/840\n",
      "Batch loss: 0.8042426109313965 batch: 200/840\n",
      "Batch loss: 0.5270053744316101 batch: 201/840\n",
      "Batch loss: 0.6068867444992065 batch: 202/840\n",
      "Batch loss: 0.6071258783340454 batch: 203/840\n",
      "Batch loss: 0.7038097977638245 batch: 204/840\n",
      "Batch loss: 0.6880822777748108 batch: 205/840\n",
      "Batch loss: 0.6491600871086121 batch: 206/840\n",
      "Batch loss: 0.5509613156318665 batch: 207/840\n",
      "Batch loss: 0.6924440860748291 batch: 208/840\n",
      "Batch loss: 0.6627405285835266 batch: 209/840\n",
      "Batch loss: 0.5289117693901062 batch: 210/840\n",
      "Batch loss: 0.47052496671676636 batch: 211/840\n",
      "Batch loss: 0.5426260828971863 batch: 212/840\n",
      "Batch loss: 0.6512241959571838 batch: 213/840\n",
      "Batch loss: 0.758857011795044 batch: 214/840\n",
      "Batch loss: 0.6841474175453186 batch: 215/840\n",
      "Batch loss: 0.5296105742454529 batch: 216/840\n",
      "Batch loss: 0.5560452938079834 batch: 217/840\n",
      "Batch loss: 0.7355831265449524 batch: 218/840\n",
      "Batch loss: 0.6217175722122192 batch: 219/840\n",
      "Batch loss: 0.9117429852485657 batch: 220/840\n",
      "Batch loss: 0.5337100028991699 batch: 221/840\n",
      "Batch loss: 0.6882537603378296 batch: 222/840\n",
      "Batch loss: 0.6339002847671509 batch: 223/840\n",
      "Batch loss: 0.8520715832710266 batch: 224/840\n",
      "Batch loss: 0.7145794630050659 batch: 225/840\n",
      "Batch loss: 0.7467772960662842 batch: 226/840\n",
      "Batch loss: 0.7811605334281921 batch: 227/840\n",
      "Batch loss: 0.478240430355072 batch: 228/840\n",
      "Batch loss: 0.49657881259918213 batch: 229/840\n",
      "Batch loss: 0.5630367398262024 batch: 230/840\n",
      "Batch loss: 0.5254836082458496 batch: 231/840\n",
      "Batch loss: 0.5807086825370789 batch: 232/840\n",
      "Batch loss: 0.7056688666343689 batch: 233/840\n",
      "Batch loss: 0.636397123336792 batch: 234/840\n",
      "Batch loss: 0.6188623905181885 batch: 235/840\n",
      "Batch loss: 0.6856730580329895 batch: 236/840\n",
      "Batch loss: 0.5533219575881958 batch: 237/840\n",
      "Batch loss: 0.6670284867286682 batch: 238/840\n",
      "Batch loss: 0.6602880358695984 batch: 239/840\n",
      "Batch loss: 0.551135241985321 batch: 240/840\n",
      "Batch loss: 0.7450410723686218 batch: 241/840\n",
      "Batch loss: 0.6874421834945679 batch: 242/840\n",
      "Batch loss: 0.6053969264030457 batch: 243/840\n",
      "Batch loss: 0.7505248188972473 batch: 244/840\n",
      "Batch loss: 0.43699198961257935 batch: 245/840\n",
      "Batch loss: 0.6138367652893066 batch: 246/840\n",
      "Batch loss: 0.7114616632461548 batch: 247/840\n",
      "Batch loss: 0.7395645380020142 batch: 248/840\n",
      "Batch loss: 0.8904611468315125 batch: 249/840\n",
      "Batch loss: 0.5109243392944336 batch: 250/840\n",
      "Batch loss: 0.5889716744422913 batch: 251/840\n",
      "Batch loss: 0.5236707925796509 batch: 252/840\n",
      "Batch loss: 0.6290516257286072 batch: 253/840\n",
      "Batch loss: 0.7749561071395874 batch: 254/840\n",
      "Batch loss: 0.6234575510025024 batch: 255/840\n",
      "Batch loss: 0.639094352722168 batch: 256/840\n",
      "Batch loss: 0.5978379249572754 batch: 257/840\n",
      "Batch loss: 0.7948617339134216 batch: 258/840\n",
      "Batch loss: 0.5169516801834106 batch: 259/840\n",
      "Batch loss: 0.4444468319416046 batch: 260/840\n",
      "Batch loss: 0.6360113620758057 batch: 261/840\n",
      "Batch loss: 0.47538816928863525 batch: 262/840\n",
      "Batch loss: 0.6124269366264343 batch: 263/840\n",
      "Batch loss: 0.6310022473335266 batch: 264/840\n",
      "Batch loss: 0.75091552734375 batch: 265/840\n",
      "Batch loss: 0.608864426612854 batch: 266/840\n",
      "Batch loss: 0.7118677496910095 batch: 267/840\n",
      "Batch loss: 0.49979594349861145 batch: 268/840\n",
      "Batch loss: 0.5513308644294739 batch: 269/840\n",
      "Batch loss: 0.6063387393951416 batch: 270/840\n",
      "Batch loss: 0.6669006943702698 batch: 271/840\n",
      "Batch loss: 0.9047394394874573 batch: 272/840\n",
      "Batch loss: 0.6289561986923218 batch: 273/840\n",
      "Batch loss: 0.5795419812202454 batch: 274/840\n",
      "Batch loss: 0.7358126044273376 batch: 275/840\n",
      "Batch loss: 0.5803314447402954 batch: 276/840\n",
      "Batch loss: 0.6084353923797607 batch: 277/840\n",
      "Batch loss: 0.728380024433136 batch: 278/840\n",
      "Batch loss: 0.6887983679771423 batch: 279/840\n",
      "Batch loss: 0.7005833387374878 batch: 280/840\n",
      "Batch loss: 0.5484254360198975 batch: 281/840\n",
      "Batch loss: 0.6141672134399414 batch: 282/840\n",
      "Batch loss: 0.6281553506851196 batch: 283/840\n",
      "Batch loss: 0.504077136516571 batch: 284/840\n",
      "Batch loss: 0.5332781076431274 batch: 285/840\n",
      "Batch loss: 0.6748799681663513 batch: 286/840\n",
      "Batch loss: 0.47527220845222473 batch: 287/840\n",
      "Batch loss: 0.5603943467140198 batch: 288/840\n",
      "Batch loss: 0.7911734580993652 batch: 289/840\n",
      "Batch loss: 0.7677627801895142 batch: 290/840\n",
      "Batch loss: 0.813340961933136 batch: 291/840\n",
      "Batch loss: 0.5136921405792236 batch: 292/840\n",
      "Batch loss: 0.6962223649024963 batch: 293/840\n",
      "Batch loss: 0.6030760407447815 batch: 294/840\n",
      "Batch loss: 0.535897433757782 batch: 295/840\n",
      "Batch loss: 0.6593750715255737 batch: 296/840\n",
      "Batch loss: 0.631087064743042 batch: 297/840\n",
      "Batch loss: 0.643197774887085 batch: 298/840\n",
      "Batch loss: 0.5123768448829651 batch: 299/840\n",
      "Batch loss: 0.8565994501113892 batch: 300/840\n",
      "Batch loss: 0.6560291051864624 batch: 301/840\n",
      "Batch loss: 0.5821976065635681 batch: 302/840\n",
      "Batch loss: 0.7374840378761292 batch: 303/840\n",
      "Batch loss: 0.5474157929420471 batch: 304/840\n",
      "Batch loss: 0.5860393643379211 batch: 305/840\n",
      "Batch loss: 0.6008128523826599 batch: 306/840\n",
      "Batch loss: 0.5235388278961182 batch: 307/840\n",
      "Batch loss: 0.8066163063049316 batch: 308/840\n",
      "Batch loss: 0.6270027756690979 batch: 309/840\n",
      "Batch loss: 0.8264867663383484 batch: 310/840\n",
      "Batch loss: 0.7351660132408142 batch: 311/840\n",
      "Batch loss: 0.7218633890151978 batch: 312/840\n",
      "Batch loss: 0.7683191895484924 batch: 313/840\n",
      "Batch loss: 0.4809969663619995 batch: 314/840\n",
      "Batch loss: 0.6515759229660034 batch: 315/840\n",
      "Batch loss: 0.5515299439430237 batch: 316/840\n",
      "Batch loss: 0.6664415001869202 batch: 317/840\n",
      "Batch loss: 0.634474515914917 batch: 318/840\n",
      "Batch loss: 0.58404940366745 batch: 319/840\n",
      "Batch loss: 0.647600531578064 batch: 320/840\n",
      "Batch loss: 0.5668606758117676 batch: 321/840\n",
      "Batch loss: 0.6858119368553162 batch: 322/840\n",
      "Batch loss: 0.6640471816062927 batch: 323/840\n",
      "Batch loss: 0.7169957160949707 batch: 324/840\n",
      "Batch loss: 0.5065438747406006 batch: 325/840\n",
      "Batch loss: 0.5879951119422913 batch: 326/840\n",
      "Batch loss: 0.41949859261512756 batch: 327/840\n",
      "Batch loss: 0.7942771911621094 batch: 328/840\n",
      "Batch loss: 0.7122738361358643 batch: 329/840\n",
      "Batch loss: 0.6949917674064636 batch: 330/840\n",
      "Batch loss: 0.6375444531440735 batch: 331/840\n",
      "Batch loss: 0.6621589064598083 batch: 332/840\n",
      "Batch loss: 0.5877184867858887 batch: 333/840\n",
      "Batch loss: 0.6188361644744873 batch: 334/840\n",
      "Batch loss: 0.6312813758850098 batch: 335/840\n",
      "Batch loss: 0.6852208971977234 batch: 336/840\n",
      "Batch loss: 0.8250558376312256 batch: 337/840\n",
      "Batch loss: 0.7640107870101929 batch: 338/840\n",
      "Batch loss: 0.5312594175338745 batch: 339/840\n",
      "Batch loss: 0.7877328395843506 batch: 340/840\n",
      "Batch loss: 0.5613624453544617 batch: 341/840\n",
      "Batch loss: 0.5918743014335632 batch: 342/840\n",
      "Batch loss: 0.805877685546875 batch: 343/840\n",
      "Batch loss: 0.6479107737541199 batch: 344/840\n",
      "Batch loss: 0.4609880745410919 batch: 345/840\n",
      "Batch loss: 0.6274882555007935 batch: 346/840\n",
      "Batch loss: 0.6103746294975281 batch: 347/840\n",
      "Batch loss: 0.630774736404419 batch: 348/840\n",
      "Batch loss: 0.6847634315490723 batch: 349/840\n",
      "Batch loss: 0.45685139298439026 batch: 350/840\n",
      "Batch loss: 0.7477538585662842 batch: 351/840\n",
      "Batch loss: 0.6501067280769348 batch: 352/840\n",
      "Batch loss: 0.6386587023735046 batch: 353/840\n",
      "Batch loss: 0.6710692048072815 batch: 354/840\n",
      "Batch loss: 0.5383981466293335 batch: 355/840\n",
      "Batch loss: 0.6149774789810181 batch: 356/840\n",
      "Batch loss: 0.5128980875015259 batch: 357/840\n",
      "Batch loss: 0.9200528860092163 batch: 358/840\n",
      "Batch loss: 0.6109926700592041 batch: 359/840\n",
      "Batch loss: 0.8950120806694031 batch: 360/840\n",
      "Batch loss: 0.6149871945381165 batch: 361/840\n",
      "Batch loss: 0.6411935687065125 batch: 362/840\n",
      "Batch loss: 0.6431906223297119 batch: 363/840\n",
      "Batch loss: 0.7086677551269531 batch: 364/840\n",
      "Batch loss: 0.5767311453819275 batch: 365/840\n",
      "Batch loss: 0.5924399495124817 batch: 366/840\n",
      "Batch loss: 0.5009058117866516 batch: 367/840\n",
      "Batch loss: 0.8020642995834351 batch: 368/840\n",
      "Batch loss: 0.5946717262268066 batch: 369/840\n",
      "Batch loss: 0.6878343820571899 batch: 370/840\n",
      "Batch loss: 0.6062782406806946 batch: 371/840\n",
      "Batch loss: 0.5169615745544434 batch: 372/840\n",
      "Batch loss: 0.6597989797592163 batch: 373/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6950103044509888 batch: 374/840\n",
      "Batch loss: 0.58251953125 batch: 375/840\n",
      "Batch loss: 0.5506236553192139 batch: 376/840\n",
      "Batch loss: 0.5947486758232117 batch: 377/840\n",
      "Batch loss: 0.5970924496650696 batch: 378/840\n",
      "Batch loss: 0.5059450268745422 batch: 379/840\n",
      "Batch loss: 0.7921249270439148 batch: 380/840\n",
      "Batch loss: 0.7739621996879578 batch: 381/840\n",
      "Batch loss: 0.7752132415771484 batch: 382/840\n",
      "Batch loss: 0.6751151084899902 batch: 383/840\n",
      "Batch loss: 0.6006791591644287 batch: 384/840\n",
      "Batch loss: 0.6985270977020264 batch: 385/840\n",
      "Batch loss: 0.707266628742218 batch: 386/840\n",
      "Batch loss: 0.6318323016166687 batch: 387/840\n",
      "Batch loss: 0.702701210975647 batch: 388/840\n",
      "Batch loss: 0.5843770503997803 batch: 389/840\n",
      "Batch loss: 0.7741184234619141 batch: 390/840\n",
      "Batch loss: 0.7277212738990784 batch: 391/840\n",
      "Batch loss: 0.5786960124969482 batch: 392/840\n",
      "Batch loss: 0.4499836266040802 batch: 393/840\n",
      "Batch loss: 0.7524163722991943 batch: 394/840\n",
      "Batch loss: 0.5092244148254395 batch: 395/840\n",
      "Batch loss: 0.6885395050048828 batch: 396/840\n",
      "Batch loss: 0.5826348662376404 batch: 397/840\n",
      "Batch loss: 0.6463583111763 batch: 398/840\n",
      "Batch loss: 0.45144152641296387 batch: 399/840\n",
      "Batch loss: 0.6001462936401367 batch: 400/840\n",
      "Batch loss: 0.7334311008453369 batch: 401/840\n",
      "Batch loss: 0.5403964519500732 batch: 402/840\n",
      "Batch loss: 0.6097347736358643 batch: 403/840\n",
      "Batch loss: 0.6495705246925354 batch: 404/840\n",
      "Batch loss: 0.6139786243438721 batch: 405/840\n",
      "Batch loss: 0.6608815789222717 batch: 406/840\n",
      "Batch loss: 0.5540510416030884 batch: 407/840\n",
      "Batch loss: 0.7441685199737549 batch: 408/840\n",
      "Batch loss: 0.6713257431983948 batch: 409/840\n",
      "Batch loss: 0.8265271782875061 batch: 410/840\n",
      "Batch loss: 0.6187098026275635 batch: 411/840\n",
      "Batch loss: 0.6751717329025269 batch: 412/840\n",
      "Batch loss: 0.6298096179962158 batch: 413/840\n",
      "Batch loss: 0.5987210869789124 batch: 414/840\n",
      "Batch loss: 0.9023919105529785 batch: 415/840\n",
      "Batch loss: 0.4413343071937561 batch: 416/840\n",
      "Batch loss: 0.5743884444236755 batch: 417/840\n",
      "Batch loss: 0.7418853044509888 batch: 418/840\n",
      "Batch loss: 0.7351320385932922 batch: 419/840\n",
      "Batch loss: 0.7620505690574646 batch: 420/840\n",
      "Batch loss: 0.5278995037078857 batch: 421/840\n",
      "Batch loss: 0.5861415266990662 batch: 422/840\n",
      "Batch loss: 0.6290108561515808 batch: 423/840\n",
      "Batch loss: 0.6483418345451355 batch: 424/840\n",
      "Batch loss: 0.7163377404212952 batch: 425/840\n",
      "Batch loss: 0.5260224342346191 batch: 426/840\n",
      "Batch loss: 0.6351656913757324 batch: 427/840\n",
      "Batch loss: 0.734892725944519 batch: 428/840\n",
      "Batch loss: 0.5384059548377991 batch: 429/840\n",
      "Batch loss: 0.8295059204101562 batch: 430/840\n",
      "Batch loss: 0.6652156710624695 batch: 431/840\n",
      "Batch loss: 0.5547491908073425 batch: 432/840\n",
      "Batch loss: 0.3946911692619324 batch: 433/840\n",
      "Batch loss: 0.5595471858978271 batch: 434/840\n",
      "Batch loss: 0.8154705762863159 batch: 435/840\n",
      "Batch loss: 0.551982045173645 batch: 436/840\n",
      "Batch loss: 0.8011026978492737 batch: 437/840\n",
      "Batch loss: 0.4248652160167694 batch: 438/840\n",
      "Batch loss: 0.6358175873756409 batch: 439/840\n",
      "Batch loss: 0.6759458780288696 batch: 440/840\n",
      "Batch loss: 0.7571753859519958 batch: 441/840\n",
      "Batch loss: 0.654792845249176 batch: 442/840\n",
      "Batch loss: 0.6235220432281494 batch: 443/840\n",
      "Batch loss: 0.6343485116958618 batch: 444/840\n",
      "Batch loss: 0.5721696615219116 batch: 445/840\n",
      "Batch loss: 0.7296584248542786 batch: 446/840\n",
      "Batch loss: 0.7586239576339722 batch: 447/840\n",
      "Batch loss: 0.7352136373519897 batch: 448/840\n",
      "Batch loss: 0.5243989825248718 batch: 449/840\n",
      "Batch loss: 0.6125354170799255 batch: 450/840\n",
      "Batch loss: 0.7054579854011536 batch: 451/840\n",
      "Batch loss: 0.7419140338897705 batch: 452/840\n",
      "Batch loss: 0.6257967948913574 batch: 453/840\n",
      "Batch loss: 0.512904703617096 batch: 454/840\n",
      "Batch loss: 0.6940696239471436 batch: 455/840\n",
      "Batch loss: 0.6174683570861816 batch: 456/840\n",
      "Batch loss: 0.6131125092506409 batch: 457/840\n",
      "Batch loss: 0.6158431172370911 batch: 458/840\n",
      "Batch loss: 0.584854245185852 batch: 459/840\n",
      "Batch loss: 0.5207778811454773 batch: 460/840\n",
      "Batch loss: 0.6977701783180237 batch: 461/840\n",
      "Batch loss: 0.6300509572029114 batch: 462/840\n",
      "Batch loss: 0.8351242542266846 batch: 463/840\n",
      "Batch loss: 0.49059849977493286 batch: 464/840\n",
      "Batch loss: 0.9362572431564331 batch: 465/840\n",
      "Batch loss: 0.6676695346832275 batch: 466/840\n",
      "Batch loss: 0.9055573344230652 batch: 467/840\n",
      "Batch loss: 0.542641818523407 batch: 468/840\n",
      "Batch loss: 0.549261212348938 batch: 469/840\n",
      "Batch loss: 0.6532405614852905 batch: 470/840\n",
      "Batch loss: 0.6800944805145264 batch: 471/840\n",
      "Batch loss: 0.7075679302215576 batch: 472/840\n",
      "Batch loss: 0.6782256960868835 batch: 473/840\n",
      "Batch loss: 0.6117275953292847 batch: 474/840\n",
      "Batch loss: 0.6472597718238831 batch: 475/840\n",
      "Batch loss: 0.5821405649185181 batch: 476/840\n",
      "Batch loss: 0.48928746581077576 batch: 477/840\n",
      "Batch loss: 0.4486038088798523 batch: 478/840\n",
      "Batch loss: 0.470508873462677 batch: 479/840\n",
      "Batch loss: 0.6727488040924072 batch: 480/840\n",
      "Batch loss: 0.6903244256973267 batch: 481/840\n",
      "Batch loss: 0.6837485432624817 batch: 482/840\n",
      "Batch loss: 0.8176692724227905 batch: 483/840\n",
      "Batch loss: 0.6721621751785278 batch: 484/840\n",
      "Batch loss: 0.5631776452064514 batch: 485/840\n",
      "Batch loss: 0.6762894988059998 batch: 486/840\n",
      "Batch loss: 0.4905852973461151 batch: 487/840\n",
      "Batch loss: 0.6507008075714111 batch: 488/840\n",
      "Batch loss: 0.6022608876228333 batch: 489/840\n",
      "Batch loss: 0.7543479800224304 batch: 490/840\n",
      "Batch loss: 0.40588366985321045 batch: 491/840\n",
      "Batch loss: 0.6862834095954895 batch: 492/840\n",
      "Batch loss: 0.6041942834854126 batch: 493/840\n",
      "Batch loss: 0.40902113914489746 batch: 494/840\n",
      "Batch loss: 0.7598133683204651 batch: 495/840\n",
      "Batch loss: 0.6255684494972229 batch: 496/840\n",
      "Batch loss: 0.7084730267524719 batch: 497/840\n",
      "Batch loss: 0.4604382812976837 batch: 498/840\n",
      "Batch loss: 0.7205734252929688 batch: 499/840\n",
      "Batch loss: 0.5967740416526794 batch: 500/840\n",
      "Batch loss: 0.5505401492118835 batch: 501/840\n",
      "Batch loss: 0.5784646272659302 batch: 502/840\n",
      "Batch loss: 0.5270225405693054 batch: 503/840\n",
      "Batch loss: 0.7294197082519531 batch: 504/840\n",
      "Batch loss: 0.7538618445396423 batch: 505/840\n",
      "Batch loss: 0.5219528079032898 batch: 506/840\n",
      "Batch loss: 0.6802753210067749 batch: 507/840\n",
      "Batch loss: 0.6614090204238892 batch: 508/840\n",
      "Batch loss: 0.7407369017601013 batch: 509/840\n",
      "Batch loss: 0.6581206321716309 batch: 510/840\n",
      "Batch loss: 0.4833451807498932 batch: 511/840\n",
      "Batch loss: 0.5640300512313843 batch: 512/840\n",
      "Batch loss: 0.5592382550239563 batch: 513/840\n",
      "Batch loss: 0.7883748412132263 batch: 514/840\n",
      "Batch loss: 0.43570277094841003 batch: 515/840\n",
      "Batch loss: 0.6773020029067993 batch: 516/840\n",
      "Batch loss: 0.6795194149017334 batch: 517/840\n",
      "Batch loss: 0.6606696248054504 batch: 518/840\n",
      "Batch loss: 0.7349486351013184 batch: 519/840\n",
      "Batch loss: 0.6490892171859741 batch: 520/840\n",
      "Batch loss: 0.6945838928222656 batch: 521/840\n",
      "Batch loss: 0.6090558171272278 batch: 522/840\n",
      "Batch loss: 0.4657961130142212 batch: 523/840\n",
      "Batch loss: 0.6956377625465393 batch: 524/840\n",
      "Batch loss: 0.6152979731559753 batch: 525/840\n",
      "Batch loss: 0.628535270690918 batch: 526/840\n",
      "Batch loss: 0.5694553852081299 batch: 527/840\n",
      "Batch loss: 0.5689449906349182 batch: 528/840\n",
      "Batch loss: 0.496592253446579 batch: 529/840\n",
      "Batch loss: 0.5939871668815613 batch: 530/840\n",
      "Batch loss: 0.5058181881904602 batch: 531/840\n",
      "Batch loss: 0.3957855701446533 batch: 532/840\n",
      "Batch loss: 0.7466510534286499 batch: 533/840\n",
      "Batch loss: 0.6477225422859192 batch: 534/840\n",
      "Batch loss: 0.5638877749443054 batch: 535/840\n",
      "Batch loss: 0.8032858371734619 batch: 536/840\n",
      "Batch loss: 0.6614334583282471 batch: 537/840\n",
      "Batch loss: 0.531877875328064 batch: 538/840\n",
      "Batch loss: 0.5927383899688721 batch: 539/840\n",
      "Batch loss: 0.8021265268325806 batch: 540/840\n",
      "Batch loss: 0.6580359935760498 batch: 541/840\n",
      "Batch loss: 0.6078060865402222 batch: 542/840\n",
      "Batch loss: 0.4566941559314728 batch: 543/840\n",
      "Batch loss: 0.6346369385719299 batch: 544/840\n",
      "Batch loss: 0.5862240195274353 batch: 545/840\n",
      "Batch loss: 0.6425704956054688 batch: 546/840\n",
      "Batch loss: 0.6344125270843506 batch: 547/840\n",
      "Batch loss: 0.5028977990150452 batch: 548/840\n",
      "Batch loss: 0.5747265219688416 batch: 549/840\n",
      "Batch loss: 0.4882573187351227 batch: 550/840\n",
      "Batch loss: 0.6076930165290833 batch: 551/840\n",
      "Batch loss: 0.5421692132949829 batch: 552/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6754202842712402 batch: 553/840\n",
      "Batch loss: 0.541429340839386 batch: 554/840\n",
      "Batch loss: 0.6940234899520874 batch: 555/840\n",
      "Batch loss: 0.6613345146179199 batch: 556/840\n",
      "Batch loss: 0.6002936363220215 batch: 557/840\n",
      "Batch loss: 0.6536957621574402 batch: 558/840\n",
      "Batch loss: 0.5243127942085266 batch: 559/840\n",
      "Batch loss: 0.7536185383796692 batch: 560/840\n",
      "Batch loss: 0.6129105687141418 batch: 561/840\n",
      "Batch loss: 0.5706142783164978 batch: 562/840\n",
      "Batch loss: 0.48910441994667053 batch: 563/840\n",
      "Batch loss: 0.6633398532867432 batch: 564/840\n",
      "Batch loss: 0.6747941374778748 batch: 565/840\n",
      "Batch loss: 0.8206098675727844 batch: 566/840\n",
      "Batch loss: 0.8087664842605591 batch: 567/840\n",
      "Batch loss: 0.6687734127044678 batch: 568/840\n",
      "Batch loss: 0.653992235660553 batch: 569/840\n",
      "Batch loss: 0.40633055567741394 batch: 570/840\n",
      "Batch loss: 0.7254889607429504 batch: 571/840\n",
      "Batch loss: 0.7042340636253357 batch: 572/840\n",
      "Batch loss: 0.6295430660247803 batch: 573/840\n",
      "Batch loss: 0.7542447447776794 batch: 574/840\n",
      "Batch loss: 0.44722476601600647 batch: 575/840\n",
      "Batch loss: 0.6478039622306824 batch: 576/840\n",
      "Batch loss: 0.5677650570869446 batch: 577/840\n",
      "Batch loss: 0.8119224309921265 batch: 578/840\n",
      "Batch loss: 0.5901545286178589 batch: 579/840\n",
      "Batch loss: 0.8547584414482117 batch: 580/840\n",
      "Batch loss: 0.6949098706245422 batch: 581/840\n",
      "Batch loss: 0.8310518860816956 batch: 582/840\n",
      "Batch loss: 0.5756945013999939 batch: 583/840\n",
      "Batch loss: 0.7443556785583496 batch: 584/840\n",
      "Batch loss: 0.6627311110496521 batch: 585/840\n",
      "Batch loss: 0.6370465159416199 batch: 586/840\n",
      "Batch loss: 0.6379265785217285 batch: 587/840\n",
      "Batch loss: 0.5545445680618286 batch: 588/840\n",
      "Batch loss: 0.7732437252998352 batch: 589/840\n",
      "Batch loss: 0.5394793152809143 batch: 590/840\n",
      "Batch loss: 0.4362233877182007 batch: 591/840\n",
      "Batch loss: 0.48998382687568665 batch: 592/840\n",
      "Batch loss: 0.6788117289543152 batch: 593/840\n",
      "Batch loss: 0.6910362243652344 batch: 594/840\n",
      "Batch loss: 0.3517408072948456 batch: 595/840\n",
      "Batch loss: 0.5282801985740662 batch: 596/840\n",
      "Batch loss: 0.6674201488494873 batch: 597/840\n",
      "Batch loss: 0.468922883272171 batch: 598/840\n",
      "Batch loss: 0.5190978050231934 batch: 599/840\n",
      "Batch loss: 0.7331921458244324 batch: 600/840\n",
      "Batch loss: 0.6708659529685974 batch: 601/840\n",
      "Batch loss: 0.5696110725402832 batch: 602/840\n",
      "Batch loss: 0.5931873917579651 batch: 603/840\n",
      "Batch loss: 0.5839389562606812 batch: 604/840\n",
      "Batch loss: 0.8620595335960388 batch: 605/840\n",
      "Batch loss: 0.7768409848213196 batch: 606/840\n",
      "Batch loss: 0.7212597131729126 batch: 607/840\n",
      "Batch loss: 0.6328830122947693 batch: 608/840\n",
      "Batch loss: 0.5365079641342163 batch: 609/840\n",
      "Batch loss: 0.7041205763816833 batch: 610/840\n",
      "Batch loss: 0.7281132340431213 batch: 611/840\n",
      "Batch loss: 0.7282586693763733 batch: 612/840\n",
      "Batch loss: 0.7065761089324951 batch: 613/840\n",
      "Batch loss: 0.6617617011070251 batch: 614/840\n",
      "Batch loss: 0.5464808940887451 batch: 615/840\n",
      "Batch loss: 0.5140575170516968 batch: 616/840\n",
      "Batch loss: 0.5269164443016052 batch: 617/840\n",
      "Batch loss: 0.6009019613265991 batch: 618/840\n",
      "Batch loss: 0.7663611769676208 batch: 619/840\n",
      "Batch loss: 0.47877976298332214 batch: 620/840\n",
      "Batch loss: 0.7065886855125427 batch: 621/840\n",
      "Batch loss: 0.6650624871253967 batch: 622/840\n",
      "Batch loss: 0.6641265153884888 batch: 623/840\n",
      "Batch loss: 0.8205538988113403 batch: 624/840\n",
      "Batch loss: 0.7360163927078247 batch: 625/840\n",
      "Batch loss: 0.676708996295929 batch: 626/840\n",
      "Batch loss: 0.5510892868041992 batch: 627/840\n",
      "Batch loss: 0.7143524289131165 batch: 628/840\n",
      "Batch loss: 0.6760152578353882 batch: 629/840\n",
      "Batch loss: 0.7005516290664673 batch: 630/840\n",
      "Batch loss: 0.5888760089874268 batch: 631/840\n",
      "Batch loss: 0.6742052435874939 batch: 632/840\n",
      "Batch loss: 0.5965539813041687 batch: 633/840\n",
      "Batch loss: 0.5754886269569397 batch: 634/840\n",
      "Batch loss: 0.5193946957588196 batch: 635/840\n",
      "Batch loss: 0.5477371215820312 batch: 636/840\n",
      "Batch loss: 0.44439274072647095 batch: 637/840\n",
      "Batch loss: 0.6678550839424133 batch: 638/840\n",
      "Batch loss: 0.7212159037590027 batch: 639/840\n",
      "Batch loss: 0.6429563164710999 batch: 640/840\n",
      "Batch loss: 0.9451457262039185 batch: 641/840\n",
      "Batch loss: 0.5529916882514954 batch: 642/840\n",
      "Batch loss: 1.0147366523742676 batch: 643/840\n",
      "Batch loss: 0.633291482925415 batch: 644/840\n",
      "Batch loss: 0.5699343681335449 batch: 645/840\n",
      "Batch loss: 0.5314952731132507 batch: 646/840\n",
      "Batch loss: 0.7094234228134155 batch: 647/840\n",
      "Batch loss: 0.7619422078132629 batch: 648/840\n",
      "Batch loss: 0.5717331171035767 batch: 649/840\n",
      "Batch loss: 0.5180701613426208 batch: 650/840\n",
      "Batch loss: 0.6570606231689453 batch: 651/840\n",
      "Batch loss: 0.666256308555603 batch: 652/840\n",
      "Batch loss: 0.5457398891448975 batch: 653/840\n",
      "Batch loss: 0.8084059357643127 batch: 654/840\n",
      "Batch loss: 0.6377151012420654 batch: 655/840\n",
      "Batch loss: 0.7102484703063965 batch: 656/840\n",
      "Batch loss: 0.6723938584327698 batch: 657/840\n",
      "Batch loss: 0.7509786486625671 batch: 658/840\n",
      "Batch loss: 0.716604471206665 batch: 659/840\n",
      "Batch loss: 0.5762666463851929 batch: 660/840\n",
      "Batch loss: 0.7291757464408875 batch: 661/840\n",
      "Batch loss: 0.6644465923309326 batch: 662/840\n",
      "Batch loss: 0.49115076661109924 batch: 663/840\n",
      "Batch loss: 0.560172438621521 batch: 664/840\n",
      "Batch loss: 0.7044457793235779 batch: 665/840\n",
      "Batch loss: 0.6429381370544434 batch: 666/840\n",
      "Batch loss: 0.6677320599555969 batch: 667/840\n",
      "Batch loss: 0.7118233442306519 batch: 668/840\n",
      "Batch loss: 0.5810787677764893 batch: 669/840\n",
      "Batch loss: 0.6918520331382751 batch: 670/840\n",
      "Batch loss: 0.6610472798347473 batch: 671/840\n",
      "Batch loss: 0.5902615189552307 batch: 672/840\n",
      "Batch loss: 0.7235674858093262 batch: 673/840\n",
      "Batch loss: 0.8931844234466553 batch: 674/840\n",
      "Batch loss: 0.5422303676605225 batch: 675/840\n",
      "Batch loss: 0.517580509185791 batch: 676/840\n",
      "Batch loss: 0.5844162702560425 batch: 677/840\n",
      "Batch loss: 0.7530702948570251 batch: 678/840\n",
      "Batch loss: 0.8320772647857666 batch: 679/840\n",
      "Batch loss: 0.660532534122467 batch: 680/840\n",
      "Batch loss: 0.6942828893661499 batch: 681/840\n",
      "Batch loss: 0.5967377424240112 batch: 682/840\n",
      "Batch loss: 0.4977582097053528 batch: 683/840\n",
      "Batch loss: 0.8070029616355896 batch: 684/840\n",
      "Batch loss: 0.6293387413024902 batch: 685/840\n",
      "Batch loss: 0.7754269242286682 batch: 686/840\n",
      "Batch loss: 0.6489070653915405 batch: 687/840\n",
      "Batch loss: 0.6023315191268921 batch: 688/840\n",
      "Batch loss: 0.5951921939849854 batch: 689/840\n",
      "Batch loss: 0.550735592842102 batch: 690/840\n",
      "Batch loss: 0.6469448804855347 batch: 691/840\n",
      "Batch loss: 0.6167688369750977 batch: 692/840\n",
      "Batch loss: 0.6065544486045837 batch: 693/840\n",
      "Batch loss: 0.8258855938911438 batch: 694/840\n",
      "Batch loss: 0.7379267811775208 batch: 695/840\n",
      "Batch loss: 0.6458595395088196 batch: 696/840\n",
      "Batch loss: 0.6925046443939209 batch: 697/840\n",
      "Batch loss: 0.5744650363922119 batch: 698/840\n",
      "Batch loss: 0.7124172449111938 batch: 699/840\n",
      "Batch loss: 0.7162750363349915 batch: 700/840\n",
      "Batch loss: 0.7688170075416565 batch: 701/840\n",
      "Batch loss: 0.5565046072006226 batch: 702/840\n",
      "Batch loss: 0.6408361196517944 batch: 703/840\n",
      "Batch loss: 0.7376033663749695 batch: 704/840\n",
      "Batch loss: 0.6220942139625549 batch: 705/840\n",
      "Batch loss: 0.5600770711898804 batch: 706/840\n",
      "Batch loss: 0.7889516949653625 batch: 707/840\n",
      "Batch loss: 0.7623723745346069 batch: 708/840\n",
      "Batch loss: 0.7202586531639099 batch: 709/840\n",
      "Batch loss: 0.7877023220062256 batch: 710/840\n",
      "Batch loss: 0.4320884048938751 batch: 711/840\n",
      "Batch loss: 0.6665486693382263 batch: 712/840\n",
      "Batch loss: 0.518699049949646 batch: 713/840\n",
      "Batch loss: 0.618338406085968 batch: 714/840\n",
      "Batch loss: 0.7853578925132751 batch: 715/840\n",
      "Batch loss: 0.7550447583198547 batch: 716/840\n",
      "Batch loss: 0.6521439552307129 batch: 717/840\n",
      "Batch loss: 0.5792748332023621 batch: 718/840\n",
      "Batch loss: 0.4456826448440552 batch: 719/840\n",
      "Batch loss: 0.5528707504272461 batch: 720/840\n",
      "Batch loss: 0.7656733989715576 batch: 721/840\n",
      "Batch loss: 0.7401716113090515 batch: 722/840\n",
      "Batch loss: 0.5606415271759033 batch: 723/840\n",
      "Batch loss: 0.7160631418228149 batch: 724/840\n",
      "Batch loss: 0.5635524392127991 batch: 725/840\n",
      "Batch loss: 0.701865553855896 batch: 726/840\n",
      "Batch loss: 0.8343378305435181 batch: 727/840\n",
      "Batch loss: 0.7212944626808167 batch: 728/840\n",
      "Batch loss: 0.6481443047523499 batch: 729/840\n",
      "Batch loss: 0.5855706930160522 batch: 730/840\n",
      "Batch loss: 0.5035767555236816 batch: 731/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8091269135475159 batch: 732/840\n",
      "Batch loss: 0.6594135165214539 batch: 733/840\n",
      "Batch loss: 0.5829812288284302 batch: 734/840\n",
      "Batch loss: 0.5434207916259766 batch: 735/840\n",
      "Batch loss: 0.7424229383468628 batch: 736/840\n",
      "Batch loss: 0.6686879992485046 batch: 737/840\n",
      "Batch loss: 0.4742799997329712 batch: 738/840\n",
      "Batch loss: 0.7717787027359009 batch: 739/840\n",
      "Batch loss: 0.651549756526947 batch: 740/840\n",
      "Batch loss: 0.5491855144500732 batch: 741/840\n",
      "Batch loss: 0.5602543354034424 batch: 742/840\n",
      "Batch loss: 0.44938868284225464 batch: 743/840\n",
      "Batch loss: 0.4995308816432953 batch: 744/840\n",
      "Batch loss: 0.5449714660644531 batch: 745/840\n",
      "Batch loss: 0.595782995223999 batch: 746/840\n",
      "Batch loss: 0.7084508538246155 batch: 747/840\n",
      "Batch loss: 0.6481449007987976 batch: 748/840\n",
      "Batch loss: 0.5973760485649109 batch: 749/840\n",
      "Batch loss: 0.5755674242973328 batch: 750/840\n",
      "Batch loss: 0.7710118889808655 batch: 751/840\n",
      "Batch loss: 0.8273326754570007 batch: 752/840\n",
      "Batch loss: 0.4902692139148712 batch: 753/840\n",
      "Batch loss: 0.6465353965759277 batch: 754/840\n",
      "Batch loss: 0.6007705330848694 batch: 755/840\n",
      "Batch loss: 0.5691342353820801 batch: 756/840\n",
      "Batch loss: 0.7042222023010254 batch: 757/840\n",
      "Batch loss: 0.5512230396270752 batch: 758/840\n",
      "Batch loss: 0.5494673252105713 batch: 759/840\n",
      "Batch loss: 0.5621391534805298 batch: 760/840\n",
      "Batch loss: 0.5492712259292603 batch: 761/840\n",
      "Batch loss: 0.7709680795669556 batch: 762/840\n",
      "Batch loss: 0.4586610794067383 batch: 763/840\n",
      "Batch loss: 0.6774532794952393 batch: 764/840\n",
      "Batch loss: 0.5040139555931091 batch: 765/840\n",
      "Batch loss: 0.5297149419784546 batch: 766/840\n",
      "Batch loss: 0.7836582660675049 batch: 767/840\n",
      "Batch loss: 0.7105072140693665 batch: 768/840\n",
      "Batch loss: 0.6826052665710449 batch: 769/840\n",
      "Batch loss: 0.5728565454483032 batch: 770/840\n",
      "Batch loss: 0.6118748188018799 batch: 771/840\n",
      "Batch loss: 0.6148011684417725 batch: 772/840\n",
      "Batch loss: 0.5686990022659302 batch: 773/840\n",
      "Batch loss: 0.5024869441986084 batch: 774/840\n",
      "Batch loss: 0.5429863929748535 batch: 775/840\n",
      "Batch loss: 0.567298173904419 batch: 776/840\n",
      "Batch loss: 0.5612900257110596 batch: 777/840\n",
      "Batch loss: 0.49022936820983887 batch: 778/840\n",
      "Batch loss: 0.627288818359375 batch: 779/840\n",
      "Batch loss: 0.5643578171730042 batch: 780/840\n",
      "Batch loss: 0.5702828764915466 batch: 781/840\n",
      "Batch loss: 0.6859006285667419 batch: 782/840\n",
      "Batch loss: 0.4701229929924011 batch: 783/840\n",
      "Batch loss: 0.727378785610199 batch: 784/840\n",
      "Batch loss: 0.5586891770362854 batch: 785/840\n",
      "Batch loss: 0.6049075126647949 batch: 786/840\n",
      "Batch loss: 0.6046062111854553 batch: 787/840\n",
      "Batch loss: 0.7555950284004211 batch: 788/840\n",
      "Batch loss: 0.7515838742256165 batch: 789/840\n",
      "Batch loss: 0.6312552094459534 batch: 790/840\n",
      "Batch loss: 0.6321010589599609 batch: 791/840\n",
      "Batch loss: 0.3865014612674713 batch: 792/840\n",
      "Batch loss: 0.5931577682495117 batch: 793/840\n",
      "Batch loss: 0.6005823612213135 batch: 794/840\n",
      "Batch loss: 0.6243168115615845 batch: 795/840\n",
      "Batch loss: 0.7261375188827515 batch: 796/840\n",
      "Batch loss: 0.6901390552520752 batch: 797/840\n",
      "Batch loss: 0.7115092277526855 batch: 798/840\n",
      "Batch loss: 0.6551702618598938 batch: 799/840\n",
      "Batch loss: 0.5123776197433472 batch: 800/840\n",
      "Batch loss: 0.7067309617996216 batch: 801/840\n",
      "Batch loss: 0.47986751794815063 batch: 802/840\n",
      "Batch loss: 0.4314838647842407 batch: 803/840\n",
      "Batch loss: 0.632016658782959 batch: 804/840\n",
      "Batch loss: 0.493897408246994 batch: 805/840\n",
      "Batch loss: 0.64646315574646 batch: 806/840\n",
      "Batch loss: 0.600757360458374 batch: 807/840\n",
      "Batch loss: 0.552214503288269 batch: 808/840\n",
      "Batch loss: 0.69975745677948 batch: 809/840\n",
      "Batch loss: 0.5758298635482788 batch: 810/840\n",
      "Batch loss: 0.516801118850708 batch: 811/840\n",
      "Batch loss: 0.7210677862167358 batch: 812/840\n",
      "Batch loss: 0.5130940079689026 batch: 813/840\n",
      "Batch loss: 0.6214785575866699 batch: 814/840\n",
      "Batch loss: 0.7905107140541077 batch: 815/840\n",
      "Batch loss: 0.6656181812286377 batch: 816/840\n",
      "Batch loss: 0.743142306804657 batch: 817/840\n",
      "Batch loss: 0.6585374474525452 batch: 818/840\n",
      "Batch loss: 0.47324272990226746 batch: 819/840\n",
      "Batch loss: 0.6770607233047485 batch: 820/840\n",
      "Batch loss: 0.7119659185409546 batch: 821/840\n",
      "Batch loss: 0.602100133895874 batch: 822/840\n",
      "Batch loss: 0.7676972150802612 batch: 823/840\n",
      "Batch loss: 0.8251024484634399 batch: 824/840\n",
      "Batch loss: 0.7770984172821045 batch: 825/840\n",
      "Batch loss: 0.6224884986877441 batch: 826/840\n",
      "Batch loss: 0.5191988348960876 batch: 827/840\n",
      "Batch loss: 0.6534721851348877 batch: 828/840\n",
      "Batch loss: 0.5416591167449951 batch: 829/840\n",
      "Batch loss: 0.6203359365463257 batch: 830/840\n",
      "Batch loss: 0.6184760332107544 batch: 831/840\n",
      "Batch loss: 0.7197799682617188 batch: 832/840\n",
      "Batch loss: 0.7508650422096252 batch: 833/840\n",
      "Batch loss: 0.6609027981758118 batch: 834/840\n",
      "Batch loss: 0.4837178885936737 batch: 835/840\n",
      "Batch loss: 0.6018986105918884 batch: 836/840\n",
      "Batch loss: 0.6085525751113892 batch: 837/840\n",
      "Batch loss: 0.6546699404716492 batch: 838/840\n",
      "Batch loss: 0.5248937606811523 batch: 839/840\n",
      "Batch loss: 0.6634663939476013 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 8/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.806\n",
      "Running epoch 9/15\n",
      "Batch loss: 0.46720263361930847 batch: 1/840\n",
      "Batch loss: 1.0408592224121094 batch: 2/840\n",
      "Batch loss: 0.5417430996894836 batch: 3/840\n",
      "Batch loss: 0.5443840622901917 batch: 4/840\n",
      "Batch loss: 0.551097571849823 batch: 5/840\n",
      "Batch loss: 0.5295368432998657 batch: 6/840\n",
      "Batch loss: 0.6012303829193115 batch: 7/840\n",
      "Batch loss: 0.6103538274765015 batch: 8/840\n",
      "Batch loss: 0.5110390186309814 batch: 9/840\n",
      "Batch loss: 0.5599561929702759 batch: 10/840\n",
      "Batch loss: 0.5831788182258606 batch: 11/840\n",
      "Batch loss: 0.6176919341087341 batch: 12/840\n",
      "Batch loss: 0.5633613467216492 batch: 13/840\n",
      "Batch loss: 0.6592355370521545 batch: 14/840\n",
      "Batch loss: 0.5872313380241394 batch: 15/840\n",
      "Batch loss: 0.5403078198432922 batch: 16/840\n",
      "Batch loss: 0.4774814546108246 batch: 17/840\n",
      "Batch loss: 0.6605782508850098 batch: 18/840\n",
      "Batch loss: 0.6618796586990356 batch: 19/840\n",
      "Batch loss: 0.7750101685523987 batch: 20/840\n",
      "Batch loss: 0.7389453649520874 batch: 21/840\n",
      "Batch loss: 0.5828856825828552 batch: 22/840\n",
      "Batch loss: 0.5732805132865906 batch: 23/840\n",
      "Batch loss: 0.5425131320953369 batch: 24/840\n",
      "Batch loss: 0.5150629878044128 batch: 25/840\n",
      "Batch loss: 0.6963707804679871 batch: 26/840\n",
      "Batch loss: 0.5917718410491943 batch: 27/840\n",
      "Batch loss: 0.7525048851966858 batch: 28/840\n",
      "Batch loss: 0.6910750865936279 batch: 29/840\n",
      "Batch loss: 0.5332231521606445 batch: 30/840\n",
      "Batch loss: 0.5956131219863892 batch: 31/840\n",
      "Batch loss: 0.6351809501647949 batch: 32/840\n",
      "Batch loss: 0.589358389377594 batch: 33/840\n",
      "Batch loss: 0.5904303193092346 batch: 34/840\n",
      "Batch loss: 0.6284778118133545 batch: 35/840\n",
      "Batch loss: 0.5360521674156189 batch: 36/840\n",
      "Batch loss: 0.7765170335769653 batch: 37/840\n",
      "Batch loss: 0.7147554159164429 batch: 38/840\n",
      "Batch loss: 0.6684427857398987 batch: 39/840\n",
      "Batch loss: 0.544118344783783 batch: 40/840\n",
      "Batch loss: 0.7523673176765442 batch: 41/840\n",
      "Batch loss: 0.6949252486228943 batch: 42/840\n",
      "Batch loss: 0.5981256365776062 batch: 43/840\n",
      "Batch loss: 0.6327112317085266 batch: 44/840\n",
      "Batch loss: 0.6149328351020813 batch: 45/840\n",
      "Batch loss: 0.5614184737205505 batch: 46/840\n",
      "Batch loss: 0.5062647461891174 batch: 47/840\n",
      "Batch loss: 0.5324628353118896 batch: 48/840\n",
      "Batch loss: 0.632505476474762 batch: 49/840\n",
      "Batch loss: 0.6285065412521362 batch: 50/840\n",
      "Batch loss: 0.6711486577987671 batch: 51/840\n",
      "Batch loss: 0.6612865328788757 batch: 52/840\n",
      "Batch loss: 0.5347152948379517 batch: 53/840\n",
      "Batch loss: 0.5816335082054138 batch: 54/840\n",
      "Batch loss: 0.6325497627258301 batch: 55/840\n",
      "Batch loss: 0.5468953847885132 batch: 56/840\n",
      "Batch loss: 0.7303562760353088 batch: 57/840\n",
      "Batch loss: 0.5903746485710144 batch: 58/840\n",
      "Batch loss: 0.6116459369659424 batch: 59/840\n",
      "Batch loss: 0.46725738048553467 batch: 60/840\n",
      "Batch loss: 0.8085153102874756 batch: 61/840\n",
      "Batch loss: 0.7693091034889221 batch: 62/840\n",
      "Batch loss: 0.5733938217163086 batch: 63/840\n",
      "Batch loss: 0.5485685467720032 batch: 64/840\n",
      "Batch loss: 0.4877672493457794 batch: 65/840\n",
      "Batch loss: 0.7208139300346375 batch: 66/840\n",
      "Batch loss: 0.7109994292259216 batch: 67/840\n",
      "Batch loss: 0.5740928053855896 batch: 68/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.726094663143158 batch: 69/840\n",
      "Batch loss: 0.6016557216644287 batch: 70/840\n",
      "Batch loss: 0.8730433583259583 batch: 71/840\n",
      "Batch loss: 0.783261775970459 batch: 72/840\n",
      "Batch loss: 0.6854133605957031 batch: 73/840\n",
      "Batch loss: 0.6141713857650757 batch: 74/840\n",
      "Batch loss: 0.6643725037574768 batch: 75/840\n",
      "Batch loss: 0.43803471326828003 batch: 76/840\n",
      "Batch loss: 0.6693471670150757 batch: 77/840\n",
      "Batch loss: 0.7206944823265076 batch: 78/840\n",
      "Batch loss: 0.6168774366378784 batch: 79/840\n",
      "Batch loss: 0.6286092400550842 batch: 80/840\n",
      "Batch loss: 0.5526739358901978 batch: 81/840\n",
      "Batch loss: 0.610927402973175 batch: 82/840\n",
      "Batch loss: 0.5013668537139893 batch: 83/840\n",
      "Batch loss: 0.8300849795341492 batch: 84/840\n",
      "Batch loss: 0.685204267501831 batch: 85/840\n",
      "Batch loss: 0.9446369409561157 batch: 86/840\n",
      "Batch loss: 0.563470721244812 batch: 87/840\n",
      "Batch loss: 0.44857245683670044 batch: 88/840\n",
      "Batch loss: 0.499497652053833 batch: 89/840\n",
      "Batch loss: 0.5819955468177795 batch: 90/840\n",
      "Batch loss: 0.5885211229324341 batch: 91/840\n",
      "Batch loss: 0.6645589470863342 batch: 92/840\n",
      "Batch loss: 0.6516312956809998 batch: 93/840\n",
      "Batch loss: 0.6007585525512695 batch: 94/840\n",
      "Batch loss: 0.5833510756492615 batch: 95/840\n",
      "Batch loss: 0.6375114321708679 batch: 96/840\n",
      "Batch loss: 0.6397817730903625 batch: 97/840\n",
      "Batch loss: 0.7646418213844299 batch: 98/840\n",
      "Batch loss: 0.5357962846755981 batch: 99/840\n",
      "Batch loss: 0.6275628209114075 batch: 100/840\n",
      "Batch loss: 0.577477216720581 batch: 101/840\n",
      "Batch loss: 0.5862029790878296 batch: 102/840\n",
      "Batch loss: 0.6221598386764526 batch: 103/840\n",
      "Batch loss: 0.5311405658721924 batch: 104/840\n",
      "Batch loss: 0.47299060225486755 batch: 105/840\n",
      "Batch loss: 0.6188169717788696 batch: 106/840\n",
      "Batch loss: 0.564983606338501 batch: 107/840\n",
      "Batch loss: 0.7508234977722168 batch: 108/840\n",
      "Batch loss: 0.6467620134353638 batch: 109/840\n",
      "Batch loss: 0.4969959259033203 batch: 110/840\n",
      "Batch loss: 0.5419116616249084 batch: 111/840\n",
      "Batch loss: 0.7590692043304443 batch: 112/840\n",
      "Batch loss: 0.6796351075172424 batch: 113/840\n",
      "Batch loss: 0.5895488262176514 batch: 114/840\n",
      "Batch loss: 0.5403008460998535 batch: 115/840\n",
      "Batch loss: 0.5938689708709717 batch: 116/840\n",
      "Batch loss: 0.5596352219581604 batch: 117/840\n",
      "Batch loss: 0.49695098400115967 batch: 118/840\n",
      "Batch loss: 0.7308501601219177 batch: 119/840\n",
      "Batch loss: 0.588097333908081 batch: 120/840\n",
      "Batch loss: 0.6943177580833435 batch: 121/840\n",
      "Batch loss: 0.8194280862808228 batch: 122/840\n",
      "Batch loss: 0.5373057126998901 batch: 123/840\n",
      "Batch loss: 0.529222309589386 batch: 124/840\n",
      "Batch loss: 0.5766758322715759 batch: 125/840\n",
      "Batch loss: 0.6766937375068665 batch: 126/840\n",
      "Batch loss: 0.6598759293556213 batch: 127/840\n",
      "Batch loss: 0.6353331804275513 batch: 128/840\n",
      "Batch loss: 0.6623866558074951 batch: 129/840\n",
      "Batch loss: 0.6130945086479187 batch: 130/840\n",
      "Batch loss: 0.7066322565078735 batch: 131/840\n",
      "Batch loss: 1.003556251525879 batch: 132/840\n",
      "Batch loss: 0.7326041460037231 batch: 133/840\n",
      "Batch loss: 0.5925211906433105 batch: 134/840\n",
      "Batch loss: 0.6189296841621399 batch: 135/840\n",
      "Batch loss: 0.6705384850502014 batch: 136/840\n",
      "Batch loss: 0.5816745162010193 batch: 137/840\n",
      "Batch loss: 0.4990716874599457 batch: 138/840\n",
      "Batch loss: 0.5566556453704834 batch: 139/840\n",
      "Batch loss: 0.6004728674888611 batch: 140/840\n",
      "Batch loss: 0.5152343511581421 batch: 141/840\n",
      "Batch loss: 0.563664972782135 batch: 142/840\n",
      "Batch loss: 0.5220342874526978 batch: 143/840\n",
      "Batch loss: 0.5329620838165283 batch: 144/840\n",
      "Batch loss: 0.8052594661712646 batch: 145/840\n",
      "Batch loss: 0.7767630219459534 batch: 146/840\n",
      "Batch loss: 0.5245287418365479 batch: 147/840\n",
      "Batch loss: 0.8402785658836365 batch: 148/840\n",
      "Batch loss: 0.6218195557594299 batch: 149/840\n",
      "Batch loss: 0.6388680338859558 batch: 150/840\n",
      "Batch loss: 0.5590250492095947 batch: 151/840\n",
      "Batch loss: 0.6389867067337036 batch: 152/840\n",
      "Batch loss: 0.46252527832984924 batch: 153/840\n",
      "Batch loss: 0.7392736673355103 batch: 154/840\n",
      "Batch loss: 0.5379275679588318 batch: 155/840\n",
      "Batch loss: 0.6769118309020996 batch: 156/840\n",
      "Batch loss: 0.6956487894058228 batch: 157/840\n",
      "Batch loss: 0.49961498379707336 batch: 158/840\n",
      "Batch loss: 0.5220372676849365 batch: 159/840\n",
      "Batch loss: 0.6233189105987549 batch: 160/840\n",
      "Batch loss: 0.7220887541770935 batch: 161/840\n",
      "Batch loss: 0.6924154162406921 batch: 162/840\n",
      "Batch loss: 0.6900289058685303 batch: 163/840\n",
      "Batch loss: 0.5129395127296448 batch: 164/840\n",
      "Batch loss: 0.6187071204185486 batch: 165/840\n",
      "Batch loss: 0.49052563309669495 batch: 166/840\n",
      "Batch loss: 0.659264326095581 batch: 167/840\n",
      "Batch loss: 0.6302458047866821 batch: 168/840\n",
      "Batch loss: 0.5610060095787048 batch: 169/840\n",
      "Batch loss: 0.6965833902359009 batch: 170/840\n",
      "Batch loss: 0.5635627508163452 batch: 171/840\n",
      "Batch loss: 0.724855899810791 batch: 172/840\n",
      "Batch loss: 0.5704315304756165 batch: 173/840\n",
      "Batch loss: 0.628697395324707 batch: 174/840\n",
      "Batch loss: 0.5625576972961426 batch: 175/840\n",
      "Batch loss: 0.7547378540039062 batch: 176/840\n",
      "Batch loss: 0.6924172043800354 batch: 177/840\n",
      "Batch loss: 0.6620611548423767 batch: 178/840\n",
      "Batch loss: 0.7298150658607483 batch: 179/840\n",
      "Batch loss: 0.5038092732429504 batch: 180/840\n",
      "Batch loss: 0.5595798492431641 batch: 181/840\n",
      "Batch loss: 0.5277445912361145 batch: 182/840\n",
      "Batch loss: 0.6912953853607178 batch: 183/840\n",
      "Batch loss: 0.5870241522789001 batch: 184/840\n",
      "Batch loss: 0.4195987284183502 batch: 185/840\n",
      "Batch loss: 0.592597246170044 batch: 186/840\n",
      "Batch loss: 0.6088815927505493 batch: 187/840\n",
      "Batch loss: 0.5834867358207703 batch: 188/840\n",
      "Batch loss: 0.5653294920921326 batch: 189/840\n",
      "Batch loss: 0.6381814479827881 batch: 190/840\n",
      "Batch loss: 0.7387114763259888 batch: 191/840\n",
      "Batch loss: 0.4822213053703308 batch: 192/840\n",
      "Batch loss: 0.4724225103855133 batch: 193/840\n",
      "Batch loss: 0.45537468791007996 batch: 194/840\n",
      "Batch loss: 0.5151609182357788 batch: 195/840\n",
      "Batch loss: 0.7187432050704956 batch: 196/840\n",
      "Batch loss: 0.5636433959007263 batch: 197/840\n",
      "Batch loss: 0.4743410348892212 batch: 198/840\n",
      "Batch loss: 0.692157506942749 batch: 199/840\n",
      "Batch loss: 0.923482358455658 batch: 200/840\n",
      "Batch loss: 0.5161690711975098 batch: 201/840\n",
      "Batch loss: 0.5673556327819824 batch: 202/840\n",
      "Batch loss: 0.5670406818389893 batch: 203/840\n",
      "Batch loss: 0.715119481086731 batch: 204/840\n",
      "Batch loss: 0.7232906222343445 batch: 205/840\n",
      "Batch loss: 0.6807544827461243 batch: 206/840\n",
      "Batch loss: 0.6682493686676025 batch: 207/840\n",
      "Batch loss: 0.6469703912734985 batch: 208/840\n",
      "Batch loss: 0.5367289185523987 batch: 209/840\n",
      "Batch loss: 0.5781431198120117 batch: 210/840\n",
      "Batch loss: 0.4495828151702881 batch: 211/840\n",
      "Batch loss: 0.5235375761985779 batch: 212/840\n",
      "Batch loss: 0.6664482951164246 batch: 213/840\n",
      "Batch loss: 0.8076543211936951 batch: 214/840\n",
      "Batch loss: 0.6273176670074463 batch: 215/840\n",
      "Batch loss: 0.6696894764900208 batch: 216/840\n",
      "Batch loss: 0.6810657382011414 batch: 217/840\n",
      "Batch loss: 0.6512558460235596 batch: 218/840\n",
      "Batch loss: 0.6904773116111755 batch: 219/840\n",
      "Batch loss: 0.7544159889221191 batch: 220/840\n",
      "Batch loss: 0.6118414402008057 batch: 221/840\n",
      "Batch loss: 0.7840908169746399 batch: 222/840\n",
      "Batch loss: 0.5773178339004517 batch: 223/840\n",
      "Batch loss: 0.7119357585906982 batch: 224/840\n",
      "Batch loss: 0.6078453063964844 batch: 225/840\n",
      "Batch loss: 0.7250950336456299 batch: 226/840\n",
      "Batch loss: 0.6826357245445251 batch: 227/840\n",
      "Batch loss: 0.46527794003486633 batch: 228/840\n",
      "Batch loss: 0.5081445574760437 batch: 229/840\n",
      "Batch loss: 0.6575700640678406 batch: 230/840\n",
      "Batch loss: 0.5097560286521912 batch: 231/840\n",
      "Batch loss: 0.7012931704521179 batch: 232/840\n",
      "Batch loss: 0.7078735828399658 batch: 233/840\n",
      "Batch loss: 0.6225784420967102 batch: 234/840\n",
      "Batch loss: 0.6406348943710327 batch: 235/840\n",
      "Batch loss: 0.7185800075531006 batch: 236/840\n",
      "Batch loss: 0.6542635560035706 batch: 237/840\n",
      "Batch loss: 0.7244508862495422 batch: 238/840\n",
      "Batch loss: 0.5831475257873535 batch: 239/840\n",
      "Batch loss: 0.6225013732910156 batch: 240/840\n",
      "Batch loss: 0.7297075390815735 batch: 241/840\n",
      "Batch loss: 0.5721263885498047 batch: 242/840\n",
      "Batch loss: 0.5832617282867432 batch: 243/840\n",
      "Batch loss: 0.7841969132423401 batch: 244/840\n",
      "Batch loss: 0.47845128178596497 batch: 245/840\n",
      "Batch loss: 0.6089494228363037 batch: 246/840\n",
      "Batch loss: 0.6881828904151917 batch: 247/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7087507843971252 batch: 248/840\n",
      "Batch loss: 0.6444986462593079 batch: 249/840\n",
      "Batch loss: 0.5033085942268372 batch: 250/840\n",
      "Batch loss: 0.5960314273834229 batch: 251/840\n",
      "Batch loss: 0.5817121267318726 batch: 252/840\n",
      "Batch loss: 0.7002370953559875 batch: 253/840\n",
      "Batch loss: 0.6930816173553467 batch: 254/840\n",
      "Batch loss: 0.6040250658988953 batch: 255/840\n",
      "Batch loss: 0.666151225566864 batch: 256/840\n",
      "Batch loss: 0.5316101908683777 batch: 257/840\n",
      "Batch loss: 0.7246439456939697 batch: 258/840\n",
      "Batch loss: 0.5317834615707397 batch: 259/840\n",
      "Batch loss: 0.4934249520301819 batch: 260/840\n",
      "Batch loss: 0.5574193000793457 batch: 261/840\n",
      "Batch loss: 0.42644762992858887 batch: 262/840\n",
      "Batch loss: 0.6393835544586182 batch: 263/840\n",
      "Batch loss: 0.564897894859314 batch: 264/840\n",
      "Batch loss: 0.6426345705986023 batch: 265/840\n",
      "Batch loss: 0.5702565908432007 batch: 266/840\n",
      "Batch loss: 0.6365152597427368 batch: 267/840\n",
      "Batch loss: 0.5330783724784851 batch: 268/840\n",
      "Batch loss: 0.5758811831474304 batch: 269/840\n",
      "Batch loss: 0.6102586388587952 batch: 270/840\n",
      "Batch loss: 0.6480127573013306 batch: 271/840\n",
      "Batch loss: 0.8386380672454834 batch: 272/840\n",
      "Batch loss: 0.7252113223075867 batch: 273/840\n",
      "Batch loss: 0.5572139620780945 batch: 274/840\n",
      "Batch loss: 0.7327607870101929 batch: 275/840\n",
      "Batch loss: 0.5352132320404053 batch: 276/840\n",
      "Batch loss: 0.6601620316505432 batch: 277/840\n",
      "Batch loss: 0.6721561551094055 batch: 278/840\n",
      "Batch loss: 0.721869707107544 batch: 279/840\n",
      "Batch loss: 0.8063673973083496 batch: 280/840\n",
      "Batch loss: 0.4981197118759155 batch: 281/840\n",
      "Batch loss: 0.5153654217720032 batch: 282/840\n",
      "Batch loss: 0.6291202306747437 batch: 283/840\n",
      "Batch loss: 0.5662968754768372 batch: 284/840\n",
      "Batch loss: 0.4710562229156494 batch: 285/840\n",
      "Batch loss: 0.5889464020729065 batch: 286/840\n",
      "Batch loss: 0.466562956571579 batch: 287/840\n",
      "Batch loss: 0.581459105014801 batch: 288/840\n",
      "Batch loss: 0.7579061388969421 batch: 289/840\n",
      "Batch loss: 0.7178129553794861 batch: 290/840\n",
      "Batch loss: 0.6338863968849182 batch: 291/840\n",
      "Batch loss: 0.5876119136810303 batch: 292/840\n",
      "Batch loss: 0.6693970561027527 batch: 293/840\n",
      "Batch loss: 0.6035677194595337 batch: 294/840\n",
      "Batch loss: 0.4999287724494934 batch: 295/840\n",
      "Batch loss: 0.6385448575019836 batch: 296/840\n",
      "Batch loss: 0.8171305060386658 batch: 297/840\n",
      "Batch loss: 0.8131267428398132 batch: 298/840\n",
      "Batch loss: 0.5232344269752502 batch: 299/840\n",
      "Batch loss: 0.7412283420562744 batch: 300/840\n",
      "Batch loss: 0.7362081408500671 batch: 301/840\n",
      "Batch loss: 0.6223866939544678 batch: 302/840\n",
      "Batch loss: 0.8825304508209229 batch: 303/840\n",
      "Batch loss: 0.5551559329032898 batch: 304/840\n",
      "Batch loss: 0.5126925110816956 batch: 305/840\n",
      "Batch loss: 0.6695334911346436 batch: 306/840\n",
      "Batch loss: 0.6047444939613342 batch: 307/840\n",
      "Batch loss: 0.7012946605682373 batch: 308/840\n",
      "Batch loss: 0.6243370771408081 batch: 309/840\n",
      "Batch loss: 0.7346993088722229 batch: 310/840\n",
      "Batch loss: 0.779778242111206 batch: 311/840\n",
      "Batch loss: 0.697098970413208 batch: 312/840\n",
      "Batch loss: 0.6612234711647034 batch: 313/840\n",
      "Batch loss: 0.45357921719551086 batch: 314/840\n",
      "Batch loss: 0.6578208804130554 batch: 315/840\n",
      "Batch loss: 0.5333945751190186 batch: 316/840\n",
      "Batch loss: 0.7013885378837585 batch: 317/840\n",
      "Batch loss: 0.6084938049316406 batch: 318/840\n",
      "Batch loss: 0.7731484174728394 batch: 319/840\n",
      "Batch loss: 0.5187990665435791 batch: 320/840\n",
      "Batch loss: 0.5753445625305176 batch: 321/840\n",
      "Batch loss: 0.7177255749702454 batch: 322/840\n",
      "Batch loss: 0.7794089317321777 batch: 323/840\n",
      "Batch loss: 0.6192091703414917 batch: 324/840\n",
      "Batch loss: 0.5406458377838135 batch: 325/840\n",
      "Batch loss: 0.6065355539321899 batch: 326/840\n",
      "Batch loss: 0.49999329447746277 batch: 327/840\n",
      "Batch loss: 0.7351995706558228 batch: 328/840\n",
      "Batch loss: 0.6491515636444092 batch: 329/840\n",
      "Batch loss: 0.6540685892105103 batch: 330/840\n",
      "Batch loss: 0.679820716381073 batch: 331/840\n",
      "Batch loss: 0.6678908467292786 batch: 332/840\n",
      "Batch loss: 0.6358050107955933 batch: 333/840\n",
      "Batch loss: 0.6856387257575989 batch: 334/840\n",
      "Batch loss: 0.6450189352035522 batch: 335/840\n",
      "Batch loss: 0.6653608083724976 batch: 336/840\n",
      "Batch loss: 0.8806698322296143 batch: 337/840\n",
      "Batch loss: 0.7247281074523926 batch: 338/840\n",
      "Batch loss: 0.5281143188476562 batch: 339/840\n",
      "Batch loss: 0.7915657162666321 batch: 340/840\n",
      "Batch loss: 0.47628268599510193 batch: 341/840\n",
      "Batch loss: 0.49686622619628906 batch: 342/840\n",
      "Batch loss: 0.8016006350517273 batch: 343/840\n",
      "Batch loss: 0.6507316827774048 batch: 344/840\n",
      "Batch loss: 0.4530785083770752 batch: 345/840\n",
      "Batch loss: 0.6090896129608154 batch: 346/840\n",
      "Batch loss: 0.5621417760848999 batch: 347/840\n",
      "Batch loss: 0.5607439875602722 batch: 348/840\n",
      "Batch loss: 0.5801187753677368 batch: 349/840\n",
      "Batch loss: 0.5820907354354858 batch: 350/840\n",
      "Batch loss: 0.6797874569892883 batch: 351/840\n",
      "Batch loss: 0.6267884373664856 batch: 352/840\n",
      "Batch loss: 0.6605251431465149 batch: 353/840\n",
      "Batch loss: 0.6346376538276672 batch: 354/840\n",
      "Batch loss: 0.5461602210998535 batch: 355/840\n",
      "Batch loss: 0.5482287406921387 batch: 356/840\n",
      "Batch loss: 0.5091579556465149 batch: 357/840\n",
      "Batch loss: 0.768073558807373 batch: 358/840\n",
      "Batch loss: 0.596660315990448 batch: 359/840\n",
      "Batch loss: 0.7261320352554321 batch: 360/840\n",
      "Batch loss: 0.7475717067718506 batch: 361/840\n",
      "Batch loss: 0.5446472764015198 batch: 362/840\n",
      "Batch loss: 0.5893754363059998 batch: 363/840\n",
      "Batch loss: 0.689147412776947 batch: 364/840\n",
      "Batch loss: 0.5456902384757996 batch: 365/840\n",
      "Batch loss: 0.6875455975532532 batch: 366/840\n",
      "Batch loss: 0.47878336906433105 batch: 367/840\n",
      "Batch loss: 0.7714394330978394 batch: 368/840\n",
      "Batch loss: 0.6474930047988892 batch: 369/840\n",
      "Batch loss: 0.7640664577484131 batch: 370/840\n",
      "Batch loss: 0.6692766547203064 batch: 371/840\n",
      "Batch loss: 0.5357468128204346 batch: 372/840\n",
      "Batch loss: 0.5458059906959534 batch: 373/840\n",
      "Batch loss: 0.6717793345451355 batch: 374/840\n",
      "Batch loss: 0.4835397005081177 batch: 375/840\n",
      "Batch loss: 0.5004908442497253 batch: 376/840\n",
      "Batch loss: 0.6054301261901855 batch: 377/840\n",
      "Batch loss: 0.5738227963447571 batch: 378/840\n",
      "Batch loss: 0.49981769919395447 batch: 379/840\n",
      "Batch loss: 0.8595103621482849 batch: 380/840\n",
      "Batch loss: 1.060224175453186 batch: 381/840\n",
      "Batch loss: 0.6785922050476074 batch: 382/840\n",
      "Batch loss: 0.6505854725837708 batch: 383/840\n",
      "Batch loss: 0.6097143292427063 batch: 384/840\n",
      "Batch loss: 0.6597146391868591 batch: 385/840\n",
      "Batch loss: 0.7805274724960327 batch: 386/840\n",
      "Batch loss: 0.5755336880683899 batch: 387/840\n",
      "Batch loss: 0.6476947665214539 batch: 388/840\n",
      "Batch loss: 0.5252013206481934 batch: 389/840\n",
      "Batch loss: 0.9203460812568665 batch: 390/840\n",
      "Batch loss: 0.776908278465271 batch: 391/840\n",
      "Batch loss: 0.6030365228652954 batch: 392/840\n",
      "Batch loss: 0.501693606376648 batch: 393/840\n",
      "Batch loss: 0.7699711322784424 batch: 394/840\n",
      "Batch loss: 0.6636087894439697 batch: 395/840\n",
      "Batch loss: 0.605646014213562 batch: 396/840\n",
      "Batch loss: 0.5570612549781799 batch: 397/840\n",
      "Batch loss: 0.7639591097831726 batch: 398/840\n",
      "Batch loss: 0.4175383746623993 batch: 399/840\n",
      "Batch loss: 0.6302525997161865 batch: 400/840\n",
      "Batch loss: 0.6472092270851135 batch: 401/840\n",
      "Batch loss: 0.4988885819911957 batch: 402/840\n",
      "Batch loss: 0.6092717051506042 batch: 403/840\n",
      "Batch loss: 0.5347450375556946 batch: 404/840\n",
      "Batch loss: 0.6186445355415344 batch: 405/840\n",
      "Batch loss: 0.5636435151100159 batch: 406/840\n",
      "Batch loss: 0.5421745181083679 batch: 407/840\n",
      "Batch loss: 0.721964418888092 batch: 408/840\n",
      "Batch loss: 0.6653881072998047 batch: 409/840\n",
      "Batch loss: 0.7459123134613037 batch: 410/840\n",
      "Batch loss: 0.7331175804138184 batch: 411/840\n",
      "Batch loss: 0.7644063830375671 batch: 412/840\n",
      "Batch loss: 0.648150622844696 batch: 413/840\n",
      "Batch loss: 0.6566672325134277 batch: 414/840\n",
      "Batch loss: 0.9184335470199585 batch: 415/840\n",
      "Batch loss: 0.48588505387306213 batch: 416/840\n",
      "Batch loss: 0.6007694005966187 batch: 417/840\n",
      "Batch loss: 0.7957248091697693 batch: 418/840\n",
      "Batch loss: 0.7179077863693237 batch: 419/840\n",
      "Batch loss: 0.6772482395172119 batch: 420/840\n",
      "Batch loss: 0.45881417393684387 batch: 421/840\n",
      "Batch loss: 0.5124198794364929 batch: 422/840\n",
      "Batch loss: 0.606976330280304 batch: 423/840\n",
      "Batch loss: 0.7746378183364868 batch: 424/840\n",
      "Batch loss: 0.7141510248184204 batch: 425/840\n",
      "Batch loss: 0.6013335585594177 batch: 426/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6328994631767273 batch: 427/840\n",
      "Batch loss: 0.7144467830657959 batch: 428/840\n",
      "Batch loss: 0.5882222056388855 batch: 429/840\n",
      "Batch loss: 0.7345538139343262 batch: 430/840\n",
      "Batch loss: 0.6331673860549927 batch: 431/840\n",
      "Batch loss: 0.6205217838287354 batch: 432/840\n",
      "Batch loss: 0.5064971446990967 batch: 433/840\n",
      "Batch loss: 0.5621923804283142 batch: 434/840\n",
      "Batch loss: 0.6718695163726807 batch: 435/840\n",
      "Batch loss: 0.6152183413505554 batch: 436/840\n",
      "Batch loss: 0.6221634149551392 batch: 437/840\n",
      "Batch loss: 0.5131189227104187 batch: 438/840\n",
      "Batch loss: 0.6231229901313782 batch: 439/840\n",
      "Batch loss: 0.6852729320526123 batch: 440/840\n",
      "Batch loss: 0.7857697010040283 batch: 441/840\n",
      "Batch loss: 0.7132149338722229 batch: 442/840\n",
      "Batch loss: 0.6094950437545776 batch: 443/840\n",
      "Batch loss: 0.6353698372840881 batch: 444/840\n",
      "Batch loss: 0.5493463277816772 batch: 445/840\n",
      "Batch loss: 0.6397413015365601 batch: 446/840\n",
      "Batch loss: 0.6949372887611389 batch: 447/840\n",
      "Batch loss: 0.691754937171936 batch: 448/840\n",
      "Batch loss: 0.598717987537384 batch: 449/840\n",
      "Batch loss: 0.6283212900161743 batch: 450/840\n",
      "Batch loss: 0.680530846118927 batch: 451/840\n",
      "Batch loss: 0.7440336346626282 batch: 452/840\n",
      "Batch loss: 0.7721909284591675 batch: 453/840\n",
      "Batch loss: 0.5697265267372131 batch: 454/840\n",
      "Batch loss: 0.6409485340118408 batch: 455/840\n",
      "Batch loss: 0.621028482913971 batch: 456/840\n",
      "Batch loss: 0.5181517004966736 batch: 457/840\n",
      "Batch loss: 0.6751169562339783 batch: 458/840\n",
      "Batch loss: 0.5094811916351318 batch: 459/840\n",
      "Batch loss: 0.42637377977371216 batch: 460/840\n",
      "Batch loss: 0.7682836651802063 batch: 461/840\n",
      "Batch loss: 0.6302329897880554 batch: 462/840\n",
      "Batch loss: 0.7539163827896118 batch: 463/840\n",
      "Batch loss: 0.5110216736793518 batch: 464/840\n",
      "Batch loss: 0.9628555178642273 batch: 465/840\n",
      "Batch loss: 0.6553127765655518 batch: 466/840\n",
      "Batch loss: 0.882129967212677 batch: 467/840\n",
      "Batch loss: 0.5431798696517944 batch: 468/840\n",
      "Batch loss: 0.5773367285728455 batch: 469/840\n",
      "Batch loss: 0.596451997756958 batch: 470/840\n",
      "Batch loss: 0.6144998073577881 batch: 471/840\n",
      "Batch loss: 0.5997008681297302 batch: 472/840\n",
      "Batch loss: 0.7094058990478516 batch: 473/840\n",
      "Batch loss: 0.5421209931373596 batch: 474/840\n",
      "Batch loss: 0.6227898597717285 batch: 475/840\n",
      "Batch loss: 0.6131453514099121 batch: 476/840\n",
      "Batch loss: 0.5326719284057617 batch: 477/840\n",
      "Batch loss: 0.5856667757034302 batch: 478/840\n",
      "Batch loss: 0.5880134105682373 batch: 479/840\n",
      "Batch loss: 0.624143123626709 batch: 480/840\n",
      "Batch loss: 0.6788923144340515 batch: 481/840\n",
      "Batch loss: 0.6408188343048096 batch: 482/840\n",
      "Batch loss: 0.6973316669464111 batch: 483/840\n",
      "Batch loss: 0.46519744396209717 batch: 484/840\n",
      "Batch loss: 0.6651962399482727 batch: 485/840\n",
      "Batch loss: 0.6220008730888367 batch: 486/840\n",
      "Batch loss: 0.43814021348953247 batch: 487/840\n",
      "Batch loss: 0.5929620265960693 batch: 488/840\n",
      "Batch loss: 0.6297224760055542 batch: 489/840\n",
      "Batch loss: 0.7363564968109131 batch: 490/840\n",
      "Batch loss: 0.4379744827747345 batch: 491/840\n",
      "Batch loss: 0.7565906643867493 batch: 492/840\n",
      "Batch loss: 0.7995694875717163 batch: 493/840\n",
      "Batch loss: 0.4488976001739502 batch: 494/840\n",
      "Batch loss: 0.8084660172462463 batch: 495/840\n",
      "Batch loss: 0.6769286394119263 batch: 496/840\n",
      "Batch loss: 0.6035841107368469 batch: 497/840\n",
      "Batch loss: 0.5223285555839539 batch: 498/840\n",
      "Batch loss: 0.6674758791923523 batch: 499/840\n",
      "Batch loss: 0.639729380607605 batch: 500/840\n",
      "Batch loss: 0.5880715847015381 batch: 501/840\n",
      "Batch loss: 0.729850172996521 batch: 502/840\n",
      "Batch loss: 0.4137401282787323 batch: 503/840\n",
      "Batch loss: 0.6609017848968506 batch: 504/840\n",
      "Batch loss: 0.6762059926986694 batch: 505/840\n",
      "Batch loss: 0.5947221517562866 batch: 506/840\n",
      "Batch loss: 0.6732006669044495 batch: 507/840\n",
      "Batch loss: 0.5864303708076477 batch: 508/840\n",
      "Batch loss: 0.7164042592048645 batch: 509/840\n",
      "Batch loss: 0.5779170989990234 batch: 510/840\n",
      "Batch loss: 0.5035150051116943 batch: 511/840\n",
      "Batch loss: 0.6259011030197144 batch: 512/840\n",
      "Batch loss: 0.5798521041870117 batch: 513/840\n",
      "Batch loss: 0.7219563126564026 batch: 514/840\n",
      "Batch loss: 0.5226141214370728 batch: 515/840\n",
      "Batch loss: 0.6249282956123352 batch: 516/840\n",
      "Batch loss: 0.6151735782623291 batch: 517/840\n",
      "Batch loss: 0.6456295847892761 batch: 518/840\n",
      "Batch loss: 0.7289748191833496 batch: 519/840\n",
      "Batch loss: 0.6547396779060364 batch: 520/840\n",
      "Batch loss: 0.7423204183578491 batch: 521/840\n",
      "Batch loss: 0.5689185857772827 batch: 522/840\n",
      "Batch loss: 0.5803030133247375 batch: 523/840\n",
      "Batch loss: 0.6124246120452881 batch: 524/840\n",
      "Batch loss: 0.6873654723167419 batch: 525/840\n",
      "Batch loss: 0.6957244873046875 batch: 526/840\n",
      "Batch loss: 0.6735248565673828 batch: 527/840\n",
      "Batch loss: 0.6507965922355652 batch: 528/840\n",
      "Batch loss: 0.532738208770752 batch: 529/840\n",
      "Batch loss: 0.6741690039634705 batch: 530/840\n",
      "Batch loss: 0.5082652568817139 batch: 531/840\n",
      "Batch loss: 0.4433060586452484 batch: 532/840\n",
      "Batch loss: 0.6434652209281921 batch: 533/840\n",
      "Batch loss: 0.6610107421875 batch: 534/840\n",
      "Batch loss: 0.6186215281486511 batch: 535/840\n",
      "Batch loss: 0.6628591418266296 batch: 536/840\n",
      "Batch loss: 0.6044527888298035 batch: 537/840\n",
      "Batch loss: 0.5527300238609314 batch: 538/840\n",
      "Batch loss: 0.636502206325531 batch: 539/840\n",
      "Batch loss: 0.7095894813537598 batch: 540/840\n",
      "Batch loss: 0.4884856939315796 batch: 541/840\n",
      "Batch loss: 0.6744760274887085 batch: 542/840\n",
      "Batch loss: 0.4183388948440552 batch: 543/840\n",
      "Batch loss: 0.6248883605003357 batch: 544/840\n",
      "Batch loss: 0.6168196201324463 batch: 545/840\n",
      "Batch loss: 0.5851783752441406 batch: 546/840\n",
      "Batch loss: 0.5676496028900146 batch: 547/840\n",
      "Batch loss: 0.5078338980674744 batch: 548/840\n",
      "Batch loss: 0.5453600287437439 batch: 549/840\n",
      "Batch loss: 0.5066419243812561 batch: 550/840\n",
      "Batch loss: 0.6838148236274719 batch: 551/840\n",
      "Batch loss: 0.5665812492370605 batch: 552/840\n",
      "Batch loss: 0.6268386840820312 batch: 553/840\n",
      "Batch loss: 0.5492491722106934 batch: 554/840\n",
      "Batch loss: 0.6305602788925171 batch: 555/840\n",
      "Batch loss: 0.6541185975074768 batch: 556/840\n",
      "Batch loss: 0.5713297128677368 batch: 557/840\n",
      "Batch loss: 0.6015369296073914 batch: 558/840\n",
      "Batch loss: 0.580501914024353 batch: 559/840\n",
      "Batch loss: 0.6459366083145142 batch: 560/840\n",
      "Batch loss: 0.6161648631095886 batch: 561/840\n",
      "Batch loss: 0.5980812311172485 batch: 562/840\n",
      "Batch loss: 0.49247822165489197 batch: 563/840\n",
      "Batch loss: 0.6508483290672302 batch: 564/840\n",
      "Batch loss: 0.7647207379341125 batch: 565/840\n",
      "Batch loss: 0.7099173665046692 batch: 566/840\n",
      "Batch loss: 0.7666503190994263 batch: 567/840\n",
      "Batch loss: 0.5226521492004395 batch: 568/840\n",
      "Batch loss: 0.593743622303009 batch: 569/840\n",
      "Batch loss: 0.4752233922481537 batch: 570/840\n",
      "Batch loss: 0.7167566418647766 batch: 571/840\n",
      "Batch loss: 0.6764801740646362 batch: 572/840\n",
      "Batch loss: 0.6590465307235718 batch: 573/840\n",
      "Batch loss: 0.7365267276763916 batch: 574/840\n",
      "Batch loss: 0.502659261226654 batch: 575/840\n",
      "Batch loss: 0.5723166465759277 batch: 576/840\n",
      "Batch loss: 0.5284430980682373 batch: 577/840\n",
      "Batch loss: 0.7214901447296143 batch: 578/840\n",
      "Batch loss: 0.7110830545425415 batch: 579/840\n",
      "Batch loss: 0.7723095417022705 batch: 580/840\n",
      "Batch loss: 0.693021833896637 batch: 581/840\n",
      "Batch loss: 0.8698728084564209 batch: 582/840\n",
      "Batch loss: 0.5653095841407776 batch: 583/840\n",
      "Batch loss: 0.7263783812522888 batch: 584/840\n",
      "Batch loss: 0.711723268032074 batch: 585/840\n",
      "Batch loss: 0.7351190447807312 batch: 586/840\n",
      "Batch loss: 0.6298427581787109 batch: 587/840\n",
      "Batch loss: 0.6160877346992493 batch: 588/840\n",
      "Batch loss: 0.6715015172958374 batch: 589/840\n",
      "Batch loss: 0.5681446194648743 batch: 590/840\n",
      "Batch loss: 0.4448176920413971 batch: 591/840\n",
      "Batch loss: 0.565345048904419 batch: 592/840\n",
      "Batch loss: 0.9413208961486816 batch: 593/840\n",
      "Batch loss: 0.6282541155815125 batch: 594/840\n",
      "Batch loss: 0.36272957921028137 batch: 595/840\n",
      "Batch loss: 0.4617297053337097 batch: 596/840\n",
      "Batch loss: 0.5990068316459656 batch: 597/840\n",
      "Batch loss: 0.4196324050426483 batch: 598/840\n",
      "Batch loss: 0.533980131149292 batch: 599/840\n",
      "Batch loss: 0.5819435119628906 batch: 600/840\n",
      "Batch loss: 0.7333360314369202 batch: 601/840\n",
      "Batch loss: 0.5777857303619385 batch: 602/840\n",
      "Batch loss: 0.6273977756500244 batch: 603/840\n",
      "Batch loss: 0.5771207213401794 batch: 604/840\n",
      "Batch loss: 0.7579383850097656 batch: 605/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8905407786369324 batch: 606/840\n",
      "Batch loss: 0.6224891543388367 batch: 607/840\n",
      "Batch loss: 0.6245429515838623 batch: 608/840\n",
      "Batch loss: 0.46295538544654846 batch: 609/840\n",
      "Batch loss: 0.6474604606628418 batch: 610/840\n",
      "Batch loss: 0.6580077409744263 batch: 611/840\n",
      "Batch loss: 0.6199028491973877 batch: 612/840\n",
      "Batch loss: 0.6779490113258362 batch: 613/840\n",
      "Batch loss: 0.6177284717559814 batch: 614/840\n",
      "Batch loss: 0.5120131969451904 batch: 615/840\n",
      "Batch loss: 0.5481239557266235 batch: 616/840\n",
      "Batch loss: 0.49344608187675476 batch: 617/840\n",
      "Batch loss: 0.5900804400444031 batch: 618/840\n",
      "Batch loss: 0.7559494972229004 batch: 619/840\n",
      "Batch loss: 0.5270478129386902 batch: 620/840\n",
      "Batch loss: 0.700214684009552 batch: 621/840\n",
      "Batch loss: 0.6552430987358093 batch: 622/840\n",
      "Batch loss: 0.5820823907852173 batch: 623/840\n",
      "Batch loss: 0.7617287635803223 batch: 624/840\n",
      "Batch loss: 0.6986879706382751 batch: 625/840\n",
      "Batch loss: 0.57233065366745 batch: 626/840\n",
      "Batch loss: 0.6244034767150879 batch: 627/840\n",
      "Batch loss: 0.7105811834335327 batch: 628/840\n",
      "Batch loss: 0.6230621337890625 batch: 629/840\n",
      "Batch loss: 0.6410508155822754 batch: 630/840\n",
      "Batch loss: 0.8000884056091309 batch: 631/840\n",
      "Batch loss: 0.6701948642730713 batch: 632/840\n",
      "Batch loss: 0.5859460234642029 batch: 633/840\n",
      "Batch loss: 0.7683612108230591 batch: 634/840\n",
      "Batch loss: 0.5701433420181274 batch: 635/840\n",
      "Batch loss: 0.5657908916473389 batch: 636/840\n",
      "Batch loss: 0.5180866122245789 batch: 637/840\n",
      "Batch loss: 0.6776191592216492 batch: 638/840\n",
      "Batch loss: 0.6971752047538757 batch: 639/840\n",
      "Batch loss: 0.5787554383277893 batch: 640/840\n",
      "Batch loss: 0.8839860558509827 batch: 641/840\n",
      "Batch loss: 0.6696061491966248 batch: 642/840\n",
      "Batch loss: 0.8841133713722229 batch: 643/840\n",
      "Batch loss: 0.5773835778236389 batch: 644/840\n",
      "Batch loss: 0.5998947024345398 batch: 645/840\n",
      "Batch loss: 0.6231186985969543 batch: 646/840\n",
      "Batch loss: 0.7159653306007385 batch: 647/840\n",
      "Batch loss: 0.7251223921775818 batch: 648/840\n",
      "Batch loss: 0.6150240898132324 batch: 649/840\n",
      "Batch loss: 0.5479413866996765 batch: 650/840\n",
      "Batch loss: 0.6568450927734375 batch: 651/840\n",
      "Batch loss: 0.646634578704834 batch: 652/840\n",
      "Batch loss: 0.5081339478492737 batch: 653/840\n",
      "Batch loss: 0.8220161199569702 batch: 654/840\n",
      "Batch loss: 0.5166146159172058 batch: 655/840\n",
      "Batch loss: 0.6701714992523193 batch: 656/840\n",
      "Batch loss: 0.5688493251800537 batch: 657/840\n",
      "Batch loss: 0.7071205377578735 batch: 658/840\n",
      "Batch loss: 0.7447535991668701 batch: 659/840\n",
      "Batch loss: 0.5232247710227966 batch: 660/840\n",
      "Batch loss: 0.7114764451980591 batch: 661/840\n",
      "Batch loss: 0.6494996547698975 batch: 662/840\n",
      "Batch loss: 0.5457444190979004 batch: 663/840\n",
      "Batch loss: 0.5854930281639099 batch: 664/840\n",
      "Batch loss: 0.6652767658233643 batch: 665/840\n",
      "Batch loss: 0.6370000243186951 batch: 666/840\n",
      "Batch loss: 0.6670354604721069 batch: 667/840\n",
      "Batch loss: 0.5564190745353699 batch: 668/840\n",
      "Batch loss: 0.5215683579444885 batch: 669/840\n",
      "Batch loss: 0.6575211882591248 batch: 670/840\n",
      "Batch loss: 0.725362241268158 batch: 671/840\n",
      "Batch loss: 0.6533451080322266 batch: 672/840\n",
      "Batch loss: 0.8773183226585388 batch: 673/840\n",
      "Batch loss: 0.7314255237579346 batch: 674/840\n",
      "Batch loss: 0.5599275827407837 batch: 675/840\n",
      "Batch loss: 0.5920734405517578 batch: 676/840\n",
      "Batch loss: 0.6657184362411499 batch: 677/840\n",
      "Batch loss: 0.6335951089859009 batch: 678/840\n",
      "Batch loss: 0.7349227666854858 batch: 679/840\n",
      "Batch loss: 0.6496559381484985 batch: 680/840\n",
      "Batch loss: 0.6333274245262146 batch: 681/840\n",
      "Batch loss: 0.6146484613418579 batch: 682/840\n",
      "Batch loss: 0.5256410241127014 batch: 683/840\n",
      "Batch loss: 0.8869006633758545 batch: 684/840\n",
      "Batch loss: 0.6677177548408508 batch: 685/840\n",
      "Batch loss: 0.7395106554031372 batch: 686/840\n",
      "Batch loss: 0.6916980147361755 batch: 687/840\n",
      "Batch loss: 0.6579289436340332 batch: 688/840\n",
      "Batch loss: 0.592282772064209 batch: 689/840\n",
      "Batch loss: 0.5451173782348633 batch: 690/840\n",
      "Batch loss: 0.6431192755699158 batch: 691/840\n",
      "Batch loss: 0.5949462652206421 batch: 692/840\n",
      "Batch loss: 0.6153609752655029 batch: 693/840\n",
      "Batch loss: 0.7707385420799255 batch: 694/840\n",
      "Batch loss: 0.7738090753555298 batch: 695/840\n",
      "Batch loss: 0.561694324016571 batch: 696/840\n",
      "Batch loss: 0.5958281755447388 batch: 697/840\n",
      "Batch loss: 0.5904597640037537 batch: 698/840\n",
      "Batch loss: 0.8444886803627014 batch: 699/840\n",
      "Batch loss: 0.7711362242698669 batch: 700/840\n",
      "Batch loss: 0.9070918560028076 batch: 701/840\n",
      "Batch loss: 0.6389248371124268 batch: 702/840\n",
      "Batch loss: 0.5205205082893372 batch: 703/840\n",
      "Batch loss: 0.6841217875480652 batch: 704/840\n",
      "Batch loss: 0.5860050916671753 batch: 705/840\n",
      "Batch loss: 0.554890513420105 batch: 706/840\n",
      "Batch loss: 0.6666091680526733 batch: 707/840\n",
      "Batch loss: 0.6604236364364624 batch: 708/840\n",
      "Batch loss: 0.6376886367797852 batch: 709/840\n",
      "Batch loss: 0.7377636432647705 batch: 710/840\n",
      "Batch loss: 0.443945974111557 batch: 711/840\n",
      "Batch loss: 0.719869077205658 batch: 712/840\n",
      "Batch loss: 0.5041095018386841 batch: 713/840\n",
      "Batch loss: 0.6638249158859253 batch: 714/840\n",
      "Batch loss: 0.6516139507293701 batch: 715/840\n",
      "Batch loss: 0.6666204333305359 batch: 716/840\n",
      "Batch loss: 0.6982252597808838 batch: 717/840\n",
      "Batch loss: 0.6134570837020874 batch: 718/840\n",
      "Batch loss: 0.4371623694896698 batch: 719/840\n",
      "Batch loss: 0.529442548751831 batch: 720/840\n",
      "Batch loss: 0.7161738872528076 batch: 721/840\n",
      "Batch loss: 0.7838053703308105 batch: 722/840\n",
      "Batch loss: 0.5167933702468872 batch: 723/840\n",
      "Batch loss: 0.6924545764923096 batch: 724/840\n",
      "Batch loss: 0.6809466481208801 batch: 725/840\n",
      "Batch loss: 0.6774118542671204 batch: 726/840\n",
      "Batch loss: 0.7679069638252258 batch: 727/840\n",
      "Batch loss: 0.705568253993988 batch: 728/840\n",
      "Batch loss: 0.6572784185409546 batch: 729/840\n",
      "Batch loss: 0.6313426494598389 batch: 730/840\n",
      "Batch loss: 0.532088041305542 batch: 731/840\n",
      "Batch loss: 0.785306990146637 batch: 732/840\n",
      "Batch loss: 0.5797634720802307 batch: 733/840\n",
      "Batch loss: 0.5542693734169006 batch: 734/840\n",
      "Batch loss: 0.6646729111671448 batch: 735/840\n",
      "Batch loss: 0.6157766580581665 batch: 736/840\n",
      "Batch loss: 0.692656397819519 batch: 737/840\n",
      "Batch loss: 0.5788323879241943 batch: 738/840\n",
      "Batch loss: 0.7294967770576477 batch: 739/840\n",
      "Batch loss: 0.7701783776283264 batch: 740/840\n",
      "Batch loss: 0.4750896394252777 batch: 741/840\n",
      "Batch loss: 0.542731523513794 batch: 742/840\n",
      "Batch loss: 0.4493386447429657 batch: 743/840\n",
      "Batch loss: 0.4894118905067444 batch: 744/840\n",
      "Batch loss: 0.548532247543335 batch: 745/840\n",
      "Batch loss: 0.6210891604423523 batch: 746/840\n",
      "Batch loss: 0.667487621307373 batch: 747/840\n",
      "Batch loss: 0.6108214855194092 batch: 748/840\n",
      "Batch loss: 0.4532324969768524 batch: 749/840\n",
      "Batch loss: 0.5488715171813965 batch: 750/840\n",
      "Batch loss: 0.6559700965881348 batch: 751/840\n",
      "Batch loss: 0.8128030896186829 batch: 752/840\n",
      "Batch loss: 0.5424684286117554 batch: 753/840\n",
      "Batch loss: 0.6893182992935181 batch: 754/840\n",
      "Batch loss: 0.6081587076187134 batch: 755/840\n",
      "Batch loss: 0.533118486404419 batch: 756/840\n",
      "Batch loss: 0.7526369690895081 batch: 757/840\n",
      "Batch loss: 0.6217887997627258 batch: 758/840\n",
      "Batch loss: 0.5069513916969299 batch: 759/840\n",
      "Batch loss: 0.5940898656845093 batch: 760/840\n",
      "Batch loss: 0.5690922737121582 batch: 761/840\n",
      "Batch loss: 0.8119156360626221 batch: 762/840\n",
      "Batch loss: 0.4909963607788086 batch: 763/840\n",
      "Batch loss: 0.6195173859596252 batch: 764/840\n",
      "Batch loss: 0.4668423533439636 batch: 765/840\n",
      "Batch loss: 0.5721601247787476 batch: 766/840\n",
      "Batch loss: 0.681535542011261 batch: 767/840\n",
      "Batch loss: 0.7161763906478882 batch: 768/840\n",
      "Batch loss: 0.6298815011978149 batch: 769/840\n",
      "Batch loss: 0.498581200838089 batch: 770/840\n",
      "Batch loss: 0.6339809894561768 batch: 771/840\n",
      "Batch loss: 0.5388587713241577 batch: 772/840\n",
      "Batch loss: 0.5467380285263062 batch: 773/840\n",
      "Batch loss: 0.5055928230285645 batch: 774/840\n",
      "Batch loss: 0.5798878073692322 batch: 775/840\n",
      "Batch loss: 0.6065549254417419 batch: 776/840\n",
      "Batch loss: 0.5407194495201111 batch: 777/840\n",
      "Batch loss: 0.44620567560195923 batch: 778/840\n",
      "Batch loss: 0.6831965446472168 batch: 779/840\n",
      "Batch loss: 0.583029568195343 batch: 780/840\n",
      "Batch loss: 0.6099979281425476 batch: 781/840\n",
      "Batch loss: 0.5408332943916321 batch: 782/840\n",
      "Batch loss: 0.4399915635585785 batch: 783/840\n",
      "Batch loss: 0.6496766805648804 batch: 784/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5507640242576599 batch: 785/840\n",
      "Batch loss: 0.6922746300697327 batch: 786/840\n",
      "Batch loss: 0.5607686042785645 batch: 787/840\n",
      "Batch loss: 0.6133044362068176 batch: 788/840\n",
      "Batch loss: 0.7778906226158142 batch: 789/840\n",
      "Batch loss: 0.6819035410881042 batch: 790/840\n",
      "Batch loss: 0.5766227841377258 batch: 791/840\n",
      "Batch loss: 0.3831547200679779 batch: 792/840\n",
      "Batch loss: 0.6479565501213074 batch: 793/840\n",
      "Batch loss: 0.6930228471755981 batch: 794/840\n",
      "Batch loss: 0.6564703583717346 batch: 795/840\n",
      "Batch loss: 0.7637691497802734 batch: 796/840\n",
      "Batch loss: 0.6773735880851746 batch: 797/840\n",
      "Batch loss: 0.5854198932647705 batch: 798/840\n",
      "Batch loss: 0.5863327980041504 batch: 799/840\n",
      "Batch loss: 0.5609017014503479 batch: 800/840\n",
      "Batch loss: 0.7572056651115417 batch: 801/840\n",
      "Batch loss: 0.5263804197311401 batch: 802/840\n",
      "Batch loss: 0.5134181380271912 batch: 803/840\n",
      "Batch loss: 0.7484831213951111 batch: 804/840\n",
      "Batch loss: 0.5787115693092346 batch: 805/840\n",
      "Batch loss: 0.6113894581794739 batch: 806/840\n",
      "Batch loss: 0.6210883855819702 batch: 807/840\n",
      "Batch loss: 0.7202314138412476 batch: 808/840\n",
      "Batch loss: 0.5570850968360901 batch: 809/840\n",
      "Batch loss: 0.5343998670578003 batch: 810/840\n",
      "Batch loss: 0.5468782186508179 batch: 811/840\n",
      "Batch loss: 0.5960303544998169 batch: 812/840\n",
      "Batch loss: 0.5229883193969727 batch: 813/840\n",
      "Batch loss: 0.6990527510643005 batch: 814/840\n",
      "Batch loss: 0.6368551850318909 batch: 815/840\n",
      "Batch loss: 0.6107497215270996 batch: 816/840\n",
      "Batch loss: 0.6708396673202515 batch: 817/840\n",
      "Batch loss: 0.6992252469062805 batch: 818/840\n",
      "Batch loss: 0.47287991642951965 batch: 819/840\n",
      "Batch loss: 0.6510565876960754 batch: 820/840\n",
      "Batch loss: 0.6185251474380493 batch: 821/840\n",
      "Batch loss: 0.6285527348518372 batch: 822/840\n",
      "Batch loss: 0.6929678320884705 batch: 823/840\n",
      "Batch loss: 0.678557276725769 batch: 824/840\n",
      "Batch loss: 0.7349182963371277 batch: 825/840\n",
      "Batch loss: 0.5646944642066956 batch: 826/840\n",
      "Batch loss: 0.593255341053009 batch: 827/840\n",
      "Batch loss: 0.7133530974388123 batch: 828/840\n",
      "Batch loss: 0.6338998079299927 batch: 829/840\n",
      "Batch loss: 0.6362488269805908 batch: 830/840\n",
      "Batch loss: 0.62517249584198 batch: 831/840\n",
      "Batch loss: 0.6545860767364502 batch: 832/840\n",
      "Batch loss: 0.7156873941421509 batch: 833/840\n",
      "Batch loss: 0.654600977897644 batch: 834/840\n",
      "Batch loss: 0.5639805793762207 batch: 835/840\n",
      "Batch loss: 0.6401152014732361 batch: 836/840\n",
      "Batch loss: 0.5936282873153687 batch: 837/840\n",
      "Batch loss: 0.7049600481987 batch: 838/840\n",
      "Batch loss: 0.5707842707633972 batch: 839/840\n",
      "Batch loss: 0.6732403635978699 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 9/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.834\n",
      "Running epoch 10/15\n",
      "Batch loss: 0.5156034827232361 batch: 1/840\n",
      "Batch loss: 0.9341583251953125 batch: 2/840\n",
      "Batch loss: 0.6470323801040649 batch: 3/840\n",
      "Batch loss: 0.5626633167266846 batch: 4/840\n",
      "Batch loss: 0.6379145979881287 batch: 5/840\n",
      "Batch loss: 0.48413124680519104 batch: 6/840\n",
      "Batch loss: 0.5442413687705994 batch: 7/840\n",
      "Batch loss: 0.7210080027580261 batch: 8/840\n",
      "Batch loss: 0.5164779424667358 batch: 9/840\n",
      "Batch loss: 0.5880047082901001 batch: 10/840\n",
      "Batch loss: 0.5885134339332581 batch: 11/840\n",
      "Batch loss: 0.5477438569068909 batch: 12/840\n",
      "Batch loss: 0.5649979114532471 batch: 13/840\n",
      "Batch loss: 0.6307011842727661 batch: 14/840\n",
      "Batch loss: 0.6549804210662842 batch: 15/840\n",
      "Batch loss: 0.49406468868255615 batch: 16/840\n",
      "Batch loss: 0.4550822377204895 batch: 17/840\n",
      "Batch loss: 0.7457714080810547 batch: 18/840\n",
      "Batch loss: 0.7046574950218201 batch: 19/840\n",
      "Batch loss: 0.7068007588386536 batch: 20/840\n",
      "Batch loss: 0.6761229038238525 batch: 21/840\n",
      "Batch loss: 0.5820375680923462 batch: 22/840\n",
      "Batch loss: 0.6032527685165405 batch: 23/840\n",
      "Batch loss: 0.516638994216919 batch: 24/840\n",
      "Batch loss: 0.5061919689178467 batch: 25/840\n",
      "Batch loss: 0.6978904604911804 batch: 26/840\n",
      "Batch loss: 0.6445889472961426 batch: 27/840\n",
      "Batch loss: 0.6575186252593994 batch: 28/840\n",
      "Batch loss: 0.7240080237388611 batch: 29/840\n",
      "Batch loss: 0.5249836444854736 batch: 30/840\n",
      "Batch loss: 0.5960837006568909 batch: 31/840\n",
      "Batch loss: 0.5918891429901123 batch: 32/840\n",
      "Batch loss: 0.5885050296783447 batch: 33/840\n",
      "Batch loss: 0.5906667709350586 batch: 34/840\n",
      "Batch loss: 0.47565406560897827 batch: 35/840\n",
      "Batch loss: 0.4652889668941498 batch: 36/840\n",
      "Batch loss: 0.6947287917137146 batch: 37/840\n",
      "Batch loss: 0.6632173657417297 batch: 38/840\n",
      "Batch loss: 0.6315122842788696 batch: 39/840\n",
      "Batch loss: 0.5887441039085388 batch: 40/840\n",
      "Batch loss: 0.7262008786201477 batch: 41/840\n",
      "Batch loss: 0.606738269329071 batch: 42/840\n",
      "Batch loss: 0.6513682007789612 batch: 43/840\n",
      "Batch loss: 0.6474194526672363 batch: 44/840\n",
      "Batch loss: 0.6365832090377808 batch: 45/840\n",
      "Batch loss: 0.49882152676582336 batch: 46/840\n",
      "Batch loss: 0.4427679181098938 batch: 47/840\n",
      "Batch loss: 0.7035892605781555 batch: 48/840\n",
      "Batch loss: 0.6302986145019531 batch: 49/840\n",
      "Batch loss: 0.6806619763374329 batch: 50/840\n",
      "Batch loss: 0.6796970367431641 batch: 51/840\n",
      "Batch loss: 0.7780486941337585 batch: 52/840\n",
      "Batch loss: 0.6325130462646484 batch: 53/840\n",
      "Batch loss: 0.6437392234802246 batch: 54/840\n",
      "Batch loss: 0.6931252479553223 batch: 55/840\n",
      "Batch loss: 0.5292301774024963 batch: 56/840\n",
      "Batch loss: 0.5974635481834412 batch: 57/840\n",
      "Batch loss: 0.5221115946769714 batch: 58/840\n",
      "Batch loss: 0.5576427578926086 batch: 59/840\n",
      "Batch loss: 0.5578440427780151 batch: 60/840\n",
      "Batch loss: 0.6524484157562256 batch: 61/840\n",
      "Batch loss: 0.7135108709335327 batch: 62/840\n",
      "Batch loss: 0.6336075067520142 batch: 63/840\n",
      "Batch loss: 0.5895082354545593 batch: 64/840\n",
      "Batch loss: 0.5790234804153442 batch: 65/840\n",
      "Batch loss: 0.5617774128913879 batch: 66/840\n",
      "Batch loss: 0.6852574944496155 batch: 67/840\n",
      "Batch loss: 0.5736647844314575 batch: 68/840\n",
      "Batch loss: 0.6803371906280518 batch: 69/840\n",
      "Batch loss: 0.563851535320282 batch: 70/840\n",
      "Batch loss: 0.794110119342804 batch: 71/840\n",
      "Batch loss: 0.6777949333190918 batch: 72/840\n",
      "Batch loss: 0.6254041790962219 batch: 73/840\n",
      "Batch loss: 0.6126729249954224 batch: 74/840\n",
      "Batch loss: 0.6747308969497681 batch: 75/840\n",
      "Batch loss: 0.44298475980758667 batch: 76/840\n",
      "Batch loss: 0.5798364281654358 batch: 77/840\n",
      "Batch loss: 0.826558530330658 batch: 78/840\n",
      "Batch loss: 0.5963432192802429 batch: 79/840\n",
      "Batch loss: 0.6516745686531067 batch: 80/840\n",
      "Batch loss: 0.557762861251831 batch: 81/840\n",
      "Batch loss: 0.6632461547851562 batch: 82/840\n",
      "Batch loss: 0.5036998987197876 batch: 83/840\n",
      "Batch loss: 0.7683155536651611 batch: 84/840\n",
      "Batch loss: 0.6274816393852234 batch: 85/840\n",
      "Batch loss: 0.9212642908096313 batch: 86/840\n",
      "Batch loss: 0.5485422015190125 batch: 87/840\n",
      "Batch loss: 0.5050503611564636 batch: 88/840\n",
      "Batch loss: 0.49499690532684326 batch: 89/840\n",
      "Batch loss: 0.5202465057373047 batch: 90/840\n",
      "Batch loss: 0.5828481316566467 batch: 91/840\n",
      "Batch loss: 0.6644712090492249 batch: 92/840\n",
      "Batch loss: 0.7447974681854248 batch: 93/840\n",
      "Batch loss: 0.6908990740776062 batch: 94/840\n",
      "Batch loss: 0.6392658352851868 batch: 95/840\n",
      "Batch loss: 0.5595837831497192 batch: 96/840\n",
      "Batch loss: 0.6030725836753845 batch: 97/840\n",
      "Batch loss: 0.7563343048095703 batch: 98/840\n",
      "Batch loss: 0.5927428007125854 batch: 99/840\n",
      "Batch loss: 0.5480600595474243 batch: 100/840\n",
      "Batch loss: 0.5144731998443604 batch: 101/840\n",
      "Batch loss: 0.5252973437309265 batch: 102/840\n",
      "Batch loss: 0.6481150984764099 batch: 103/840\n",
      "Batch loss: 0.5125983953475952 batch: 104/840\n",
      "Batch loss: 0.5305662155151367 batch: 105/840\n",
      "Batch loss: 0.6830368638038635 batch: 106/840\n",
      "Batch loss: 0.5619137287139893 batch: 107/840\n",
      "Batch loss: 0.7414697408676147 batch: 108/840\n",
      "Batch loss: 0.6020026206970215 batch: 109/840\n",
      "Batch loss: 0.5692604184150696 batch: 110/840\n",
      "Batch loss: 0.5318817496299744 batch: 111/840\n",
      "Batch loss: 0.8098549842834473 batch: 112/840\n",
      "Batch loss: 0.6526928544044495 batch: 113/840\n",
      "Batch loss: 0.5297205448150635 batch: 114/840\n",
      "Batch loss: 0.6431067585945129 batch: 115/840\n",
      "Batch loss: 0.5252368450164795 batch: 116/840\n",
      "Batch loss: 0.5854933261871338 batch: 117/840\n",
      "Batch loss: 0.43133145570755005 batch: 118/840\n",
      "Batch loss: 0.7354788184165955 batch: 119/840\n",
      "Batch loss: 0.681439995765686 batch: 120/840\n",
      "Batch loss: 0.7172375321388245 batch: 121/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.85808265209198 batch: 122/840\n",
      "Batch loss: 0.47007134556770325 batch: 123/840\n",
      "Batch loss: 0.6022940874099731 batch: 124/840\n",
      "Batch loss: 0.5806018710136414 batch: 125/840\n",
      "Batch loss: 0.6389834880828857 batch: 126/840\n",
      "Batch loss: 0.6662684082984924 batch: 127/840\n",
      "Batch loss: 0.611802339553833 batch: 128/840\n",
      "Batch loss: 0.7805414795875549 batch: 129/840\n",
      "Batch loss: 0.6538532376289368 batch: 130/840\n",
      "Batch loss: 0.6897634267807007 batch: 131/840\n",
      "Batch loss: 1.1014901399612427 batch: 132/840\n",
      "Batch loss: 0.7001873254776001 batch: 133/840\n",
      "Batch loss: 0.5872253179550171 batch: 134/840\n",
      "Batch loss: 0.5693249106407166 batch: 135/840\n",
      "Batch loss: 0.728101909160614 batch: 136/840\n",
      "Batch loss: 0.5534405708312988 batch: 137/840\n",
      "Batch loss: 0.4803513288497925 batch: 138/840\n",
      "Batch loss: 0.678789496421814 batch: 139/840\n",
      "Batch loss: 0.6421746611595154 batch: 140/840\n",
      "Batch loss: 0.5144826769828796 batch: 141/840\n",
      "Batch loss: 0.5039138197898865 batch: 142/840\n",
      "Batch loss: 0.48080652952194214 batch: 143/840\n",
      "Batch loss: 0.5809071660041809 batch: 144/840\n",
      "Batch loss: 0.6554514169692993 batch: 145/840\n",
      "Batch loss: 0.6502225995063782 batch: 146/840\n",
      "Batch loss: 0.5442054271697998 batch: 147/840\n",
      "Batch loss: 0.7705468535423279 batch: 148/840\n",
      "Batch loss: 0.7098168730735779 batch: 149/840\n",
      "Batch loss: 0.7226535677909851 batch: 150/840\n",
      "Batch loss: 0.4311835765838623 batch: 151/840\n",
      "Batch loss: 0.6196616291999817 batch: 152/840\n",
      "Batch loss: 0.46357443928718567 batch: 153/840\n",
      "Batch loss: 0.6084699034690857 batch: 154/840\n",
      "Batch loss: 0.5426554679870605 batch: 155/840\n",
      "Batch loss: 0.6472299695014954 batch: 156/840\n",
      "Batch loss: 0.6951054930686951 batch: 157/840\n",
      "Batch loss: 0.4946034550666809 batch: 158/840\n",
      "Batch loss: 0.5082846283912659 batch: 159/840\n",
      "Batch loss: 0.5373414754867554 batch: 160/840\n",
      "Batch loss: 0.710702121257782 batch: 161/840\n",
      "Batch loss: 0.6031584739685059 batch: 162/840\n",
      "Batch loss: 0.7258912920951843 batch: 163/840\n",
      "Batch loss: 0.4631927013397217 batch: 164/840\n",
      "Batch loss: 0.6078360080718994 batch: 165/840\n",
      "Batch loss: 0.5554391741752625 batch: 166/840\n",
      "Batch loss: 0.6997144818305969 batch: 167/840\n",
      "Batch loss: 0.6293696761131287 batch: 168/840\n",
      "Batch loss: 0.5949747562408447 batch: 169/840\n",
      "Batch loss: 0.7539219856262207 batch: 170/840\n",
      "Batch loss: 0.6262609958648682 batch: 171/840\n",
      "Batch loss: 0.7953009009361267 batch: 172/840\n",
      "Batch loss: 0.5457433462142944 batch: 173/840\n",
      "Batch loss: 0.5086389183998108 batch: 174/840\n",
      "Batch loss: 0.4754925072193146 batch: 175/840\n",
      "Batch loss: 0.6239048838615417 batch: 176/840\n",
      "Batch loss: 0.650941014289856 batch: 177/840\n",
      "Batch loss: 0.7013711333274841 batch: 178/840\n",
      "Batch loss: 0.7786467671394348 batch: 179/840\n",
      "Batch loss: 0.5436460375785828 batch: 180/840\n",
      "Batch loss: 0.570050060749054 batch: 181/840\n",
      "Batch loss: 0.5581393241882324 batch: 182/840\n",
      "Batch loss: 0.6594743132591248 batch: 183/840\n",
      "Batch loss: 0.5624778270721436 batch: 184/840\n",
      "Batch loss: 0.4970320463180542 batch: 185/840\n",
      "Batch loss: 0.47438767552375793 batch: 186/840\n",
      "Batch loss: 0.7411839962005615 batch: 187/840\n",
      "Batch loss: 0.5577676296234131 batch: 188/840\n",
      "Batch loss: 0.581987202167511 batch: 189/840\n",
      "Batch loss: 0.62544846534729 batch: 190/840\n",
      "Batch loss: 0.8190702795982361 batch: 191/840\n",
      "Batch loss: 0.48560860753059387 batch: 192/840\n",
      "Batch loss: 0.47107642889022827 batch: 193/840\n",
      "Batch loss: 0.465311199426651 batch: 194/840\n",
      "Batch loss: 0.5066946148872375 batch: 195/840\n",
      "Batch loss: 0.7612977623939514 batch: 196/840\n",
      "Batch loss: 0.5883588194847107 batch: 197/840\n",
      "Batch loss: 0.6426523327827454 batch: 198/840\n",
      "Batch loss: 0.5752250552177429 batch: 199/840\n",
      "Batch loss: 0.8933876752853394 batch: 200/840\n",
      "Batch loss: 0.45950251817703247 batch: 201/840\n",
      "Batch loss: 0.5450718402862549 batch: 202/840\n",
      "Batch loss: 0.6489993929862976 batch: 203/840\n",
      "Batch loss: 0.7386883497238159 batch: 204/840\n",
      "Batch loss: 0.7348618507385254 batch: 205/840\n",
      "Batch loss: 0.555081307888031 batch: 206/840\n",
      "Batch loss: 0.5973495841026306 batch: 207/840\n",
      "Batch loss: 0.6048895716667175 batch: 208/840\n",
      "Batch loss: 0.5476621389389038 batch: 209/840\n",
      "Batch loss: 0.5602977871894836 batch: 210/840\n",
      "Batch loss: 0.5392705798149109 batch: 211/840\n",
      "Batch loss: 0.5472573637962341 batch: 212/840\n",
      "Batch loss: 0.7398367524147034 batch: 213/840\n",
      "Batch loss: 0.7710826992988586 batch: 214/840\n",
      "Batch loss: 0.6474626064300537 batch: 215/840\n",
      "Batch loss: 0.6350417137145996 batch: 216/840\n",
      "Batch loss: 0.5832796096801758 batch: 217/840\n",
      "Batch loss: 0.6468190550804138 batch: 218/840\n",
      "Batch loss: 0.6628109216690063 batch: 219/840\n",
      "Batch loss: 0.731863260269165 batch: 220/840\n",
      "Batch loss: 0.6052515506744385 batch: 221/840\n",
      "Batch loss: 0.7152622938156128 batch: 222/840\n",
      "Batch loss: 0.5620042681694031 batch: 223/840\n",
      "Batch loss: 0.731839120388031 batch: 224/840\n",
      "Batch loss: 0.7188791632652283 batch: 225/840\n",
      "Batch loss: 0.7566434741020203 batch: 226/840\n",
      "Batch loss: 0.6433821320533752 batch: 227/840\n",
      "Batch loss: 0.4652290642261505 batch: 228/840\n",
      "Batch loss: 0.5143759250640869 batch: 229/840\n",
      "Batch loss: 0.6549944877624512 batch: 230/840\n",
      "Batch loss: 0.5525201559066772 batch: 231/840\n",
      "Batch loss: 0.49014490842819214 batch: 232/840\n",
      "Batch loss: 0.7852862477302551 batch: 233/840\n",
      "Batch loss: 0.6583282351493835 batch: 234/840\n",
      "Batch loss: 0.66693514585495 batch: 235/840\n",
      "Batch loss: 0.6706840395927429 batch: 236/840\n",
      "Batch loss: 0.5459635257720947 batch: 237/840\n",
      "Batch loss: 0.7702005505561829 batch: 238/840\n",
      "Batch loss: 0.6588619351387024 batch: 239/840\n",
      "Batch loss: 0.5923012495040894 batch: 240/840\n",
      "Batch loss: 0.7253470420837402 batch: 241/840\n",
      "Batch loss: 0.6417287588119507 batch: 242/840\n",
      "Batch loss: 0.6200616955757141 batch: 243/840\n",
      "Batch loss: 0.8066010475158691 batch: 244/840\n",
      "Batch loss: 0.43965157866477966 batch: 245/840\n",
      "Batch loss: 0.6451036930084229 batch: 246/840\n",
      "Batch loss: 0.7025034427642822 batch: 247/840\n",
      "Batch loss: 0.647916316986084 batch: 248/840\n",
      "Batch loss: 0.8018209934234619 batch: 249/840\n",
      "Batch loss: 0.43869999051094055 batch: 250/840\n",
      "Batch loss: 0.6212867498397827 batch: 251/840\n",
      "Batch loss: 0.5892409682273865 batch: 252/840\n",
      "Batch loss: 0.6034302115440369 batch: 253/840\n",
      "Batch loss: 0.6965065598487854 batch: 254/840\n",
      "Batch loss: 0.5048626065254211 batch: 255/840\n",
      "Batch loss: 0.7076438665390015 batch: 256/840\n",
      "Batch loss: 0.600963830947876 batch: 257/840\n",
      "Batch loss: 0.7468297481536865 batch: 258/840\n",
      "Batch loss: 0.5696364045143127 batch: 259/840\n",
      "Batch loss: 0.5102096796035767 batch: 260/840\n",
      "Batch loss: 0.5477589964866638 batch: 261/840\n",
      "Batch loss: 0.4164660573005676 batch: 262/840\n",
      "Batch loss: 0.6571597456932068 batch: 263/840\n",
      "Batch loss: 0.5781073570251465 batch: 264/840\n",
      "Batch loss: 0.6601701974868774 batch: 265/840\n",
      "Batch loss: 0.5084666609764099 batch: 266/840\n",
      "Batch loss: 0.6811909675598145 batch: 267/840\n",
      "Batch loss: 0.5724907517433167 batch: 268/840\n",
      "Batch loss: 0.48473864793777466 batch: 269/840\n",
      "Batch loss: 0.5934624075889587 batch: 270/840\n",
      "Batch loss: 0.4868421256542206 batch: 271/840\n",
      "Batch loss: 0.8137737512588501 batch: 272/840\n",
      "Batch loss: 0.7279167771339417 batch: 273/840\n",
      "Batch loss: 0.7796854376792908 batch: 274/840\n",
      "Batch loss: 0.6824537515640259 batch: 275/840\n",
      "Batch loss: 0.480102002620697 batch: 276/840\n",
      "Batch loss: 0.6255128979682922 batch: 277/840\n",
      "Batch loss: 0.8515970706939697 batch: 278/840\n",
      "Batch loss: 0.7094508409500122 batch: 279/840\n",
      "Batch loss: 0.6763995289802551 batch: 280/840\n",
      "Batch loss: 0.5603367686271667 batch: 281/840\n",
      "Batch loss: 0.5352768301963806 batch: 282/840\n",
      "Batch loss: 0.7089381217956543 batch: 283/840\n",
      "Batch loss: 0.4373922646045685 batch: 284/840\n",
      "Batch loss: 0.500007688999176 batch: 285/840\n",
      "Batch loss: 0.5678704977035522 batch: 286/840\n",
      "Batch loss: 0.4730653464794159 batch: 287/840\n",
      "Batch loss: 0.5723750591278076 batch: 288/840\n",
      "Batch loss: 0.7724435329437256 batch: 289/840\n",
      "Batch loss: 0.8841294646263123 batch: 290/840\n",
      "Batch loss: 0.7487313747406006 batch: 291/840\n",
      "Batch loss: 0.6323630809783936 batch: 292/840\n",
      "Batch loss: 0.7307391166687012 batch: 293/840\n",
      "Batch loss: 0.6075073480606079 batch: 294/840\n",
      "Batch loss: 0.5237523913383484 batch: 295/840\n",
      "Batch loss: 0.6780654191970825 batch: 296/840\n",
      "Batch loss: 0.6344388723373413 batch: 297/840\n",
      "Batch loss: 0.7120099663734436 batch: 298/840\n",
      "Batch loss: 0.5571194291114807 batch: 299/840\n",
      "Batch loss: 0.7308780550956726 batch: 300/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7248691320419312 batch: 301/840\n",
      "Batch loss: 0.660478949546814 batch: 302/840\n",
      "Batch loss: 0.6644818186759949 batch: 303/840\n",
      "Batch loss: 0.4748244881629944 batch: 304/840\n",
      "Batch loss: 0.5441550612449646 batch: 305/840\n",
      "Batch loss: 0.6619676947593689 batch: 306/840\n",
      "Batch loss: 0.6131755709648132 batch: 307/840\n",
      "Batch loss: 0.6947618126869202 batch: 308/840\n",
      "Batch loss: 0.600753128528595 batch: 309/840\n",
      "Batch loss: 0.9204639196395874 batch: 310/840\n",
      "Batch loss: 0.7619420886039734 batch: 311/840\n",
      "Batch loss: 0.6731660962104797 batch: 312/840\n",
      "Batch loss: 0.645439863204956 batch: 313/840\n",
      "Batch loss: 0.5992175340652466 batch: 314/840\n",
      "Batch loss: 0.6717221736907959 batch: 315/840\n",
      "Batch loss: 0.41551512479782104 batch: 316/840\n",
      "Batch loss: 0.6989560127258301 batch: 317/840\n",
      "Batch loss: 0.7524411678314209 batch: 318/840\n",
      "Batch loss: 0.7142200469970703 batch: 319/840\n",
      "Batch loss: 0.5410881042480469 batch: 320/840\n",
      "Batch loss: 0.5324452519416809 batch: 321/840\n",
      "Batch loss: 0.7725279927253723 batch: 322/840\n",
      "Batch loss: 0.745714008808136 batch: 323/840\n",
      "Batch loss: 0.5405919551849365 batch: 324/840\n",
      "Batch loss: 0.42661240696907043 batch: 325/840\n",
      "Batch loss: 0.6172152757644653 batch: 326/840\n",
      "Batch loss: 0.5136899948120117 batch: 327/840\n",
      "Batch loss: 0.8008798956871033 batch: 328/840\n",
      "Batch loss: 0.6574237942695618 batch: 329/840\n",
      "Batch loss: 0.6696288585662842 batch: 330/840\n",
      "Batch loss: 0.6591758131980896 batch: 331/840\n",
      "Batch loss: 0.6891536712646484 batch: 332/840\n",
      "Batch loss: 0.5991501808166504 batch: 333/840\n",
      "Batch loss: 0.6305783987045288 batch: 334/840\n",
      "Batch loss: 0.5726598501205444 batch: 335/840\n",
      "Batch loss: 0.6874281167984009 batch: 336/840\n",
      "Batch loss: 0.8549606800079346 batch: 337/840\n",
      "Batch loss: 0.6549007296562195 batch: 338/840\n",
      "Batch loss: 0.5408294200897217 batch: 339/840\n",
      "Batch loss: 0.7732564806938171 batch: 340/840\n",
      "Batch loss: 0.5445316433906555 batch: 341/840\n",
      "Batch loss: 0.48168304562568665 batch: 342/840\n",
      "Batch loss: 0.9169927835464478 batch: 343/840\n",
      "Batch loss: 0.5897066593170166 batch: 344/840\n",
      "Batch loss: 0.47626614570617676 batch: 345/840\n",
      "Batch loss: 0.6149392127990723 batch: 346/840\n",
      "Batch loss: 0.694369912147522 batch: 347/840\n",
      "Batch loss: 0.6418939232826233 batch: 348/840\n",
      "Batch loss: 0.7429834008216858 batch: 349/840\n",
      "Batch loss: 0.5560045838356018 batch: 350/840\n",
      "Batch loss: 0.6497114300727844 batch: 351/840\n",
      "Batch loss: 0.5518596172332764 batch: 352/840\n",
      "Batch loss: 0.7561669945716858 batch: 353/840\n",
      "Batch loss: 0.6491620540618896 batch: 354/840\n",
      "Batch loss: 0.5802427530288696 batch: 355/840\n",
      "Batch loss: 0.5394189357757568 batch: 356/840\n",
      "Batch loss: 0.5457513332366943 batch: 357/840\n",
      "Batch loss: 0.8576971292495728 batch: 358/840\n",
      "Batch loss: 0.5921418070793152 batch: 359/840\n",
      "Batch loss: 0.7934030294418335 batch: 360/840\n",
      "Batch loss: 0.7579109072685242 batch: 361/840\n",
      "Batch loss: 0.6141137480735779 batch: 362/840\n",
      "Batch loss: 0.5443708300590515 batch: 363/840\n",
      "Batch loss: 0.7237794399261475 batch: 364/840\n",
      "Batch loss: 0.5964557528495789 batch: 365/840\n",
      "Batch loss: 0.6260215640068054 batch: 366/840\n",
      "Batch loss: 0.49365857243537903 batch: 367/840\n",
      "Batch loss: 0.6828106045722961 batch: 368/840\n",
      "Batch loss: 0.6126893758773804 batch: 369/840\n",
      "Batch loss: 0.7683783769607544 batch: 370/840\n",
      "Batch loss: 0.66542649269104 batch: 371/840\n",
      "Batch loss: 0.4618566930294037 batch: 372/840\n",
      "Batch loss: 0.5975565314292908 batch: 373/840\n",
      "Batch loss: 0.6995528936386108 batch: 374/840\n",
      "Batch loss: 0.4640723764896393 batch: 375/840\n",
      "Batch loss: 0.42196109890937805 batch: 376/840\n",
      "Batch loss: 0.6778570413589478 batch: 377/840\n",
      "Batch loss: 0.6461448669433594 batch: 378/840\n",
      "Batch loss: 0.5675933957099915 batch: 379/840\n",
      "Batch loss: 0.8500401377677917 batch: 380/840\n",
      "Batch loss: 1.107835054397583 batch: 381/840\n",
      "Batch loss: 0.6730055212974548 batch: 382/840\n",
      "Batch loss: 0.7037563323974609 batch: 383/840\n",
      "Batch loss: 0.6855656504631042 batch: 384/840\n",
      "Batch loss: 0.7334461212158203 batch: 385/840\n",
      "Batch loss: 0.7022340893745422 batch: 386/840\n",
      "Batch loss: 0.5455726981163025 batch: 387/840\n",
      "Batch loss: 0.6374369263648987 batch: 388/840\n",
      "Batch loss: 0.6057223677635193 batch: 389/840\n",
      "Batch loss: 0.7531105279922485 batch: 390/840\n",
      "Batch loss: 0.6524584293365479 batch: 391/840\n",
      "Batch loss: 0.5584481954574585 batch: 392/840\n",
      "Batch loss: 0.5352826714515686 batch: 393/840\n",
      "Batch loss: 0.7147871255874634 batch: 394/840\n",
      "Batch loss: 0.5341747999191284 batch: 395/840\n",
      "Batch loss: 0.6427792906761169 batch: 396/840\n",
      "Batch loss: 0.6111310720443726 batch: 397/840\n",
      "Batch loss: 0.6519888043403625 batch: 398/840\n",
      "Batch loss: 0.45747125148773193 batch: 399/840\n",
      "Batch loss: 0.5488172173500061 batch: 400/840\n",
      "Batch loss: 0.7603127956390381 batch: 401/840\n",
      "Batch loss: 0.5127843618392944 batch: 402/840\n",
      "Batch loss: 0.598891019821167 batch: 403/840\n",
      "Batch loss: 0.6139239072799683 batch: 404/840\n",
      "Batch loss: 0.5437890887260437 batch: 405/840\n",
      "Batch loss: 0.6584349870681763 batch: 406/840\n",
      "Batch loss: 0.5328506231307983 batch: 407/840\n",
      "Batch loss: 0.7096216082572937 batch: 408/840\n",
      "Batch loss: 0.7576215267181396 batch: 409/840\n",
      "Batch loss: 0.7674565315246582 batch: 410/840\n",
      "Batch loss: 0.5460376739501953 batch: 411/840\n",
      "Batch loss: 0.7310774326324463 batch: 412/840\n",
      "Batch loss: 0.6205249428749084 batch: 413/840\n",
      "Batch loss: 0.6583556532859802 batch: 414/840\n",
      "Batch loss: 0.8577286601066589 batch: 415/840\n",
      "Batch loss: 0.46268534660339355 batch: 416/840\n",
      "Batch loss: 0.7328569293022156 batch: 417/840\n",
      "Batch loss: 0.774833619594574 batch: 418/840\n",
      "Batch loss: 0.7086333632469177 batch: 419/840\n",
      "Batch loss: 0.7012643218040466 batch: 420/840\n",
      "Batch loss: 0.47138717770576477 batch: 421/840\n",
      "Batch loss: 0.5793026685714722 batch: 422/840\n",
      "Batch loss: 0.6418306231498718 batch: 423/840\n",
      "Batch loss: 0.6837193369865417 batch: 424/840\n",
      "Batch loss: 0.7773699164390564 batch: 425/840\n",
      "Batch loss: 0.6499770879745483 batch: 426/840\n",
      "Batch loss: 0.6087520718574524 batch: 427/840\n",
      "Batch loss: 0.723166286945343 batch: 428/840\n",
      "Batch loss: 0.5989924669265747 batch: 429/840\n",
      "Batch loss: 0.8077564835548401 batch: 430/840\n",
      "Batch loss: 0.6737662553787231 batch: 431/840\n",
      "Batch loss: 0.6299004554748535 batch: 432/840\n",
      "Batch loss: 0.4718434810638428 batch: 433/840\n",
      "Batch loss: 0.5994789600372314 batch: 434/840\n",
      "Batch loss: 0.7146170735359192 batch: 435/840\n",
      "Batch loss: 0.7190622091293335 batch: 436/840\n",
      "Batch loss: 0.7028583288192749 batch: 437/840\n",
      "Batch loss: 0.42191755771636963 batch: 438/840\n",
      "Batch loss: 0.6692763566970825 batch: 439/840\n",
      "Batch loss: 0.6835814118385315 batch: 440/840\n",
      "Batch loss: 0.6253061294555664 batch: 441/840\n",
      "Batch loss: 0.7181488275527954 batch: 442/840\n",
      "Batch loss: 0.5524372458457947 batch: 443/840\n",
      "Batch loss: 0.577187716960907 batch: 444/840\n",
      "Batch loss: 0.5305356383323669 batch: 445/840\n",
      "Batch loss: 0.7924652099609375 batch: 446/840\n",
      "Batch loss: 0.8005893230438232 batch: 447/840\n",
      "Batch loss: 0.6786934733390808 batch: 448/840\n",
      "Batch loss: 0.5896832346916199 batch: 449/840\n",
      "Batch loss: 0.6255072355270386 batch: 450/840\n",
      "Batch loss: 0.5718363523483276 batch: 451/840\n",
      "Batch loss: 0.7032983303070068 batch: 452/840\n",
      "Batch loss: 0.6017478704452515 batch: 453/840\n",
      "Batch loss: 0.6018060445785522 batch: 454/840\n",
      "Batch loss: 0.6944901347160339 batch: 455/840\n",
      "Batch loss: 0.6037296652793884 batch: 456/840\n",
      "Batch loss: 0.4834633767604828 batch: 457/840\n",
      "Batch loss: 0.5691044330596924 batch: 458/840\n",
      "Batch loss: 0.5547276139259338 batch: 459/840\n",
      "Batch loss: 0.42298758029937744 batch: 460/840\n",
      "Batch loss: 0.7000934481620789 batch: 461/840\n",
      "Batch loss: 0.616281270980835 batch: 462/840\n",
      "Batch loss: 0.7218050360679626 batch: 463/840\n",
      "Batch loss: 0.5526705384254456 batch: 464/840\n",
      "Batch loss: 0.850565493106842 batch: 465/840\n",
      "Batch loss: 0.6395118236541748 batch: 466/840\n",
      "Batch loss: 0.9431568384170532 batch: 467/840\n",
      "Batch loss: 0.5549964308738708 batch: 468/840\n",
      "Batch loss: 0.5717161893844604 batch: 469/840\n",
      "Batch loss: 0.636761486530304 batch: 470/840\n",
      "Batch loss: 0.6470252275466919 batch: 471/840\n",
      "Batch loss: 0.5970724821090698 batch: 472/840\n",
      "Batch loss: 0.6505590081214905 batch: 473/840\n",
      "Batch loss: 0.6154570579528809 batch: 474/840\n",
      "Batch loss: 0.5800915360450745 batch: 475/840\n",
      "Batch loss: 0.6573874950408936 batch: 476/840\n",
      "Batch loss: 0.5922496318817139 batch: 477/840\n",
      "Batch loss: 0.5509419441223145 batch: 478/840\n",
      "Batch loss: 0.5281808376312256 batch: 479/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6400166153907776 batch: 480/840\n",
      "Batch loss: 0.6102261543273926 batch: 481/840\n",
      "Batch loss: 0.6746479272842407 batch: 482/840\n",
      "Batch loss: 0.854095458984375 batch: 483/840\n",
      "Batch loss: 0.4890179932117462 batch: 484/840\n",
      "Batch loss: 0.6781836748123169 batch: 485/840\n",
      "Batch loss: 0.6319714784622192 batch: 486/840\n",
      "Batch loss: 0.47598525881767273 batch: 487/840\n",
      "Batch loss: 0.6770341396331787 batch: 488/840\n",
      "Batch loss: 0.6584159135818481 batch: 489/840\n",
      "Batch loss: 0.7568680047988892 batch: 490/840\n",
      "Batch loss: 0.3946806788444519 batch: 491/840\n",
      "Batch loss: 0.6279184818267822 batch: 492/840\n",
      "Batch loss: 0.6859356164932251 batch: 493/840\n",
      "Batch loss: 0.34391099214553833 batch: 494/840\n",
      "Batch loss: 0.7006009817123413 batch: 495/840\n",
      "Batch loss: 0.6318884491920471 batch: 496/840\n",
      "Batch loss: 0.8168532848358154 batch: 497/840\n",
      "Batch loss: 0.5404656529426575 batch: 498/840\n",
      "Batch loss: 0.6486827731132507 batch: 499/840\n",
      "Batch loss: 0.5450182557106018 batch: 500/840\n",
      "Batch loss: 0.5592153668403625 batch: 501/840\n",
      "Batch loss: 0.704395592212677 batch: 502/840\n",
      "Batch loss: 0.39704442024230957 batch: 503/840\n",
      "Batch loss: 0.5835890769958496 batch: 504/840\n",
      "Batch loss: 0.6823760867118835 batch: 505/840\n",
      "Batch loss: 0.4935762882232666 batch: 506/840\n",
      "Batch loss: 0.6624924540519714 batch: 507/840\n",
      "Batch loss: 0.47839340567588806 batch: 508/840\n",
      "Batch loss: 0.690491259098053 batch: 509/840\n",
      "Batch loss: 0.6803860664367676 batch: 510/840\n",
      "Batch loss: 0.5494964718818665 batch: 511/840\n",
      "Batch loss: 0.6194460391998291 batch: 512/840\n",
      "Batch loss: 0.5484060645103455 batch: 513/840\n",
      "Batch loss: 0.5938004851341248 batch: 514/840\n",
      "Batch loss: 0.42980873584747314 batch: 515/840\n",
      "Batch loss: 0.6952607035636902 batch: 516/840\n",
      "Batch loss: 0.5565450191497803 batch: 517/840\n",
      "Batch loss: 0.7180718183517456 batch: 518/840\n",
      "Batch loss: 0.7858428955078125 batch: 519/840\n",
      "Batch loss: 0.5875067710876465 batch: 520/840\n",
      "Batch loss: 0.7906810641288757 batch: 521/840\n",
      "Batch loss: 0.7209058403968811 batch: 522/840\n",
      "Batch loss: 0.4643230140209198 batch: 523/840\n",
      "Batch loss: 0.5951334238052368 batch: 524/840\n",
      "Batch loss: 0.6425721049308777 batch: 525/840\n",
      "Batch loss: 0.7842915058135986 batch: 526/840\n",
      "Batch loss: 0.6293084621429443 batch: 527/840\n",
      "Batch loss: 0.5416092872619629 batch: 528/840\n",
      "Batch loss: 0.5831548571586609 batch: 529/840\n",
      "Batch loss: 0.6274606585502625 batch: 530/840\n",
      "Batch loss: 0.5732704401016235 batch: 531/840\n",
      "Batch loss: 0.6270616054534912 batch: 532/840\n",
      "Batch loss: 0.6995614767074585 batch: 533/840\n",
      "Batch loss: 0.6238203048706055 batch: 534/840\n",
      "Batch loss: 0.6965978741645813 batch: 535/840\n",
      "Batch loss: 0.6748632192611694 batch: 536/840\n",
      "Batch loss: 0.6617642045021057 batch: 537/840\n",
      "Batch loss: 0.487862229347229 batch: 538/840\n",
      "Batch loss: 0.5442208647727966 batch: 539/840\n",
      "Batch loss: 0.7587061524391174 batch: 540/840\n",
      "Batch loss: 0.5540359616279602 batch: 541/840\n",
      "Batch loss: 0.5977858304977417 batch: 542/840\n",
      "Batch loss: 0.4832744896411896 batch: 543/840\n",
      "Batch loss: 0.6388744711875916 batch: 544/840\n",
      "Batch loss: 0.6789518594741821 batch: 545/840\n",
      "Batch loss: 0.6129878163337708 batch: 546/840\n",
      "Batch loss: 0.6022384762763977 batch: 547/840\n",
      "Batch loss: 0.4635598659515381 batch: 548/840\n",
      "Batch loss: 0.49367088079452515 batch: 549/840\n",
      "Batch loss: 0.5321460366249084 batch: 550/840\n",
      "Batch loss: 0.669858992099762 batch: 551/840\n",
      "Batch loss: 0.5870617628097534 batch: 552/840\n",
      "Batch loss: 0.7245472073554993 batch: 553/840\n",
      "Batch loss: 0.6582044363021851 batch: 554/840\n",
      "Batch loss: 0.6007446646690369 batch: 555/840\n",
      "Batch loss: 0.6792578101158142 batch: 556/840\n",
      "Batch loss: 0.6296538710594177 batch: 557/840\n",
      "Batch loss: 0.5882282853126526 batch: 558/840\n",
      "Batch loss: 0.6192449927330017 batch: 559/840\n",
      "Batch loss: 0.672254741191864 batch: 560/840\n",
      "Batch loss: 0.5947195887565613 batch: 561/840\n",
      "Batch loss: 0.5249969959259033 batch: 562/840\n",
      "Batch loss: 0.5185790061950684 batch: 563/840\n",
      "Batch loss: 0.578837513923645 batch: 564/840\n",
      "Batch loss: 0.6588268280029297 batch: 565/840\n",
      "Batch loss: 0.7145860195159912 batch: 566/840\n",
      "Batch loss: 0.64548659324646 batch: 567/840\n",
      "Batch loss: 0.5547193288803101 batch: 568/840\n",
      "Batch loss: 0.5698255300521851 batch: 569/840\n",
      "Batch loss: 0.4391598403453827 batch: 570/840\n",
      "Batch loss: 0.6838642954826355 batch: 571/840\n",
      "Batch loss: 0.6220462322235107 batch: 572/840\n",
      "Batch loss: 0.6766452193260193 batch: 573/840\n",
      "Batch loss: 0.7668163180351257 batch: 574/840\n",
      "Batch loss: 0.4624524414539337 batch: 575/840\n",
      "Batch loss: 0.5962066054344177 batch: 576/840\n",
      "Batch loss: 0.5802807807922363 batch: 577/840\n",
      "Batch loss: 0.7130531072616577 batch: 578/840\n",
      "Batch loss: 0.6771485805511475 batch: 579/840\n",
      "Batch loss: 0.8405178785324097 batch: 580/840\n",
      "Batch loss: 0.7453600168228149 batch: 581/840\n",
      "Batch loss: 0.7534856200218201 batch: 582/840\n",
      "Batch loss: 0.5842110514640808 batch: 583/840\n",
      "Batch loss: 0.7460736036300659 batch: 584/840\n",
      "Batch loss: 0.6566017866134644 batch: 585/840\n",
      "Batch loss: 0.6030992865562439 batch: 586/840\n",
      "Batch loss: 0.5768106579780579 batch: 587/840\n",
      "Batch loss: 0.6181769371032715 batch: 588/840\n",
      "Batch loss: 0.6171802878379822 batch: 589/840\n",
      "Batch loss: 0.4962034225463867 batch: 590/840\n",
      "Batch loss: 0.5244624614715576 batch: 591/840\n",
      "Batch loss: 0.5650731325149536 batch: 592/840\n",
      "Batch loss: 0.7383825182914734 batch: 593/840\n",
      "Batch loss: 0.6676669120788574 batch: 594/840\n",
      "Batch loss: 0.37350308895111084 batch: 595/840\n",
      "Batch loss: 0.5296244025230408 batch: 596/840\n",
      "Batch loss: 0.5061792731285095 batch: 597/840\n",
      "Batch loss: 0.4511631727218628 batch: 598/840\n",
      "Batch loss: 0.5059530735015869 batch: 599/840\n",
      "Batch loss: 0.6426483392715454 batch: 600/840\n",
      "Batch loss: 0.8198427557945251 batch: 601/840\n",
      "Batch loss: 0.5631794333457947 batch: 602/840\n",
      "Batch loss: 0.6413595080375671 batch: 603/840\n",
      "Batch loss: 0.7092986106872559 batch: 604/840\n",
      "Batch loss: 0.6953784823417664 batch: 605/840\n",
      "Batch loss: 0.74864661693573 batch: 606/840\n",
      "Batch loss: 0.7407167553901672 batch: 607/840\n",
      "Batch loss: 0.6345645785331726 batch: 608/840\n",
      "Batch loss: 0.5156901478767395 batch: 609/840\n",
      "Batch loss: 0.7406778931617737 batch: 610/840\n",
      "Batch loss: 0.6792089343070984 batch: 611/840\n",
      "Batch loss: 0.7626140713691711 batch: 612/840\n",
      "Batch loss: 0.6776488423347473 batch: 613/840\n",
      "Batch loss: 0.7079627513885498 batch: 614/840\n",
      "Batch loss: 0.5269591212272644 batch: 615/840\n",
      "Batch loss: 0.48758941888809204 batch: 616/840\n",
      "Batch loss: 0.5316817760467529 batch: 617/840\n",
      "Batch loss: 0.7176138162612915 batch: 618/840\n",
      "Batch loss: 0.7788301110267639 batch: 619/840\n",
      "Batch loss: 0.5722135901451111 batch: 620/840\n",
      "Batch loss: 0.6155101656913757 batch: 621/840\n",
      "Batch loss: 0.5978101491928101 batch: 622/840\n",
      "Batch loss: 0.5845263004302979 batch: 623/840\n",
      "Batch loss: 0.6753244996070862 batch: 624/840\n",
      "Batch loss: 0.6955524682998657 batch: 625/840\n",
      "Batch loss: 0.5967716574668884 batch: 626/840\n",
      "Batch loss: 0.5439770221710205 batch: 627/840\n",
      "Batch loss: 0.7463212013244629 batch: 628/840\n",
      "Batch loss: 0.6135264039039612 batch: 629/840\n",
      "Batch loss: 0.5849329829216003 batch: 630/840\n",
      "Batch loss: 0.6378041505813599 batch: 631/840\n",
      "Batch loss: 0.677885890007019 batch: 632/840\n",
      "Batch loss: 0.6129451990127563 batch: 633/840\n",
      "Batch loss: 0.6252392530441284 batch: 634/840\n",
      "Batch loss: 0.6161195635795593 batch: 635/840\n",
      "Batch loss: 0.6214549541473389 batch: 636/840\n",
      "Batch loss: 0.5188069343566895 batch: 637/840\n",
      "Batch loss: 0.6913556456565857 batch: 638/840\n",
      "Batch loss: 0.6484885215759277 batch: 639/840\n",
      "Batch loss: 0.5717871189117432 batch: 640/840\n",
      "Batch loss: 0.8873674869537354 batch: 641/840\n",
      "Batch loss: 0.5375291109085083 batch: 642/840\n",
      "Batch loss: 0.9642254114151001 batch: 643/840\n",
      "Batch loss: 0.5969851016998291 batch: 644/840\n",
      "Batch loss: 0.5115501880645752 batch: 645/840\n",
      "Batch loss: 0.665250837802887 batch: 646/840\n",
      "Batch loss: 0.6960679888725281 batch: 647/840\n",
      "Batch loss: 0.705376148223877 batch: 648/840\n",
      "Batch loss: 0.6688805222511292 batch: 649/840\n",
      "Batch loss: 0.5876144170761108 batch: 650/840\n",
      "Batch loss: 0.6140949726104736 batch: 651/840\n",
      "Batch loss: 0.6747235059738159 batch: 652/840\n",
      "Batch loss: 0.513200044631958 batch: 653/840\n",
      "Batch loss: 0.7995308041572571 batch: 654/840\n",
      "Batch loss: 0.5486242771148682 batch: 655/840\n",
      "Batch loss: 0.6685651540756226 batch: 656/840\n",
      "Batch loss: 0.5690696835517883 batch: 657/840\n",
      "Batch loss: 0.6598943471908569 batch: 658/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6479200124740601 batch: 659/840\n",
      "Batch loss: 0.6304791569709778 batch: 660/840\n",
      "Batch loss: 0.7200886011123657 batch: 661/840\n",
      "Batch loss: 0.6676404476165771 batch: 662/840\n",
      "Batch loss: 0.5243040919303894 batch: 663/840\n",
      "Batch loss: 0.5975248217582703 batch: 664/840\n",
      "Batch loss: 0.6842653155326843 batch: 665/840\n",
      "Batch loss: 0.6129787564277649 batch: 666/840\n",
      "Batch loss: 0.8465477228164673 batch: 667/840\n",
      "Batch loss: 0.6948869824409485 batch: 668/840\n",
      "Batch loss: 0.6032808423042297 batch: 669/840\n",
      "Batch loss: 0.5751833915710449 batch: 670/840\n",
      "Batch loss: 0.6816482543945312 batch: 671/840\n",
      "Batch loss: 0.5310954451560974 batch: 672/840\n",
      "Batch loss: 0.7622016668319702 batch: 673/840\n",
      "Batch loss: 0.7061046361923218 batch: 674/840\n",
      "Batch loss: 0.609058678150177 batch: 675/840\n",
      "Batch loss: 0.5835261344909668 batch: 676/840\n",
      "Batch loss: 0.6031098365783691 batch: 677/840\n",
      "Batch loss: 0.5985111594200134 batch: 678/840\n",
      "Batch loss: 0.805738091468811 batch: 679/840\n",
      "Batch loss: 0.7073810696601868 batch: 680/840\n",
      "Batch loss: 0.8023204207420349 batch: 681/840\n",
      "Batch loss: 0.6111665964126587 batch: 682/840\n",
      "Batch loss: 0.5723575353622437 batch: 683/840\n",
      "Batch loss: 0.8579539656639099 batch: 684/840\n",
      "Batch loss: 0.6646499037742615 batch: 685/840\n",
      "Batch loss: 0.6933334469795227 batch: 686/840\n",
      "Batch loss: 0.6062986254692078 batch: 687/840\n",
      "Batch loss: 0.6051098108291626 batch: 688/840\n",
      "Batch loss: 0.5512608289718628 batch: 689/840\n",
      "Batch loss: 0.6072788238525391 batch: 690/840\n",
      "Batch loss: 0.5559338331222534 batch: 691/840\n",
      "Batch loss: 0.6547913551330566 batch: 692/840\n",
      "Batch loss: 0.6361003518104553 batch: 693/840\n",
      "Batch loss: 0.8223639726638794 batch: 694/840\n",
      "Batch loss: 0.7462331652641296 batch: 695/840\n",
      "Batch loss: 0.5676196217536926 batch: 696/840\n",
      "Batch loss: 0.6716977953910828 batch: 697/840\n",
      "Batch loss: 0.588202714920044 batch: 698/840\n",
      "Batch loss: 0.6962552070617676 batch: 699/840\n",
      "Batch loss: 0.7223047018051147 batch: 700/840\n",
      "Batch loss: 0.7586857080459595 batch: 701/840\n",
      "Batch loss: 0.6042057275772095 batch: 702/840\n",
      "Batch loss: 0.582892119884491 batch: 703/840\n",
      "Batch loss: 0.6388885378837585 batch: 704/840\n",
      "Batch loss: 0.6441075801849365 batch: 705/840\n",
      "Batch loss: 0.5871047973632812 batch: 706/840\n",
      "Batch loss: 0.6269508004188538 batch: 707/840\n",
      "Batch loss: 0.7537602186203003 batch: 708/840\n",
      "Batch loss: 0.6834703087806702 batch: 709/840\n",
      "Batch loss: 0.69140625 batch: 710/840\n",
      "Batch loss: 0.48029834032058716 batch: 711/840\n",
      "Batch loss: 0.8642103672027588 batch: 712/840\n",
      "Batch loss: 0.5471847057342529 batch: 713/840\n",
      "Batch loss: 0.6095269918441772 batch: 714/840\n",
      "Batch loss: 0.7618958950042725 batch: 715/840\n",
      "Batch loss: 0.7453261613845825 batch: 716/840\n",
      "Batch loss: 0.5867282748222351 batch: 717/840\n",
      "Batch loss: 0.5337288975715637 batch: 718/840\n",
      "Batch loss: 0.41003334522247314 batch: 719/840\n",
      "Batch loss: 0.4887089133262634 batch: 720/840\n",
      "Batch loss: 0.651093065738678 batch: 721/840\n",
      "Batch loss: 0.7319099307060242 batch: 722/840\n",
      "Batch loss: 0.502285897731781 batch: 723/840\n",
      "Batch loss: 0.7033671736717224 batch: 724/840\n",
      "Batch loss: 0.5881521105766296 batch: 725/840\n",
      "Batch loss: 0.5609804391860962 batch: 726/840\n",
      "Batch loss: 0.7733530402183533 batch: 727/840\n",
      "Batch loss: 0.7807145714759827 batch: 728/840\n",
      "Batch loss: 0.7482145428657532 batch: 729/840\n",
      "Batch loss: 0.6646482944488525 batch: 730/840\n",
      "Batch loss: 0.49838733673095703 batch: 731/840\n",
      "Batch loss: 0.8587275743484497 batch: 732/840\n",
      "Batch loss: 0.6673755645751953 batch: 733/840\n",
      "Batch loss: 0.5079648494720459 batch: 734/840\n",
      "Batch loss: 0.5969047546386719 batch: 735/840\n",
      "Batch loss: 0.6648111939430237 batch: 736/840\n",
      "Batch loss: 0.7211116552352905 batch: 737/840\n",
      "Batch loss: 0.5363407731056213 batch: 738/840\n",
      "Batch loss: 0.7482039928436279 batch: 739/840\n",
      "Batch loss: 0.7033200860023499 batch: 740/840\n",
      "Batch loss: 0.5247112512588501 batch: 741/840\n",
      "Batch loss: 0.5526149868965149 batch: 742/840\n",
      "Batch loss: 0.43693894147872925 batch: 743/840\n",
      "Batch loss: 0.5529811978340149 batch: 744/840\n",
      "Batch loss: 0.6023853421211243 batch: 745/840\n",
      "Batch loss: 0.6446006894111633 batch: 746/840\n",
      "Batch loss: 0.7156597375869751 batch: 747/840\n",
      "Batch loss: 0.7544658184051514 batch: 748/840\n",
      "Batch loss: 0.5190081000328064 batch: 749/840\n",
      "Batch loss: 0.5431143045425415 batch: 750/840\n",
      "Batch loss: 0.7175548076629639 batch: 751/840\n",
      "Batch loss: 0.9057502150535583 batch: 752/840\n",
      "Batch loss: 0.47085925936698914 batch: 753/840\n",
      "Batch loss: 0.6089333295822144 batch: 754/840\n",
      "Batch loss: 0.6364467144012451 batch: 755/840\n",
      "Batch loss: 0.5906474590301514 batch: 756/840\n",
      "Batch loss: 0.7192997932434082 batch: 757/840\n",
      "Batch loss: 0.5708943009376526 batch: 758/840\n",
      "Batch loss: 0.5286414623260498 batch: 759/840\n",
      "Batch loss: 0.6024566888809204 batch: 760/840\n",
      "Batch loss: 0.5182852745056152 batch: 761/840\n",
      "Batch loss: 0.6861023902893066 batch: 762/840\n",
      "Batch loss: 0.4679015278816223 batch: 763/840\n",
      "Batch loss: 0.5966505408287048 batch: 764/840\n",
      "Batch loss: 0.46661436557769775 batch: 765/840\n",
      "Batch loss: 0.5410227179527283 batch: 766/840\n",
      "Batch loss: 0.7521384358406067 batch: 767/840\n",
      "Batch loss: 0.6488821506500244 batch: 768/840\n",
      "Batch loss: 0.6214582324028015 batch: 769/840\n",
      "Batch loss: 0.5836074352264404 batch: 770/840\n",
      "Batch loss: 0.6027745008468628 batch: 771/840\n",
      "Batch loss: 0.5540893077850342 batch: 772/840\n",
      "Batch loss: 0.4529048204421997 batch: 773/840\n",
      "Batch loss: 0.43530070781707764 batch: 774/840\n",
      "Batch loss: 0.5337888598442078 batch: 775/840\n",
      "Batch loss: 0.5035765767097473 batch: 776/840\n",
      "Batch loss: 0.5120211243629456 batch: 777/840\n",
      "Batch loss: 0.4454631805419922 batch: 778/840\n",
      "Batch loss: 0.7633280158042908 batch: 779/840\n",
      "Batch loss: 0.722367525100708 batch: 780/840\n",
      "Batch loss: 0.6323766112327576 batch: 781/840\n",
      "Batch loss: 0.5973341464996338 batch: 782/840\n",
      "Batch loss: 0.4813854694366455 batch: 783/840\n",
      "Batch loss: 0.7353065609931946 batch: 784/840\n",
      "Batch loss: 0.5492451786994934 batch: 785/840\n",
      "Batch loss: 0.5896636247634888 batch: 786/840\n",
      "Batch loss: 0.5247074365615845 batch: 787/840\n",
      "Batch loss: 0.7103655934333801 batch: 788/840\n",
      "Batch loss: 0.8111789226531982 batch: 789/840\n",
      "Batch loss: 0.6544373035430908 batch: 790/840\n",
      "Batch loss: 0.5989986658096313 batch: 791/840\n",
      "Batch loss: 0.3720971941947937 batch: 792/840\n",
      "Batch loss: 0.5687772035598755 batch: 793/840\n",
      "Batch loss: 0.46948373317718506 batch: 794/840\n",
      "Batch loss: 0.6355822086334229 batch: 795/840\n",
      "Batch loss: 0.7033043503761292 batch: 796/840\n",
      "Batch loss: 0.7200834155082703 batch: 797/840\n",
      "Batch loss: 0.6240745186805725 batch: 798/840\n",
      "Batch loss: 0.664207935333252 batch: 799/840\n",
      "Batch loss: 0.5373173356056213 batch: 800/840\n",
      "Batch loss: 0.697365403175354 batch: 801/840\n",
      "Batch loss: 0.5407512187957764 batch: 802/840\n",
      "Batch loss: 0.481623113155365 batch: 803/840\n",
      "Batch loss: 0.7082231044769287 batch: 804/840\n",
      "Batch loss: 0.5324283838272095 batch: 805/840\n",
      "Batch loss: 0.6712545156478882 batch: 806/840\n",
      "Batch loss: 0.6272537708282471 batch: 807/840\n",
      "Batch loss: 0.6466551423072815 batch: 808/840\n",
      "Batch loss: 0.7423643469810486 batch: 809/840\n",
      "Batch loss: 0.5139543414115906 batch: 810/840\n",
      "Batch loss: 0.5538565516471863 batch: 811/840\n",
      "Batch loss: 0.6091011762619019 batch: 812/840\n",
      "Batch loss: 0.606443464756012 batch: 813/840\n",
      "Batch loss: 0.5883239507675171 batch: 814/840\n",
      "Batch loss: 0.6952037811279297 batch: 815/840\n",
      "Batch loss: 0.6671690940856934 batch: 816/840\n",
      "Batch loss: 0.6767839193344116 batch: 817/840\n",
      "Batch loss: 0.6396218538284302 batch: 818/840\n",
      "Batch loss: 0.5644533634185791 batch: 819/840\n",
      "Batch loss: 0.7171074748039246 batch: 820/840\n",
      "Batch loss: 0.6634662747383118 batch: 821/840\n",
      "Batch loss: 0.674709141254425 batch: 822/840\n",
      "Batch loss: 0.7202149033546448 batch: 823/840\n",
      "Batch loss: 0.6861310005187988 batch: 824/840\n",
      "Batch loss: 0.7832096219062805 batch: 825/840\n",
      "Batch loss: 0.6008729338645935 batch: 826/840\n",
      "Batch loss: 0.5396113395690918 batch: 827/840\n",
      "Batch loss: 0.6639668345451355 batch: 828/840\n",
      "Batch loss: 0.5453647375106812 batch: 829/840\n",
      "Batch loss: 0.6432309746742249 batch: 830/840\n",
      "Batch loss: 0.646078884601593 batch: 831/840\n",
      "Batch loss: 0.649441123008728 batch: 832/840\n",
      "Batch loss: 0.6421871781349182 batch: 833/840\n",
      "Batch loss: 0.6203076839447021 batch: 834/840\n",
      "Batch loss: 0.5567871332168579 batch: 835/840\n",
      "Batch loss: 0.5497341752052307 batch: 836/840\n",
      "Batch loss: 0.6331059336662292 batch: 837/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7984111905097961 batch: 838/840\n",
      "Batch loss: 0.6110010147094727 batch: 839/840\n",
      "Batch loss: 0.6561132073402405 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 10/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.810\n",
      "Running epoch 11/15\n",
      "Batch loss: 0.48212963342666626 batch: 1/840\n",
      "Batch loss: 1.1387251615524292 batch: 2/840\n",
      "Batch loss: 0.5886335968971252 batch: 3/840\n",
      "Batch loss: 0.7219880819320679 batch: 4/840\n",
      "Batch loss: 0.6514614224433899 batch: 5/840\n",
      "Batch loss: 0.41225555539131165 batch: 6/840\n",
      "Batch loss: 0.5937109589576721 batch: 7/840\n",
      "Batch loss: 0.5579603314399719 batch: 8/840\n",
      "Batch loss: 0.5902275443077087 batch: 9/840\n",
      "Batch loss: 0.619544506072998 batch: 10/840\n",
      "Batch loss: 0.5584364533424377 batch: 11/840\n",
      "Batch loss: 0.5585454702377319 batch: 12/840\n",
      "Batch loss: 0.49924957752227783 batch: 13/840\n",
      "Batch loss: 0.6949397325515747 batch: 14/840\n",
      "Batch loss: 0.5625876784324646 batch: 15/840\n",
      "Batch loss: 0.41041359305381775 batch: 16/840\n",
      "Batch loss: 0.5144999027252197 batch: 17/840\n",
      "Batch loss: 0.6517093181610107 batch: 18/840\n",
      "Batch loss: 0.6933761835098267 batch: 19/840\n",
      "Batch loss: 0.8004652261734009 batch: 20/840\n",
      "Batch loss: 0.7746126055717468 batch: 21/840\n",
      "Batch loss: 0.5430502891540527 batch: 22/840\n",
      "Batch loss: 0.6072236895561218 batch: 23/840\n",
      "Batch loss: 0.5008646249771118 batch: 24/840\n",
      "Batch loss: 0.5199167132377625 batch: 25/840\n",
      "Batch loss: 0.7125617265701294 batch: 26/840\n",
      "Batch loss: 0.7252448797225952 batch: 27/840\n",
      "Batch loss: 0.768647313117981 batch: 28/840\n",
      "Batch loss: 0.7551062703132629 batch: 29/840\n",
      "Batch loss: 0.46983805298805237 batch: 30/840\n",
      "Batch loss: 0.5592632293701172 batch: 31/840\n",
      "Batch loss: 0.5872201323509216 batch: 32/840\n",
      "Batch loss: 0.5794694423675537 batch: 33/840\n",
      "Batch loss: 0.5439127683639526 batch: 34/840\n",
      "Batch loss: 0.5506951808929443 batch: 35/840\n",
      "Batch loss: 0.5168095231056213 batch: 36/840\n",
      "Batch loss: 0.7159707546234131 batch: 37/840\n",
      "Batch loss: 0.6366446018218994 batch: 38/840\n",
      "Batch loss: 0.6265219449996948 batch: 39/840\n",
      "Batch loss: 0.5056718587875366 batch: 40/840\n",
      "Batch loss: 0.8025867342948914 batch: 41/840\n",
      "Batch loss: 0.6640019416809082 batch: 42/840\n",
      "Batch loss: 0.6632906198501587 batch: 43/840\n",
      "Batch loss: 0.6911576986312866 batch: 44/840\n",
      "Batch loss: 0.6978386640548706 batch: 45/840\n",
      "Batch loss: 0.5651458501815796 batch: 46/840\n",
      "Batch loss: 0.5505440831184387 batch: 47/840\n",
      "Batch loss: 0.6312680244445801 batch: 48/840\n",
      "Batch loss: 0.5202031135559082 batch: 49/840\n",
      "Batch loss: 0.628359317779541 batch: 50/840\n",
      "Batch loss: 0.714043378829956 batch: 51/840\n",
      "Batch loss: 0.7080774903297424 batch: 52/840\n",
      "Batch loss: 0.532762885093689 batch: 53/840\n",
      "Batch loss: 0.5417526960372925 batch: 54/840\n",
      "Batch loss: 0.589073121547699 batch: 55/840\n",
      "Batch loss: 0.5343450307846069 batch: 56/840\n",
      "Batch loss: 0.664099931716919 batch: 57/840\n",
      "Batch loss: 0.6139915585517883 batch: 58/840\n",
      "Batch loss: 0.5390165448188782 batch: 59/840\n",
      "Batch loss: 0.49988409876823425 batch: 60/840\n",
      "Batch loss: 0.7869651317596436 batch: 61/840\n",
      "Batch loss: 0.6569085121154785 batch: 62/840\n",
      "Batch loss: 0.6224052309989929 batch: 63/840\n",
      "Batch loss: 0.5836744904518127 batch: 64/840\n",
      "Batch loss: 0.638283371925354 batch: 65/840\n",
      "Batch loss: 0.6478522419929504 batch: 66/840\n",
      "Batch loss: 0.6953718662261963 batch: 67/840\n",
      "Batch loss: 0.5985553860664368 batch: 68/840\n",
      "Batch loss: 0.7054363489151001 batch: 69/840\n",
      "Batch loss: 0.6493691205978394 batch: 70/840\n",
      "Batch loss: 0.7169927954673767 batch: 71/840\n",
      "Batch loss: 0.752501368522644 batch: 72/840\n",
      "Batch loss: 0.6823654770851135 batch: 73/840\n",
      "Batch loss: 0.6028344035148621 batch: 74/840\n",
      "Batch loss: 0.6890268921852112 batch: 75/840\n",
      "Batch loss: 0.4407024085521698 batch: 76/840\n",
      "Batch loss: 0.5686256289482117 batch: 77/840\n",
      "Batch loss: 0.8172934651374817 batch: 78/840\n",
      "Batch loss: 0.5390734672546387 batch: 79/840\n",
      "Batch loss: 0.6021066904067993 batch: 80/840\n",
      "Batch loss: 0.5197808146476746 batch: 81/840\n",
      "Batch loss: 0.6171934008598328 batch: 82/840\n",
      "Batch loss: 0.48892417550086975 batch: 83/840\n",
      "Batch loss: 0.8130548000335693 batch: 84/840\n",
      "Batch loss: 0.6640989184379578 batch: 85/840\n",
      "Batch loss: 0.9094142317771912 batch: 86/840\n",
      "Batch loss: 0.5097246766090393 batch: 87/840\n",
      "Batch loss: 0.5130137801170349 batch: 88/840\n",
      "Batch loss: 0.42213454842567444 batch: 89/840\n",
      "Batch loss: 0.5045348405838013 batch: 90/840\n",
      "Batch loss: 0.6188385486602783 batch: 91/840\n",
      "Batch loss: 0.6417184472084045 batch: 92/840\n",
      "Batch loss: 0.6172778606414795 batch: 93/840\n",
      "Batch loss: 0.55791175365448 batch: 94/840\n",
      "Batch loss: 0.5441178679466248 batch: 95/840\n",
      "Batch loss: 0.6896888613700867 batch: 96/840\n",
      "Batch loss: 0.6596980094909668 batch: 97/840\n",
      "Batch loss: 0.8840702176094055 batch: 98/840\n",
      "Batch loss: 0.514919102191925 batch: 99/840\n",
      "Batch loss: 0.5906257629394531 batch: 100/840\n",
      "Batch loss: 0.5673127770423889 batch: 101/840\n",
      "Batch loss: 0.5282472372055054 batch: 102/840\n",
      "Batch loss: 0.6924615502357483 batch: 103/840\n",
      "Batch loss: 0.511655867099762 batch: 104/840\n",
      "Batch loss: 0.6105637550354004 batch: 105/840\n",
      "Batch loss: 0.7224284410476685 batch: 106/840\n",
      "Batch loss: 0.5322980284690857 batch: 107/840\n",
      "Batch loss: 0.7810952067375183 batch: 108/840\n",
      "Batch loss: 0.7356829643249512 batch: 109/840\n",
      "Batch loss: 0.5319753289222717 batch: 110/840\n",
      "Batch loss: 0.5581657886505127 batch: 111/840\n",
      "Batch loss: 0.6597445011138916 batch: 112/840\n",
      "Batch loss: 0.6975297331809998 batch: 113/840\n",
      "Batch loss: 0.5446218252182007 batch: 114/840\n",
      "Batch loss: 0.583782970905304 batch: 115/840\n",
      "Batch loss: 0.5712478160858154 batch: 116/840\n",
      "Batch loss: 0.6456698775291443 batch: 117/840\n",
      "Batch loss: 0.44062885642051697 batch: 118/840\n",
      "Batch loss: 0.6945160031318665 batch: 119/840\n",
      "Batch loss: 0.49108776450157166 batch: 120/840\n",
      "Batch loss: 0.6688363552093506 batch: 121/840\n",
      "Batch loss: 0.845913827419281 batch: 122/840\n",
      "Batch loss: 0.5275859236717224 batch: 123/840\n",
      "Batch loss: 0.6066744923591614 batch: 124/840\n",
      "Batch loss: 0.5926982760429382 batch: 125/840\n",
      "Batch loss: 0.6152421236038208 batch: 126/840\n",
      "Batch loss: 0.7042284607887268 batch: 127/840\n",
      "Batch loss: 0.6920726299285889 batch: 128/840\n",
      "Batch loss: 0.6570947766304016 batch: 129/840\n",
      "Batch loss: 0.597707986831665 batch: 130/840\n",
      "Batch loss: 0.7142142653465271 batch: 131/840\n",
      "Batch loss: 0.958871066570282 batch: 132/840\n",
      "Batch loss: 0.6194848418235779 batch: 133/840\n",
      "Batch loss: 0.5471434593200684 batch: 134/840\n",
      "Batch loss: 0.4943316578865051 batch: 135/840\n",
      "Batch loss: 0.6974179744720459 batch: 136/840\n",
      "Batch loss: 0.6186684966087341 batch: 137/840\n",
      "Batch loss: 0.46287691593170166 batch: 138/840\n",
      "Batch loss: 0.5030199885368347 batch: 139/840\n",
      "Batch loss: 0.5758378505706787 batch: 140/840\n",
      "Batch loss: 0.4937826097011566 batch: 141/840\n",
      "Batch loss: 0.6110595464706421 batch: 142/840\n",
      "Batch loss: 0.5886669754981995 batch: 143/840\n",
      "Batch loss: 0.5649693608283997 batch: 144/840\n",
      "Batch loss: 0.7084394097328186 batch: 145/840\n",
      "Batch loss: 0.6526944637298584 batch: 146/840\n",
      "Batch loss: 0.4879588782787323 batch: 147/840\n",
      "Batch loss: 0.7186241745948792 batch: 148/840\n",
      "Batch loss: 0.7143006324768066 batch: 149/840\n",
      "Batch loss: 0.508246660232544 batch: 150/840\n",
      "Batch loss: 0.6044313311576843 batch: 151/840\n",
      "Batch loss: 0.6391510963439941 batch: 152/840\n",
      "Batch loss: 0.48625388741493225 batch: 153/840\n",
      "Batch loss: 0.7941035628318787 batch: 154/840\n",
      "Batch loss: 0.5025700330734253 batch: 155/840\n",
      "Batch loss: 0.5183013677597046 batch: 156/840\n",
      "Batch loss: 0.6225952506065369 batch: 157/840\n",
      "Batch loss: 0.5736949443817139 batch: 158/840\n",
      "Batch loss: 0.5302385687828064 batch: 159/840\n",
      "Batch loss: 0.5131946206092834 batch: 160/840\n",
      "Batch loss: 0.6595540642738342 batch: 161/840\n",
      "Batch loss: 0.6147220134735107 batch: 162/840\n",
      "Batch loss: 0.7631446123123169 batch: 163/840\n",
      "Batch loss: 0.5486522912979126 batch: 164/840\n",
      "Batch loss: 0.6516784429550171 batch: 165/840\n",
      "Batch loss: 0.5039762854576111 batch: 166/840\n",
      "Batch loss: 0.6303538680076599 batch: 167/840\n",
      "Batch loss: 0.5760409235954285 batch: 168/840\n",
      "Batch loss: 0.48307299613952637 batch: 169/840\n",
      "Batch loss: 0.7797552347183228 batch: 170/840\n",
      "Batch loss: 0.6339987516403198 batch: 171/840\n",
      "Batch loss: 0.669222891330719 batch: 172/840\n",
      "Batch loss: 0.6361620426177979 batch: 173/840\n",
      "Batch loss: 0.6285378932952881 batch: 174/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5352247357368469 batch: 175/840\n",
      "Batch loss: 0.6610172986984253 batch: 176/840\n",
      "Batch loss: 0.6838523745536804 batch: 177/840\n",
      "Batch loss: 0.6734927296638489 batch: 178/840\n",
      "Batch loss: 0.7160698771476746 batch: 179/840\n",
      "Batch loss: 0.616433322429657 batch: 180/840\n",
      "Batch loss: 0.5423415899276733 batch: 181/840\n",
      "Batch loss: 0.5703869462013245 batch: 182/840\n",
      "Batch loss: 0.6112351417541504 batch: 183/840\n",
      "Batch loss: 0.5383538007736206 batch: 184/840\n",
      "Batch loss: 0.5080475211143494 batch: 185/840\n",
      "Batch loss: 0.5157424807548523 batch: 186/840\n",
      "Batch loss: 0.674416184425354 batch: 187/840\n",
      "Batch loss: 0.5744113922119141 batch: 188/840\n",
      "Batch loss: 0.6316797137260437 batch: 189/840\n",
      "Batch loss: 0.7286738753318787 batch: 190/840\n",
      "Batch loss: 0.7214086055755615 batch: 191/840\n",
      "Batch loss: 0.4670702815055847 batch: 192/840\n",
      "Batch loss: 0.47772225737571716 batch: 193/840\n",
      "Batch loss: 0.3689665198326111 batch: 194/840\n",
      "Batch loss: 0.48119503259658813 batch: 195/840\n",
      "Batch loss: 0.7525362372398376 batch: 196/840\n",
      "Batch loss: 0.636305034160614 batch: 197/840\n",
      "Batch loss: 0.4908089339733124 batch: 198/840\n",
      "Batch loss: 0.5107195973396301 batch: 199/840\n",
      "Batch loss: 0.8036348819732666 batch: 200/840\n",
      "Batch loss: 0.5757729411125183 batch: 201/840\n",
      "Batch loss: 0.6402754783630371 batch: 202/840\n",
      "Batch loss: 0.6475813984870911 batch: 203/840\n",
      "Batch loss: 0.6731213927268982 batch: 204/840\n",
      "Batch loss: 0.6925098896026611 batch: 205/840\n",
      "Batch loss: 0.6133071184158325 batch: 206/840\n",
      "Batch loss: 0.6297067999839783 batch: 207/840\n",
      "Batch loss: 0.7528130412101746 batch: 208/840\n",
      "Batch loss: 0.545138418674469 batch: 209/840\n",
      "Batch loss: 0.5094629526138306 batch: 210/840\n",
      "Batch loss: 0.46279990673065186 batch: 211/840\n",
      "Batch loss: 0.5234050154685974 batch: 212/840\n",
      "Batch loss: 0.7487180233001709 batch: 213/840\n",
      "Batch loss: 0.801124095916748 batch: 214/840\n",
      "Batch loss: 0.595729649066925 batch: 215/840\n",
      "Batch loss: 0.6494072079658508 batch: 216/840\n",
      "Batch loss: 0.6361168026924133 batch: 217/840\n",
      "Batch loss: 0.6502296924591064 batch: 218/840\n",
      "Batch loss: 0.6518279314041138 batch: 219/840\n",
      "Batch loss: 0.8015499711036682 batch: 220/840\n",
      "Batch loss: 0.5896168351173401 batch: 221/840\n",
      "Batch loss: 0.618461549282074 batch: 222/840\n",
      "Batch loss: 0.5730424523353577 batch: 223/840\n",
      "Batch loss: 0.658729612827301 batch: 224/840\n",
      "Batch loss: 0.6740398406982422 batch: 225/840\n",
      "Batch loss: 0.6829239130020142 batch: 226/840\n",
      "Batch loss: 0.8046543002128601 batch: 227/840\n",
      "Batch loss: 0.5831114053726196 batch: 228/840\n",
      "Batch loss: 0.46200647950172424 batch: 229/840\n",
      "Batch loss: 0.5874596238136292 batch: 230/840\n",
      "Batch loss: 0.5458914637565613 batch: 231/840\n",
      "Batch loss: 0.5522717237472534 batch: 232/840\n",
      "Batch loss: 0.7099617719650269 batch: 233/840\n",
      "Batch loss: 0.5777141451835632 batch: 234/840\n",
      "Batch loss: 0.5513278841972351 batch: 235/840\n",
      "Batch loss: 0.6319887042045593 batch: 236/840\n",
      "Batch loss: 0.6185004711151123 batch: 237/840\n",
      "Batch loss: 0.6954331994056702 batch: 238/840\n",
      "Batch loss: 0.6435100436210632 batch: 239/840\n",
      "Batch loss: 0.6522231101989746 batch: 240/840\n",
      "Batch loss: 0.6164042353630066 batch: 241/840\n",
      "Batch loss: 0.5655720829963684 batch: 242/840\n",
      "Batch loss: 0.597773015499115 batch: 243/840\n",
      "Batch loss: 0.6709017157554626 batch: 244/840\n",
      "Batch loss: 0.5329881906509399 batch: 245/840\n",
      "Batch loss: 0.615744411945343 batch: 246/840\n",
      "Batch loss: 0.7233905792236328 batch: 247/840\n",
      "Batch loss: 0.7581228613853455 batch: 248/840\n",
      "Batch loss: 0.9620964527130127 batch: 249/840\n",
      "Batch loss: 0.5000712871551514 batch: 250/840\n",
      "Batch loss: 0.5524039268493652 batch: 251/840\n",
      "Batch loss: 0.5554615259170532 batch: 252/840\n",
      "Batch loss: 0.7138356566429138 batch: 253/840\n",
      "Batch loss: 0.6505328416824341 batch: 254/840\n",
      "Batch loss: 0.6168713569641113 batch: 255/840\n",
      "Batch loss: 0.7361772060394287 batch: 256/840\n",
      "Batch loss: 0.5962104797363281 batch: 257/840\n",
      "Batch loss: 0.6556529402732849 batch: 258/840\n",
      "Batch loss: 0.5002491474151611 batch: 259/840\n",
      "Batch loss: 0.523862898349762 batch: 260/840\n",
      "Batch loss: 0.6518430113792419 batch: 261/840\n",
      "Batch loss: 0.4321621358394623 batch: 262/840\n",
      "Batch loss: 0.5801606178283691 batch: 263/840\n",
      "Batch loss: 0.5740020871162415 batch: 264/840\n",
      "Batch loss: 0.6863394975662231 batch: 265/840\n",
      "Batch loss: 0.6168959140777588 batch: 266/840\n",
      "Batch loss: 0.6387074589729309 batch: 267/840\n",
      "Batch loss: 0.5454217195510864 batch: 268/840\n",
      "Batch loss: 0.5153777599334717 batch: 269/840\n",
      "Batch loss: 0.6094896793365479 batch: 270/840\n",
      "Batch loss: 0.5927059650421143 batch: 271/840\n",
      "Batch loss: 0.7046089768409729 batch: 272/840\n",
      "Batch loss: 0.7795941233634949 batch: 273/840\n",
      "Batch loss: 0.6065580248832703 batch: 274/840\n",
      "Batch loss: 0.6836289167404175 batch: 275/840\n",
      "Batch loss: 0.546527087688446 batch: 276/840\n",
      "Batch loss: 0.6325732469558716 batch: 277/840\n",
      "Batch loss: 0.7588146328926086 batch: 278/840\n",
      "Batch loss: 0.730597972869873 batch: 279/840\n",
      "Batch loss: 0.6785079836845398 batch: 280/840\n",
      "Batch loss: 0.48754647374153137 batch: 281/840\n",
      "Batch loss: 0.6216078996658325 batch: 282/840\n",
      "Batch loss: 0.5843985080718994 batch: 283/840\n",
      "Batch loss: 0.45744720101356506 batch: 284/840\n",
      "Batch loss: 0.5098998546600342 batch: 285/840\n",
      "Batch loss: 0.7211723923683167 batch: 286/840\n",
      "Batch loss: 0.42535656690597534 batch: 287/840\n",
      "Batch loss: 0.502098560333252 batch: 288/840\n",
      "Batch loss: 0.7545637488365173 batch: 289/840\n",
      "Batch loss: 0.761573076248169 batch: 290/840\n",
      "Batch loss: 0.7098955512046814 batch: 291/840\n",
      "Batch loss: 0.6332080960273743 batch: 292/840\n",
      "Batch loss: 0.7288223505020142 batch: 293/840\n",
      "Batch loss: 0.6451202630996704 batch: 294/840\n",
      "Batch loss: 0.49561968445777893 batch: 295/840\n",
      "Batch loss: 0.6643081903457642 batch: 296/840\n",
      "Batch loss: 0.7373221516609192 batch: 297/840\n",
      "Batch loss: 0.7489427328109741 batch: 298/840\n",
      "Batch loss: 0.5699107646942139 batch: 299/840\n",
      "Batch loss: 0.8364415764808655 batch: 300/840\n",
      "Batch loss: 0.7523632645606995 batch: 301/840\n",
      "Batch loss: 0.6329967975616455 batch: 302/840\n",
      "Batch loss: 0.8133912682533264 batch: 303/840\n",
      "Batch loss: 0.5524521470069885 batch: 304/840\n",
      "Batch loss: 0.5621199607849121 batch: 305/840\n",
      "Batch loss: 0.6776449680328369 batch: 306/840\n",
      "Batch loss: 0.5810739398002625 batch: 307/840\n",
      "Batch loss: 0.6892416477203369 batch: 308/840\n",
      "Batch loss: 0.6207008957862854 batch: 309/840\n",
      "Batch loss: 0.8362237811088562 batch: 310/840\n",
      "Batch loss: 0.7731156945228577 batch: 311/840\n",
      "Batch loss: 0.6510261297225952 batch: 312/840\n",
      "Batch loss: 0.7915855646133423 batch: 313/840\n",
      "Batch loss: 0.5585525631904602 batch: 314/840\n",
      "Batch loss: 0.7135074138641357 batch: 315/840\n",
      "Batch loss: 0.4858730435371399 batch: 316/840\n",
      "Batch loss: 0.6610887050628662 batch: 317/840\n",
      "Batch loss: 0.5952386260032654 batch: 318/840\n",
      "Batch loss: 0.675484299659729 batch: 319/840\n",
      "Batch loss: 0.501805305480957 batch: 320/840\n",
      "Batch loss: 0.6297560334205627 batch: 321/840\n",
      "Batch loss: 0.6187903881072998 batch: 322/840\n",
      "Batch loss: 0.7072058916091919 batch: 323/840\n",
      "Batch loss: 0.6717208027839661 batch: 324/840\n",
      "Batch loss: 0.5537462830543518 batch: 325/840\n",
      "Batch loss: 0.6470598578453064 batch: 326/840\n",
      "Batch loss: 0.45946136116981506 batch: 327/840\n",
      "Batch loss: 0.6895774602890015 batch: 328/840\n",
      "Batch loss: 0.636417031288147 batch: 329/840\n",
      "Batch loss: 0.6674756407737732 batch: 330/840\n",
      "Batch loss: 0.7723917365074158 batch: 331/840\n",
      "Batch loss: 0.6373410224914551 batch: 332/840\n",
      "Batch loss: 0.6186498999595642 batch: 333/840\n",
      "Batch loss: 0.7227056622505188 batch: 334/840\n",
      "Batch loss: 0.6543221473693848 batch: 335/840\n",
      "Batch loss: 0.7780990600585938 batch: 336/840\n",
      "Batch loss: 0.8249236345291138 batch: 337/840\n",
      "Batch loss: 0.7265445590019226 batch: 338/840\n",
      "Batch loss: 0.5028283596038818 batch: 339/840\n",
      "Batch loss: 0.7465398907661438 batch: 340/840\n",
      "Batch loss: 0.5230203866958618 batch: 341/840\n",
      "Batch loss: 0.5226600766181946 batch: 342/840\n",
      "Batch loss: 0.7689890265464783 batch: 343/840\n",
      "Batch loss: 0.5992405414581299 batch: 344/840\n",
      "Batch loss: 0.4587220251560211 batch: 345/840\n",
      "Batch loss: 0.572675347328186 batch: 346/840\n",
      "Batch loss: 0.5607019662857056 batch: 347/840\n",
      "Batch loss: 0.5998653173446655 batch: 348/840\n",
      "Batch loss: 0.6683011651039124 batch: 349/840\n",
      "Batch loss: 0.5149974226951599 batch: 350/840\n",
      "Batch loss: 0.6952240467071533 batch: 351/840\n",
      "Batch loss: 0.5068896412849426 batch: 352/840\n",
      "Batch loss: 0.7956270575523376 batch: 353/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5774341821670532 batch: 354/840\n",
      "Batch loss: 0.47021159529685974 batch: 355/840\n",
      "Batch loss: 0.5130252242088318 batch: 356/840\n",
      "Batch loss: 0.5381221175193787 batch: 357/840\n",
      "Batch loss: 0.6335855722427368 batch: 358/840\n",
      "Batch loss: 0.5527399182319641 batch: 359/840\n",
      "Batch loss: 0.7753105163574219 batch: 360/840\n",
      "Batch loss: 0.7184547185897827 batch: 361/840\n",
      "Batch loss: 0.6072542071342468 batch: 362/840\n",
      "Batch loss: 0.6265612244606018 batch: 363/840\n",
      "Batch loss: 0.70276939868927 batch: 364/840\n",
      "Batch loss: 0.5392575263977051 batch: 365/840\n",
      "Batch loss: 0.6650264859199524 batch: 366/840\n",
      "Batch loss: 0.46301379799842834 batch: 367/840\n",
      "Batch loss: 0.7149473428726196 batch: 368/840\n",
      "Batch loss: 0.6183743476867676 batch: 369/840\n",
      "Batch loss: 0.7559217214584351 batch: 370/840\n",
      "Batch loss: 0.6615894436836243 batch: 371/840\n",
      "Batch loss: 0.4207928776741028 batch: 372/840\n",
      "Batch loss: 0.6401746869087219 batch: 373/840\n",
      "Batch loss: 0.6749857068061829 batch: 374/840\n",
      "Batch loss: 0.5259241461753845 batch: 375/840\n",
      "Batch loss: 0.5407330989837646 batch: 376/840\n",
      "Batch loss: 0.6887375712394714 batch: 377/840\n",
      "Batch loss: 0.5730250477790833 batch: 378/840\n",
      "Batch loss: 0.5649513602256775 batch: 379/840\n",
      "Batch loss: 0.8885581493377686 batch: 380/840\n",
      "Batch loss: 0.9486663937568665 batch: 381/840\n",
      "Batch loss: 0.6724711656570435 batch: 382/840\n",
      "Batch loss: 0.7295798659324646 batch: 383/840\n",
      "Batch loss: 0.7144272327423096 batch: 384/840\n",
      "Batch loss: 0.5815638303756714 batch: 385/840\n",
      "Batch loss: 0.7526681423187256 batch: 386/840\n",
      "Batch loss: 0.6527717709541321 batch: 387/840\n",
      "Batch loss: 0.7031699419021606 batch: 388/840\n",
      "Batch loss: 0.5343906879425049 batch: 389/840\n",
      "Batch loss: 0.879642128944397 batch: 390/840\n",
      "Batch loss: 0.5888370871543884 batch: 391/840\n",
      "Batch loss: 0.5434045195579529 batch: 392/840\n",
      "Batch loss: 0.5489419102668762 batch: 393/840\n",
      "Batch loss: 0.7589874863624573 batch: 394/840\n",
      "Batch loss: 0.5599560141563416 batch: 395/840\n",
      "Batch loss: 0.6624257564544678 batch: 396/840\n",
      "Batch loss: 0.5388152003288269 batch: 397/840\n",
      "Batch loss: 0.630560576915741 batch: 398/840\n",
      "Batch loss: 0.4898044466972351 batch: 399/840\n",
      "Batch loss: 0.6017852425575256 batch: 400/840\n",
      "Batch loss: 0.6379911303520203 batch: 401/840\n",
      "Batch loss: 0.5613909363746643 batch: 402/840\n",
      "Batch loss: 0.5978772640228271 batch: 403/840\n",
      "Batch loss: 0.5619625449180603 batch: 404/840\n",
      "Batch loss: 0.6102907657623291 batch: 405/840\n",
      "Batch loss: 0.6686891913414001 batch: 406/840\n",
      "Batch loss: 0.5962076783180237 batch: 407/840\n",
      "Batch loss: 0.6778951287269592 batch: 408/840\n",
      "Batch loss: 0.7211300730705261 batch: 409/840\n",
      "Batch loss: 0.6676865220069885 batch: 410/840\n",
      "Batch loss: 0.7164149284362793 batch: 411/840\n",
      "Batch loss: 0.7584356069564819 batch: 412/840\n",
      "Batch loss: 0.6137763857841492 batch: 413/840\n",
      "Batch loss: 0.5258175134658813 batch: 414/840\n",
      "Batch loss: 0.8836101293563843 batch: 415/840\n",
      "Batch loss: 0.41200169920921326 batch: 416/840\n",
      "Batch loss: 0.7118211388587952 batch: 417/840\n",
      "Batch loss: 0.7943006157875061 batch: 418/840\n",
      "Batch loss: 0.6120506525039673 batch: 419/840\n",
      "Batch loss: 0.7288551330566406 batch: 420/840\n",
      "Batch loss: 0.4945109486579895 batch: 421/840\n",
      "Batch loss: 0.5372191071510315 batch: 422/840\n",
      "Batch loss: 0.7223101258277893 batch: 423/840\n",
      "Batch loss: 0.6715006232261658 batch: 424/840\n",
      "Batch loss: 0.6999607086181641 batch: 425/840\n",
      "Batch loss: 0.6254245042800903 batch: 426/840\n",
      "Batch loss: 0.5809639096260071 batch: 427/840\n",
      "Batch loss: 0.7167237997055054 batch: 428/840\n",
      "Batch loss: 0.5096691846847534 batch: 429/840\n",
      "Batch loss: 0.7330420613288879 batch: 430/840\n",
      "Batch loss: 0.5859423279762268 batch: 431/840\n",
      "Batch loss: 0.6000044941902161 batch: 432/840\n",
      "Batch loss: 0.45828714966773987 batch: 433/840\n",
      "Batch loss: 0.5675672888755798 batch: 434/840\n",
      "Batch loss: 0.6948762536048889 batch: 435/840\n",
      "Batch loss: 0.45674529671669006 batch: 436/840\n",
      "Batch loss: 0.6803572177886963 batch: 437/840\n",
      "Batch loss: 0.43782153725624084 batch: 438/840\n",
      "Batch loss: 0.5967720150947571 batch: 439/840\n",
      "Batch loss: 0.7559598684310913 batch: 440/840\n",
      "Batch loss: 0.5883198976516724 batch: 441/840\n",
      "Batch loss: 0.6928516626358032 batch: 442/840\n",
      "Batch loss: 0.6275257468223572 batch: 443/840\n",
      "Batch loss: 0.5762888193130493 batch: 444/840\n",
      "Batch loss: 0.4958713948726654 batch: 445/840\n",
      "Batch loss: 0.7757415771484375 batch: 446/840\n",
      "Batch loss: 0.6524879336357117 batch: 447/840\n",
      "Batch loss: 0.6912010312080383 batch: 448/840\n",
      "Batch loss: 0.6007521152496338 batch: 449/840\n",
      "Batch loss: 0.5712286233901978 batch: 450/840\n",
      "Batch loss: 0.6758580803871155 batch: 451/840\n",
      "Batch loss: 0.6013553738594055 batch: 452/840\n",
      "Batch loss: 0.6625661253929138 batch: 453/840\n",
      "Batch loss: 0.567706823348999 batch: 454/840\n",
      "Batch loss: 0.6908236742019653 batch: 455/840\n",
      "Batch loss: 0.5328333377838135 batch: 456/840\n",
      "Batch loss: 0.5388508439064026 batch: 457/840\n",
      "Batch loss: 0.5909692645072937 batch: 458/840\n",
      "Batch loss: 0.5912476181983948 batch: 459/840\n",
      "Batch loss: 0.47942492365837097 batch: 460/840\n",
      "Batch loss: 0.6727205514907837 batch: 461/840\n",
      "Batch loss: 0.5618679523468018 batch: 462/840\n",
      "Batch loss: 0.8347499370574951 batch: 463/840\n",
      "Batch loss: 0.5160371661186218 batch: 464/840\n",
      "Batch loss: 0.8193652629852295 batch: 465/840\n",
      "Batch loss: 0.7074258923530579 batch: 466/840\n",
      "Batch loss: 0.8404009342193604 batch: 467/840\n",
      "Batch loss: 0.6076866388320923 batch: 468/840\n",
      "Batch loss: 0.48401129245758057 batch: 469/840\n",
      "Batch loss: 0.6727699041366577 batch: 470/840\n",
      "Batch loss: 0.6895259022712708 batch: 471/840\n",
      "Batch loss: 0.6969108581542969 batch: 472/840\n",
      "Batch loss: 0.6868835687637329 batch: 473/840\n",
      "Batch loss: 0.7373639941215515 batch: 474/840\n",
      "Batch loss: 0.5797313451766968 batch: 475/840\n",
      "Batch loss: 0.5955809950828552 batch: 476/840\n",
      "Batch loss: 0.5664308667182922 batch: 477/840\n",
      "Batch loss: 0.5152543187141418 batch: 478/840\n",
      "Batch loss: 0.4400499761104584 batch: 479/840\n",
      "Batch loss: 0.6213248372077942 batch: 480/840\n",
      "Batch loss: 0.5708577036857605 batch: 481/840\n",
      "Batch loss: 0.6484124064445496 batch: 482/840\n",
      "Batch loss: 1.047716498374939 batch: 483/840\n",
      "Batch loss: 0.5013293623924255 batch: 484/840\n",
      "Batch loss: 0.6479350924491882 batch: 485/840\n",
      "Batch loss: 0.6260578632354736 batch: 486/840\n",
      "Batch loss: 0.4591911733150482 batch: 487/840\n",
      "Batch loss: 0.7086358070373535 batch: 488/840\n",
      "Batch loss: 0.6597133874893188 batch: 489/840\n",
      "Batch loss: 0.8059128522872925 batch: 490/840\n",
      "Batch loss: 0.44360870122909546 batch: 491/840\n",
      "Batch loss: 0.712211012840271 batch: 492/840\n",
      "Batch loss: 0.6798829436302185 batch: 493/840\n",
      "Batch loss: 0.4063206911087036 batch: 494/840\n",
      "Batch loss: 0.7776581048965454 batch: 495/840\n",
      "Batch loss: 0.6824923157691956 batch: 496/840\n",
      "Batch loss: 0.755309522151947 batch: 497/840\n",
      "Batch loss: 0.47847992181777954 batch: 498/840\n",
      "Batch loss: 0.6461611390113831 batch: 499/840\n",
      "Batch loss: 0.5759636163711548 batch: 500/840\n",
      "Batch loss: 0.5137537121772766 batch: 501/840\n",
      "Batch loss: 0.7019696235656738 batch: 502/840\n",
      "Batch loss: 0.511606752872467 batch: 503/840\n",
      "Batch loss: 0.6516848802566528 batch: 504/840\n",
      "Batch loss: 0.6327969431877136 batch: 505/840\n",
      "Batch loss: 0.5411402583122253 batch: 506/840\n",
      "Batch loss: 0.746734082698822 batch: 507/840\n",
      "Batch loss: 0.5622652173042297 batch: 508/840\n",
      "Batch loss: 0.6662608981132507 batch: 509/840\n",
      "Batch loss: 0.6929928064346313 batch: 510/840\n",
      "Batch loss: 0.47081294655799866 batch: 511/840\n",
      "Batch loss: 0.6651179790496826 batch: 512/840\n",
      "Batch loss: 0.5659851431846619 batch: 513/840\n",
      "Batch loss: 0.7954630851745605 batch: 514/840\n",
      "Batch loss: 0.48552796244621277 batch: 515/840\n",
      "Batch loss: 0.698771595954895 batch: 516/840\n",
      "Batch loss: 0.6555611491203308 batch: 517/840\n",
      "Batch loss: 0.5961056351661682 batch: 518/840\n",
      "Batch loss: 0.8060529828071594 batch: 519/840\n",
      "Batch loss: 0.6707257628440857 batch: 520/840\n",
      "Batch loss: 0.6692768931388855 batch: 521/840\n",
      "Batch loss: 0.6024744510650635 batch: 522/840\n",
      "Batch loss: 0.45037129521369934 batch: 523/840\n",
      "Batch loss: 0.6553751230239868 batch: 524/840\n",
      "Batch loss: 0.548714816570282 batch: 525/840\n",
      "Batch loss: 0.688291072845459 batch: 526/840\n",
      "Batch loss: 0.5734502673149109 batch: 527/840\n",
      "Batch loss: 0.7324258685112 batch: 528/840\n",
      "Batch loss: 0.47395843267440796 batch: 529/840\n",
      "Batch loss: 0.5930563807487488 batch: 530/840\n",
      "Batch loss: 0.5225529670715332 batch: 531/840\n",
      "Batch loss: 0.538625955581665 batch: 532/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6248078346252441 batch: 533/840\n",
      "Batch loss: 0.6168057918548584 batch: 534/840\n",
      "Batch loss: 0.5891121029853821 batch: 535/840\n",
      "Batch loss: 0.5479736924171448 batch: 536/840\n",
      "Batch loss: 0.5828794836997986 batch: 537/840\n",
      "Batch loss: 0.4764338433742523 batch: 538/840\n",
      "Batch loss: 0.6247014999389648 batch: 539/840\n",
      "Batch loss: 0.7235237956047058 batch: 540/840\n",
      "Batch loss: 0.6950569152832031 batch: 541/840\n",
      "Batch loss: 0.6540377736091614 batch: 542/840\n",
      "Batch loss: 0.4914195239543915 batch: 543/840\n",
      "Batch loss: 0.6903573870658875 batch: 544/840\n",
      "Batch loss: 0.6398258805274963 batch: 545/840\n",
      "Batch loss: 0.5092463493347168 batch: 546/840\n",
      "Batch loss: 0.547671377658844 batch: 547/840\n",
      "Batch loss: 0.5350499749183655 batch: 548/840\n",
      "Batch loss: 0.537991464138031 batch: 549/840\n",
      "Batch loss: 0.5111928582191467 batch: 550/840\n",
      "Batch loss: 0.671984076499939 batch: 551/840\n",
      "Batch loss: 0.5269877314567566 batch: 552/840\n",
      "Batch loss: 0.6273808479309082 batch: 553/840\n",
      "Batch loss: 0.6380963325500488 batch: 554/840\n",
      "Batch loss: 0.6137915849685669 batch: 555/840\n",
      "Batch loss: 0.6666883230209351 batch: 556/840\n",
      "Batch loss: 0.6436110138893127 batch: 557/840\n",
      "Batch loss: 0.667589545249939 batch: 558/840\n",
      "Batch loss: 0.6263022422790527 batch: 559/840\n",
      "Batch loss: 0.7021798491477966 batch: 560/840\n",
      "Batch loss: 0.5633125305175781 batch: 561/840\n",
      "Batch loss: 0.5747899413108826 batch: 562/840\n",
      "Batch loss: 0.49955517053604126 batch: 563/840\n",
      "Batch loss: 0.6699550151824951 batch: 564/840\n",
      "Batch loss: 0.8067774772644043 batch: 565/840\n",
      "Batch loss: 0.7213531732559204 batch: 566/840\n",
      "Batch loss: 0.6883195638656616 batch: 567/840\n",
      "Batch loss: 0.6107850670814514 batch: 568/840\n",
      "Batch loss: 0.6622929573059082 batch: 569/840\n",
      "Batch loss: 0.41993218660354614 batch: 570/840\n",
      "Batch loss: 0.6880878210067749 batch: 571/840\n",
      "Batch loss: 0.7831026315689087 batch: 572/840\n",
      "Batch loss: 0.6478096842765808 batch: 573/840\n",
      "Batch loss: 0.6635504961013794 batch: 574/840\n",
      "Batch loss: 0.4911515712738037 batch: 575/840\n",
      "Batch loss: 0.6217777132987976 batch: 576/840\n",
      "Batch loss: 0.6102506518363953 batch: 577/840\n",
      "Batch loss: 0.7023346424102783 batch: 578/840\n",
      "Batch loss: 0.6872687339782715 batch: 579/840\n",
      "Batch loss: 0.8482749462127686 batch: 580/840\n",
      "Batch loss: 0.5737634897232056 batch: 581/840\n",
      "Batch loss: 0.8089525699615479 batch: 582/840\n",
      "Batch loss: 0.6924209594726562 batch: 583/840\n",
      "Batch loss: 0.7430517077445984 batch: 584/840\n",
      "Batch loss: 0.631074845790863 batch: 585/840\n",
      "Batch loss: 0.6090020537376404 batch: 586/840\n",
      "Batch loss: 0.6364651918411255 batch: 587/840\n",
      "Batch loss: 0.6652446985244751 batch: 588/840\n",
      "Batch loss: 0.6983340382575989 batch: 589/840\n",
      "Batch loss: 0.5452775955200195 batch: 590/840\n",
      "Batch loss: 0.4711754620075226 batch: 591/840\n",
      "Batch loss: 0.5474410057067871 batch: 592/840\n",
      "Batch loss: 0.774263322353363 batch: 593/840\n",
      "Batch loss: 0.5977291464805603 batch: 594/840\n",
      "Batch loss: 0.36913174390792847 batch: 595/840\n",
      "Batch loss: 0.5132715702056885 batch: 596/840\n",
      "Batch loss: 0.6031216979026794 batch: 597/840\n",
      "Batch loss: 0.48223987221717834 batch: 598/840\n",
      "Batch loss: 0.5497651100158691 batch: 599/840\n",
      "Batch loss: 0.6927094459533691 batch: 600/840\n",
      "Batch loss: 0.7203418016433716 batch: 601/840\n",
      "Batch loss: 0.5221142768859863 batch: 602/840\n",
      "Batch loss: 0.6629581451416016 batch: 603/840\n",
      "Batch loss: 0.6240310668945312 batch: 604/840\n",
      "Batch loss: 0.7629945874214172 batch: 605/840\n",
      "Batch loss: 0.8055536150932312 batch: 606/840\n",
      "Batch loss: 0.6775684356689453 batch: 607/840\n",
      "Batch loss: 0.5261516571044922 batch: 608/840\n",
      "Batch loss: 0.5036051273345947 batch: 609/840\n",
      "Batch loss: 0.7142711877822876 batch: 610/840\n",
      "Batch loss: 0.5654492378234863 batch: 611/840\n",
      "Batch loss: 0.7528452277183533 batch: 612/840\n",
      "Batch loss: 0.6181116700172424 batch: 613/840\n",
      "Batch loss: 0.6887692809104919 batch: 614/840\n",
      "Batch loss: 0.5359849333763123 batch: 615/840\n",
      "Batch loss: 0.47809693217277527 batch: 616/840\n",
      "Batch loss: 0.4876502752304077 batch: 617/840\n",
      "Batch loss: 0.6221016049385071 batch: 618/840\n",
      "Batch loss: 0.8104419708251953 batch: 619/840\n",
      "Batch loss: 0.5315373539924622 batch: 620/840\n",
      "Batch loss: 0.6667008996009827 batch: 621/840\n",
      "Batch loss: 0.618478536605835 batch: 622/840\n",
      "Batch loss: 0.6343956589698792 batch: 623/840\n",
      "Batch loss: 0.7983565330505371 batch: 624/840\n",
      "Batch loss: 0.7842166423797607 batch: 625/840\n",
      "Batch loss: 0.6240938305854797 batch: 626/840\n",
      "Batch loss: 0.5860567688941956 batch: 627/840\n",
      "Batch loss: 0.6147133111953735 batch: 628/840\n",
      "Batch loss: 0.5979399681091309 batch: 629/840\n",
      "Batch loss: 0.5852211117744446 batch: 630/840\n",
      "Batch loss: 0.673064649105072 batch: 631/840\n",
      "Batch loss: 0.7035287618637085 batch: 632/840\n",
      "Batch loss: 0.6051787734031677 batch: 633/840\n",
      "Batch loss: 0.6858683228492737 batch: 634/840\n",
      "Batch loss: 0.6187995672225952 batch: 635/840\n",
      "Batch loss: 0.500257670879364 batch: 636/840\n",
      "Batch loss: 0.502443790435791 batch: 637/840\n",
      "Batch loss: 0.6585706472396851 batch: 638/840\n",
      "Batch loss: 0.6501950025558472 batch: 639/840\n",
      "Batch loss: 0.588151216506958 batch: 640/840\n",
      "Batch loss: 0.8008013367652893 batch: 641/840\n",
      "Batch loss: 0.6159433126449585 batch: 642/840\n",
      "Batch loss: 0.9209333658218384 batch: 643/840\n",
      "Batch loss: 0.6037842631340027 batch: 644/840\n",
      "Batch loss: 0.5752376317977905 batch: 645/840\n",
      "Batch loss: 0.5681985020637512 batch: 646/840\n",
      "Batch loss: 0.7153096199035645 batch: 647/840\n",
      "Batch loss: 0.6096168160438538 batch: 648/840\n",
      "Batch loss: 0.6391587257385254 batch: 649/840\n",
      "Batch loss: 0.5394704341888428 batch: 650/840\n",
      "Batch loss: 0.6226799488067627 batch: 651/840\n",
      "Batch loss: 0.6910282373428345 batch: 652/840\n",
      "Batch loss: 0.48749926686286926 batch: 653/840\n",
      "Batch loss: 0.7727555632591248 batch: 654/840\n",
      "Batch loss: 0.5997195839881897 batch: 655/840\n",
      "Batch loss: 0.6907598972320557 batch: 656/840\n",
      "Batch loss: 0.597812831401825 batch: 657/840\n",
      "Batch loss: 0.7830519080162048 batch: 658/840\n",
      "Batch loss: 0.6939258575439453 batch: 659/840\n",
      "Batch loss: 0.565787136554718 batch: 660/840\n",
      "Batch loss: 0.7147308588027954 batch: 661/840\n",
      "Batch loss: 0.652850866317749 batch: 662/840\n",
      "Batch loss: 0.5608698129653931 batch: 663/840\n",
      "Batch loss: 0.6581705212593079 batch: 664/840\n",
      "Batch loss: 0.7011933922767639 batch: 665/840\n",
      "Batch loss: 0.6022598147392273 batch: 666/840\n",
      "Batch loss: 0.7274447679519653 batch: 667/840\n",
      "Batch loss: 0.5887876749038696 batch: 668/840\n",
      "Batch loss: 0.6244484186172485 batch: 669/840\n",
      "Batch loss: 0.6637090444564819 batch: 670/840\n",
      "Batch loss: 0.6816152930259705 batch: 671/840\n",
      "Batch loss: 0.5869839787483215 batch: 672/840\n",
      "Batch loss: 0.7839730978012085 batch: 673/840\n",
      "Batch loss: 0.870576024055481 batch: 674/840\n",
      "Batch loss: 0.5920211672782898 batch: 675/840\n",
      "Batch loss: 0.5549470782279968 batch: 676/840\n",
      "Batch loss: 0.6625103950500488 batch: 677/840\n",
      "Batch loss: 0.6661130785942078 batch: 678/840\n",
      "Batch loss: 0.7649558782577515 batch: 679/840\n",
      "Batch loss: 0.654171347618103 batch: 680/840\n",
      "Batch loss: 0.7202010154724121 batch: 681/840\n",
      "Batch loss: 0.5785489082336426 batch: 682/840\n",
      "Batch loss: 0.4957887530326843 batch: 683/840\n",
      "Batch loss: 0.8070517778396606 batch: 684/840\n",
      "Batch loss: 0.6914578080177307 batch: 685/840\n",
      "Batch loss: 0.6694381833076477 batch: 686/840\n",
      "Batch loss: 0.7426594495773315 batch: 687/840\n",
      "Batch loss: 0.6128717064857483 batch: 688/840\n",
      "Batch loss: 0.5139272212982178 batch: 689/840\n",
      "Batch loss: 0.6817173957824707 batch: 690/840\n",
      "Batch loss: 0.5578391551971436 batch: 691/840\n",
      "Batch loss: 0.6967315077781677 batch: 692/840\n",
      "Batch loss: 0.5924703478813171 batch: 693/840\n",
      "Batch loss: 0.7490501403808594 batch: 694/840\n",
      "Batch loss: 0.7722417712211609 batch: 695/840\n",
      "Batch loss: 0.6602903604507446 batch: 696/840\n",
      "Batch loss: 0.5840091109275818 batch: 697/840\n",
      "Batch loss: 0.5865405201911926 batch: 698/840\n",
      "Batch loss: 0.709688127040863 batch: 699/840\n",
      "Batch loss: 0.6862548589706421 batch: 700/840\n",
      "Batch loss: 0.6672569513320923 batch: 701/840\n",
      "Batch loss: 0.5964654088020325 batch: 702/840\n",
      "Batch loss: 0.5965949892997742 batch: 703/840\n",
      "Batch loss: 0.8083702921867371 batch: 704/840\n",
      "Batch loss: 0.5853164196014404 batch: 705/840\n",
      "Batch loss: 0.5556325316429138 batch: 706/840\n",
      "Batch loss: 0.6601017117500305 batch: 707/840\n",
      "Batch loss: 0.608816385269165 batch: 708/840\n",
      "Batch loss: 0.6286625862121582 batch: 709/840\n",
      "Batch loss: 0.788809061050415 batch: 710/840\n",
      "Batch loss: 0.500723659992218 batch: 711/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6669734716415405 batch: 712/840\n",
      "Batch loss: 0.5472900867462158 batch: 713/840\n",
      "Batch loss: 0.6427754759788513 batch: 714/840\n",
      "Batch loss: 0.6929024457931519 batch: 715/840\n",
      "Batch loss: 0.6409072875976562 batch: 716/840\n",
      "Batch loss: 0.7421362400054932 batch: 717/840\n",
      "Batch loss: 0.558096706867218 batch: 718/840\n",
      "Batch loss: 0.4780447483062744 batch: 719/840\n",
      "Batch loss: 0.5471028089523315 batch: 720/840\n",
      "Batch loss: 0.6687490344047546 batch: 721/840\n",
      "Batch loss: 0.7669888138771057 batch: 722/840\n",
      "Batch loss: 0.5260406732559204 batch: 723/840\n",
      "Batch loss: 0.7502405643463135 batch: 724/840\n",
      "Batch loss: 0.6517887115478516 batch: 725/840\n",
      "Batch loss: 0.6425604820251465 batch: 726/840\n",
      "Batch loss: 0.7572811841964722 batch: 727/840\n",
      "Batch loss: 0.719482958316803 batch: 728/840\n",
      "Batch loss: 0.6163515448570251 batch: 729/840\n",
      "Batch loss: 0.5720213651657104 batch: 730/840\n",
      "Batch loss: 0.5333706736564636 batch: 731/840\n",
      "Batch loss: 0.7822756171226501 batch: 732/840\n",
      "Batch loss: 0.6251577138900757 batch: 733/840\n",
      "Batch loss: 0.5786844491958618 batch: 734/840\n",
      "Batch loss: 0.6100634932518005 batch: 735/840\n",
      "Batch loss: 0.6544278860092163 batch: 736/840\n",
      "Batch loss: 0.6222019791603088 batch: 737/840\n",
      "Batch loss: 0.5060386061668396 batch: 738/840\n",
      "Batch loss: 0.7395063638687134 batch: 739/840\n",
      "Batch loss: 0.6491712927818298 batch: 740/840\n",
      "Batch loss: 0.5938634872436523 batch: 741/840\n",
      "Batch loss: 0.6331207752227783 batch: 742/840\n",
      "Batch loss: 0.4518930912017822 batch: 743/840\n",
      "Batch loss: 0.55628502368927 batch: 744/840\n",
      "Batch loss: 0.5775545835494995 batch: 745/840\n",
      "Batch loss: 0.606928825378418 batch: 746/840\n",
      "Batch loss: 0.6543765068054199 batch: 747/840\n",
      "Batch loss: 0.7099611163139343 batch: 748/840\n",
      "Batch loss: 0.4717333912849426 batch: 749/840\n",
      "Batch loss: 0.4757290184497833 batch: 750/840\n",
      "Batch loss: 0.6659216284751892 batch: 751/840\n",
      "Batch loss: 0.8754583597183228 batch: 752/840\n",
      "Batch loss: 0.5045087337493896 batch: 753/840\n",
      "Batch loss: 0.5994372367858887 batch: 754/840\n",
      "Batch loss: 0.6415192484855652 batch: 755/840\n",
      "Batch loss: 0.5460530519485474 batch: 756/840\n",
      "Batch loss: 0.7078565359115601 batch: 757/840\n",
      "Batch loss: 0.6304905414581299 batch: 758/840\n",
      "Batch loss: 0.560441792011261 batch: 759/840\n",
      "Batch loss: 0.5471179485321045 batch: 760/840\n",
      "Batch loss: 0.5668050050735474 batch: 761/840\n",
      "Batch loss: 0.7130134105682373 batch: 762/840\n",
      "Batch loss: 0.4923572242259979 batch: 763/840\n",
      "Batch loss: 0.6646933555603027 batch: 764/840\n",
      "Batch loss: 0.5287564992904663 batch: 765/840\n",
      "Batch loss: 0.5181254744529724 batch: 766/840\n",
      "Batch loss: 0.70682692527771 batch: 767/840\n",
      "Batch loss: 0.7283497452735901 batch: 768/840\n",
      "Batch loss: 0.6316289305686951 batch: 769/840\n",
      "Batch loss: 0.5187748670578003 batch: 770/840\n",
      "Batch loss: 0.6488980054855347 batch: 771/840\n",
      "Batch loss: 0.5102744102478027 batch: 772/840\n",
      "Batch loss: 0.5196408629417419 batch: 773/840\n",
      "Batch loss: 0.49401623010635376 batch: 774/840\n",
      "Batch loss: 0.538907527923584 batch: 775/840\n",
      "Batch loss: 0.5857123136520386 batch: 776/840\n",
      "Batch loss: 0.48165589570999146 batch: 777/840\n",
      "Batch loss: 0.41252583265304565 batch: 778/840\n",
      "Batch loss: 0.7042356729507446 batch: 779/840\n",
      "Batch loss: 0.6159151792526245 batch: 780/840\n",
      "Batch loss: 0.5744382739067078 batch: 781/840\n",
      "Batch loss: 0.6096013784408569 batch: 782/840\n",
      "Batch loss: 0.49323880672454834 batch: 783/840\n",
      "Batch loss: 0.670413613319397 batch: 784/840\n",
      "Batch loss: 0.4823669195175171 batch: 785/840\n",
      "Batch loss: 0.664002537727356 batch: 786/840\n",
      "Batch loss: 0.4861821234226227 batch: 787/840\n",
      "Batch loss: 0.7840666770935059 batch: 788/840\n",
      "Batch loss: 0.7377727031707764 batch: 789/840\n",
      "Batch loss: 0.6312302350997925 batch: 790/840\n",
      "Batch loss: 0.7410916686058044 batch: 791/840\n",
      "Batch loss: 0.4208015203475952 batch: 792/840\n",
      "Batch loss: 0.6493114233016968 batch: 793/840\n",
      "Batch loss: 0.6289752721786499 batch: 794/840\n",
      "Batch loss: 0.6428574919700623 batch: 795/840\n",
      "Batch loss: 0.7370687127113342 batch: 796/840\n",
      "Batch loss: 0.6269704103469849 batch: 797/840\n",
      "Batch loss: 0.6830365657806396 batch: 798/840\n",
      "Batch loss: 0.6864765286445618 batch: 799/840\n",
      "Batch loss: 0.5177352428436279 batch: 800/840\n",
      "Batch loss: 0.6601610779762268 batch: 801/840\n",
      "Batch loss: 0.49141764640808105 batch: 802/840\n",
      "Batch loss: 0.5361680388450623 batch: 803/840\n",
      "Batch loss: 0.7106014490127563 batch: 804/840\n",
      "Batch loss: 0.556903064250946 batch: 805/840\n",
      "Batch loss: 0.5837802290916443 batch: 806/840\n",
      "Batch loss: 0.6048977375030518 batch: 807/840\n",
      "Batch loss: 0.6254823207855225 batch: 808/840\n",
      "Batch loss: 0.6552113890647888 batch: 809/840\n",
      "Batch loss: 0.5410829782485962 batch: 810/840\n",
      "Batch loss: 0.6040348410606384 batch: 811/840\n",
      "Batch loss: 0.5823773145675659 batch: 812/840\n",
      "Batch loss: 0.5301353335380554 batch: 813/840\n",
      "Batch loss: 0.6505773067474365 batch: 814/840\n",
      "Batch loss: 0.6637790203094482 batch: 815/840\n",
      "Batch loss: 0.6504641175270081 batch: 816/840\n",
      "Batch loss: 0.613717794418335 batch: 817/840\n",
      "Batch loss: 0.7171165943145752 batch: 818/840\n",
      "Batch loss: 0.5029095411300659 batch: 819/840\n",
      "Batch loss: 0.6309099197387695 batch: 820/840\n",
      "Batch loss: 0.7621666193008423 batch: 821/840\n",
      "Batch loss: 0.5560552477836609 batch: 822/840\n",
      "Batch loss: 0.7635759115219116 batch: 823/840\n",
      "Batch loss: 0.7136638164520264 batch: 824/840\n",
      "Batch loss: 0.708266019821167 batch: 825/840\n",
      "Batch loss: 0.583705723285675 batch: 826/840\n",
      "Batch loss: 0.5621187090873718 batch: 827/840\n",
      "Batch loss: 0.6698652505874634 batch: 828/840\n",
      "Batch loss: 0.6183133125305176 batch: 829/840\n",
      "Batch loss: 0.6858757734298706 batch: 830/840\n",
      "Batch loss: 0.4966627061367035 batch: 831/840\n",
      "Batch loss: 0.6701873540878296 batch: 832/840\n",
      "Batch loss: 0.660747230052948 batch: 833/840\n",
      "Batch loss: 0.5679607391357422 batch: 834/840\n",
      "Batch loss: 0.579388439655304 batch: 835/840\n",
      "Batch loss: 0.5304995775222778 batch: 836/840\n",
      "Batch loss: 0.5751087069511414 batch: 837/840\n",
      "Batch loss: 0.7243891954421997 batch: 838/840\n",
      "Batch loss: 0.5373466610908508 batch: 839/840\n",
      "Batch loss: 0.6742927432060242 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 11/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.816\n",
      "Running epoch 12/15\n",
      "Batch loss: 0.5078858733177185 batch: 1/840\n",
      "Batch loss: 1.055004596710205 batch: 2/840\n",
      "Batch loss: 0.6738668084144592 batch: 3/840\n",
      "Batch loss: 0.648699164390564 batch: 4/840\n",
      "Batch loss: 0.6116102337837219 batch: 5/840\n",
      "Batch loss: 0.45341405272483826 batch: 6/840\n",
      "Batch loss: 0.703925371170044 batch: 7/840\n",
      "Batch loss: 0.5588546991348267 batch: 8/840\n",
      "Batch loss: 0.4920863211154938 batch: 9/840\n",
      "Batch loss: 0.547751247882843 batch: 10/840\n",
      "Batch loss: 0.5555068254470825 batch: 11/840\n",
      "Batch loss: 0.5659759044647217 batch: 12/840\n",
      "Batch loss: 0.5622050762176514 batch: 13/840\n",
      "Batch loss: 0.6865777373313904 batch: 14/840\n",
      "Batch loss: 0.5681748986244202 batch: 15/840\n",
      "Batch loss: 0.5454229116439819 batch: 16/840\n",
      "Batch loss: 0.5707806944847107 batch: 17/840\n",
      "Batch loss: 0.7532150745391846 batch: 18/840\n",
      "Batch loss: 0.7337008118629456 batch: 19/840\n",
      "Batch loss: 0.72660231590271 batch: 20/840\n",
      "Batch loss: 0.6559630036354065 batch: 21/840\n",
      "Batch loss: 0.5657322406768799 batch: 22/840\n",
      "Batch loss: 0.4985848367214203 batch: 23/840\n",
      "Batch loss: 0.43953609466552734 batch: 24/840\n",
      "Batch loss: 0.5621222853660583 batch: 25/840\n",
      "Batch loss: 0.7496586441993713 batch: 26/840\n",
      "Batch loss: 0.784464418888092 batch: 27/840\n",
      "Batch loss: 0.5488311648368835 batch: 28/840\n",
      "Batch loss: 0.5666686296463013 batch: 29/840\n",
      "Batch loss: 0.4600774645805359 batch: 30/840\n",
      "Batch loss: 0.5200440287590027 batch: 31/840\n",
      "Batch loss: 0.5211588740348816 batch: 32/840\n",
      "Batch loss: 0.6125946640968323 batch: 33/840\n",
      "Batch loss: 0.5700834393501282 batch: 34/840\n",
      "Batch loss: 0.659364640712738 batch: 35/840\n",
      "Batch loss: 0.4723080098628998 batch: 36/840\n",
      "Batch loss: 0.6621981263160706 batch: 37/840\n",
      "Batch loss: 0.6379610896110535 batch: 38/840\n",
      "Batch loss: 0.65578293800354 batch: 39/840\n",
      "Batch loss: 0.5111795663833618 batch: 40/840\n",
      "Batch loss: 0.7490615248680115 batch: 41/840\n",
      "Batch loss: 0.6656551361083984 batch: 42/840\n",
      "Batch loss: 0.6147017478942871 batch: 43/840\n",
      "Batch loss: 0.6517998576164246 batch: 44/840\n",
      "Batch loss: 0.6698867678642273 batch: 45/840\n",
      "Batch loss: 0.5427138805389404 batch: 46/840\n",
      "Batch loss: 0.577460765838623 batch: 47/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5860925912857056 batch: 48/840\n",
      "Batch loss: 0.579903781414032 batch: 49/840\n",
      "Batch loss: 0.6079249382019043 batch: 50/840\n",
      "Batch loss: 0.676777720451355 batch: 51/840\n",
      "Batch loss: 0.6834861636161804 batch: 52/840\n",
      "Batch loss: 0.5628201365470886 batch: 53/840\n",
      "Batch loss: 0.677206814289093 batch: 54/840\n",
      "Batch loss: 0.6455696821212769 batch: 55/840\n",
      "Batch loss: 0.46384918689727783 batch: 56/840\n",
      "Batch loss: 0.6445857286453247 batch: 57/840\n",
      "Batch loss: 0.5905929803848267 batch: 58/840\n",
      "Batch loss: 0.559836208820343 batch: 59/840\n",
      "Batch loss: 0.6566406488418579 batch: 60/840\n",
      "Batch loss: 0.7475599050521851 batch: 61/840\n",
      "Batch loss: 0.7049005031585693 batch: 62/840\n",
      "Batch loss: 0.5744215250015259 batch: 63/840\n",
      "Batch loss: 0.6995124816894531 batch: 64/840\n",
      "Batch loss: 0.5484561324119568 batch: 65/840\n",
      "Batch loss: 0.5363996028900146 batch: 66/840\n",
      "Batch loss: 0.640399158000946 batch: 67/840\n",
      "Batch loss: 0.6737123727798462 batch: 68/840\n",
      "Batch loss: 0.7087306976318359 batch: 69/840\n",
      "Batch loss: 0.5679227709770203 batch: 70/840\n",
      "Batch loss: 0.7181552052497864 batch: 71/840\n",
      "Batch loss: 0.7963203191757202 batch: 72/840\n",
      "Batch loss: 0.6137147545814514 batch: 73/840\n",
      "Batch loss: 0.5997427701950073 batch: 74/840\n",
      "Batch loss: 0.6529029011726379 batch: 75/840\n",
      "Batch loss: 0.45703747868537903 batch: 76/840\n",
      "Batch loss: 0.5403192043304443 batch: 77/840\n",
      "Batch loss: 0.7848278284072876 batch: 78/840\n",
      "Batch loss: 0.5529543161392212 batch: 79/840\n",
      "Batch loss: 0.6481285095214844 batch: 80/840\n",
      "Batch loss: 0.5373689532279968 batch: 81/840\n",
      "Batch loss: 0.6050200462341309 batch: 82/840\n",
      "Batch loss: 0.5493042469024658 batch: 83/840\n",
      "Batch loss: 0.7558041214942932 batch: 84/840\n",
      "Batch loss: 0.542695939540863 batch: 85/840\n",
      "Batch loss: 0.8697994947433472 batch: 86/840\n",
      "Batch loss: 0.5075879096984863 batch: 87/840\n",
      "Batch loss: 0.47869911789894104 batch: 88/840\n",
      "Batch loss: 0.4630211591720581 batch: 89/840\n",
      "Batch loss: 0.48419320583343506 batch: 90/840\n",
      "Batch loss: 0.5568878650665283 batch: 91/840\n",
      "Batch loss: 0.6204941272735596 batch: 92/840\n",
      "Batch loss: 0.6210339069366455 batch: 93/840\n",
      "Batch loss: 0.6314504146575928 batch: 94/840\n",
      "Batch loss: 0.5654972195625305 batch: 95/840\n",
      "Batch loss: 0.5937183499336243 batch: 96/840\n",
      "Batch loss: 0.6624322533607483 batch: 97/840\n",
      "Batch loss: 0.6393072009086609 batch: 98/840\n",
      "Batch loss: 0.6203866004943848 batch: 99/840\n",
      "Batch loss: 0.6100143194198608 batch: 100/840\n",
      "Batch loss: 0.5303901433944702 batch: 101/840\n",
      "Batch loss: 0.5213860869407654 batch: 102/840\n",
      "Batch loss: 0.7049787640571594 batch: 103/840\n",
      "Batch loss: 0.477385938167572 batch: 104/840\n",
      "Batch loss: 0.5240910053253174 batch: 105/840\n",
      "Batch loss: 0.6466855406761169 batch: 106/840\n",
      "Batch loss: 0.5660803318023682 batch: 107/840\n",
      "Batch loss: 0.722374677658081 batch: 108/840\n",
      "Batch loss: 0.5732280611991882 batch: 109/840\n",
      "Batch loss: 0.577753484249115 batch: 110/840\n",
      "Batch loss: 0.47488486766815186 batch: 111/840\n",
      "Batch loss: 0.6360474228858948 batch: 112/840\n",
      "Batch loss: 0.710164487361908 batch: 113/840\n",
      "Batch loss: 0.5213271379470825 batch: 114/840\n",
      "Batch loss: 0.7024255990982056 batch: 115/840\n",
      "Batch loss: 0.5902411937713623 batch: 116/840\n",
      "Batch loss: 0.6357455253601074 batch: 117/840\n",
      "Batch loss: 0.4141247868537903 batch: 118/840\n",
      "Batch loss: 0.6085447669029236 batch: 119/840\n",
      "Batch loss: 0.5804646015167236 batch: 120/840\n",
      "Batch loss: 0.6477624773979187 batch: 121/840\n",
      "Batch loss: 0.8142582774162292 batch: 122/840\n",
      "Batch loss: 0.4956533908843994 batch: 123/840\n",
      "Batch loss: 0.4757557809352875 batch: 124/840\n",
      "Batch loss: 0.5371763110160828 batch: 125/840\n",
      "Batch loss: 0.6862155795097351 batch: 126/840\n",
      "Batch loss: 0.6956419348716736 batch: 127/840\n",
      "Batch loss: 0.6955790519714355 batch: 128/840\n",
      "Batch loss: 0.7407054305076599 batch: 129/840\n",
      "Batch loss: 0.4779677987098694 batch: 130/840\n",
      "Batch loss: 0.8024429082870483 batch: 131/840\n",
      "Batch loss: 1.0476856231689453 batch: 132/840\n",
      "Batch loss: 0.6529677510261536 batch: 133/840\n",
      "Batch loss: 0.5306893587112427 batch: 134/840\n",
      "Batch loss: 0.5000134110450745 batch: 135/840\n",
      "Batch loss: 0.6596812605857849 batch: 136/840\n",
      "Batch loss: 0.6152721047401428 batch: 137/840\n",
      "Batch loss: 0.4012182354927063 batch: 138/840\n",
      "Batch loss: 0.49002835154533386 batch: 139/840\n",
      "Batch loss: 0.5945455431938171 batch: 140/840\n",
      "Batch loss: 0.4819200038909912 batch: 141/840\n",
      "Batch loss: 0.5916224718093872 batch: 142/840\n",
      "Batch loss: 0.4792841076850891 batch: 143/840\n",
      "Batch loss: 0.5909359455108643 batch: 144/840\n",
      "Batch loss: 0.6838566064834595 batch: 145/840\n",
      "Batch loss: 0.5631658434867859 batch: 146/840\n",
      "Batch loss: 0.5657466053962708 batch: 147/840\n",
      "Batch loss: 0.7708255052566528 batch: 148/840\n",
      "Batch loss: 0.6588873863220215 batch: 149/840\n",
      "Batch loss: 0.6154116988182068 batch: 150/840\n",
      "Batch loss: 0.6483846306800842 batch: 151/840\n",
      "Batch loss: 0.7440237998962402 batch: 152/840\n",
      "Batch loss: 0.5171553492546082 batch: 153/840\n",
      "Batch loss: 0.6980178952217102 batch: 154/840\n",
      "Batch loss: 0.5047342777252197 batch: 155/840\n",
      "Batch loss: 0.5972023010253906 batch: 156/840\n",
      "Batch loss: 0.6109591126441956 batch: 157/840\n",
      "Batch loss: 0.44982239603996277 batch: 158/840\n",
      "Batch loss: 0.5380505919456482 batch: 159/840\n",
      "Batch loss: 0.6154554486274719 batch: 160/840\n",
      "Batch loss: 0.7193471789360046 batch: 161/840\n",
      "Batch loss: 0.6675150990486145 batch: 162/840\n",
      "Batch loss: 0.789152979850769 batch: 163/840\n",
      "Batch loss: 0.5308387875556946 batch: 164/840\n",
      "Batch loss: 0.625759482383728 batch: 165/840\n",
      "Batch loss: 0.47974473237991333 batch: 166/840\n",
      "Batch loss: 0.6617190837860107 batch: 167/840\n",
      "Batch loss: 0.5627362728118896 batch: 168/840\n",
      "Batch loss: 0.46505919098854065 batch: 169/840\n",
      "Batch loss: 0.6913007497787476 batch: 170/840\n",
      "Batch loss: 0.6743324995040894 batch: 171/840\n",
      "Batch loss: 0.6046643257141113 batch: 172/840\n",
      "Batch loss: 0.6380993127822876 batch: 173/840\n",
      "Batch loss: 0.5428820252418518 batch: 174/840\n",
      "Batch loss: 0.5404475927352905 batch: 175/840\n",
      "Batch loss: 0.6964809894561768 batch: 176/840\n",
      "Batch loss: 0.6275684237480164 batch: 177/840\n",
      "Batch loss: 0.6775177121162415 batch: 178/840\n",
      "Batch loss: 0.6391416788101196 batch: 179/840\n",
      "Batch loss: 0.5724939107894897 batch: 180/840\n",
      "Batch loss: 0.5138239860534668 batch: 181/840\n",
      "Batch loss: 0.7013383507728577 batch: 182/840\n",
      "Batch loss: 0.7408989071846008 batch: 183/840\n",
      "Batch loss: 0.5150234699249268 batch: 184/840\n",
      "Batch loss: 0.5819714069366455 batch: 185/840\n",
      "Batch loss: 0.5165325403213501 batch: 186/840\n",
      "Batch loss: 0.7404168844223022 batch: 187/840\n",
      "Batch loss: 0.5348853468894958 batch: 188/840\n",
      "Batch loss: 0.5721961855888367 batch: 189/840\n",
      "Batch loss: 0.6265196800231934 batch: 190/840\n",
      "Batch loss: 0.6770831346511841 batch: 191/840\n",
      "Batch loss: 0.5083160996437073 batch: 192/840\n",
      "Batch loss: 0.5148143172264099 batch: 193/840\n",
      "Batch loss: 0.4453507661819458 batch: 194/840\n",
      "Batch loss: 0.5690106749534607 batch: 195/840\n",
      "Batch loss: 0.7021971344947815 batch: 196/840\n",
      "Batch loss: 0.6481334567070007 batch: 197/840\n",
      "Batch loss: 0.5766311883926392 batch: 198/840\n",
      "Batch loss: 0.597041666507721 batch: 199/840\n",
      "Batch loss: 0.7789863348007202 batch: 200/840\n",
      "Batch loss: 0.5591420531272888 batch: 201/840\n",
      "Batch loss: 0.5618953108787537 batch: 202/840\n",
      "Batch loss: 0.5274844169616699 batch: 203/840\n",
      "Batch loss: 0.6430310606956482 batch: 204/840\n",
      "Batch loss: 0.7338588237762451 batch: 205/840\n",
      "Batch loss: 0.6157914996147156 batch: 206/840\n",
      "Batch loss: 0.5912761092185974 batch: 207/840\n",
      "Batch loss: 0.5601957440376282 batch: 208/840\n",
      "Batch loss: 0.5361407995223999 batch: 209/840\n",
      "Batch loss: 0.580913782119751 batch: 210/840\n",
      "Batch loss: 0.4727729558944702 batch: 211/840\n",
      "Batch loss: 0.6735657453536987 batch: 212/840\n",
      "Batch loss: 0.646751344203949 batch: 213/840\n",
      "Batch loss: 0.7153763771057129 batch: 214/840\n",
      "Batch loss: 0.5622495412826538 batch: 215/840\n",
      "Batch loss: 0.6127844452857971 batch: 216/840\n",
      "Batch loss: 0.49888771772384644 batch: 217/840\n",
      "Batch loss: 0.6371602416038513 batch: 218/840\n",
      "Batch loss: 0.6959574818611145 batch: 219/840\n",
      "Batch loss: 0.6744900345802307 batch: 220/840\n",
      "Batch loss: 0.6264640688896179 batch: 221/840\n",
      "Batch loss: 0.7960702776908875 batch: 222/840\n",
      "Batch loss: 0.5253309011459351 batch: 223/840\n",
      "Batch loss: 0.7610868811607361 batch: 224/840\n",
      "Batch loss: 0.6196381449699402 batch: 225/840\n",
      "Batch loss: 0.7118554711341858 batch: 226/840\n",
      "Batch loss: 0.6961031556129456 batch: 227/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4773269593715668 batch: 228/840\n",
      "Batch loss: 0.46884995698928833 batch: 229/840\n",
      "Batch loss: 0.598045289516449 batch: 230/840\n",
      "Batch loss: 0.48795369267463684 batch: 231/840\n",
      "Batch loss: 0.5640665292739868 batch: 232/840\n",
      "Batch loss: 0.7348002791404724 batch: 233/840\n",
      "Batch loss: 0.5155380964279175 batch: 234/840\n",
      "Batch loss: 0.6067073345184326 batch: 235/840\n",
      "Batch loss: 0.7087889909744263 batch: 236/840\n",
      "Batch loss: 0.5229277610778809 batch: 237/840\n",
      "Batch loss: 0.6974466443061829 batch: 238/840\n",
      "Batch loss: 0.6268895864486694 batch: 239/840\n",
      "Batch loss: 0.662769615650177 batch: 240/840\n",
      "Batch loss: 0.6142366528511047 batch: 241/840\n",
      "Batch loss: 0.5581411719322205 batch: 242/840\n",
      "Batch loss: 0.6437787413597107 batch: 243/840\n",
      "Batch loss: 0.6744087338447571 batch: 244/840\n",
      "Batch loss: 0.478080153465271 batch: 245/840\n",
      "Batch loss: 0.5332430005073547 batch: 246/840\n",
      "Batch loss: 0.7403622269630432 batch: 247/840\n",
      "Batch loss: 0.6882920265197754 batch: 248/840\n",
      "Batch loss: 0.9368149638175964 batch: 249/840\n",
      "Batch loss: 0.43540066480636597 batch: 250/840\n",
      "Batch loss: 0.5858629941940308 batch: 251/840\n",
      "Batch loss: 0.4874819219112396 batch: 252/840\n",
      "Batch loss: 0.6435514092445374 batch: 253/840\n",
      "Batch loss: 0.6417816877365112 batch: 254/840\n",
      "Batch loss: 0.5163751244544983 batch: 255/840\n",
      "Batch loss: 0.6889575719833374 batch: 256/840\n",
      "Batch loss: 0.5596023201942444 batch: 257/840\n",
      "Batch loss: 0.7109804749488831 batch: 258/840\n",
      "Batch loss: 0.4930587410926819 batch: 259/840\n",
      "Batch loss: 0.4898175299167633 batch: 260/840\n",
      "Batch loss: 0.5712277293205261 batch: 261/840\n",
      "Batch loss: 0.3335697650909424 batch: 262/840\n",
      "Batch loss: 0.5774818062782288 batch: 263/840\n",
      "Batch loss: 0.6350709795951843 batch: 264/840\n",
      "Batch loss: 0.5921159982681274 batch: 265/840\n",
      "Batch loss: 0.5579919815063477 batch: 266/840\n",
      "Batch loss: 0.6881041526794434 batch: 267/840\n",
      "Batch loss: 0.5739734172821045 batch: 268/840\n",
      "Batch loss: 0.5001147985458374 batch: 269/840\n",
      "Batch loss: 0.7107992768287659 batch: 270/840\n",
      "Batch loss: 0.5163523554801941 batch: 271/840\n",
      "Batch loss: 0.7518008351325989 batch: 272/840\n",
      "Batch loss: 0.7253058552742004 batch: 273/840\n",
      "Batch loss: 0.6631917357444763 batch: 274/840\n",
      "Batch loss: 0.7234178781509399 batch: 275/840\n",
      "Batch loss: 0.5562602281570435 batch: 276/840\n",
      "Batch loss: 0.5305415987968445 batch: 277/840\n",
      "Batch loss: 0.7874813079833984 batch: 278/840\n",
      "Batch loss: 0.7479070425033569 batch: 279/840\n",
      "Batch loss: 0.6355640888214111 batch: 280/840\n",
      "Batch loss: 0.5255761742591858 batch: 281/840\n",
      "Batch loss: 0.4969816207885742 batch: 282/840\n",
      "Batch loss: 0.6478201150894165 batch: 283/840\n",
      "Batch loss: 0.4885184168815613 batch: 284/840\n",
      "Batch loss: 0.6214166879653931 batch: 285/840\n",
      "Batch loss: 0.5666925311088562 batch: 286/840\n",
      "Batch loss: 0.4049116373062134 batch: 287/840\n",
      "Batch loss: 0.5336592197418213 batch: 288/840\n",
      "Batch loss: 0.7877749800682068 batch: 289/840\n",
      "Batch loss: 0.7540885210037231 batch: 290/840\n",
      "Batch loss: 0.7228567600250244 batch: 291/840\n",
      "Batch loss: 0.6873675584793091 batch: 292/840\n",
      "Batch loss: 0.7019603848457336 batch: 293/840\n",
      "Batch loss: 0.6395918130874634 batch: 294/840\n",
      "Batch loss: 0.6331131458282471 batch: 295/840\n",
      "Batch loss: 0.6607256531715393 batch: 296/840\n",
      "Batch loss: 0.5814876556396484 batch: 297/840\n",
      "Batch loss: 0.8164293169975281 batch: 298/840\n",
      "Batch loss: 0.5395269989967346 batch: 299/840\n",
      "Batch loss: 0.7369803786277771 batch: 300/840\n",
      "Batch loss: 0.733113706111908 batch: 301/840\n",
      "Batch loss: 0.6032648682594299 batch: 302/840\n",
      "Batch loss: 0.7064704895019531 batch: 303/840\n",
      "Batch loss: 0.5299069285392761 batch: 304/840\n",
      "Batch loss: 0.5422686338424683 batch: 305/840\n",
      "Batch loss: 0.6374047994613647 batch: 306/840\n",
      "Batch loss: 0.5754794478416443 batch: 307/840\n",
      "Batch loss: 0.6937443614006042 batch: 308/840\n",
      "Batch loss: 0.5803583264350891 batch: 309/840\n",
      "Batch loss: 0.8131462931632996 batch: 310/840\n",
      "Batch loss: 0.6851099133491516 batch: 311/840\n",
      "Batch loss: 0.7514159679412842 batch: 312/840\n",
      "Batch loss: 0.5761722326278687 batch: 313/840\n",
      "Batch loss: 0.5453816652297974 batch: 314/840\n",
      "Batch loss: 0.6038063168525696 batch: 315/840\n",
      "Batch loss: 0.504753828048706 batch: 316/840\n",
      "Batch loss: 0.6800634264945984 batch: 317/840\n",
      "Batch loss: 0.6271861791610718 batch: 318/840\n",
      "Batch loss: 0.71265709400177 batch: 319/840\n",
      "Batch loss: 0.5165711045265198 batch: 320/840\n",
      "Batch loss: 0.6236017942428589 batch: 321/840\n",
      "Batch loss: 0.6454260349273682 batch: 322/840\n",
      "Batch loss: 0.7941861748695374 batch: 323/840\n",
      "Batch loss: 0.6810275316238403 batch: 324/840\n",
      "Batch loss: 0.455497145652771 batch: 325/840\n",
      "Batch loss: 0.6309040784835815 batch: 326/840\n",
      "Batch loss: 0.52277010679245 batch: 327/840\n",
      "Batch loss: 0.7511200904846191 batch: 328/840\n",
      "Batch loss: 0.6133603453636169 batch: 329/840\n",
      "Batch loss: 0.6721003651618958 batch: 330/840\n",
      "Batch loss: 0.6665071249008179 batch: 331/840\n",
      "Batch loss: 0.6385632753372192 batch: 332/840\n",
      "Batch loss: 0.6156360507011414 batch: 333/840\n",
      "Batch loss: 0.6447070240974426 batch: 334/840\n",
      "Batch loss: 0.566608190536499 batch: 335/840\n",
      "Batch loss: 0.707190752029419 batch: 336/840\n",
      "Batch loss: 0.8057771921157837 batch: 337/840\n",
      "Batch loss: 0.7245209217071533 batch: 338/840\n",
      "Batch loss: 0.5529037714004517 batch: 339/840\n",
      "Batch loss: 0.7384510636329651 batch: 340/840\n",
      "Batch loss: 0.4790711998939514 batch: 341/840\n",
      "Batch loss: 0.5245950818061829 batch: 342/840\n",
      "Batch loss: 0.7435960173606873 batch: 343/840\n",
      "Batch loss: 0.6374321579933167 batch: 344/840\n",
      "Batch loss: 0.4030791223049164 batch: 345/840\n",
      "Batch loss: 0.5618811845779419 batch: 346/840\n",
      "Batch loss: 0.71349036693573 batch: 347/840\n",
      "Batch loss: 0.6041473150253296 batch: 348/840\n",
      "Batch loss: 0.6305667757987976 batch: 349/840\n",
      "Batch loss: 0.5596164464950562 batch: 350/840\n",
      "Batch loss: 0.6631383299827576 batch: 351/840\n",
      "Batch loss: 0.6361976861953735 batch: 352/840\n",
      "Batch loss: 0.7139974236488342 batch: 353/840\n",
      "Batch loss: 0.6189944744110107 batch: 354/840\n",
      "Batch loss: 0.592892050743103 batch: 355/840\n",
      "Batch loss: 0.6152547597885132 batch: 356/840\n",
      "Batch loss: 0.6042603850364685 batch: 357/840\n",
      "Batch loss: 0.7850867509841919 batch: 358/840\n",
      "Batch loss: 0.6357825398445129 batch: 359/840\n",
      "Batch loss: 0.7210673689842224 batch: 360/840\n",
      "Batch loss: 0.7305688261985779 batch: 361/840\n",
      "Batch loss: 0.5715579986572266 batch: 362/840\n",
      "Batch loss: 0.5546834468841553 batch: 363/840\n",
      "Batch loss: 0.6585351824760437 batch: 364/840\n",
      "Batch loss: 0.5751511454582214 batch: 365/840\n",
      "Batch loss: 0.6195489764213562 batch: 366/840\n",
      "Batch loss: 0.4860106408596039 batch: 367/840\n",
      "Batch loss: 0.7821192145347595 batch: 368/840\n",
      "Batch loss: 0.5608119368553162 batch: 369/840\n",
      "Batch loss: 0.6553118824958801 batch: 370/840\n",
      "Batch loss: 0.6232532262802124 batch: 371/840\n",
      "Batch loss: 0.4939485192298889 batch: 372/840\n",
      "Batch loss: 0.5703048706054688 batch: 373/840\n",
      "Batch loss: 0.7222312092781067 batch: 374/840\n",
      "Batch loss: 0.5650689601898193 batch: 375/840\n",
      "Batch loss: 0.5015692114830017 batch: 376/840\n",
      "Batch loss: 0.5947932600975037 batch: 377/840\n",
      "Batch loss: 0.6930209398269653 batch: 378/840\n",
      "Batch loss: 0.4695763885974884 batch: 379/840\n",
      "Batch loss: 0.9400700330734253 batch: 380/840\n",
      "Batch loss: 1.0033034086227417 batch: 381/840\n",
      "Batch loss: 0.640249490737915 batch: 382/840\n",
      "Batch loss: 0.6380525231361389 batch: 383/840\n",
      "Batch loss: 0.6244138479232788 batch: 384/840\n",
      "Batch loss: 0.5806334614753723 batch: 385/840\n",
      "Batch loss: 0.7280637621879578 batch: 386/840\n",
      "Batch loss: 0.5876635313034058 batch: 387/840\n",
      "Batch loss: 0.6960479617118835 batch: 388/840\n",
      "Batch loss: 0.6369596719741821 batch: 389/840\n",
      "Batch loss: 0.8316701650619507 batch: 390/840\n",
      "Batch loss: 0.5951426029205322 batch: 391/840\n",
      "Batch loss: 0.5901085138320923 batch: 392/840\n",
      "Batch loss: 0.44032588601112366 batch: 393/840\n",
      "Batch loss: 0.7182424068450928 batch: 394/840\n",
      "Batch loss: 0.5516662001609802 batch: 395/840\n",
      "Batch loss: 0.6636819243431091 batch: 396/840\n",
      "Batch loss: 0.5117454528808594 batch: 397/840\n",
      "Batch loss: 0.6943795084953308 batch: 398/840\n",
      "Batch loss: 0.4684395492076874 batch: 399/840\n",
      "Batch loss: 0.5705519914627075 batch: 400/840\n",
      "Batch loss: 0.7371229529380798 batch: 401/840\n",
      "Batch loss: 0.5631465911865234 batch: 402/840\n",
      "Batch loss: 0.6322831511497498 batch: 403/840\n",
      "Batch loss: 0.5282383561134338 batch: 404/840\n",
      "Batch loss: 0.584873378276825 batch: 405/840\n",
      "Batch loss: 0.6373178362846375 batch: 406/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5322454571723938 batch: 407/840\n",
      "Batch loss: 0.6290424466133118 batch: 408/840\n",
      "Batch loss: 0.699280321598053 batch: 409/840\n",
      "Batch loss: 0.87884122133255 batch: 410/840\n",
      "Batch loss: 0.7878567576408386 batch: 411/840\n",
      "Batch loss: 0.7522141933441162 batch: 412/840\n",
      "Batch loss: 0.672089159488678 batch: 413/840\n",
      "Batch loss: 0.5225752592086792 batch: 414/840\n",
      "Batch loss: 0.8961448669433594 batch: 415/840\n",
      "Batch loss: 0.44602280855178833 batch: 416/840\n",
      "Batch loss: 0.5941513180732727 batch: 417/840\n",
      "Batch loss: 0.7514727115631104 batch: 418/840\n",
      "Batch loss: 0.6251924633979797 batch: 419/840\n",
      "Batch loss: 0.6919043064117432 batch: 420/840\n",
      "Batch loss: 0.5423755049705505 batch: 421/840\n",
      "Batch loss: 0.5515317320823669 batch: 422/840\n",
      "Batch loss: 0.7362473011016846 batch: 423/840\n",
      "Batch loss: 0.6893961429595947 batch: 424/840\n",
      "Batch loss: 0.6998827457427979 batch: 425/840\n",
      "Batch loss: 0.5869777202606201 batch: 426/840\n",
      "Batch loss: 0.5497910976409912 batch: 427/840\n",
      "Batch loss: 0.7341428399085999 batch: 428/840\n",
      "Batch loss: 0.5130681991577148 batch: 429/840\n",
      "Batch loss: 0.7187865376472473 batch: 430/840\n",
      "Batch loss: 0.555692732334137 batch: 431/840\n",
      "Batch loss: 0.6318865418434143 batch: 432/840\n",
      "Batch loss: 0.4912225604057312 batch: 433/840\n",
      "Batch loss: 0.4782577157020569 batch: 434/840\n",
      "Batch loss: 0.7751396894454956 batch: 435/840\n",
      "Batch loss: 0.6663411855697632 batch: 436/840\n",
      "Batch loss: 0.689359724521637 batch: 437/840\n",
      "Batch loss: 0.40141844749450684 batch: 438/840\n",
      "Batch loss: 0.6580355763435364 batch: 439/840\n",
      "Batch loss: 0.7116983532905579 batch: 440/840\n",
      "Batch loss: 0.6846888065338135 batch: 441/840\n",
      "Batch loss: 0.8853726387023926 batch: 442/840\n",
      "Batch loss: 0.6224777102470398 batch: 443/840\n",
      "Batch loss: 0.6931198239326477 batch: 444/840\n",
      "Batch loss: 0.5522392988204956 batch: 445/840\n",
      "Batch loss: 0.6112095713615417 batch: 446/840\n",
      "Batch loss: 0.6228811144828796 batch: 447/840\n",
      "Batch loss: 0.6417009830474854 batch: 448/840\n",
      "Batch loss: 0.5908865928649902 batch: 449/840\n",
      "Batch loss: 0.6161062121391296 batch: 450/840\n",
      "Batch loss: 0.6174285411834717 batch: 451/840\n",
      "Batch loss: 0.6432651281356812 batch: 452/840\n",
      "Batch loss: 0.7003354430198669 batch: 453/840\n",
      "Batch loss: 0.4770389199256897 batch: 454/840\n",
      "Batch loss: 0.7082405686378479 batch: 455/840\n",
      "Batch loss: 0.5891145467758179 batch: 456/840\n",
      "Batch loss: 0.5714473128318787 batch: 457/840\n",
      "Batch loss: 0.6320388913154602 batch: 458/840\n",
      "Batch loss: 0.5740613341331482 batch: 459/840\n",
      "Batch loss: 0.4391363263130188 batch: 460/840\n",
      "Batch loss: 0.6688092947006226 batch: 461/840\n",
      "Batch loss: 0.5568661093711853 batch: 462/840\n",
      "Batch loss: 0.8135719895362854 batch: 463/840\n",
      "Batch loss: 0.5150141716003418 batch: 464/840\n",
      "Batch loss: 0.8374230265617371 batch: 465/840\n",
      "Batch loss: 0.5917041897773743 batch: 466/840\n",
      "Batch loss: 0.9203794002532959 batch: 467/840\n",
      "Batch loss: 0.5760747790336609 batch: 468/840\n",
      "Batch loss: 0.5706791281700134 batch: 469/840\n",
      "Batch loss: 0.6049445867538452 batch: 470/840\n",
      "Batch loss: 0.6623214483261108 batch: 471/840\n",
      "Batch loss: 0.6437536478042603 batch: 472/840\n",
      "Batch loss: 0.7042834758758545 batch: 473/840\n",
      "Batch loss: 0.5222921967506409 batch: 474/840\n",
      "Batch loss: 0.5830422043800354 batch: 475/840\n",
      "Batch loss: 0.677880048751831 batch: 476/840\n",
      "Batch loss: 0.48653343319892883 batch: 477/840\n",
      "Batch loss: 0.5570404529571533 batch: 478/840\n",
      "Batch loss: 0.48558157682418823 batch: 479/840\n",
      "Batch loss: 0.5792885422706604 batch: 480/840\n",
      "Batch loss: 0.6762422323226929 batch: 481/840\n",
      "Batch loss: 0.6298931837081909 batch: 482/840\n",
      "Batch loss: 0.7584133744239807 batch: 483/840\n",
      "Batch loss: 0.5640512704849243 batch: 484/840\n",
      "Batch loss: 0.656666100025177 batch: 485/840\n",
      "Batch loss: 0.6453942656517029 batch: 486/840\n",
      "Batch loss: 0.5590928196907043 batch: 487/840\n",
      "Batch loss: 0.630646824836731 batch: 488/840\n",
      "Batch loss: 0.6053840517997742 batch: 489/840\n",
      "Batch loss: 0.6382092833518982 batch: 490/840\n",
      "Batch loss: 0.3880001902580261 batch: 491/840\n",
      "Batch loss: 0.7423239946365356 batch: 492/840\n",
      "Batch loss: 0.6853811740875244 batch: 493/840\n",
      "Batch loss: 0.4157203733921051 batch: 494/840\n",
      "Batch loss: 0.7081514000892639 batch: 495/840\n",
      "Batch loss: 0.6532291173934937 batch: 496/840\n",
      "Batch loss: 0.5861707925796509 batch: 497/840\n",
      "Batch loss: 0.5022900104522705 batch: 498/840\n",
      "Batch loss: 0.5769599676132202 batch: 499/840\n",
      "Batch loss: 0.5476093292236328 batch: 500/840\n",
      "Batch loss: 0.5724483728408813 batch: 501/840\n",
      "Batch loss: 0.6793115735054016 batch: 502/840\n",
      "Batch loss: 0.4528874456882477 batch: 503/840\n",
      "Batch loss: 0.6106045842170715 batch: 504/840\n",
      "Batch loss: 0.7340139150619507 batch: 505/840\n",
      "Batch loss: 0.5603891015052795 batch: 506/840\n",
      "Batch loss: 0.6797479391098022 batch: 507/840\n",
      "Batch loss: 0.5305485725402832 batch: 508/840\n",
      "Batch loss: 0.6130673885345459 batch: 509/840\n",
      "Batch loss: 0.6615445017814636 batch: 510/840\n",
      "Batch loss: 0.46072593331336975 batch: 511/840\n",
      "Batch loss: 0.6092372536659241 batch: 512/840\n",
      "Batch loss: 0.593991219997406 batch: 513/840\n",
      "Batch loss: 0.6389920711517334 batch: 514/840\n",
      "Batch loss: 0.4751697778701782 batch: 515/840\n",
      "Batch loss: 0.7003337740898132 batch: 516/840\n",
      "Batch loss: 0.5924117565155029 batch: 517/840\n",
      "Batch loss: 0.6383491158485413 batch: 518/840\n",
      "Batch loss: 0.7018968462944031 batch: 519/840\n",
      "Batch loss: 0.7343624234199524 batch: 520/840\n",
      "Batch loss: 0.6930654644966125 batch: 521/840\n",
      "Batch loss: 0.5693054795265198 batch: 522/840\n",
      "Batch loss: 0.46109557151794434 batch: 523/840\n",
      "Batch loss: 0.6391705870628357 batch: 524/840\n",
      "Batch loss: 0.5721644759178162 batch: 525/840\n",
      "Batch loss: 0.6759772300720215 batch: 526/840\n",
      "Batch loss: 0.7159601449966431 batch: 527/840\n",
      "Batch loss: 0.6130024194717407 batch: 528/840\n",
      "Batch loss: 0.5437930822372437 batch: 529/840\n",
      "Batch loss: 0.691387951374054 batch: 530/840\n",
      "Batch loss: 0.5081195831298828 batch: 531/840\n",
      "Batch loss: 0.47150662541389465 batch: 532/840\n",
      "Batch loss: 0.727719247341156 batch: 533/840\n",
      "Batch loss: 0.6557338237762451 batch: 534/840\n",
      "Batch loss: 0.619204044342041 batch: 535/840\n",
      "Batch loss: 0.7233704924583435 batch: 536/840\n",
      "Batch loss: 0.5716590881347656 batch: 537/840\n",
      "Batch loss: 0.5236033201217651 batch: 538/840\n",
      "Batch loss: 0.6348752975463867 batch: 539/840\n",
      "Batch loss: 0.830704927444458 batch: 540/840\n",
      "Batch loss: 0.5134446024894714 batch: 541/840\n",
      "Batch loss: 0.6709269881248474 batch: 542/840\n",
      "Batch loss: 0.48126038908958435 batch: 543/840\n",
      "Batch loss: 0.5670129656791687 batch: 544/840\n",
      "Batch loss: 0.6787556409835815 batch: 545/840\n",
      "Batch loss: 0.6542748808860779 batch: 546/840\n",
      "Batch loss: 0.53883957862854 batch: 547/840\n",
      "Batch loss: 0.44796866178512573 batch: 548/840\n",
      "Batch loss: 0.5121161937713623 batch: 549/840\n",
      "Batch loss: 0.6021077632904053 batch: 550/840\n",
      "Batch loss: 0.6962472796440125 batch: 551/840\n",
      "Batch loss: 0.6412487626075745 batch: 552/840\n",
      "Batch loss: 0.6218100190162659 batch: 553/840\n",
      "Batch loss: 0.6584031581878662 batch: 554/840\n",
      "Batch loss: 0.616638720035553 batch: 555/840\n",
      "Batch loss: 0.6216123700141907 batch: 556/840\n",
      "Batch loss: 0.6779518723487854 batch: 557/840\n",
      "Batch loss: 0.6520467400550842 batch: 558/840\n",
      "Batch loss: 0.5430906414985657 batch: 559/840\n",
      "Batch loss: 0.7107716202735901 batch: 560/840\n",
      "Batch loss: 0.5549176931381226 batch: 561/840\n",
      "Batch loss: 0.5646111965179443 batch: 562/840\n",
      "Batch loss: 0.4895932078361511 batch: 563/840\n",
      "Batch loss: 0.6568499803543091 batch: 564/840\n",
      "Batch loss: 0.7979480028152466 batch: 565/840\n",
      "Batch loss: 0.7082401514053345 batch: 566/840\n",
      "Batch loss: 0.7598574161529541 batch: 567/840\n",
      "Batch loss: 0.5942183136940002 batch: 568/840\n",
      "Batch loss: 0.5962017178535461 batch: 569/840\n",
      "Batch loss: 0.4254729151725769 batch: 570/840\n",
      "Batch loss: 0.6534597873687744 batch: 571/840\n",
      "Batch loss: 0.7653396129608154 batch: 572/840\n",
      "Batch loss: 0.6088857054710388 batch: 573/840\n",
      "Batch loss: 0.7031378746032715 batch: 574/840\n",
      "Batch loss: 0.5820451974868774 batch: 575/840\n",
      "Batch loss: 0.6112561225891113 batch: 576/840\n",
      "Batch loss: 0.5256468057632446 batch: 577/840\n",
      "Batch loss: 0.738126277923584 batch: 578/840\n",
      "Batch loss: 0.630729079246521 batch: 579/840\n",
      "Batch loss: 0.7612872123718262 batch: 580/840\n",
      "Batch loss: 0.6521339416503906 batch: 581/840\n",
      "Batch loss: 0.7361297607421875 batch: 582/840\n",
      "Batch loss: 0.5905354022979736 batch: 583/840\n",
      "Batch loss: 0.8549981713294983 batch: 584/840\n",
      "Batch loss: 0.6025715470314026 batch: 585/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6413366198539734 batch: 586/840\n",
      "Batch loss: 0.6614829301834106 batch: 587/840\n",
      "Batch loss: 0.6117152571678162 batch: 588/840\n",
      "Batch loss: 0.7502248287200928 batch: 589/840\n",
      "Batch loss: 0.4205474853515625 batch: 590/840\n",
      "Batch loss: 0.4354659616947174 batch: 591/840\n",
      "Batch loss: 0.4998418390750885 batch: 592/840\n",
      "Batch loss: 0.8253275156021118 batch: 593/840\n",
      "Batch loss: 0.6586718559265137 batch: 594/840\n",
      "Batch loss: 0.30704745650291443 batch: 595/840\n",
      "Batch loss: 0.4726310968399048 batch: 596/840\n",
      "Batch loss: 0.6004575490951538 batch: 597/840\n",
      "Batch loss: 0.43487539887428284 batch: 598/840\n",
      "Batch loss: 0.6150572299957275 batch: 599/840\n",
      "Batch loss: 0.785199761390686 batch: 600/840\n",
      "Batch loss: 0.7335411310195923 batch: 601/840\n",
      "Batch loss: 0.49271923303604126 batch: 602/840\n",
      "Batch loss: 0.5401470065116882 batch: 603/840\n",
      "Batch loss: 0.6728925108909607 batch: 604/840\n",
      "Batch loss: 0.7537436485290527 batch: 605/840\n",
      "Batch loss: 0.8493638634681702 batch: 606/840\n",
      "Batch loss: 0.5750709772109985 batch: 607/840\n",
      "Batch loss: 0.6423200964927673 batch: 608/840\n",
      "Batch loss: 0.4950275719165802 batch: 609/840\n",
      "Batch loss: 0.6399682760238647 batch: 610/840\n",
      "Batch loss: 0.6422601938247681 batch: 611/840\n",
      "Batch loss: 0.740812361240387 batch: 612/840\n",
      "Batch loss: 0.7536125779151917 batch: 613/840\n",
      "Batch loss: 0.6058425307273865 batch: 614/840\n",
      "Batch loss: 0.47112154960632324 batch: 615/840\n",
      "Batch loss: 0.5629440546035767 batch: 616/840\n",
      "Batch loss: 0.540911078453064 batch: 617/840\n",
      "Batch loss: 0.5821003913879395 batch: 618/840\n",
      "Batch loss: 0.8178130388259888 batch: 619/840\n",
      "Batch loss: 0.52071213722229 batch: 620/840\n",
      "Batch loss: 0.6995669603347778 batch: 621/840\n",
      "Batch loss: 0.6207782626152039 batch: 622/840\n",
      "Batch loss: 0.6682758331298828 batch: 623/840\n",
      "Batch loss: 0.7379850149154663 batch: 624/840\n",
      "Batch loss: 0.6389577984809875 batch: 625/840\n",
      "Batch loss: 0.5161448121070862 batch: 626/840\n",
      "Batch loss: 0.550246000289917 batch: 627/840\n",
      "Batch loss: 0.7079382538795471 batch: 628/840\n",
      "Batch loss: 0.6662198901176453 batch: 629/840\n",
      "Batch loss: 0.5283733606338501 batch: 630/840\n",
      "Batch loss: 0.6407064199447632 batch: 631/840\n",
      "Batch loss: 0.5921967625617981 batch: 632/840\n",
      "Batch loss: 0.5122734904289246 batch: 633/840\n",
      "Batch loss: 0.6022642254829407 batch: 634/840\n",
      "Batch loss: 0.517062783241272 batch: 635/840\n",
      "Batch loss: 0.560640811920166 batch: 636/840\n",
      "Batch loss: 0.4987196624279022 batch: 637/840\n",
      "Batch loss: 0.6446515917778015 batch: 638/840\n",
      "Batch loss: 0.6830532550811768 batch: 639/840\n",
      "Batch loss: 0.6406967639923096 batch: 640/840\n",
      "Batch loss: 0.8255777955055237 batch: 641/840\n",
      "Batch loss: 0.5968844294548035 batch: 642/840\n",
      "Batch loss: 0.9632092118263245 batch: 643/840\n",
      "Batch loss: 0.6343184113502502 batch: 644/840\n",
      "Batch loss: 0.47930821776390076 batch: 645/840\n",
      "Batch loss: 0.6015228033065796 batch: 646/840\n",
      "Batch loss: 0.7505815625190735 batch: 647/840\n",
      "Batch loss: 0.6175368428230286 batch: 648/840\n",
      "Batch loss: 0.5970281362533569 batch: 649/840\n",
      "Batch loss: 0.5421432256698608 batch: 650/840\n",
      "Batch loss: 0.6532044410705566 batch: 651/840\n",
      "Batch loss: 0.6572248935699463 batch: 652/840\n",
      "Batch loss: 0.49922168254852295 batch: 653/840\n",
      "Batch loss: 0.8757549524307251 batch: 654/840\n",
      "Batch loss: 0.5755211114883423 batch: 655/840\n",
      "Batch loss: 0.6856259107589722 batch: 656/840\n",
      "Batch loss: 0.530108630657196 batch: 657/840\n",
      "Batch loss: 0.63470059633255 batch: 658/840\n",
      "Batch loss: 0.6625553369522095 batch: 659/840\n",
      "Batch loss: 0.5570699572563171 batch: 660/840\n",
      "Batch loss: 0.741970419883728 batch: 661/840\n",
      "Batch loss: 0.638063371181488 batch: 662/840\n",
      "Batch loss: 0.6140555143356323 batch: 663/840\n",
      "Batch loss: 0.6626144647598267 batch: 664/840\n",
      "Batch loss: 0.7665949463844299 batch: 665/840\n",
      "Batch loss: 0.6187500357627869 batch: 666/840\n",
      "Batch loss: 0.6756354570388794 batch: 667/840\n",
      "Batch loss: 0.6380197405815125 batch: 668/840\n",
      "Batch loss: 0.5836050510406494 batch: 669/840\n",
      "Batch loss: 0.6006343364715576 batch: 670/840\n",
      "Batch loss: 0.6133929491043091 batch: 671/840\n",
      "Batch loss: 0.5626880526542664 batch: 672/840\n",
      "Batch loss: 1.0222645998001099 batch: 673/840\n",
      "Batch loss: 0.6874274611473083 batch: 674/840\n",
      "Batch loss: 0.6189028024673462 batch: 675/840\n",
      "Batch loss: 0.5260934829711914 batch: 676/840\n",
      "Batch loss: 0.5852232575416565 batch: 677/840\n",
      "Batch loss: 0.6703851222991943 batch: 678/840\n",
      "Batch loss: 0.738777220249176 batch: 679/840\n",
      "Batch loss: 0.7162445783615112 batch: 680/840\n",
      "Batch loss: 0.6515851020812988 batch: 681/840\n",
      "Batch loss: 0.5594914555549622 batch: 682/840\n",
      "Batch loss: 0.5701388716697693 batch: 683/840\n",
      "Batch loss: 0.7541883587837219 batch: 684/840\n",
      "Batch loss: 0.6398715376853943 batch: 685/840\n",
      "Batch loss: 0.7824050784111023 batch: 686/840\n",
      "Batch loss: 0.6483238935470581 batch: 687/840\n",
      "Batch loss: 0.5333116054534912 batch: 688/840\n",
      "Batch loss: 0.5017670392990112 batch: 689/840\n",
      "Batch loss: 0.547770619392395 batch: 690/840\n",
      "Batch loss: 0.6755842566490173 batch: 691/840\n",
      "Batch loss: 0.5508931875228882 batch: 692/840\n",
      "Batch loss: 0.6494296193122864 batch: 693/840\n",
      "Batch loss: 0.7977785468101501 batch: 694/840\n",
      "Batch loss: 0.751020610332489 batch: 695/840\n",
      "Batch loss: 0.633201003074646 batch: 696/840\n",
      "Batch loss: 0.5432584285736084 batch: 697/840\n",
      "Batch loss: 0.5485438704490662 batch: 698/840\n",
      "Batch loss: 0.7369927167892456 batch: 699/840\n",
      "Batch loss: 0.810921847820282 batch: 700/840\n",
      "Batch loss: 0.844235897064209 batch: 701/840\n",
      "Batch loss: 0.670396625995636 batch: 702/840\n",
      "Batch loss: 0.6128900647163391 batch: 703/840\n",
      "Batch loss: 0.7186647057533264 batch: 704/840\n",
      "Batch loss: 0.6414781212806702 batch: 705/840\n",
      "Batch loss: 0.635908305644989 batch: 706/840\n",
      "Batch loss: 0.6343871355056763 batch: 707/840\n",
      "Batch loss: 0.6925700306892395 batch: 708/840\n",
      "Batch loss: 0.7156246304512024 batch: 709/840\n",
      "Batch loss: 0.7661969661712646 batch: 710/840\n",
      "Batch loss: 0.43140342831611633 batch: 711/840\n",
      "Batch loss: 0.642937421798706 batch: 712/840\n",
      "Batch loss: 0.4982008635997772 batch: 713/840\n",
      "Batch loss: 0.6785765290260315 batch: 714/840\n",
      "Batch loss: 0.5914632678031921 batch: 715/840\n",
      "Batch loss: 0.6616377830505371 batch: 716/840\n",
      "Batch loss: 0.5408468842506409 batch: 717/840\n",
      "Batch loss: 0.5442501306533813 batch: 718/840\n",
      "Batch loss: 0.5109111666679382 batch: 719/840\n",
      "Batch loss: 0.569847822189331 batch: 720/840\n",
      "Batch loss: 0.7650351524353027 batch: 721/840\n",
      "Batch loss: 0.8529992699623108 batch: 722/840\n",
      "Batch loss: 0.44808492064476013 batch: 723/840\n",
      "Batch loss: 0.7166257500648499 batch: 724/840\n",
      "Batch loss: 0.6248912811279297 batch: 725/840\n",
      "Batch loss: 0.6326414346694946 batch: 726/840\n",
      "Batch loss: 0.8157463669776917 batch: 727/840\n",
      "Batch loss: 0.7429158687591553 batch: 728/840\n",
      "Batch loss: 0.6954141855239868 batch: 729/840\n",
      "Batch loss: 0.5844292640686035 batch: 730/840\n",
      "Batch loss: 0.49715760350227356 batch: 731/840\n",
      "Batch loss: 0.7326433062553406 batch: 732/840\n",
      "Batch loss: 0.5528890490531921 batch: 733/840\n",
      "Batch loss: 0.5999974012374878 batch: 734/840\n",
      "Batch loss: 0.6222541332244873 batch: 735/840\n",
      "Batch loss: 0.7393093109130859 batch: 736/840\n",
      "Batch loss: 0.7482357621192932 batch: 737/840\n",
      "Batch loss: 0.47458842396736145 batch: 738/840\n",
      "Batch loss: 0.7574465870857239 batch: 739/840\n",
      "Batch loss: 0.802153468132019 batch: 740/840\n",
      "Batch loss: 0.5751532912254333 batch: 741/840\n",
      "Batch loss: 0.5620622038841248 batch: 742/840\n",
      "Batch loss: 0.4065576493740082 batch: 743/840\n",
      "Batch loss: 0.5117288827896118 batch: 744/840\n",
      "Batch loss: 0.5750569701194763 batch: 745/840\n",
      "Batch loss: 0.6023653149604797 batch: 746/840\n",
      "Batch loss: 0.6358316540718079 batch: 747/840\n",
      "Batch loss: 0.6256625652313232 batch: 748/840\n",
      "Batch loss: 0.48964595794677734 batch: 749/840\n",
      "Batch loss: 0.5520215034484863 batch: 750/840\n",
      "Batch loss: 0.6385996341705322 batch: 751/840\n",
      "Batch loss: 0.7684612274169922 batch: 752/840\n",
      "Batch loss: 0.5197076201438904 batch: 753/840\n",
      "Batch loss: 0.6656026244163513 batch: 754/840\n",
      "Batch loss: 0.6621477007865906 batch: 755/840\n",
      "Batch loss: 0.5325576663017273 batch: 756/840\n",
      "Batch loss: 0.6455357074737549 batch: 757/840\n",
      "Batch loss: 0.5632897019386292 batch: 758/840\n",
      "Batch loss: 0.5511366724967957 batch: 759/840\n",
      "Batch loss: 0.5777307748794556 batch: 760/840\n",
      "Batch loss: 0.5406115651130676 batch: 761/840\n",
      "Batch loss: 0.7301223278045654 batch: 762/840\n",
      "Batch loss: 0.45933136343955994 batch: 763/840\n",
      "Batch loss: 0.6179913878440857 batch: 764/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.41795018315315247 batch: 765/840\n",
      "Batch loss: 0.619662880897522 batch: 766/840\n",
      "Batch loss: 0.898394763469696 batch: 767/840\n",
      "Batch loss: 0.7107377648353577 batch: 768/840\n",
      "Batch loss: 0.6453225612640381 batch: 769/840\n",
      "Batch loss: 0.5436524748802185 batch: 770/840\n",
      "Batch loss: 0.6763499975204468 batch: 771/840\n",
      "Batch loss: 0.483007550239563 batch: 772/840\n",
      "Batch loss: 0.5038214325904846 batch: 773/840\n",
      "Batch loss: 0.50376296043396 batch: 774/840\n",
      "Batch loss: 0.6219906806945801 batch: 775/840\n",
      "Batch loss: 0.5490787029266357 batch: 776/840\n",
      "Batch loss: 0.48236599564552307 batch: 777/840\n",
      "Batch loss: 0.5030001997947693 batch: 778/840\n",
      "Batch loss: 0.7370665669441223 batch: 779/840\n",
      "Batch loss: 0.5727481245994568 batch: 780/840\n",
      "Batch loss: 0.5302107930183411 batch: 781/840\n",
      "Batch loss: 0.6718466877937317 batch: 782/840\n",
      "Batch loss: 0.4620470404624939 batch: 783/840\n",
      "Batch loss: 0.6622456312179565 batch: 784/840\n",
      "Batch loss: 0.5017804503440857 batch: 785/840\n",
      "Batch loss: 0.6025637984275818 batch: 786/840\n",
      "Batch loss: 0.6070781946182251 batch: 787/840\n",
      "Batch loss: 0.7285681366920471 batch: 788/840\n",
      "Batch loss: 0.7319190502166748 batch: 789/840\n",
      "Batch loss: 0.6576252579689026 batch: 790/840\n",
      "Batch loss: 0.6787466406822205 batch: 791/840\n",
      "Batch loss: 0.4076040983200073 batch: 792/840\n",
      "Batch loss: 0.5607892274856567 batch: 793/840\n",
      "Batch loss: 0.6331566572189331 batch: 794/840\n",
      "Batch loss: 0.6934766173362732 batch: 795/840\n",
      "Batch loss: 0.6609005928039551 batch: 796/840\n",
      "Batch loss: 0.663100004196167 batch: 797/840\n",
      "Batch loss: 0.624847948551178 batch: 798/840\n",
      "Batch loss: 0.615920901298523 batch: 799/840\n",
      "Batch loss: 0.5331419706344604 batch: 800/840\n",
      "Batch loss: 0.7074882984161377 batch: 801/840\n",
      "Batch loss: 0.5343488454818726 batch: 802/840\n",
      "Batch loss: 0.4762311577796936 batch: 803/840\n",
      "Batch loss: 0.7285926342010498 batch: 804/840\n",
      "Batch loss: 0.5508307814598083 batch: 805/840\n",
      "Batch loss: 0.5847902894020081 batch: 806/840\n",
      "Batch loss: 0.5580186247825623 batch: 807/840\n",
      "Batch loss: 0.5846338868141174 batch: 808/840\n",
      "Batch loss: 0.5958237648010254 batch: 809/840\n",
      "Batch loss: 0.6315876245498657 batch: 810/840\n",
      "Batch loss: 0.5400842428207397 batch: 811/840\n",
      "Batch loss: 0.6397668719291687 batch: 812/840\n",
      "Batch loss: 0.5495477318763733 batch: 813/840\n",
      "Batch loss: 0.5912976264953613 batch: 814/840\n",
      "Batch loss: 0.6755450367927551 batch: 815/840\n",
      "Batch loss: 0.6696791052818298 batch: 816/840\n",
      "Batch loss: 0.8227161169052124 batch: 817/840\n",
      "Batch loss: 0.6898368000984192 batch: 818/840\n",
      "Batch loss: 0.4849810004234314 batch: 819/840\n",
      "Batch loss: 0.5515204071998596 batch: 820/840\n",
      "Batch loss: 0.7222538590431213 batch: 821/840\n",
      "Batch loss: 0.5643680691719055 batch: 822/840\n",
      "Batch loss: 0.6878578066825867 batch: 823/840\n",
      "Batch loss: 0.786053478717804 batch: 824/840\n",
      "Batch loss: 0.7635923027992249 batch: 825/840\n",
      "Batch loss: 0.6444634795188904 batch: 826/840\n",
      "Batch loss: 0.5222848653793335 batch: 827/840\n",
      "Batch loss: 0.6700548529624939 batch: 828/840\n",
      "Batch loss: 0.5348095893859863 batch: 829/840\n",
      "Batch loss: 0.7286996245384216 batch: 830/840\n",
      "Batch loss: 0.5699361562728882 batch: 831/840\n",
      "Batch loss: 0.7051595449447632 batch: 832/840\n",
      "Batch loss: 0.6281506419181824 batch: 833/840\n",
      "Batch loss: 0.6106591820716858 batch: 834/840\n",
      "Batch loss: 0.47078973054885864 batch: 835/840\n",
      "Batch loss: 0.630265474319458 batch: 836/840\n",
      "Batch loss: 0.6286699175834656 batch: 837/840\n",
      "Batch loss: 0.6714129447937012 batch: 838/840\n",
      "Batch loss: 0.5562722682952881 batch: 839/840\n",
      "Batch loss: 0.5839744806289673 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 12/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.816\n",
      "Running epoch 13/15\n",
      "Batch loss: 0.5036607980728149 batch: 1/840\n",
      "Batch loss: 0.9804290533065796 batch: 2/840\n",
      "Batch loss: 0.6159551739692688 batch: 3/840\n",
      "Batch loss: 0.6258542537689209 batch: 4/840\n",
      "Batch loss: 0.5491620898246765 batch: 5/840\n",
      "Batch loss: 0.40424561500549316 batch: 6/840\n",
      "Batch loss: 0.5102386474609375 batch: 7/840\n",
      "Batch loss: 0.6457424759864807 batch: 8/840\n",
      "Batch loss: 0.5184339880943298 batch: 9/840\n",
      "Batch loss: 0.5641844272613525 batch: 10/840\n",
      "Batch loss: 0.5643116235733032 batch: 11/840\n",
      "Batch loss: 0.5190249085426331 batch: 12/840\n",
      "Batch loss: 0.542433500289917 batch: 13/840\n",
      "Batch loss: 0.6189667582511902 batch: 14/840\n",
      "Batch loss: 0.6765131950378418 batch: 15/840\n",
      "Batch loss: 0.43194279074668884 batch: 16/840\n",
      "Batch loss: 0.4812396764755249 batch: 17/840\n",
      "Batch loss: 0.6684117317199707 batch: 18/840\n",
      "Batch loss: 0.6707934737205505 batch: 19/840\n",
      "Batch loss: 0.7447673678398132 batch: 20/840\n",
      "Batch loss: 0.6327390074729919 batch: 21/840\n",
      "Batch loss: 0.5921540260314941 batch: 22/840\n",
      "Batch loss: 0.5779668092727661 batch: 23/840\n",
      "Batch loss: 0.4744459092617035 batch: 24/840\n",
      "Batch loss: 0.573089599609375 batch: 25/840\n",
      "Batch loss: 0.6618281602859497 batch: 26/840\n",
      "Batch loss: 0.5594446659088135 batch: 27/840\n",
      "Batch loss: 0.6392179727554321 batch: 28/840\n",
      "Batch loss: 0.6788428425788879 batch: 29/840\n",
      "Batch loss: 0.4903554618358612 batch: 30/840\n",
      "Batch loss: 0.6167500615119934 batch: 31/840\n",
      "Batch loss: 0.6243274211883545 batch: 32/840\n",
      "Batch loss: 0.6036558747291565 batch: 33/840\n",
      "Batch loss: 0.5205501914024353 batch: 34/840\n",
      "Batch loss: 0.5708192586898804 batch: 35/840\n",
      "Batch loss: 0.6097880005836487 batch: 36/840\n",
      "Batch loss: 0.633003294467926 batch: 37/840\n",
      "Batch loss: 0.5637083053588867 batch: 38/840\n",
      "Batch loss: 0.7452789545059204 batch: 39/840\n",
      "Batch loss: 0.5830608606338501 batch: 40/840\n",
      "Batch loss: 0.7264846563339233 batch: 41/840\n",
      "Batch loss: 0.6240135431289673 batch: 42/840\n",
      "Batch loss: 0.6399986743927002 batch: 43/840\n",
      "Batch loss: 0.7261914610862732 batch: 44/840\n",
      "Batch loss: 0.6864904761314392 batch: 45/840\n",
      "Batch loss: 0.5862768888473511 batch: 46/840\n",
      "Batch loss: 0.47258347272872925 batch: 47/840\n",
      "Batch loss: 0.6745566725730896 batch: 48/840\n",
      "Batch loss: 0.5301948189735413 batch: 49/840\n",
      "Batch loss: 0.7220573425292969 batch: 50/840\n",
      "Batch loss: 0.6334478855133057 batch: 51/840\n",
      "Batch loss: 0.8267577290534973 batch: 52/840\n",
      "Batch loss: 0.5465923547744751 batch: 53/840\n",
      "Batch loss: 0.6656922101974487 batch: 54/840\n",
      "Batch loss: 0.5564695596694946 batch: 55/840\n",
      "Batch loss: 0.576797604560852 batch: 56/840\n",
      "Batch loss: 0.6597208380699158 batch: 57/840\n",
      "Batch loss: 0.5676340460777283 batch: 58/840\n",
      "Batch loss: 0.5264391303062439 batch: 59/840\n",
      "Batch loss: 0.48545515537261963 batch: 60/840\n",
      "Batch loss: 0.738350510597229 batch: 61/840\n",
      "Batch loss: 0.5960401892662048 batch: 62/840\n",
      "Batch loss: 0.6468268632888794 batch: 63/840\n",
      "Batch loss: 0.6247274875640869 batch: 64/840\n",
      "Batch loss: 0.5536025762557983 batch: 65/840\n",
      "Batch loss: 0.6301259398460388 batch: 66/840\n",
      "Batch loss: 0.6281054615974426 batch: 67/840\n",
      "Batch loss: 0.6497933864593506 batch: 68/840\n",
      "Batch loss: 0.7119879722595215 batch: 69/840\n",
      "Batch loss: 0.5394319295883179 batch: 70/840\n",
      "Batch loss: 0.6318526268005371 batch: 71/840\n",
      "Batch loss: 0.7176962494850159 batch: 72/840\n",
      "Batch loss: 0.6988381743431091 batch: 73/840\n",
      "Batch loss: 0.7547297477722168 batch: 74/840\n",
      "Batch loss: 0.5561135411262512 batch: 75/840\n",
      "Batch loss: 0.4378223419189453 batch: 76/840\n",
      "Batch loss: 0.5701354146003723 batch: 77/840\n",
      "Batch loss: 0.8553322553634644 batch: 78/840\n",
      "Batch loss: 0.5467815399169922 batch: 79/840\n",
      "Batch loss: 0.6398679614067078 batch: 80/840\n",
      "Batch loss: 0.5594854950904846 batch: 81/840\n",
      "Batch loss: 0.6121453046798706 batch: 82/840\n",
      "Batch loss: 0.5264843106269836 batch: 83/840\n",
      "Batch loss: 0.7233469486236572 batch: 84/840\n",
      "Batch loss: 0.6251479983329773 batch: 85/840\n",
      "Batch loss: 0.9217491745948792 batch: 86/840\n",
      "Batch loss: 0.5426027178764343 batch: 87/840\n",
      "Batch loss: 0.39191004633903503 batch: 88/840\n",
      "Batch loss: 0.41579753160476685 batch: 89/840\n",
      "Batch loss: 0.5240668654441833 batch: 90/840\n",
      "Batch loss: 0.6592557430267334 batch: 91/840\n",
      "Batch loss: 0.5711017847061157 batch: 92/840\n",
      "Batch loss: 0.6396788358688354 batch: 93/840\n",
      "Batch loss: 0.5283372402191162 batch: 94/840\n",
      "Batch loss: 0.528456449508667 batch: 95/840\n",
      "Batch loss: 0.6642158031463623 batch: 96/840\n",
      "Batch loss: 0.654343843460083 batch: 97/840\n",
      "Batch loss: 0.6957263946533203 batch: 98/840\n",
      "Batch loss: 0.5752315521240234 batch: 99/840\n",
      "Batch loss: 0.6812949180603027 batch: 100/840\n",
      "Batch loss: 0.48289793729782104 batch: 101/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4575197100639343 batch: 102/840\n",
      "Batch loss: 0.6288957595825195 batch: 103/840\n",
      "Batch loss: 0.494482159614563 batch: 104/840\n",
      "Batch loss: 0.6672325134277344 batch: 105/840\n",
      "Batch loss: 0.6609136462211609 batch: 106/840\n",
      "Batch loss: 0.5712193846702576 batch: 107/840\n",
      "Batch loss: 0.9120755791664124 batch: 108/840\n",
      "Batch loss: 0.5141875147819519 batch: 109/840\n",
      "Batch loss: 0.5728437304496765 batch: 110/840\n",
      "Batch loss: 0.4922439157962799 batch: 111/840\n",
      "Batch loss: 0.7980489134788513 batch: 112/840\n",
      "Batch loss: 0.6867132782936096 batch: 113/840\n",
      "Batch loss: 0.5374689698219299 batch: 114/840\n",
      "Batch loss: 0.5895994305610657 batch: 115/840\n",
      "Batch loss: 0.560984194278717 batch: 116/840\n",
      "Batch loss: 0.7223750948905945 batch: 117/840\n",
      "Batch loss: 0.42967385053634644 batch: 118/840\n",
      "Batch loss: 0.5247234106063843 batch: 119/840\n",
      "Batch loss: 0.6045535802841187 batch: 120/840\n",
      "Batch loss: 0.6964160799980164 batch: 121/840\n",
      "Batch loss: 0.7588688731193542 batch: 122/840\n",
      "Batch loss: 0.5578256845474243 batch: 123/840\n",
      "Batch loss: 0.5515294671058655 batch: 124/840\n",
      "Batch loss: 0.5199217796325684 batch: 125/840\n",
      "Batch loss: 0.6890340447425842 batch: 126/840\n",
      "Batch loss: 0.6620494723320007 batch: 127/840\n",
      "Batch loss: 0.6871533989906311 batch: 128/840\n",
      "Batch loss: 0.7877343893051147 batch: 129/840\n",
      "Batch loss: 0.5739404559135437 batch: 130/840\n",
      "Batch loss: 0.7159053087234497 batch: 131/840\n",
      "Batch loss: 1.0222910642623901 batch: 132/840\n",
      "Batch loss: 0.6988839507102966 batch: 133/840\n",
      "Batch loss: 0.6188451051712036 batch: 134/840\n",
      "Batch loss: 0.4748421609401703 batch: 135/840\n",
      "Batch loss: 0.7792260646820068 batch: 136/840\n",
      "Batch loss: 0.5997462272644043 batch: 137/840\n",
      "Batch loss: 0.5717076659202576 batch: 138/840\n",
      "Batch loss: 0.5237694382667542 batch: 139/840\n",
      "Batch loss: 0.6595207452774048 batch: 140/840\n",
      "Batch loss: 0.4355350136756897 batch: 141/840\n",
      "Batch loss: 0.6084500551223755 batch: 142/840\n",
      "Batch loss: 0.5054313540458679 batch: 143/840\n",
      "Batch loss: 0.535926342010498 batch: 144/840\n",
      "Batch loss: 0.6250299215316772 batch: 145/840\n",
      "Batch loss: 0.6152370572090149 batch: 146/840\n",
      "Batch loss: 0.45909878611564636 batch: 147/840\n",
      "Batch loss: 0.751797616481781 batch: 148/840\n",
      "Batch loss: 0.6822227239608765 batch: 149/840\n",
      "Batch loss: 0.677002489566803 batch: 150/840\n",
      "Batch loss: 0.5003649592399597 batch: 151/840\n",
      "Batch loss: 0.6209072470664978 batch: 152/840\n",
      "Batch loss: 0.49206021428108215 batch: 153/840\n",
      "Batch loss: 0.7565870881080627 batch: 154/840\n",
      "Batch loss: 0.5606335997581482 batch: 155/840\n",
      "Batch loss: 0.6950721740722656 batch: 156/840\n",
      "Batch loss: 0.7586179375648499 batch: 157/840\n",
      "Batch loss: 0.5254136919975281 batch: 158/840\n",
      "Batch loss: 0.5391573905944824 batch: 159/840\n",
      "Batch loss: 0.559238612651825 batch: 160/840\n",
      "Batch loss: 0.6351345181465149 batch: 161/840\n",
      "Batch loss: 0.6952626705169678 batch: 162/840\n",
      "Batch loss: 0.7531952857971191 batch: 163/840\n",
      "Batch loss: 0.41991087794303894 batch: 164/840\n",
      "Batch loss: 0.7019119262695312 batch: 165/840\n",
      "Batch loss: 0.5244496464729309 batch: 166/840\n",
      "Batch loss: 0.6257568597793579 batch: 167/840\n",
      "Batch loss: 0.6680499911308289 batch: 168/840\n",
      "Batch loss: 0.505210816860199 batch: 169/840\n",
      "Batch loss: 0.6530898809432983 batch: 170/840\n",
      "Batch loss: 0.5765596032142639 batch: 171/840\n",
      "Batch loss: 0.6533472537994385 batch: 172/840\n",
      "Batch loss: 0.6130748987197876 batch: 173/840\n",
      "Batch loss: 0.5326606035232544 batch: 174/840\n",
      "Batch loss: 0.5339324474334717 batch: 175/840\n",
      "Batch loss: 0.708071231842041 batch: 176/840\n",
      "Batch loss: 0.6557153463363647 batch: 177/840\n",
      "Batch loss: 0.6452015042304993 batch: 178/840\n",
      "Batch loss: 0.7892102003097534 batch: 179/840\n",
      "Batch loss: 0.5402370691299438 batch: 180/840\n",
      "Batch loss: 0.49937117099761963 batch: 181/840\n",
      "Batch loss: 0.5890459418296814 batch: 182/840\n",
      "Batch loss: 0.6105645298957825 batch: 183/840\n",
      "Batch loss: 0.5545132160186768 batch: 184/840\n",
      "Batch loss: 0.409498393535614 batch: 185/840\n",
      "Batch loss: 0.44438502192497253 batch: 186/840\n",
      "Batch loss: 0.6039928793907166 batch: 187/840\n",
      "Batch loss: 0.5404748320579529 batch: 188/840\n",
      "Batch loss: 0.5937586426734924 batch: 189/840\n",
      "Batch loss: 0.6132490634918213 batch: 190/840\n",
      "Batch loss: 0.7285782098770142 batch: 191/840\n",
      "Batch loss: 0.5188469290733337 batch: 192/840\n",
      "Batch loss: 0.5455370545387268 batch: 193/840\n",
      "Batch loss: 0.4363400638103485 batch: 194/840\n",
      "Batch loss: 0.6034091114997864 batch: 195/840\n",
      "Batch loss: 0.7576190233230591 batch: 196/840\n",
      "Batch loss: 0.6580401062965393 batch: 197/840\n",
      "Batch loss: 0.5203490257263184 batch: 198/840\n",
      "Batch loss: 0.5667948126792908 batch: 199/840\n",
      "Batch loss: 0.7817663550376892 batch: 200/840\n",
      "Batch loss: 0.5346764922142029 batch: 201/840\n",
      "Batch loss: 0.550764262676239 batch: 202/840\n",
      "Batch loss: 0.5563246011734009 batch: 203/840\n",
      "Batch loss: 0.7845974564552307 batch: 204/840\n",
      "Batch loss: 0.799164891242981 batch: 205/840\n",
      "Batch loss: 0.6751624345779419 batch: 206/840\n",
      "Batch loss: 0.5597259998321533 batch: 207/840\n",
      "Batch loss: 0.7120383381843567 batch: 208/840\n",
      "Batch loss: 0.6061113476753235 batch: 209/840\n",
      "Batch loss: 0.530133068561554 batch: 210/840\n",
      "Batch loss: 0.47012147307395935 batch: 211/840\n",
      "Batch loss: 0.6328648328781128 batch: 212/840\n",
      "Batch loss: 0.7064104676246643 batch: 213/840\n",
      "Batch loss: 0.7190476059913635 batch: 214/840\n",
      "Batch loss: 0.562180757522583 batch: 215/840\n",
      "Batch loss: 0.5506166219711304 batch: 216/840\n",
      "Batch loss: 0.6967542767524719 batch: 217/840\n",
      "Batch loss: 0.7145792245864868 batch: 218/840\n",
      "Batch loss: 0.6403088569641113 batch: 219/840\n",
      "Batch loss: 0.8019769787788391 batch: 220/840\n",
      "Batch loss: 0.5783253312110901 batch: 221/840\n",
      "Batch loss: 0.6525318622589111 batch: 222/840\n",
      "Batch loss: 0.5842103362083435 batch: 223/840\n",
      "Batch loss: 0.7024075984954834 batch: 224/840\n",
      "Batch loss: 0.6745067834854126 batch: 225/840\n",
      "Batch loss: 0.6219552755355835 batch: 226/840\n",
      "Batch loss: 0.6728657484054565 batch: 227/840\n",
      "Batch loss: 0.43596571683883667 batch: 228/840\n",
      "Batch loss: 0.4632633328437805 batch: 229/840\n",
      "Batch loss: 0.5698246955871582 batch: 230/840\n",
      "Batch loss: 0.5459584593772888 batch: 231/840\n",
      "Batch loss: 0.690845251083374 batch: 232/840\n",
      "Batch loss: 0.7287687659263611 batch: 233/840\n",
      "Batch loss: 0.49765563011169434 batch: 234/840\n",
      "Batch loss: 0.5679263472557068 batch: 235/840\n",
      "Batch loss: 0.6139625906944275 batch: 236/840\n",
      "Batch loss: 0.6209495663642883 batch: 237/840\n",
      "Batch loss: 0.8321919441223145 batch: 238/840\n",
      "Batch loss: 0.5087875127792358 batch: 239/840\n",
      "Batch loss: 0.590808629989624 batch: 240/840\n",
      "Batch loss: 0.731777548789978 batch: 241/840\n",
      "Batch loss: 0.668211042881012 batch: 242/840\n",
      "Batch loss: 0.5715216994285583 batch: 243/840\n",
      "Batch loss: 0.7069661021232605 batch: 244/840\n",
      "Batch loss: 0.5313319563865662 batch: 245/840\n",
      "Batch loss: 0.6816354990005493 batch: 246/840\n",
      "Batch loss: 0.7017492651939392 batch: 247/840\n",
      "Batch loss: 0.7630067467689514 batch: 248/840\n",
      "Batch loss: 0.6804342865943909 batch: 249/840\n",
      "Batch loss: 0.490859717130661 batch: 250/840\n",
      "Batch loss: 0.6287223696708679 batch: 251/840\n",
      "Batch loss: 0.5720888376235962 batch: 252/840\n",
      "Batch loss: 0.612110435962677 batch: 253/840\n",
      "Batch loss: 0.6300536394119263 batch: 254/840\n",
      "Batch loss: 0.5305514335632324 batch: 255/840\n",
      "Batch loss: 0.6195023059844971 batch: 256/840\n",
      "Batch loss: 0.5538866519927979 batch: 257/840\n",
      "Batch loss: 0.671448290348053 batch: 258/840\n",
      "Batch loss: 0.5049063563346863 batch: 259/840\n",
      "Batch loss: 0.562009871006012 batch: 260/840\n",
      "Batch loss: 0.5227091312408447 batch: 261/840\n",
      "Batch loss: 0.4199252724647522 batch: 262/840\n",
      "Batch loss: 0.6014963388442993 batch: 263/840\n",
      "Batch loss: 0.5918645262718201 batch: 264/840\n",
      "Batch loss: 0.6807258129119873 batch: 265/840\n",
      "Batch loss: 0.6309789419174194 batch: 266/840\n",
      "Batch loss: 0.7123377919197083 batch: 267/840\n",
      "Batch loss: 0.5583942532539368 batch: 268/840\n",
      "Batch loss: 0.5619924664497375 batch: 269/840\n",
      "Batch loss: 0.5927507877349854 batch: 270/840\n",
      "Batch loss: 0.5516918301582336 batch: 271/840\n",
      "Batch loss: 0.8053772449493408 batch: 272/840\n",
      "Batch loss: 0.8707010746002197 batch: 273/840\n",
      "Batch loss: 0.6182580590248108 batch: 274/840\n",
      "Batch loss: 0.6557294726371765 batch: 275/840\n",
      "Batch loss: 0.5393837094306946 batch: 276/840\n",
      "Batch loss: 0.5773231387138367 batch: 277/840\n",
      "Batch loss: 0.7513315677642822 batch: 278/840\n",
      "Batch loss: 0.6599475741386414 batch: 279/840\n",
      "Batch loss: 0.6589648723602295 batch: 280/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6347368359565735 batch: 281/840\n",
      "Batch loss: 0.5841825008392334 batch: 282/840\n",
      "Batch loss: 0.6910359859466553 batch: 283/840\n",
      "Batch loss: 0.4188976585865021 batch: 284/840\n",
      "Batch loss: 0.5642722845077515 batch: 285/840\n",
      "Batch loss: 0.6625900268554688 batch: 286/840\n",
      "Batch loss: 0.44194549322128296 batch: 287/840\n",
      "Batch loss: 0.5055350661277771 batch: 288/840\n",
      "Batch loss: 0.8952553272247314 batch: 289/840\n",
      "Batch loss: 0.6730090975761414 batch: 290/840\n",
      "Batch loss: 0.6497672200202942 batch: 291/840\n",
      "Batch loss: 0.5181186199188232 batch: 292/840\n",
      "Batch loss: 0.6637270450592041 batch: 293/840\n",
      "Batch loss: 0.6071469187736511 batch: 294/840\n",
      "Batch loss: 0.5413346290588379 batch: 295/840\n",
      "Batch loss: 0.6088092923164368 batch: 296/840\n",
      "Batch loss: 0.6883724927902222 batch: 297/840\n",
      "Batch loss: 0.6573560833930969 batch: 298/840\n",
      "Batch loss: 0.5272627472877502 batch: 299/840\n",
      "Batch loss: 0.8262540698051453 batch: 300/840\n",
      "Batch loss: 0.6046696901321411 batch: 301/840\n",
      "Batch loss: 0.6811429858207703 batch: 302/840\n",
      "Batch loss: 0.831360399723053 batch: 303/840\n",
      "Batch loss: 0.5646851062774658 batch: 304/840\n",
      "Batch loss: 0.49551546573638916 batch: 305/840\n",
      "Batch loss: 0.671011209487915 batch: 306/840\n",
      "Batch loss: 0.6623392701148987 batch: 307/840\n",
      "Batch loss: 0.7048860192298889 batch: 308/840\n",
      "Batch loss: 0.6032183766365051 batch: 309/840\n",
      "Batch loss: 0.9096603393554688 batch: 310/840\n",
      "Batch loss: 0.7651520371437073 batch: 311/840\n",
      "Batch loss: 0.6469307541847229 batch: 312/840\n",
      "Batch loss: 0.7092030048370361 batch: 313/840\n",
      "Batch loss: 0.5174869298934937 batch: 314/840\n",
      "Batch loss: 0.6211822628974915 batch: 315/840\n",
      "Batch loss: 0.4526265263557434 batch: 316/840\n",
      "Batch loss: 0.6375110745429993 batch: 317/840\n",
      "Batch loss: 0.65230393409729 batch: 318/840\n",
      "Batch loss: 0.7158960700035095 batch: 319/840\n",
      "Batch loss: 0.5466465353965759 batch: 320/840\n",
      "Batch loss: 0.5978412628173828 batch: 321/840\n",
      "Batch loss: 0.6573398113250732 batch: 322/840\n",
      "Batch loss: 0.6698611974716187 batch: 323/840\n",
      "Batch loss: 0.6334651708602905 batch: 324/840\n",
      "Batch loss: 0.4413107633590698 batch: 325/840\n",
      "Batch loss: 0.615068256855011 batch: 326/840\n",
      "Batch loss: 0.47458353638648987 batch: 327/840\n",
      "Batch loss: 0.7941391468048096 batch: 328/840\n",
      "Batch loss: 0.7269712090492249 batch: 329/840\n",
      "Batch loss: 0.6962332725524902 batch: 330/840\n",
      "Batch loss: 0.7729238271713257 batch: 331/840\n",
      "Batch loss: 0.6862074136734009 batch: 332/840\n",
      "Batch loss: 0.5410665273666382 batch: 333/840\n",
      "Batch loss: 0.6449762582778931 batch: 334/840\n",
      "Batch loss: 0.5800111889839172 batch: 335/840\n",
      "Batch loss: 0.6649373769760132 batch: 336/840\n",
      "Batch loss: 0.7240064740180969 batch: 337/840\n",
      "Batch loss: 0.7483857870101929 batch: 338/840\n",
      "Batch loss: 0.4730881154537201 batch: 339/840\n",
      "Batch loss: 0.7567125558853149 batch: 340/840\n",
      "Batch loss: 0.5136144757270813 batch: 341/840\n",
      "Batch loss: 0.448735773563385 batch: 342/840\n",
      "Batch loss: 0.8737959861755371 batch: 343/840\n",
      "Batch loss: 0.6251614689826965 batch: 344/840\n",
      "Batch loss: 0.449726402759552 batch: 345/840\n",
      "Batch loss: 0.6012592911720276 batch: 346/840\n",
      "Batch loss: 0.6468419432640076 batch: 347/840\n",
      "Batch loss: 0.5861223340034485 batch: 348/840\n",
      "Batch loss: 0.7204253673553467 batch: 349/840\n",
      "Batch loss: 0.5814214944839478 batch: 350/840\n",
      "Batch loss: 0.6866165995597839 batch: 351/840\n",
      "Batch loss: 0.5967938303947449 batch: 352/840\n",
      "Batch loss: 0.7089378237724304 batch: 353/840\n",
      "Batch loss: 0.6047922372817993 batch: 354/840\n",
      "Batch loss: 0.49097731709480286 batch: 355/840\n",
      "Batch loss: 0.613430380821228 batch: 356/840\n",
      "Batch loss: 0.5123084783554077 batch: 357/840\n",
      "Batch loss: 0.822771430015564 batch: 358/840\n",
      "Batch loss: 0.5977579355239868 batch: 359/840\n",
      "Batch loss: 0.8137902021408081 batch: 360/840\n",
      "Batch loss: 0.7422086596488953 batch: 361/840\n",
      "Batch loss: 0.5307857990264893 batch: 362/840\n",
      "Batch loss: 0.6133518218994141 batch: 363/840\n",
      "Batch loss: 0.7351024746894836 batch: 364/840\n",
      "Batch loss: 0.5530338883399963 batch: 365/840\n",
      "Batch loss: 0.6449033617973328 batch: 366/840\n",
      "Batch loss: 0.4042026102542877 batch: 367/840\n",
      "Batch loss: 0.7521933913230896 batch: 368/840\n",
      "Batch loss: 0.6257582306861877 batch: 369/840\n",
      "Batch loss: 0.6866937875747681 batch: 370/840\n",
      "Batch loss: 0.641246497631073 batch: 371/840\n",
      "Batch loss: 0.45519545674324036 batch: 372/840\n",
      "Batch loss: 0.6225239038467407 batch: 373/840\n",
      "Batch loss: 0.6619026064872742 batch: 374/840\n",
      "Batch loss: 0.5559588074684143 batch: 375/840\n",
      "Batch loss: 0.4737117290496826 batch: 376/840\n",
      "Batch loss: 0.7103269100189209 batch: 377/840\n",
      "Batch loss: 0.6428906321525574 batch: 378/840\n",
      "Batch loss: 0.5315451622009277 batch: 379/840\n",
      "Batch loss: 0.9403800368309021 batch: 380/840\n",
      "Batch loss: 0.8460927605628967 batch: 381/840\n",
      "Batch loss: 0.6498625874519348 batch: 382/840\n",
      "Batch loss: 0.6089039444923401 batch: 383/840\n",
      "Batch loss: 0.5816152095794678 batch: 384/840\n",
      "Batch loss: 0.6825273036956787 batch: 385/840\n",
      "Batch loss: 0.7573367953300476 batch: 386/840\n",
      "Batch loss: 0.5772077441215515 batch: 387/840\n",
      "Batch loss: 0.7148931622505188 batch: 388/840\n",
      "Batch loss: 0.5346153378486633 batch: 389/840\n",
      "Batch loss: 0.7576436400413513 batch: 390/840\n",
      "Batch loss: 0.6574561595916748 batch: 391/840\n",
      "Batch loss: 0.5529409646987915 batch: 392/840\n",
      "Batch loss: 0.486587256193161 batch: 393/840\n",
      "Batch loss: 0.8565599322319031 batch: 394/840\n",
      "Batch loss: 0.5155708193778992 batch: 395/840\n",
      "Batch loss: 0.664919912815094 batch: 396/840\n",
      "Batch loss: 0.6386785507202148 batch: 397/840\n",
      "Batch loss: 0.7045203447341919 batch: 398/840\n",
      "Batch loss: 0.5027633905410767 batch: 399/840\n",
      "Batch loss: 0.5500088334083557 batch: 400/840\n",
      "Batch loss: 0.6479625701904297 batch: 401/840\n",
      "Batch loss: 0.48843058943748474 batch: 402/840\n",
      "Batch loss: 0.6820830702781677 batch: 403/840\n",
      "Batch loss: 0.6472915410995483 batch: 404/840\n",
      "Batch loss: 0.6348812580108643 batch: 405/840\n",
      "Batch loss: 0.683036744594574 batch: 406/840\n",
      "Batch loss: 0.5157849788665771 batch: 407/840\n",
      "Batch loss: 0.6922891736030579 batch: 408/840\n",
      "Batch loss: 0.6785176992416382 batch: 409/840\n",
      "Batch loss: 0.715877890586853 batch: 410/840\n",
      "Batch loss: 0.6920410990715027 batch: 411/840\n",
      "Batch loss: 0.7366981506347656 batch: 412/840\n",
      "Batch loss: 0.5970507264137268 batch: 413/840\n",
      "Batch loss: 0.5615819692611694 batch: 414/840\n",
      "Batch loss: 0.7674899101257324 batch: 415/840\n",
      "Batch loss: 0.46398788690567017 batch: 416/840\n",
      "Batch loss: 0.6174025535583496 batch: 417/840\n",
      "Batch loss: 0.8162781000137329 batch: 418/840\n",
      "Batch loss: 0.5310090184211731 batch: 419/840\n",
      "Batch loss: 0.5466639995574951 batch: 420/840\n",
      "Batch loss: 0.46511349081993103 batch: 421/840\n",
      "Batch loss: 0.5221843123435974 batch: 422/840\n",
      "Batch loss: 0.6721296906471252 batch: 423/840\n",
      "Batch loss: 0.6126870512962341 batch: 424/840\n",
      "Batch loss: 0.6546462178230286 batch: 425/840\n",
      "Batch loss: 0.6237757205963135 batch: 426/840\n",
      "Batch loss: 0.6877424120903015 batch: 427/840\n",
      "Batch loss: 0.7588188052177429 batch: 428/840\n",
      "Batch loss: 0.5738441944122314 batch: 429/840\n",
      "Batch loss: 0.8197635412216187 batch: 430/840\n",
      "Batch loss: 0.6406444311141968 batch: 431/840\n",
      "Batch loss: 0.6268097162246704 batch: 432/840\n",
      "Batch loss: 0.4573434591293335 batch: 433/840\n",
      "Batch loss: 0.52681565284729 batch: 434/840\n",
      "Batch loss: 0.7278614640235901 batch: 435/840\n",
      "Batch loss: 0.6193475127220154 batch: 436/840\n",
      "Batch loss: 0.7260099053382874 batch: 437/840\n",
      "Batch loss: 0.4175192713737488 batch: 438/840\n",
      "Batch loss: 0.6747228503227234 batch: 439/840\n",
      "Batch loss: 0.7819319367408752 batch: 440/840\n",
      "Batch loss: 0.5669706463813782 batch: 441/840\n",
      "Batch loss: 0.6652353405952454 batch: 442/840\n",
      "Batch loss: 0.5425652861595154 batch: 443/840\n",
      "Batch loss: 0.6244065761566162 batch: 444/840\n",
      "Batch loss: 0.5807908773422241 batch: 445/840\n",
      "Batch loss: 0.6967543959617615 batch: 446/840\n",
      "Batch loss: 0.7204495072364807 batch: 447/840\n",
      "Batch loss: 0.7176653146743774 batch: 448/840\n",
      "Batch loss: 0.5449273586273193 batch: 449/840\n",
      "Batch loss: 0.605463445186615 batch: 450/840\n",
      "Batch loss: 0.6945136785507202 batch: 451/840\n",
      "Batch loss: 0.6796191930770874 batch: 452/840\n",
      "Batch loss: 0.5846362113952637 batch: 453/840\n",
      "Batch loss: 0.5103972554206848 batch: 454/840\n",
      "Batch loss: 0.6558580994606018 batch: 455/840\n",
      "Batch loss: 0.6220605373382568 batch: 456/840\n",
      "Batch loss: 0.5269351005554199 batch: 457/840\n",
      "Batch loss: 0.5979527235031128 batch: 458/840\n",
      "Batch loss: 0.5517020225524902 batch: 459/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4559035003185272 batch: 460/840\n",
      "Batch loss: 0.6985692381858826 batch: 461/840\n",
      "Batch loss: 0.5903876423835754 batch: 462/840\n",
      "Batch loss: 0.6932179927825928 batch: 463/840\n",
      "Batch loss: 0.5406429767608643 batch: 464/840\n",
      "Batch loss: 0.8148140907287598 batch: 465/840\n",
      "Batch loss: 0.6921818256378174 batch: 466/840\n",
      "Batch loss: 0.8492562174797058 batch: 467/840\n",
      "Batch loss: 0.5001876354217529 batch: 468/840\n",
      "Batch loss: 0.5583797693252563 batch: 469/840\n",
      "Batch loss: 0.5937869548797607 batch: 470/840\n",
      "Batch loss: 0.5899854898452759 batch: 471/840\n",
      "Batch loss: 0.7263176441192627 batch: 472/840\n",
      "Batch loss: 0.7579324841499329 batch: 473/840\n",
      "Batch loss: 0.5620824694633484 batch: 474/840\n",
      "Batch loss: 0.5539972186088562 batch: 475/840\n",
      "Batch loss: 0.5890229940414429 batch: 476/840\n",
      "Batch loss: 0.5256059169769287 batch: 477/840\n",
      "Batch loss: 0.43296322226524353 batch: 478/840\n",
      "Batch loss: 0.4556267559528351 batch: 479/840\n",
      "Batch loss: 0.6446810364723206 batch: 480/840\n",
      "Batch loss: 0.6221598386764526 batch: 481/840\n",
      "Batch loss: 0.7048999071121216 batch: 482/840\n",
      "Batch loss: 0.8119864463806152 batch: 483/840\n",
      "Batch loss: 0.44721758365631104 batch: 484/840\n",
      "Batch loss: 0.5613517165184021 batch: 485/840\n",
      "Batch loss: 0.6266236901283264 batch: 486/840\n",
      "Batch loss: 0.43929222226142883 batch: 487/840\n",
      "Batch loss: 0.6622074842453003 batch: 488/840\n",
      "Batch loss: 0.5826254487037659 batch: 489/840\n",
      "Batch loss: 0.675179660320282 batch: 490/840\n",
      "Batch loss: 0.37719330191612244 batch: 491/840\n",
      "Batch loss: 0.6617639660835266 batch: 492/840\n",
      "Batch loss: 0.6792087554931641 batch: 493/840\n",
      "Batch loss: 0.4033249616622925 batch: 494/840\n",
      "Batch loss: 0.6810980439186096 batch: 495/840\n",
      "Batch loss: 0.6374233365058899 batch: 496/840\n",
      "Batch loss: 0.7519327402114868 batch: 497/840\n",
      "Batch loss: 0.4206940531730652 batch: 498/840\n",
      "Batch loss: 0.6574695110321045 batch: 499/840\n",
      "Batch loss: 0.6043276786804199 batch: 500/840\n",
      "Batch loss: 0.522233784198761 batch: 501/840\n",
      "Batch loss: 0.7371082901954651 batch: 502/840\n",
      "Batch loss: 0.5553089380264282 batch: 503/840\n",
      "Batch loss: 0.5917670726776123 batch: 504/840\n",
      "Batch loss: 0.6529048085212708 batch: 505/840\n",
      "Batch loss: 0.47412383556365967 batch: 506/840\n",
      "Batch loss: 0.7245979905128479 batch: 507/840\n",
      "Batch loss: 0.5709931254386902 batch: 508/840\n",
      "Batch loss: 0.715624213218689 batch: 509/840\n",
      "Batch loss: 0.6726927161216736 batch: 510/840\n",
      "Batch loss: 0.49961644411087036 batch: 511/840\n",
      "Batch loss: 0.6724191308021545 batch: 512/840\n",
      "Batch loss: 0.5362192988395691 batch: 513/840\n",
      "Batch loss: 0.6239349842071533 batch: 514/840\n",
      "Batch loss: 0.4622374475002289 batch: 515/840\n",
      "Batch loss: 0.730492115020752 batch: 516/840\n",
      "Batch loss: 0.6402668952941895 batch: 517/840\n",
      "Batch loss: 0.7301598191261292 batch: 518/840\n",
      "Batch loss: 0.7605190873146057 batch: 519/840\n",
      "Batch loss: 0.6631010174751282 batch: 520/840\n",
      "Batch loss: 0.631350040435791 batch: 521/840\n",
      "Batch loss: 0.5980948805809021 batch: 522/840\n",
      "Batch loss: 0.46936455368995667 batch: 523/840\n",
      "Batch loss: 0.6872487664222717 batch: 524/840\n",
      "Batch loss: 0.5251563191413879 batch: 525/840\n",
      "Batch loss: 0.7621906995773315 batch: 526/840\n",
      "Batch loss: 0.6256245970726013 batch: 527/840\n",
      "Batch loss: 0.5552690029144287 batch: 528/840\n",
      "Batch loss: 0.49881067872047424 batch: 529/840\n",
      "Batch loss: 0.6535261273384094 batch: 530/840\n",
      "Batch loss: 0.5241398811340332 batch: 531/840\n",
      "Batch loss: 0.4910661578178406 batch: 532/840\n",
      "Batch loss: 0.7170085906982422 batch: 533/840\n",
      "Batch loss: 0.5714423656463623 batch: 534/840\n",
      "Batch loss: 0.5948826670646667 batch: 535/840\n",
      "Batch loss: 0.7203524708747864 batch: 536/840\n",
      "Batch loss: 0.6401268839836121 batch: 537/840\n",
      "Batch loss: 0.5514928698539734 batch: 538/840\n",
      "Batch loss: 0.5261951684951782 batch: 539/840\n",
      "Batch loss: 0.7093557119369507 batch: 540/840\n",
      "Batch loss: 0.6314501762390137 batch: 541/840\n",
      "Batch loss: 0.6697046756744385 batch: 542/840\n",
      "Batch loss: 0.45903250575065613 batch: 543/840\n",
      "Batch loss: 0.6588042974472046 batch: 544/840\n",
      "Batch loss: 0.5928634405136108 batch: 545/840\n",
      "Batch loss: 0.5903505682945251 batch: 546/840\n",
      "Batch loss: 0.5718152523040771 batch: 547/840\n",
      "Batch loss: 0.5083886981010437 batch: 548/840\n",
      "Batch loss: 0.5001492500305176 batch: 549/840\n",
      "Batch loss: 0.5024921894073486 batch: 550/840\n",
      "Batch loss: 0.6178650856018066 batch: 551/840\n",
      "Batch loss: 0.4987432360649109 batch: 552/840\n",
      "Batch loss: 0.7172516584396362 batch: 553/840\n",
      "Batch loss: 0.6480329036712646 batch: 554/840\n",
      "Batch loss: 0.6529022455215454 batch: 555/840\n",
      "Batch loss: 0.5917108654975891 batch: 556/840\n",
      "Batch loss: 0.6897563338279724 batch: 557/840\n",
      "Batch loss: 0.5887235403060913 batch: 558/840\n",
      "Batch loss: 0.6014201045036316 batch: 559/840\n",
      "Batch loss: 0.6391971111297607 batch: 560/840\n",
      "Batch loss: 0.6243188381195068 batch: 561/840\n",
      "Batch loss: 0.557532548904419 batch: 562/840\n",
      "Batch loss: 0.3649376332759857 batch: 563/840\n",
      "Batch loss: 0.6927154660224915 batch: 564/840\n",
      "Batch loss: 0.8055208325386047 batch: 565/840\n",
      "Batch loss: 0.6374661922454834 batch: 566/840\n",
      "Batch loss: 0.6391894221305847 batch: 567/840\n",
      "Batch loss: 0.5890471339225769 batch: 568/840\n",
      "Batch loss: 0.621655285358429 batch: 569/840\n",
      "Batch loss: 0.4187203347682953 batch: 570/840\n",
      "Batch loss: 0.8258291482925415 batch: 571/840\n",
      "Batch loss: 0.6544971466064453 batch: 572/840\n",
      "Batch loss: 0.7486458420753479 batch: 573/840\n",
      "Batch loss: 0.6883310079574585 batch: 574/840\n",
      "Batch loss: 0.5190992951393127 batch: 575/840\n",
      "Batch loss: 0.560042142868042 batch: 576/840\n",
      "Batch loss: 0.5293171405792236 batch: 577/840\n",
      "Batch loss: 0.745337963104248 batch: 578/840\n",
      "Batch loss: 0.6140998005867004 batch: 579/840\n",
      "Batch loss: 0.7845144867897034 batch: 580/840\n",
      "Batch loss: 0.6287998557090759 batch: 581/840\n",
      "Batch loss: 0.7201262712478638 batch: 582/840\n",
      "Batch loss: 0.618053138256073 batch: 583/840\n",
      "Batch loss: 0.770399272441864 batch: 584/840\n",
      "Batch loss: 0.6739243268966675 batch: 585/840\n",
      "Batch loss: 0.7095028162002563 batch: 586/840\n",
      "Batch loss: 0.5314176082611084 batch: 587/840\n",
      "Batch loss: 0.5773276686668396 batch: 588/840\n",
      "Batch loss: 0.6629116535186768 batch: 589/840\n",
      "Batch loss: 0.49788525700569153 batch: 590/840\n",
      "Batch loss: 0.4055730402469635 batch: 591/840\n",
      "Batch loss: 0.5750631093978882 batch: 592/840\n",
      "Batch loss: 0.6949238777160645 batch: 593/840\n",
      "Batch loss: 0.6346212029457092 batch: 594/840\n",
      "Batch loss: 0.36293256282806396 batch: 595/840\n",
      "Batch loss: 0.5252074599266052 batch: 596/840\n",
      "Batch loss: 0.5610987544059753 batch: 597/840\n",
      "Batch loss: 0.3853023052215576 batch: 598/840\n",
      "Batch loss: 0.5260631442070007 batch: 599/840\n",
      "Batch loss: 0.792061448097229 batch: 600/840\n",
      "Batch loss: 0.7673184275627136 batch: 601/840\n",
      "Batch loss: 0.6401754021644592 batch: 602/840\n",
      "Batch loss: 0.7118814587593079 batch: 603/840\n",
      "Batch loss: 0.5763694047927856 batch: 604/840\n",
      "Batch loss: 0.7408193945884705 batch: 605/840\n",
      "Batch loss: 0.7934651374816895 batch: 606/840\n",
      "Batch loss: 0.6147011518478394 batch: 607/840\n",
      "Batch loss: 0.6102240681648254 batch: 608/840\n",
      "Batch loss: 0.47572121024131775 batch: 609/840\n",
      "Batch loss: 0.672698974609375 batch: 610/840\n",
      "Batch loss: 0.5938866138458252 batch: 611/840\n",
      "Batch loss: 0.657716691493988 batch: 612/840\n",
      "Batch loss: 0.6686482429504395 batch: 613/840\n",
      "Batch loss: 0.5513553023338318 batch: 614/840\n",
      "Batch loss: 0.5622712969779968 batch: 615/840\n",
      "Batch loss: 0.5808941125869751 batch: 616/840\n",
      "Batch loss: 0.4953269958496094 batch: 617/840\n",
      "Batch loss: 0.5951573252677917 batch: 618/840\n",
      "Batch loss: 0.7239896655082703 batch: 619/840\n",
      "Batch loss: 0.5608450174331665 batch: 620/840\n",
      "Batch loss: 0.6289771199226379 batch: 621/840\n",
      "Batch loss: 0.630210280418396 batch: 622/840\n",
      "Batch loss: 0.5361269116401672 batch: 623/840\n",
      "Batch loss: 0.7675983309745789 batch: 624/840\n",
      "Batch loss: 0.684762179851532 batch: 625/840\n",
      "Batch loss: 0.6341497302055359 batch: 626/840\n",
      "Batch loss: 0.6066765785217285 batch: 627/840\n",
      "Batch loss: 0.6253359913825989 batch: 628/840\n",
      "Batch loss: 0.6867725849151611 batch: 629/840\n",
      "Batch loss: 0.5694056153297424 batch: 630/840\n",
      "Batch loss: 0.6284845471382141 batch: 631/840\n",
      "Batch loss: 0.7143133282661438 batch: 632/840\n",
      "Batch loss: 0.513904333114624 batch: 633/840\n",
      "Batch loss: 0.5916696786880493 batch: 634/840\n",
      "Batch loss: 0.5432551503181458 batch: 635/840\n",
      "Batch loss: 0.5241740942001343 batch: 636/840\n",
      "Batch loss: 0.4587455689907074 batch: 637/840\n",
      "Batch loss: 0.6512502431869507 batch: 638/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6829483509063721 batch: 639/840\n",
      "Batch loss: 0.5164150595664978 batch: 640/840\n",
      "Batch loss: 0.832904040813446 batch: 641/840\n",
      "Batch loss: 0.5813828110694885 batch: 642/840\n",
      "Batch loss: 0.8516293168067932 batch: 643/840\n",
      "Batch loss: 0.5536300539970398 batch: 644/840\n",
      "Batch loss: 0.5381994843482971 batch: 645/840\n",
      "Batch loss: 0.5818721652030945 batch: 646/840\n",
      "Batch loss: 0.6366590857505798 batch: 647/840\n",
      "Batch loss: 0.627524197101593 batch: 648/840\n",
      "Batch loss: 0.6363242864608765 batch: 649/840\n",
      "Batch loss: 0.5277549028396606 batch: 650/840\n",
      "Batch loss: 0.6065276861190796 batch: 651/840\n",
      "Batch loss: 0.7135367393493652 batch: 652/840\n",
      "Batch loss: 0.46122032403945923 batch: 653/840\n",
      "Batch loss: 0.9576066732406616 batch: 654/840\n",
      "Batch loss: 0.5838545560836792 batch: 655/840\n",
      "Batch loss: 0.5272199511528015 batch: 656/840\n",
      "Batch loss: 0.5417273044586182 batch: 657/840\n",
      "Batch loss: 0.5683385729789734 batch: 658/840\n",
      "Batch loss: 0.7289000153541565 batch: 659/840\n",
      "Batch loss: 0.5465776324272156 batch: 660/840\n",
      "Batch loss: 0.7092772722244263 batch: 661/840\n",
      "Batch loss: 0.6724178791046143 batch: 662/840\n",
      "Batch loss: 0.5974462032318115 batch: 663/840\n",
      "Batch loss: 0.6528881788253784 batch: 664/840\n",
      "Batch loss: 0.7103700041770935 batch: 665/840\n",
      "Batch loss: 0.6993297338485718 batch: 666/840\n",
      "Batch loss: 0.7461307644844055 batch: 667/840\n",
      "Batch loss: 0.5746815204620361 batch: 668/840\n",
      "Batch loss: 0.6023868918418884 batch: 669/840\n",
      "Batch loss: 0.6432543992996216 batch: 670/840\n",
      "Batch loss: 0.6802797913551331 batch: 671/840\n",
      "Batch loss: 0.5071035623550415 batch: 672/840\n",
      "Batch loss: 0.8530562520027161 batch: 673/840\n",
      "Batch loss: 0.7338521480560303 batch: 674/840\n",
      "Batch loss: 0.5664899349212646 batch: 675/840\n",
      "Batch loss: 0.6100568175315857 batch: 676/840\n",
      "Batch loss: 0.66742342710495 batch: 677/840\n",
      "Batch loss: 0.7012967467308044 batch: 678/840\n",
      "Batch loss: 0.8191478848457336 batch: 679/840\n",
      "Batch loss: 0.6580959558486938 batch: 680/840\n",
      "Batch loss: 0.6508958339691162 batch: 681/840\n",
      "Batch loss: 0.663603663444519 batch: 682/840\n",
      "Batch loss: 0.5346840023994446 batch: 683/840\n",
      "Batch loss: 0.8063697814941406 batch: 684/840\n",
      "Batch loss: 0.6315262317657471 batch: 685/840\n",
      "Batch loss: 0.7500048279762268 batch: 686/840\n",
      "Batch loss: 0.8155550360679626 batch: 687/840\n",
      "Batch loss: 0.5774939060211182 batch: 688/840\n",
      "Batch loss: 0.5327341556549072 batch: 689/840\n",
      "Batch loss: 0.5106289386749268 batch: 690/840\n",
      "Batch loss: 0.6981445550918579 batch: 691/840\n",
      "Batch loss: 0.6002148389816284 batch: 692/840\n",
      "Batch loss: 0.6366092562675476 batch: 693/840\n",
      "Batch loss: 0.7981806397438049 batch: 694/840\n",
      "Batch loss: 0.7313670516014099 batch: 695/840\n",
      "Batch loss: 0.6061763167381287 batch: 696/840\n",
      "Batch loss: 0.6119241714477539 batch: 697/840\n",
      "Batch loss: 0.5789543390274048 batch: 698/840\n",
      "Batch loss: 0.7268092632293701 batch: 699/840\n",
      "Batch loss: 0.7093021869659424 batch: 700/840\n",
      "Batch loss: 0.7356856465339661 batch: 701/840\n",
      "Batch loss: 0.6353018283843994 batch: 702/840\n",
      "Batch loss: 0.5808401107788086 batch: 703/840\n",
      "Batch loss: 0.6463239789009094 batch: 704/840\n",
      "Batch loss: 0.5819241404533386 batch: 705/840\n",
      "Batch loss: 0.6003208160400391 batch: 706/840\n",
      "Batch loss: 0.6768446564674377 batch: 707/840\n",
      "Batch loss: 0.7221214771270752 batch: 708/840\n",
      "Batch loss: 0.7099047899246216 batch: 709/840\n",
      "Batch loss: 0.8720389008522034 batch: 710/840\n",
      "Batch loss: 0.43140166997909546 batch: 711/840\n",
      "Batch loss: 0.7756777405738831 batch: 712/840\n",
      "Batch loss: 0.5460973381996155 batch: 713/840\n",
      "Batch loss: 0.643028974533081 batch: 714/840\n",
      "Batch loss: 0.7962911128997803 batch: 715/840\n",
      "Batch loss: 0.6516765356063843 batch: 716/840\n",
      "Batch loss: 0.6575716137886047 batch: 717/840\n",
      "Batch loss: 0.6466549634933472 batch: 718/840\n",
      "Batch loss: 0.5128429532051086 batch: 719/840\n",
      "Batch loss: 0.5961570143699646 batch: 720/840\n",
      "Batch loss: 0.7966455221176147 batch: 721/840\n",
      "Batch loss: 0.8440302014350891 batch: 722/840\n",
      "Batch loss: 0.4541102647781372 batch: 723/840\n",
      "Batch loss: 0.7526270151138306 batch: 724/840\n",
      "Batch loss: 0.6334419846534729 batch: 725/840\n",
      "Batch loss: 0.4872608184814453 batch: 726/840\n",
      "Batch loss: 0.6961987018585205 batch: 727/840\n",
      "Batch loss: 0.7519493699073792 batch: 728/840\n",
      "Batch loss: 0.6222428679466248 batch: 729/840\n",
      "Batch loss: 0.5946080684661865 batch: 730/840\n",
      "Batch loss: 0.5192051529884338 batch: 731/840\n",
      "Batch loss: 0.9016870856285095 batch: 732/840\n",
      "Batch loss: 0.6427620649337769 batch: 733/840\n",
      "Batch loss: 0.5524643659591675 batch: 734/840\n",
      "Batch loss: 0.6654998064041138 batch: 735/840\n",
      "Batch loss: 0.5807082653045654 batch: 736/840\n",
      "Batch loss: 0.6421259045600891 batch: 737/840\n",
      "Batch loss: 0.4556953012943268 batch: 738/840\n",
      "Batch loss: 0.7183523774147034 batch: 739/840\n",
      "Batch loss: 0.6486084461212158 batch: 740/840\n",
      "Batch loss: 0.514064371585846 batch: 741/840\n",
      "Batch loss: 0.4752561151981354 batch: 742/840\n",
      "Batch loss: 0.46666625142097473 batch: 743/840\n",
      "Batch loss: 0.5210162997245789 batch: 744/840\n",
      "Batch loss: 0.5552288293838501 batch: 745/840\n",
      "Batch loss: 0.6464323401451111 batch: 746/840\n",
      "Batch loss: 0.7105973958969116 batch: 747/840\n",
      "Batch loss: 0.6223199367523193 batch: 748/840\n",
      "Batch loss: 0.4951223134994507 batch: 749/840\n",
      "Batch loss: 0.478508859872818 batch: 750/840\n",
      "Batch loss: 0.6384923458099365 batch: 751/840\n",
      "Batch loss: 0.9119808077812195 batch: 752/840\n",
      "Batch loss: 0.5271556377410889 batch: 753/840\n",
      "Batch loss: 0.6159495711326599 batch: 754/840\n",
      "Batch loss: 0.5853685736656189 batch: 755/840\n",
      "Batch loss: 0.5146971344947815 batch: 756/840\n",
      "Batch loss: 0.7506248354911804 batch: 757/840\n",
      "Batch loss: 0.5817458629608154 batch: 758/840\n",
      "Batch loss: 0.5184056162834167 batch: 759/840\n",
      "Batch loss: 0.528980016708374 batch: 760/840\n",
      "Batch loss: 0.5129719972610474 batch: 761/840\n",
      "Batch loss: 0.7205377817153931 batch: 762/840\n",
      "Batch loss: 0.47291508316993713 batch: 763/840\n",
      "Batch loss: 0.6025996208190918 batch: 764/840\n",
      "Batch loss: 0.45019692182540894 batch: 765/840\n",
      "Batch loss: 0.5818818211555481 batch: 766/840\n",
      "Batch loss: 0.6903079748153687 batch: 767/840\n",
      "Batch loss: 0.6365746855735779 batch: 768/840\n",
      "Batch loss: 0.6355825066566467 batch: 769/840\n",
      "Batch loss: 0.5372568368911743 batch: 770/840\n",
      "Batch loss: 0.5753286480903625 batch: 771/840\n",
      "Batch loss: 0.529055118560791 batch: 772/840\n",
      "Batch loss: 0.5220451951026917 batch: 773/840\n",
      "Batch loss: 0.4308375120162964 batch: 774/840\n",
      "Batch loss: 0.5965903997421265 batch: 775/840\n",
      "Batch loss: 0.4809993505477905 batch: 776/840\n",
      "Batch loss: 0.5827023983001709 batch: 777/840\n",
      "Batch loss: 0.43673405051231384 batch: 778/840\n",
      "Batch loss: 0.7772752642631531 batch: 779/840\n",
      "Batch loss: 0.5574645400047302 batch: 780/840\n",
      "Batch loss: 0.677864134311676 batch: 781/840\n",
      "Batch loss: 0.602199375629425 batch: 782/840\n",
      "Batch loss: 0.46263357996940613 batch: 783/840\n",
      "Batch loss: 0.6563425660133362 batch: 784/840\n",
      "Batch loss: 0.5343401432037354 batch: 785/840\n",
      "Batch loss: 0.5673997402191162 batch: 786/840\n",
      "Batch loss: 0.6038674712181091 batch: 787/840\n",
      "Batch loss: 0.7175063490867615 batch: 788/840\n",
      "Batch loss: 0.7046661972999573 batch: 789/840\n",
      "Batch loss: 0.6989863514900208 batch: 790/840\n",
      "Batch loss: 0.5982017517089844 batch: 791/840\n",
      "Batch loss: 0.37963563203811646 batch: 792/840\n",
      "Batch loss: 0.5815944671630859 batch: 793/840\n",
      "Batch loss: 0.5772024393081665 batch: 794/840\n",
      "Batch loss: 0.5227156281471252 batch: 795/840\n",
      "Batch loss: 0.6825637817382812 batch: 796/840\n",
      "Batch loss: 0.6087549924850464 batch: 797/840\n",
      "Batch loss: 0.5992975234985352 batch: 798/840\n",
      "Batch loss: 0.6216323375701904 batch: 799/840\n",
      "Batch loss: 0.5444793701171875 batch: 800/840\n",
      "Batch loss: 0.758574903011322 batch: 801/840\n",
      "Batch loss: 0.47308576107025146 batch: 802/840\n",
      "Batch loss: 0.46111592650413513 batch: 803/840\n",
      "Batch loss: 0.6278155446052551 batch: 804/840\n",
      "Batch loss: 0.5474757552146912 batch: 805/840\n",
      "Batch loss: 0.6094765663146973 batch: 806/840\n",
      "Batch loss: 0.7319166660308838 batch: 807/840\n",
      "Batch loss: 0.5526302456855774 batch: 808/840\n",
      "Batch loss: 0.600649356842041 batch: 809/840\n",
      "Batch loss: 0.6031404137611389 batch: 810/840\n",
      "Batch loss: 0.5075522065162659 batch: 811/840\n",
      "Batch loss: 0.619204044342041 batch: 812/840\n",
      "Batch loss: 0.47737061977386475 batch: 813/840\n",
      "Batch loss: 0.6208605766296387 batch: 814/840\n",
      "Batch loss: 0.695960521697998 batch: 815/840\n",
      "Batch loss: 0.5930094718933105 batch: 816/840\n",
      "Batch loss: 0.6817011833190918 batch: 817/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7062437534332275 batch: 818/840\n",
      "Batch loss: 0.526105523109436 batch: 819/840\n",
      "Batch loss: 0.6460859179496765 batch: 820/840\n",
      "Batch loss: 0.6787360310554504 batch: 821/840\n",
      "Batch loss: 0.5791730284690857 batch: 822/840\n",
      "Batch loss: 0.7560252547264099 batch: 823/840\n",
      "Batch loss: 0.8068123459815979 batch: 824/840\n",
      "Batch loss: 0.6423029899597168 batch: 825/840\n",
      "Batch loss: 0.5552993416786194 batch: 826/840\n",
      "Batch loss: 0.5475264191627502 batch: 827/840\n",
      "Batch loss: 0.6749385595321655 batch: 828/840\n",
      "Batch loss: 0.5850094556808472 batch: 829/840\n",
      "Batch loss: 0.7395409345626831 batch: 830/840\n",
      "Batch loss: 0.5961737632751465 batch: 831/840\n",
      "Batch loss: 0.7108189463615417 batch: 832/840\n",
      "Batch loss: 0.6469309329986572 batch: 833/840\n",
      "Batch loss: 0.5631629824638367 batch: 834/840\n",
      "Batch loss: 0.47077882289886475 batch: 835/840\n",
      "Batch loss: 0.6336544156074524 batch: 836/840\n",
      "Batch loss: 0.6452784538269043 batch: 837/840\n",
      "Batch loss: 0.6852951049804688 batch: 838/840\n",
      "Batch loss: 0.5256032943725586 batch: 839/840\n",
      "Batch loss: 0.5588181018829346 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 13/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.822\n",
      "Running epoch 14/15\n",
      "Batch loss: 0.5434228181838989 batch: 1/840\n",
      "Batch loss: 1.0146644115447998 batch: 2/840\n",
      "Batch loss: 0.6566750407218933 batch: 3/840\n",
      "Batch loss: 0.6102261543273926 batch: 4/840\n",
      "Batch loss: 0.5964108109474182 batch: 5/840\n",
      "Batch loss: 0.5179630517959595 batch: 6/840\n",
      "Batch loss: 0.5236395597457886 batch: 7/840\n",
      "Batch loss: 0.5314561724662781 batch: 8/840\n",
      "Batch loss: 0.609652042388916 batch: 9/840\n",
      "Batch loss: 0.6756606101989746 batch: 10/840\n",
      "Batch loss: 0.5671285390853882 batch: 11/840\n",
      "Batch loss: 0.5279728174209595 batch: 12/840\n",
      "Batch loss: 0.5210738778114319 batch: 13/840\n",
      "Batch loss: 0.6120873093605042 batch: 14/840\n",
      "Batch loss: 0.605975866317749 batch: 15/840\n",
      "Batch loss: 0.49399295449256897 batch: 16/840\n",
      "Batch loss: 0.44361647963523865 batch: 17/840\n",
      "Batch loss: 0.7337782979011536 batch: 18/840\n",
      "Batch loss: 0.6965024471282959 batch: 19/840\n",
      "Batch loss: 0.8052114844322205 batch: 20/840\n",
      "Batch loss: 0.6924617886543274 batch: 21/840\n",
      "Batch loss: 0.5885708332061768 batch: 22/840\n",
      "Batch loss: 0.5316839814186096 batch: 23/840\n",
      "Batch loss: 0.558861494064331 batch: 24/840\n",
      "Batch loss: 0.554677426815033 batch: 25/840\n",
      "Batch loss: 0.6814932823181152 batch: 26/840\n",
      "Batch loss: 0.6315906047821045 batch: 27/840\n",
      "Batch loss: 0.655712902545929 batch: 28/840\n",
      "Batch loss: 0.6163175702095032 batch: 29/840\n",
      "Batch loss: 0.5720410346984863 batch: 30/840\n",
      "Batch loss: 0.5651705861091614 batch: 31/840\n",
      "Batch loss: 0.591777503490448 batch: 32/840\n",
      "Batch loss: 0.5696592330932617 batch: 33/840\n",
      "Batch loss: 0.5482008457183838 batch: 34/840\n",
      "Batch loss: 0.6275604963302612 batch: 35/840\n",
      "Batch loss: 0.5867372751235962 batch: 36/840\n",
      "Batch loss: 0.7018560767173767 batch: 37/840\n",
      "Batch loss: 0.6909288167953491 batch: 38/840\n",
      "Batch loss: 0.7064767479896545 batch: 39/840\n",
      "Batch loss: 0.5449248552322388 batch: 40/840\n",
      "Batch loss: 0.7829082012176514 batch: 41/840\n",
      "Batch loss: 0.6024878025054932 batch: 42/840\n",
      "Batch loss: 0.546842634677887 batch: 43/840\n",
      "Batch loss: 0.6570965647697449 batch: 44/840\n",
      "Batch loss: 0.6286100149154663 batch: 45/840\n",
      "Batch loss: 0.5600574016571045 batch: 46/840\n",
      "Batch loss: 0.48440030217170715 batch: 47/840\n",
      "Batch loss: 0.6953128576278687 batch: 48/840\n",
      "Batch loss: 0.6862406134605408 batch: 49/840\n",
      "Batch loss: 0.6186681985855103 batch: 50/840\n",
      "Batch loss: 0.6746307611465454 batch: 51/840\n",
      "Batch loss: 0.7381964325904846 batch: 52/840\n",
      "Batch loss: 0.544450044631958 batch: 53/840\n",
      "Batch loss: 0.6323606371879578 batch: 54/840\n",
      "Batch loss: 0.6681698560714722 batch: 55/840\n",
      "Batch loss: 0.5555040240287781 batch: 56/840\n",
      "Batch loss: 0.7446720004081726 batch: 57/840\n",
      "Batch loss: 0.5662447810173035 batch: 58/840\n",
      "Batch loss: 0.5309808850288391 batch: 59/840\n",
      "Batch loss: 0.5688895583152771 batch: 60/840\n",
      "Batch loss: 0.7906685471534729 batch: 61/840\n",
      "Batch loss: 0.6144466400146484 batch: 62/840\n",
      "Batch loss: 0.5645824670791626 batch: 63/840\n",
      "Batch loss: 0.7351258993148804 batch: 64/840\n",
      "Batch loss: 0.574110746383667 batch: 65/840\n",
      "Batch loss: 0.720734715461731 batch: 66/840\n",
      "Batch loss: 0.6220135688781738 batch: 67/840\n",
      "Batch loss: 0.6437782049179077 batch: 68/840\n",
      "Batch loss: 0.6653033494949341 batch: 69/840\n",
      "Batch loss: 0.58377605676651 batch: 70/840\n",
      "Batch loss: 0.6823458075523376 batch: 71/840\n",
      "Batch loss: 0.6793676018714905 batch: 72/840\n",
      "Batch loss: 0.6451705098152161 batch: 73/840\n",
      "Batch loss: 0.6346768736839294 batch: 74/840\n",
      "Batch loss: 0.6274751424789429 batch: 75/840\n",
      "Batch loss: 0.3584495484828949 batch: 76/840\n",
      "Batch loss: 0.5686748623847961 batch: 77/840\n",
      "Batch loss: 0.7450085282325745 batch: 78/840\n",
      "Batch loss: 0.5948692560195923 batch: 79/840\n",
      "Batch loss: 0.5692108273506165 batch: 80/840\n",
      "Batch loss: 0.5314992666244507 batch: 81/840\n",
      "Batch loss: 0.6825621128082275 batch: 82/840\n",
      "Batch loss: 0.516953706741333 batch: 83/840\n",
      "Batch loss: 0.73406982421875 batch: 84/840\n",
      "Batch loss: 0.6021485328674316 batch: 85/840\n",
      "Batch loss: 0.9059431552886963 batch: 86/840\n",
      "Batch loss: 0.4343537390232086 batch: 87/840\n",
      "Batch loss: 0.45001423358917236 batch: 88/840\n",
      "Batch loss: 0.3697466254234314 batch: 89/840\n",
      "Batch loss: 0.5285038948059082 batch: 90/840\n",
      "Batch loss: 0.6068570017814636 batch: 91/840\n",
      "Batch loss: 0.614423394203186 batch: 92/840\n",
      "Batch loss: 0.593168318271637 batch: 93/840\n",
      "Batch loss: 0.5430366396903992 batch: 94/840\n",
      "Batch loss: 0.5544496774673462 batch: 95/840\n",
      "Batch loss: 0.700164258480072 batch: 96/840\n",
      "Batch loss: 0.6394885182380676 batch: 97/840\n",
      "Batch loss: 0.7865153551101685 batch: 98/840\n",
      "Batch loss: 0.5991502404212952 batch: 99/840\n",
      "Batch loss: 0.6017789244651794 batch: 100/840\n",
      "Batch loss: 0.56051105260849 batch: 101/840\n",
      "Batch loss: 0.6944308280944824 batch: 102/840\n",
      "Batch loss: 0.5942488312721252 batch: 103/840\n",
      "Batch loss: 0.5270311236381531 batch: 104/840\n",
      "Batch loss: 0.5136454105377197 batch: 105/840\n",
      "Batch loss: 0.636063277721405 batch: 106/840\n",
      "Batch loss: 0.5745035409927368 batch: 107/840\n",
      "Batch loss: 0.7639236450195312 batch: 108/840\n",
      "Batch loss: 0.6834562420845032 batch: 109/840\n",
      "Batch loss: 0.5803247690200806 batch: 110/840\n",
      "Batch loss: 0.4460260272026062 batch: 111/840\n",
      "Batch loss: 0.6413333415985107 batch: 112/840\n",
      "Batch loss: 0.6607221364974976 batch: 113/840\n",
      "Batch loss: 0.5686892867088318 batch: 114/840\n",
      "Batch loss: 0.5166299939155579 batch: 115/840\n",
      "Batch loss: 0.6596108078956604 batch: 116/840\n",
      "Batch loss: 0.6431522369384766 batch: 117/840\n",
      "Batch loss: 0.4624316394329071 batch: 118/840\n",
      "Batch loss: 0.6675025820732117 batch: 119/840\n",
      "Batch loss: 0.5428414344787598 batch: 120/840\n",
      "Batch loss: 0.7335765361785889 batch: 121/840\n",
      "Batch loss: 0.7293987274169922 batch: 122/840\n",
      "Batch loss: 0.636457085609436 batch: 123/840\n",
      "Batch loss: 0.5447871685028076 batch: 124/840\n",
      "Batch loss: 0.6018778681755066 batch: 125/840\n",
      "Batch loss: 0.5830787420272827 batch: 126/840\n",
      "Batch loss: 0.6681369543075562 batch: 127/840\n",
      "Batch loss: 0.594120442867279 batch: 128/840\n",
      "Batch loss: 0.7402793169021606 batch: 129/840\n",
      "Batch loss: 0.5439771413803101 batch: 130/840\n",
      "Batch loss: 0.7124481201171875 batch: 131/840\n",
      "Batch loss: 0.9604586958885193 batch: 132/840\n",
      "Batch loss: 0.7012699842453003 batch: 133/840\n",
      "Batch loss: 0.5608540177345276 batch: 134/840\n",
      "Batch loss: 0.5108143091201782 batch: 135/840\n",
      "Batch loss: 0.7144440412521362 batch: 136/840\n",
      "Batch loss: 0.5708867907524109 batch: 137/840\n",
      "Batch loss: 0.5363368391990662 batch: 138/840\n",
      "Batch loss: 0.5564932823181152 batch: 139/840\n",
      "Batch loss: 0.6170703768730164 batch: 140/840\n",
      "Batch loss: 0.4579004943370819 batch: 141/840\n",
      "Batch loss: 0.620582103729248 batch: 142/840\n",
      "Batch loss: 0.5673085451126099 batch: 143/840\n",
      "Batch loss: 0.5736448168754578 batch: 144/840\n",
      "Batch loss: 0.6447995901107788 batch: 145/840\n",
      "Batch loss: 0.7549266219139099 batch: 146/840\n",
      "Batch loss: 0.4661344885826111 batch: 147/840\n",
      "Batch loss: 0.7113397121429443 batch: 148/840\n",
      "Batch loss: 0.6288538575172424 batch: 149/840\n",
      "Batch loss: 0.6933133602142334 batch: 150/840\n",
      "Batch loss: 0.5060579180717468 batch: 151/840\n",
      "Batch loss: 0.6460875868797302 batch: 152/840\n",
      "Batch loss: 0.5001895427703857 batch: 153/840\n",
      "Batch loss: 0.8777002096176147 batch: 154/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.500082790851593 batch: 155/840\n",
      "Batch loss: 0.579103410243988 batch: 156/840\n",
      "Batch loss: 0.6690755486488342 batch: 157/840\n",
      "Batch loss: 0.4972093999385834 batch: 158/840\n",
      "Batch loss: 0.5225533843040466 batch: 159/840\n",
      "Batch loss: 0.5444604158401489 batch: 160/840\n",
      "Batch loss: 0.6820705533027649 batch: 161/840\n",
      "Batch loss: 0.6696226596832275 batch: 162/840\n",
      "Batch loss: 0.6340273022651672 batch: 163/840\n",
      "Batch loss: 0.45905840396881104 batch: 164/840\n",
      "Batch loss: 0.6680819988250732 batch: 165/840\n",
      "Batch loss: 0.5232229232788086 batch: 166/840\n",
      "Batch loss: 0.6236520409584045 batch: 167/840\n",
      "Batch loss: 0.633539080619812 batch: 168/840\n",
      "Batch loss: 0.4581613540649414 batch: 169/840\n",
      "Batch loss: 0.6246985793113708 batch: 170/840\n",
      "Batch loss: 0.6537085175514221 batch: 171/840\n",
      "Batch loss: 0.6630408763885498 batch: 172/840\n",
      "Batch loss: 0.6177719235420227 batch: 173/840\n",
      "Batch loss: 0.6088497638702393 batch: 174/840\n",
      "Batch loss: 0.5580502152442932 batch: 175/840\n",
      "Batch loss: 0.7439872026443481 batch: 176/840\n",
      "Batch loss: 0.7598273754119873 batch: 177/840\n",
      "Batch loss: 0.6266701221466064 batch: 178/840\n",
      "Batch loss: 0.6279127597808838 batch: 179/840\n",
      "Batch loss: 0.5068793296813965 batch: 180/840\n",
      "Batch loss: 0.5197913646697998 batch: 181/840\n",
      "Batch loss: 0.5453570485115051 batch: 182/840\n",
      "Batch loss: 0.6743299961090088 batch: 183/840\n",
      "Batch loss: 0.48645949363708496 batch: 184/840\n",
      "Batch loss: 0.4028746783733368 batch: 185/840\n",
      "Batch loss: 0.4931938648223877 batch: 186/840\n",
      "Batch loss: 0.6955485343933105 batch: 187/840\n",
      "Batch loss: 0.6293899416923523 batch: 188/840\n",
      "Batch loss: 0.6329358816146851 batch: 189/840\n",
      "Batch loss: 0.6703990697860718 batch: 190/840\n",
      "Batch loss: 0.7902382612228394 batch: 191/840\n",
      "Batch loss: 0.5473521947860718 batch: 192/840\n",
      "Batch loss: 0.48152026534080505 batch: 193/840\n",
      "Batch loss: 0.3812355399131775 batch: 194/840\n",
      "Batch loss: 0.46842655539512634 batch: 195/840\n",
      "Batch loss: 0.7147849798202515 batch: 196/840\n",
      "Batch loss: 0.645285964012146 batch: 197/840\n",
      "Batch loss: 0.4852569103240967 batch: 198/840\n",
      "Batch loss: 0.6118861436843872 batch: 199/840\n",
      "Batch loss: 0.8158531188964844 batch: 200/840\n",
      "Batch loss: 0.604811429977417 batch: 201/840\n",
      "Batch loss: 0.5639132857322693 batch: 202/840\n",
      "Batch loss: 0.5127109885215759 batch: 203/840\n",
      "Batch loss: 0.7146012783050537 batch: 204/840\n",
      "Batch loss: 0.6330256462097168 batch: 205/840\n",
      "Batch loss: 0.576623260974884 batch: 206/840\n",
      "Batch loss: 0.6059650182723999 batch: 207/840\n",
      "Batch loss: 0.6775639057159424 batch: 208/840\n",
      "Batch loss: 0.6256041526794434 batch: 209/840\n",
      "Batch loss: 0.5253512859344482 batch: 210/840\n",
      "Batch loss: 0.491054505109787 batch: 211/840\n",
      "Batch loss: 0.5542595982551575 batch: 212/840\n",
      "Batch loss: 0.7654190063476562 batch: 213/840\n",
      "Batch loss: 0.7897831201553345 batch: 214/840\n",
      "Batch loss: 0.5768377184867859 batch: 215/840\n",
      "Batch loss: 0.6130542159080505 batch: 216/840\n",
      "Batch loss: 0.5466140508651733 batch: 217/840\n",
      "Batch loss: 0.6268365383148193 batch: 218/840\n",
      "Batch loss: 0.623337984085083 batch: 219/840\n",
      "Batch loss: 0.7998200058937073 batch: 220/840\n",
      "Batch loss: 0.6246489882469177 batch: 221/840\n",
      "Batch loss: 0.6223487257957458 batch: 222/840\n",
      "Batch loss: 0.5551787614822388 batch: 223/840\n",
      "Batch loss: 0.6752100586891174 batch: 224/840\n",
      "Batch loss: 0.6827126145362854 batch: 225/840\n",
      "Batch loss: 0.6218644976615906 batch: 226/840\n",
      "Batch loss: 0.7608181834220886 batch: 227/840\n",
      "Batch loss: 0.4763798117637634 batch: 228/840\n",
      "Batch loss: 0.5283304452896118 batch: 229/840\n",
      "Batch loss: 0.5747380256652832 batch: 230/840\n",
      "Batch loss: 0.45425090193748474 batch: 231/840\n",
      "Batch loss: 0.6611031889915466 batch: 232/840\n",
      "Batch loss: 0.7718625664710999 batch: 233/840\n",
      "Batch loss: 0.6428854465484619 batch: 234/840\n",
      "Batch loss: 0.6757808923721313 batch: 235/840\n",
      "Batch loss: 0.6643469929695129 batch: 236/840\n",
      "Batch loss: 0.5057214498519897 batch: 237/840\n",
      "Batch loss: 0.7344843149185181 batch: 238/840\n",
      "Batch loss: 0.522347629070282 batch: 239/840\n",
      "Batch loss: 0.6261637806892395 batch: 240/840\n",
      "Batch loss: 0.6261369585990906 batch: 241/840\n",
      "Batch loss: 0.5865603685379028 batch: 242/840\n",
      "Batch loss: 0.5974574089050293 batch: 243/840\n",
      "Batch loss: 0.6986653804779053 batch: 244/840\n",
      "Batch loss: 0.4701172709465027 batch: 245/840\n",
      "Batch loss: 0.5953604578971863 batch: 246/840\n",
      "Batch loss: 0.659411609172821 batch: 247/840\n",
      "Batch loss: 0.7031552195549011 batch: 248/840\n",
      "Batch loss: 0.8538950085639954 batch: 249/840\n",
      "Batch loss: 0.5196691751480103 batch: 250/840\n",
      "Batch loss: 0.5121750235557556 batch: 251/840\n",
      "Batch loss: 0.4876929223537445 batch: 252/840\n",
      "Batch loss: 0.6583220958709717 batch: 253/840\n",
      "Batch loss: 0.646500825881958 batch: 254/840\n",
      "Batch loss: 0.5697101950645447 batch: 255/840\n",
      "Batch loss: 0.6533088684082031 batch: 256/840\n",
      "Batch loss: 0.6544140577316284 batch: 257/840\n",
      "Batch loss: 0.7667118310928345 batch: 258/840\n",
      "Batch loss: 0.48904815316200256 batch: 259/840\n",
      "Batch loss: 0.4448833763599396 batch: 260/840\n",
      "Batch loss: 0.5247042179107666 batch: 261/840\n",
      "Batch loss: 0.3284870982170105 batch: 262/840\n",
      "Batch loss: 0.6524749994277954 batch: 263/840\n",
      "Batch loss: 0.5354492664337158 batch: 264/840\n",
      "Batch loss: 0.6610608696937561 batch: 265/840\n",
      "Batch loss: 0.5824761390686035 batch: 266/840\n",
      "Batch loss: 0.6906070113182068 batch: 267/840\n",
      "Batch loss: 0.5949495434761047 batch: 268/840\n",
      "Batch loss: 0.4683483839035034 batch: 269/840\n",
      "Batch loss: 0.5562444925308228 batch: 270/840\n",
      "Batch loss: 0.5619240403175354 batch: 271/840\n",
      "Batch loss: 0.7327542901039124 batch: 272/840\n",
      "Batch loss: 0.6559127569198608 batch: 273/840\n",
      "Batch loss: 0.60331130027771 batch: 274/840\n",
      "Batch loss: 0.7168757915496826 batch: 275/840\n",
      "Batch loss: 0.48835527896881104 batch: 276/840\n",
      "Batch loss: 0.5860775113105774 batch: 277/840\n",
      "Batch loss: 0.7410463690757751 batch: 278/840\n",
      "Batch loss: 0.7459718585014343 batch: 279/840\n",
      "Batch loss: 0.6653091311454773 batch: 280/840\n",
      "Batch loss: 0.5219674706459045 batch: 281/840\n",
      "Batch loss: 0.5505234003067017 batch: 282/840\n",
      "Batch loss: 0.615120530128479 batch: 283/840\n",
      "Batch loss: 0.4230318069458008 batch: 284/840\n",
      "Batch loss: 0.5319818258285522 batch: 285/840\n",
      "Batch loss: 0.7284862995147705 batch: 286/840\n",
      "Batch loss: 0.47111260890960693 batch: 287/840\n",
      "Batch loss: 0.5470535755157471 batch: 288/840\n",
      "Batch loss: 0.7162296175956726 batch: 289/840\n",
      "Batch loss: 0.7302055358886719 batch: 290/840\n",
      "Batch loss: 0.7320147156715393 batch: 291/840\n",
      "Batch loss: 0.5684145092964172 batch: 292/840\n",
      "Batch loss: 0.7339783310890198 batch: 293/840\n",
      "Batch loss: 0.5816679000854492 batch: 294/840\n",
      "Batch loss: 0.4520012140274048 batch: 295/840\n",
      "Batch loss: 0.6892561316490173 batch: 296/840\n",
      "Batch loss: 0.6905168890953064 batch: 297/840\n",
      "Batch loss: 0.7102023363113403 batch: 298/840\n",
      "Batch loss: 0.5304093360900879 batch: 299/840\n",
      "Batch loss: 0.6533600687980652 batch: 300/840\n",
      "Batch loss: 0.6736562848091125 batch: 301/840\n",
      "Batch loss: 0.5720511078834534 batch: 302/840\n",
      "Batch loss: 0.6803748607635498 batch: 303/840\n",
      "Batch loss: 0.5190896391868591 batch: 304/840\n",
      "Batch loss: 0.5615551471710205 batch: 305/840\n",
      "Batch loss: 0.6476691365242004 batch: 306/840\n",
      "Batch loss: 0.499863862991333 batch: 307/840\n",
      "Batch loss: 0.8315671682357788 batch: 308/840\n",
      "Batch loss: 0.5049313902854919 batch: 309/840\n",
      "Batch loss: 0.9053609371185303 batch: 310/840\n",
      "Batch loss: 0.588532567024231 batch: 311/840\n",
      "Batch loss: 0.6970455050468445 batch: 312/840\n",
      "Batch loss: 0.6611437797546387 batch: 313/840\n",
      "Batch loss: 0.5787277221679688 batch: 314/840\n",
      "Batch loss: 0.6259291172027588 batch: 315/840\n",
      "Batch loss: 0.5134378671646118 batch: 316/840\n",
      "Batch loss: 0.6507917642593384 batch: 317/840\n",
      "Batch loss: 0.635310173034668 batch: 318/840\n",
      "Batch loss: 0.6082335710525513 batch: 319/840\n",
      "Batch loss: 0.4748266339302063 batch: 320/840\n",
      "Batch loss: 0.527509331703186 batch: 321/840\n",
      "Batch loss: 0.5979211926460266 batch: 322/840\n",
      "Batch loss: 0.7088600993156433 batch: 323/840\n",
      "Batch loss: 0.6454238295555115 batch: 324/840\n",
      "Batch loss: 0.5405431985855103 batch: 325/840\n",
      "Batch loss: 0.5353599786758423 batch: 326/840\n",
      "Batch loss: 0.5348628759384155 batch: 327/840\n",
      "Batch loss: 0.7914277911186218 batch: 328/840\n",
      "Batch loss: 0.6827390789985657 batch: 329/840\n",
      "Batch loss: 0.6168255805969238 batch: 330/840\n",
      "Batch loss: 0.7079256176948547 batch: 331/840\n",
      "Batch loss: 0.6529772281646729 batch: 332/840\n",
      "Batch loss: 0.6286103129386902 batch: 333/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.676219642162323 batch: 334/840\n",
      "Batch loss: 0.5374742150306702 batch: 335/840\n",
      "Batch loss: 0.7275542616844177 batch: 336/840\n",
      "Batch loss: 0.8822558522224426 batch: 337/840\n",
      "Batch loss: 0.740756094455719 batch: 338/840\n",
      "Batch loss: 0.5489345192909241 batch: 339/840\n",
      "Batch loss: 0.7556161284446716 batch: 340/840\n",
      "Batch loss: 0.5028223991394043 batch: 341/840\n",
      "Batch loss: 0.46881648898124695 batch: 342/840\n",
      "Batch loss: 0.7942386865615845 batch: 343/840\n",
      "Batch loss: 0.564178466796875 batch: 344/840\n",
      "Batch loss: 0.4889373779296875 batch: 345/840\n",
      "Batch loss: 0.6293520331382751 batch: 346/840\n",
      "Batch loss: 0.7809052467346191 batch: 347/840\n",
      "Batch loss: 0.63685542345047 batch: 348/840\n",
      "Batch loss: 0.6130476593971252 batch: 349/840\n",
      "Batch loss: 0.53387451171875 batch: 350/840\n",
      "Batch loss: 0.7601810693740845 batch: 351/840\n",
      "Batch loss: 0.6361535787582397 batch: 352/840\n",
      "Batch loss: 0.726309061050415 batch: 353/840\n",
      "Batch loss: 0.6223248839378357 batch: 354/840\n",
      "Batch loss: 0.5553798675537109 batch: 355/840\n",
      "Batch loss: 0.576300323009491 batch: 356/840\n",
      "Batch loss: 0.5361530780792236 batch: 357/840\n",
      "Batch loss: 0.7373523712158203 batch: 358/840\n",
      "Batch loss: 0.6173052787780762 batch: 359/840\n",
      "Batch loss: 0.7253578901290894 batch: 360/840\n",
      "Batch loss: 0.7537611126899719 batch: 361/840\n",
      "Batch loss: 0.6013035774230957 batch: 362/840\n",
      "Batch loss: 0.5438396334648132 batch: 363/840\n",
      "Batch loss: 0.7037264108657837 batch: 364/840\n",
      "Batch loss: 0.629504919052124 batch: 365/840\n",
      "Batch loss: 0.6116386651992798 batch: 366/840\n",
      "Batch loss: 0.41947323083877563 batch: 367/840\n",
      "Batch loss: 0.7181267738342285 batch: 368/840\n",
      "Batch loss: 0.5994436740875244 batch: 369/840\n",
      "Batch loss: 0.7059076428413391 batch: 370/840\n",
      "Batch loss: 0.59710294008255 batch: 371/840\n",
      "Batch loss: 0.43283310532569885 batch: 372/840\n",
      "Batch loss: 0.6082074046134949 batch: 373/840\n",
      "Batch loss: 0.7554945349693298 batch: 374/840\n",
      "Batch loss: 0.5169870853424072 batch: 375/840\n",
      "Batch loss: 0.4369804859161377 batch: 376/840\n",
      "Batch loss: 0.6715614199638367 batch: 377/840\n",
      "Batch loss: 0.5355713367462158 batch: 378/840\n",
      "Batch loss: 0.5557812452316284 batch: 379/840\n",
      "Batch loss: 0.9323446154594421 batch: 380/840\n",
      "Batch loss: 0.8840396404266357 batch: 381/840\n",
      "Batch loss: 0.7376193404197693 batch: 382/840\n",
      "Batch loss: 0.6650537848472595 batch: 383/840\n",
      "Batch loss: 0.5354195237159729 batch: 384/840\n",
      "Batch loss: 0.7802238464355469 batch: 385/840\n",
      "Batch loss: 0.7731094360351562 batch: 386/840\n",
      "Batch loss: 0.6010236740112305 batch: 387/840\n",
      "Batch loss: 0.6905174255371094 batch: 388/840\n",
      "Batch loss: 0.6068141460418701 batch: 389/840\n",
      "Batch loss: 0.8023223280906677 batch: 390/840\n",
      "Batch loss: 0.6075165867805481 batch: 391/840\n",
      "Batch loss: 0.5787353515625 batch: 392/840\n",
      "Batch loss: 0.4903806447982788 batch: 393/840\n",
      "Batch loss: 0.7389135956764221 batch: 394/840\n",
      "Batch loss: 0.5810390710830688 batch: 395/840\n",
      "Batch loss: 0.6223352551460266 batch: 396/840\n",
      "Batch loss: 0.5588265061378479 batch: 397/840\n",
      "Batch loss: 0.670208215713501 batch: 398/840\n",
      "Batch loss: 0.43723341822624207 batch: 399/840\n",
      "Batch loss: 0.49313899874687195 batch: 400/840\n",
      "Batch loss: 0.697706937789917 batch: 401/840\n",
      "Batch loss: 0.5105940103530884 batch: 402/840\n",
      "Batch loss: 0.6364908814430237 batch: 403/840\n",
      "Batch loss: 0.6326229572296143 batch: 404/840\n",
      "Batch loss: 0.5362449288368225 batch: 405/840\n",
      "Batch loss: 0.7871946692466736 batch: 406/840\n",
      "Batch loss: 0.5730146169662476 batch: 407/840\n",
      "Batch loss: 0.685886025428772 batch: 408/840\n",
      "Batch loss: 0.802620530128479 batch: 409/840\n",
      "Batch loss: 0.7918369174003601 batch: 410/840\n",
      "Batch loss: 0.7062209248542786 batch: 411/840\n",
      "Batch loss: 0.732158362865448 batch: 412/840\n",
      "Batch loss: 0.6177668571472168 batch: 413/840\n",
      "Batch loss: 0.5697880387306213 batch: 414/840\n",
      "Batch loss: 0.8636430501937866 batch: 415/840\n",
      "Batch loss: 0.4725542366504669 batch: 416/840\n",
      "Batch loss: 0.7350360751152039 batch: 417/840\n",
      "Batch loss: 0.7493749856948853 batch: 418/840\n",
      "Batch loss: 0.6176608800888062 batch: 419/840\n",
      "Batch loss: 0.7105987071990967 batch: 420/840\n",
      "Batch loss: 0.526593029499054 batch: 421/840\n",
      "Batch loss: 0.5429587960243225 batch: 422/840\n",
      "Batch loss: 0.6015427708625793 batch: 423/840\n",
      "Batch loss: 0.6620270609855652 batch: 424/840\n",
      "Batch loss: 0.6756578683853149 batch: 425/840\n",
      "Batch loss: 0.6317527294158936 batch: 426/840\n",
      "Batch loss: 0.5282789468765259 batch: 427/840\n",
      "Batch loss: 0.8686708211898804 batch: 428/840\n",
      "Batch loss: 0.5022487044334412 batch: 429/840\n",
      "Batch loss: 0.7947998642921448 batch: 430/840\n",
      "Batch loss: 0.537221372127533 batch: 431/840\n",
      "Batch loss: 0.6060991883277893 batch: 432/840\n",
      "Batch loss: 0.4666135013103485 batch: 433/840\n",
      "Batch loss: 0.48665446043014526 batch: 434/840\n",
      "Batch loss: 0.7009114027023315 batch: 435/840\n",
      "Batch loss: 0.6539507508277893 batch: 436/840\n",
      "Batch loss: 0.7085855007171631 batch: 437/840\n",
      "Batch loss: 0.38463252782821655 batch: 438/840\n",
      "Batch loss: 0.5525672435760498 batch: 439/840\n",
      "Batch loss: 0.7473223209381104 batch: 440/840\n",
      "Batch loss: 0.6983915567398071 batch: 441/840\n",
      "Batch loss: 0.7239072918891907 batch: 442/840\n",
      "Batch loss: 0.6649685502052307 batch: 443/840\n",
      "Batch loss: 0.7560506463050842 batch: 444/840\n",
      "Batch loss: 0.595939040184021 batch: 445/840\n",
      "Batch loss: 0.6153905391693115 batch: 446/840\n",
      "Batch loss: 0.6047555804252625 batch: 447/840\n",
      "Batch loss: 0.6457678079605103 batch: 448/840\n",
      "Batch loss: 0.532366156578064 batch: 449/840\n",
      "Batch loss: 0.6715058088302612 batch: 450/840\n",
      "Batch loss: 0.6906469464302063 batch: 451/840\n",
      "Batch loss: 0.7555400133132935 batch: 452/840\n",
      "Batch loss: 0.6245026588439941 batch: 453/840\n",
      "Batch loss: 0.5754498243331909 batch: 454/840\n",
      "Batch loss: 0.6715438961982727 batch: 455/840\n",
      "Batch loss: 0.5982654094696045 batch: 456/840\n",
      "Batch loss: 0.46896064281463623 batch: 457/840\n",
      "Batch loss: 0.5627157092094421 batch: 458/840\n",
      "Batch loss: 0.5010339021682739 batch: 459/840\n",
      "Batch loss: 0.48482388257980347 batch: 460/840\n",
      "Batch loss: 0.786817729473114 batch: 461/840\n",
      "Batch loss: 0.589102566242218 batch: 462/840\n",
      "Batch loss: 0.6843095421791077 batch: 463/840\n",
      "Batch loss: 0.572454035282135 batch: 464/840\n",
      "Batch loss: 0.8491867780685425 batch: 465/840\n",
      "Batch loss: 0.6582213640213013 batch: 466/840\n",
      "Batch loss: 0.8399180769920349 batch: 467/840\n",
      "Batch loss: 0.6182288527488708 batch: 468/840\n",
      "Batch loss: 0.5055999755859375 batch: 469/840\n",
      "Batch loss: 0.6516063213348389 batch: 470/840\n",
      "Batch loss: 0.6176545023918152 batch: 471/840\n",
      "Batch loss: 0.7206306457519531 batch: 472/840\n",
      "Batch loss: 0.639219343662262 batch: 473/840\n",
      "Batch loss: 0.6224815249443054 batch: 474/840\n",
      "Batch loss: 0.5145986676216125 batch: 475/840\n",
      "Batch loss: 0.5450862646102905 batch: 476/840\n",
      "Batch loss: 0.5871643424034119 batch: 477/840\n",
      "Batch loss: 0.6176324486732483 batch: 478/840\n",
      "Batch loss: 0.4697718024253845 batch: 479/840\n",
      "Batch loss: 0.6454680562019348 batch: 480/840\n",
      "Batch loss: 0.7114211916923523 batch: 481/840\n",
      "Batch loss: 0.5970136523246765 batch: 482/840\n",
      "Batch loss: 0.5771339535713196 batch: 483/840\n",
      "Batch loss: 0.5808314681053162 batch: 484/840\n",
      "Batch loss: 0.6162593364715576 batch: 485/840\n",
      "Batch loss: 0.6069095134735107 batch: 486/840\n",
      "Batch loss: 0.47300949692726135 batch: 487/840\n",
      "Batch loss: 0.6657093167304993 batch: 488/840\n",
      "Batch loss: 0.5517643690109253 batch: 489/840\n",
      "Batch loss: 0.6717708110809326 batch: 490/840\n",
      "Batch loss: 0.4555032253265381 batch: 491/840\n",
      "Batch loss: 0.6150957345962524 batch: 492/840\n",
      "Batch loss: 0.5839841961860657 batch: 493/840\n",
      "Batch loss: 0.4141913652420044 batch: 494/840\n",
      "Batch loss: 0.6139199137687683 batch: 495/840\n",
      "Batch loss: 0.6303128004074097 batch: 496/840\n",
      "Batch loss: 0.8037828803062439 batch: 497/840\n",
      "Batch loss: 0.41611626744270325 batch: 498/840\n",
      "Batch loss: 0.7455827593803406 batch: 499/840\n",
      "Batch loss: 0.6284587979316711 batch: 500/840\n",
      "Batch loss: 0.4776490330696106 batch: 501/840\n",
      "Batch loss: 0.6586090326309204 batch: 502/840\n",
      "Batch loss: 0.5118610858917236 batch: 503/840\n",
      "Batch loss: 0.6648271083831787 batch: 504/840\n",
      "Batch loss: 0.6951308250427246 batch: 505/840\n",
      "Batch loss: 0.454137921333313 batch: 506/840\n",
      "Batch loss: 0.7673022747039795 batch: 507/840\n",
      "Batch loss: 0.5172027349472046 batch: 508/840\n",
      "Batch loss: 0.69987952709198 batch: 509/840\n",
      "Batch loss: 0.6462929248809814 batch: 510/840\n",
      "Batch loss: 0.5038703083992004 batch: 511/840\n",
      "Batch loss: 0.6251270771026611 batch: 512/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5871518850326538 batch: 513/840\n",
      "Batch loss: 0.6323200464248657 batch: 514/840\n",
      "Batch loss: 0.38930976390838623 batch: 515/840\n",
      "Batch loss: 0.660062313079834 batch: 516/840\n",
      "Batch loss: 0.5781707167625427 batch: 517/840\n",
      "Batch loss: 0.7400498390197754 batch: 518/840\n",
      "Batch loss: 0.9457343816757202 batch: 519/840\n",
      "Batch loss: 0.5971339344978333 batch: 520/840\n",
      "Batch loss: 0.5823533535003662 batch: 521/840\n",
      "Batch loss: 0.5669263005256653 batch: 522/840\n",
      "Batch loss: 0.5207718014717102 batch: 523/840\n",
      "Batch loss: 0.6216354370117188 batch: 524/840\n",
      "Batch loss: 0.5415240526199341 batch: 525/840\n",
      "Batch loss: 0.6962172985076904 batch: 526/840\n",
      "Batch loss: 0.7221055030822754 batch: 527/840\n",
      "Batch loss: 0.7299468517303467 batch: 528/840\n",
      "Batch loss: 0.5005067586898804 batch: 529/840\n",
      "Batch loss: 0.5308778285980225 batch: 530/840\n",
      "Batch loss: 0.5702365636825562 batch: 531/840\n",
      "Batch loss: 0.3818659484386444 batch: 532/840\n",
      "Batch loss: 0.6996857523918152 batch: 533/840\n",
      "Batch loss: 0.6518881916999817 batch: 534/840\n",
      "Batch loss: 0.584604799747467 batch: 535/840\n",
      "Batch loss: 0.62775057554245 batch: 536/840\n",
      "Batch loss: 0.661192774772644 batch: 537/840\n",
      "Batch loss: 0.5879675149917603 batch: 538/840\n",
      "Batch loss: 0.5686830282211304 batch: 539/840\n",
      "Batch loss: 0.7038653492927551 batch: 540/840\n",
      "Batch loss: 0.6101893782615662 batch: 541/840\n",
      "Batch loss: 0.6447113752365112 batch: 542/840\n",
      "Batch loss: 0.5361217260360718 batch: 543/840\n",
      "Batch loss: 0.6660280823707581 batch: 544/840\n",
      "Batch loss: 0.6025073528289795 batch: 545/840\n",
      "Batch loss: 0.5464833974838257 batch: 546/840\n",
      "Batch loss: 0.5699522495269775 batch: 547/840\n",
      "Batch loss: 0.5153961777687073 batch: 548/840\n",
      "Batch loss: 0.5019190907478333 batch: 549/840\n",
      "Batch loss: 0.46608299016952515 batch: 550/840\n",
      "Batch loss: 0.6273699998855591 batch: 551/840\n",
      "Batch loss: 0.55306077003479 batch: 552/840\n",
      "Batch loss: 0.6909062266349792 batch: 553/840\n",
      "Batch loss: 0.6681967377662659 batch: 554/840\n",
      "Batch loss: 0.677198588848114 batch: 555/840\n",
      "Batch loss: 0.5987534523010254 batch: 556/840\n",
      "Batch loss: 0.6166261434555054 batch: 557/840\n",
      "Batch loss: 0.5944342613220215 batch: 558/840\n",
      "Batch loss: 0.4914061725139618 batch: 559/840\n",
      "Batch loss: 0.5791844725608826 batch: 560/840\n",
      "Batch loss: 0.6661115288734436 batch: 561/840\n",
      "Batch loss: 0.5260601043701172 batch: 562/840\n",
      "Batch loss: 0.32206401228904724 batch: 563/840\n",
      "Batch loss: 0.6692163944244385 batch: 564/840\n",
      "Batch loss: 0.6782090067863464 batch: 565/840\n",
      "Batch loss: 0.6534122228622437 batch: 566/840\n",
      "Batch loss: 0.7235315442085266 batch: 567/840\n",
      "Batch loss: 0.5761296153068542 batch: 568/840\n",
      "Batch loss: 0.5630121827125549 batch: 569/840\n",
      "Batch loss: 0.44473230838775635 batch: 570/840\n",
      "Batch loss: 0.743385374546051 batch: 571/840\n",
      "Batch loss: 0.6430859565734863 batch: 572/840\n",
      "Batch loss: 0.5854107737541199 batch: 573/840\n",
      "Batch loss: 0.667919397354126 batch: 574/840\n",
      "Batch loss: 0.45905858278274536 batch: 575/840\n",
      "Batch loss: 0.7219913005828857 batch: 576/840\n",
      "Batch loss: 0.5731759071350098 batch: 577/840\n",
      "Batch loss: 0.649070680141449 batch: 578/840\n",
      "Batch loss: 0.5543990135192871 batch: 579/840\n",
      "Batch loss: 0.7631928324699402 batch: 580/840\n",
      "Batch loss: 0.6872274875640869 batch: 581/840\n",
      "Batch loss: 0.792815089225769 batch: 582/840\n",
      "Batch loss: 0.5171042084693909 batch: 583/840\n",
      "Batch loss: 0.7538299560546875 batch: 584/840\n",
      "Batch loss: 0.6178674101829529 batch: 585/840\n",
      "Batch loss: 0.682895302772522 batch: 586/840\n",
      "Batch loss: 0.6392226219177246 batch: 587/840\n",
      "Batch loss: 0.578021764755249 batch: 588/840\n",
      "Batch loss: 0.6661379933357239 batch: 589/840\n",
      "Batch loss: 0.5697019100189209 batch: 590/840\n",
      "Batch loss: 0.5090216994285583 batch: 591/840\n",
      "Batch loss: 0.553955078125 batch: 592/840\n",
      "Batch loss: 0.6491584181785583 batch: 593/840\n",
      "Batch loss: 0.6256996989250183 batch: 594/840\n",
      "Batch loss: 0.4078179895877838 batch: 595/840\n",
      "Batch loss: 0.4694978594779968 batch: 596/840\n",
      "Batch loss: 0.6762406229972839 batch: 597/840\n",
      "Batch loss: 0.344462513923645 batch: 598/840\n",
      "Batch loss: 0.5760067105293274 batch: 599/840\n",
      "Batch loss: 0.6863808631896973 batch: 600/840\n",
      "Batch loss: 0.7256749868392944 batch: 601/840\n",
      "Batch loss: 0.6307058930397034 batch: 602/840\n",
      "Batch loss: 0.6246170401573181 batch: 603/840\n",
      "Batch loss: 0.6399763226509094 batch: 604/840\n",
      "Batch loss: 0.8319773077964783 batch: 605/840\n",
      "Batch loss: 0.8094985485076904 batch: 606/840\n",
      "Batch loss: 0.7597669959068298 batch: 607/840\n",
      "Batch loss: 0.5461746454238892 batch: 608/840\n",
      "Batch loss: 0.4760666787624359 batch: 609/840\n",
      "Batch loss: 0.639951229095459 batch: 610/840\n",
      "Batch loss: 0.5621934533119202 batch: 611/840\n",
      "Batch loss: 0.7125011682510376 batch: 612/840\n",
      "Batch loss: 0.6234686970710754 batch: 613/840\n",
      "Batch loss: 0.652239203453064 batch: 614/840\n",
      "Batch loss: 0.5793484449386597 batch: 615/840\n",
      "Batch loss: 0.5159639716148376 batch: 616/840\n",
      "Batch loss: 0.49039188027381897 batch: 617/840\n",
      "Batch loss: 0.5908780694007874 batch: 618/840\n",
      "Batch loss: 0.8176378011703491 batch: 619/840\n",
      "Batch loss: 0.5222575068473816 batch: 620/840\n",
      "Batch loss: 0.5742634534835815 batch: 621/840\n",
      "Batch loss: 0.6579767465591431 batch: 622/840\n",
      "Batch loss: 0.6003434062004089 batch: 623/840\n",
      "Batch loss: 0.7249157428741455 batch: 624/840\n",
      "Batch loss: 0.6271501779556274 batch: 625/840\n",
      "Batch loss: 0.5494120121002197 batch: 626/840\n",
      "Batch loss: 0.548887312412262 batch: 627/840\n",
      "Batch loss: 0.6196151971817017 batch: 628/840\n",
      "Batch loss: 0.6111351251602173 batch: 629/840\n",
      "Batch loss: 0.6260139346122742 batch: 630/840\n",
      "Batch loss: 0.6390783786773682 batch: 631/840\n",
      "Batch loss: 0.6880074143409729 batch: 632/840\n",
      "Batch loss: 0.5986843705177307 batch: 633/840\n",
      "Batch loss: 0.6227086782455444 batch: 634/840\n",
      "Batch loss: 0.5835034847259521 batch: 635/840\n",
      "Batch loss: 0.5940688848495483 batch: 636/840\n",
      "Batch loss: 0.44034403562545776 batch: 637/840\n",
      "Batch loss: 0.6031391620635986 batch: 638/840\n",
      "Batch loss: 0.7063944935798645 batch: 639/840\n",
      "Batch loss: 0.5702681541442871 batch: 640/840\n",
      "Batch loss: 0.8353266716003418 batch: 641/840\n",
      "Batch loss: 0.6044111847877502 batch: 642/840\n",
      "Batch loss: 0.872016966342926 batch: 643/840\n",
      "Batch loss: 0.6514652967453003 batch: 644/840\n",
      "Batch loss: 0.5465191602706909 batch: 645/840\n",
      "Batch loss: 0.6034573912620544 batch: 646/840\n",
      "Batch loss: 0.6272298097610474 batch: 647/840\n",
      "Batch loss: 0.6348474621772766 batch: 648/840\n",
      "Batch loss: 0.586098849773407 batch: 649/840\n",
      "Batch loss: 0.4539017975330353 batch: 650/840\n",
      "Batch loss: 0.5916130542755127 batch: 651/840\n",
      "Batch loss: 0.6967926025390625 batch: 652/840\n",
      "Batch loss: 0.497819185256958 batch: 653/840\n",
      "Batch loss: 0.8845117688179016 batch: 654/840\n",
      "Batch loss: 0.5372388958930969 batch: 655/840\n",
      "Batch loss: 0.6695082187652588 batch: 656/840\n",
      "Batch loss: 0.5788385272026062 batch: 657/840\n",
      "Batch loss: 0.5986414551734924 batch: 658/840\n",
      "Batch loss: 0.7289347052574158 batch: 659/840\n",
      "Batch loss: 0.5578842759132385 batch: 660/840\n",
      "Batch loss: 0.6813848614692688 batch: 661/840\n",
      "Batch loss: 0.5924882888793945 batch: 662/840\n",
      "Batch loss: 0.5570168495178223 batch: 663/840\n",
      "Batch loss: 0.632051944732666 batch: 664/840\n",
      "Batch loss: 0.6774359345436096 batch: 665/840\n",
      "Batch loss: 0.6818607449531555 batch: 666/840\n",
      "Batch loss: 0.7125282287597656 batch: 667/840\n",
      "Batch loss: 0.5082995891571045 batch: 668/840\n",
      "Batch loss: 0.6460691690444946 batch: 669/840\n",
      "Batch loss: 0.5737528800964355 batch: 670/840\n",
      "Batch loss: 0.7095494866371155 batch: 671/840\n",
      "Batch loss: 0.547793447971344 batch: 672/840\n",
      "Batch loss: 0.8449463844299316 batch: 673/840\n",
      "Batch loss: 0.8561840057373047 batch: 674/840\n",
      "Batch loss: 0.558552086353302 batch: 675/840\n",
      "Batch loss: 0.5638185143470764 batch: 676/840\n",
      "Batch loss: 0.6434939503669739 batch: 677/840\n",
      "Batch loss: 0.6541454792022705 batch: 678/840\n",
      "Batch loss: 0.7765635848045349 batch: 679/840\n",
      "Batch loss: 0.723930835723877 batch: 680/840\n",
      "Batch loss: 0.6256083250045776 batch: 681/840\n",
      "Batch loss: 0.6062396168708801 batch: 682/840\n",
      "Batch loss: 0.5206640958786011 batch: 683/840\n",
      "Batch loss: 0.8626837134361267 batch: 684/840\n",
      "Batch loss: 0.6669989228248596 batch: 685/840\n",
      "Batch loss: 0.6814048290252686 batch: 686/840\n",
      "Batch loss: 0.6772311329841614 batch: 687/840\n",
      "Batch loss: 0.5471054315567017 batch: 688/840\n",
      "Batch loss: 0.5205010771751404 batch: 689/840\n",
      "Batch loss: 0.48536568880081177 batch: 690/840\n",
      "Batch loss: 0.7000871300697327 batch: 691/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6288873553276062 batch: 692/840\n",
      "Batch loss: 0.6288907527923584 batch: 693/840\n",
      "Batch loss: 0.7972927093505859 batch: 694/840\n",
      "Batch loss: 0.6923161149024963 batch: 695/840\n",
      "Batch loss: 0.6135925650596619 batch: 696/840\n",
      "Batch loss: 0.562557578086853 batch: 697/840\n",
      "Batch loss: 0.5995562076568604 batch: 698/840\n",
      "Batch loss: 0.6688772439956665 batch: 699/840\n",
      "Batch loss: 0.7191662788391113 batch: 700/840\n",
      "Batch loss: 0.8126928210258484 batch: 701/840\n",
      "Batch loss: 0.691344678401947 batch: 702/840\n",
      "Batch loss: 0.5445941686630249 batch: 703/840\n",
      "Batch loss: 0.8110744953155518 batch: 704/840\n",
      "Batch loss: 0.6062442660331726 batch: 705/840\n",
      "Batch loss: 0.5800601243972778 batch: 706/840\n",
      "Batch loss: 0.7102557420730591 batch: 707/840\n",
      "Batch loss: 0.6095046401023865 batch: 708/840\n",
      "Batch loss: 0.6768530011177063 batch: 709/840\n",
      "Batch loss: 0.7482885122299194 batch: 710/840\n",
      "Batch loss: 0.503308117389679 batch: 711/840\n",
      "Batch loss: 0.7782816886901855 batch: 712/840\n",
      "Batch loss: 0.4531058073043823 batch: 713/840\n",
      "Batch loss: 0.6122187972068787 batch: 714/840\n",
      "Batch loss: 0.7673240900039673 batch: 715/840\n",
      "Batch loss: 0.6242961883544922 batch: 716/840\n",
      "Batch loss: 0.6666055917739868 batch: 717/840\n",
      "Batch loss: 0.5192804932594299 batch: 718/840\n",
      "Batch loss: 0.45053547620773315 batch: 719/840\n",
      "Batch loss: 0.4906650483608246 batch: 720/840\n",
      "Batch loss: 0.6870281100273132 batch: 721/840\n",
      "Batch loss: 0.8219525814056396 batch: 722/840\n",
      "Batch loss: 0.4912257492542267 batch: 723/840\n",
      "Batch loss: 0.6976081132888794 batch: 724/840\n",
      "Batch loss: 0.7060788869857788 batch: 725/840\n",
      "Batch loss: 0.537290096282959 batch: 726/840\n",
      "Batch loss: 0.66047203540802 batch: 727/840\n",
      "Batch loss: 0.7045086622238159 batch: 728/840\n",
      "Batch loss: 0.6948326230049133 batch: 729/840\n",
      "Batch loss: 0.6488586664199829 batch: 730/840\n",
      "Batch loss: 0.48795756697654724 batch: 731/840\n",
      "Batch loss: 0.8366498351097107 batch: 732/840\n",
      "Batch loss: 0.6570034027099609 batch: 733/840\n",
      "Batch loss: 0.593604564666748 batch: 734/840\n",
      "Batch loss: 0.6592600345611572 batch: 735/840\n",
      "Batch loss: 0.626448929309845 batch: 736/840\n",
      "Batch loss: 0.5955674052238464 batch: 737/840\n",
      "Batch loss: 0.4733894467353821 batch: 738/840\n",
      "Batch loss: 0.6546578407287598 batch: 739/840\n",
      "Batch loss: 0.6860595941543579 batch: 740/840\n",
      "Batch loss: 0.4515896737575531 batch: 741/840\n",
      "Batch loss: 0.5408003330230713 batch: 742/840\n",
      "Batch loss: 0.4290908873081207 batch: 743/840\n",
      "Batch loss: 0.4867631196975708 batch: 744/840\n",
      "Batch loss: 0.6311394572257996 batch: 745/840\n",
      "Batch loss: 0.6030593514442444 batch: 746/840\n",
      "Batch loss: 0.7342594861984253 batch: 747/840\n",
      "Batch loss: 0.707928478717804 batch: 748/840\n",
      "Batch loss: 0.5122149586677551 batch: 749/840\n",
      "Batch loss: 0.5230022668838501 batch: 750/840\n",
      "Batch loss: 0.5734915733337402 batch: 751/840\n",
      "Batch loss: 0.9177488684654236 batch: 752/840\n",
      "Batch loss: 0.5210528373718262 batch: 753/840\n",
      "Batch loss: 0.615459680557251 batch: 754/840\n",
      "Batch loss: 0.5938770771026611 batch: 755/840\n",
      "Batch loss: 0.5120543837547302 batch: 756/840\n",
      "Batch loss: 0.6571048498153687 batch: 757/840\n",
      "Batch loss: 0.4998916983604431 batch: 758/840\n",
      "Batch loss: 0.4878213405609131 batch: 759/840\n",
      "Batch loss: 0.5611961483955383 batch: 760/840\n",
      "Batch loss: 0.5569649338722229 batch: 761/840\n",
      "Batch loss: 0.717897355556488 batch: 762/840\n",
      "Batch loss: 0.43728476762771606 batch: 763/840\n",
      "Batch loss: 0.609836757183075 batch: 764/840\n",
      "Batch loss: 0.4694799780845642 batch: 765/840\n",
      "Batch loss: 0.5535840392112732 batch: 766/840\n",
      "Batch loss: 0.7215271592140198 batch: 767/840\n",
      "Batch loss: 0.6430392265319824 batch: 768/840\n",
      "Batch loss: 0.5634744167327881 batch: 769/840\n",
      "Batch loss: 0.4762943983078003 batch: 770/840\n",
      "Batch loss: 0.6552064418792725 batch: 771/840\n",
      "Batch loss: 0.5667259097099304 batch: 772/840\n",
      "Batch loss: 0.5102220773696899 batch: 773/840\n",
      "Batch loss: 0.4359223246574402 batch: 774/840\n",
      "Batch loss: 0.5450590252876282 batch: 775/840\n",
      "Batch loss: 0.5498209595680237 batch: 776/840\n",
      "Batch loss: 0.5572091937065125 batch: 777/840\n",
      "Batch loss: 0.4579685926437378 batch: 778/840\n",
      "Batch loss: 0.7120427489280701 batch: 779/840\n",
      "Batch loss: 0.6163183450698853 batch: 780/840\n",
      "Batch loss: 0.5775561332702637 batch: 781/840\n",
      "Batch loss: 0.5741493105888367 batch: 782/840\n",
      "Batch loss: 0.45409417152404785 batch: 783/840\n",
      "Batch loss: 0.6794794201850891 batch: 784/840\n",
      "Batch loss: 0.5711485743522644 batch: 785/840\n",
      "Batch loss: 0.5718070864677429 batch: 786/840\n",
      "Batch loss: 0.5451628565788269 batch: 787/840\n",
      "Batch loss: 0.6055648922920227 batch: 788/840\n",
      "Batch loss: 0.6603925228118896 batch: 789/840\n",
      "Batch loss: 0.7606157064437866 batch: 790/840\n",
      "Batch loss: 0.5217701196670532 batch: 791/840\n",
      "Batch loss: 0.3636399507522583 batch: 792/840\n",
      "Batch loss: 0.6168176531791687 batch: 793/840\n",
      "Batch loss: 0.573108434677124 batch: 794/840\n",
      "Batch loss: 0.6583845615386963 batch: 795/840\n",
      "Batch loss: 0.7665239572525024 batch: 796/840\n",
      "Batch loss: 0.5959212183952332 batch: 797/840\n",
      "Batch loss: 0.5588639974594116 batch: 798/840\n",
      "Batch loss: 0.5878786444664001 batch: 799/840\n",
      "Batch loss: 0.4622865915298462 batch: 800/840\n",
      "Batch loss: 0.6487221717834473 batch: 801/840\n",
      "Batch loss: 0.4679984152317047 batch: 802/840\n",
      "Batch loss: 0.4127255976200104 batch: 803/840\n",
      "Batch loss: 0.6740542650222778 batch: 804/840\n",
      "Batch loss: 0.5768120884895325 batch: 805/840\n",
      "Batch loss: 0.6324569582939148 batch: 806/840\n",
      "Batch loss: 0.6364322304725647 batch: 807/840\n",
      "Batch loss: 0.6001479625701904 batch: 808/840\n",
      "Batch loss: 0.6289697885513306 batch: 809/840\n",
      "Batch loss: 0.5946261286735535 batch: 810/840\n",
      "Batch loss: 0.5676143765449524 batch: 811/840\n",
      "Batch loss: 0.5753506422042847 batch: 812/840\n",
      "Batch loss: 0.5575874447822571 batch: 813/840\n",
      "Batch loss: 0.5970896482467651 batch: 814/840\n",
      "Batch loss: 0.676240861415863 batch: 815/840\n",
      "Batch loss: 0.6378946304321289 batch: 816/840\n",
      "Batch loss: 0.7318201661109924 batch: 817/840\n",
      "Batch loss: 0.618467390537262 batch: 818/840\n",
      "Batch loss: 0.5015500783920288 batch: 819/840\n",
      "Batch loss: 0.7355952262878418 batch: 820/840\n",
      "Batch loss: 0.6116433143615723 batch: 821/840\n",
      "Batch loss: 0.6797624826431274 batch: 822/840\n",
      "Batch loss: 0.6835792064666748 batch: 823/840\n",
      "Batch loss: 0.6162757277488708 batch: 824/840\n",
      "Batch loss: 0.703370213508606 batch: 825/840\n",
      "Batch loss: 0.5765421986579895 batch: 826/840\n",
      "Batch loss: 0.5536050796508789 batch: 827/840\n",
      "Batch loss: 0.6472856402397156 batch: 828/840\n",
      "Batch loss: 0.6376706957817078 batch: 829/840\n",
      "Batch loss: 0.7306322455406189 batch: 830/840\n",
      "Batch loss: 0.5463257431983948 batch: 831/840\n",
      "Batch loss: 0.5763610601425171 batch: 832/840\n",
      "Batch loss: 0.7693608999252319 batch: 833/840\n",
      "Batch loss: 0.5796348452568054 batch: 834/840\n",
      "Batch loss: 0.49456536769866943 batch: 835/840\n",
      "Batch loss: 0.5077386498451233 batch: 836/840\n",
      "Batch loss: 0.5825925469398499 batch: 837/840\n",
      "Batch loss: 0.6804670691490173 batch: 838/840\n",
      "Batch loss: 0.5507738590240479 batch: 839/840\n",
      "Batch loss: 0.7478116750717163 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 14/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.816\n",
      "Running epoch 15/15\n",
      "Batch loss: 0.45811232924461365 batch: 1/840\n",
      "Batch loss: 1.1083197593688965 batch: 2/840\n",
      "Batch loss: 0.6226295232772827 batch: 3/840\n",
      "Batch loss: 0.6512033939361572 batch: 4/840\n",
      "Batch loss: 0.6108394861221313 batch: 5/840\n",
      "Batch loss: 0.4445784389972687 batch: 6/840\n",
      "Batch loss: 0.5534914135932922 batch: 7/840\n",
      "Batch loss: 0.5392876267433167 batch: 8/840\n",
      "Batch loss: 0.4923807382583618 batch: 9/840\n",
      "Batch loss: 0.5751879811286926 batch: 10/840\n",
      "Batch loss: 0.5374080538749695 batch: 11/840\n",
      "Batch loss: 0.5383071303367615 batch: 12/840\n",
      "Batch loss: 0.5275496244430542 batch: 13/840\n",
      "Batch loss: 0.6070115566253662 batch: 14/840\n",
      "Batch loss: 0.5548355579376221 batch: 15/840\n",
      "Batch loss: 0.4673941731452942 batch: 16/840\n",
      "Batch loss: 0.4524903893470764 batch: 17/840\n",
      "Batch loss: 0.6738365292549133 batch: 18/840\n",
      "Batch loss: 0.6279027462005615 batch: 19/840\n",
      "Batch loss: 0.7342907190322876 batch: 20/840\n",
      "Batch loss: 0.6132281422615051 batch: 21/840\n",
      "Batch loss: 0.5432897806167603 batch: 22/840\n",
      "Batch loss: 0.7178775072097778 batch: 23/840\n",
      "Batch loss: 0.43447577953338623 batch: 24/840\n",
      "Batch loss: 0.6223680377006531 batch: 25/840\n",
      "Batch loss: 0.5654935240745544 batch: 26/840\n",
      "Batch loss: 0.63151615858078 batch: 27/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6021156907081604 batch: 28/840\n",
      "Batch loss: 0.5735129117965698 batch: 29/840\n",
      "Batch loss: 0.5109794735908508 batch: 30/840\n",
      "Batch loss: 0.5904938578605652 batch: 31/840\n",
      "Batch loss: 0.5981833338737488 batch: 32/840\n",
      "Batch loss: 0.6113447546958923 batch: 33/840\n",
      "Batch loss: 0.4487757384777069 batch: 34/840\n",
      "Batch loss: 0.494591623544693 batch: 35/840\n",
      "Batch loss: 0.4872990548610687 batch: 36/840\n",
      "Batch loss: 0.6875902414321899 batch: 37/840\n",
      "Batch loss: 0.7168633937835693 batch: 38/840\n",
      "Batch loss: 0.7149865031242371 batch: 39/840\n",
      "Batch loss: 0.5362414717674255 batch: 40/840\n",
      "Batch loss: 0.8019624352455139 batch: 41/840\n",
      "Batch loss: 0.5821481943130493 batch: 42/840\n",
      "Batch loss: 0.5890933275222778 batch: 43/840\n",
      "Batch loss: 0.682228147983551 batch: 44/840\n",
      "Batch loss: 0.5858602523803711 batch: 45/840\n",
      "Batch loss: 0.5124677419662476 batch: 46/840\n",
      "Batch loss: 0.5184600353240967 batch: 47/840\n",
      "Batch loss: 0.5900648832321167 batch: 48/840\n",
      "Batch loss: 0.6455181837081909 batch: 49/840\n",
      "Batch loss: 0.6943686604499817 batch: 50/840\n",
      "Batch loss: 0.6112581491470337 batch: 51/840\n",
      "Batch loss: 0.7183350920677185 batch: 52/840\n",
      "Batch loss: 0.5282990336418152 batch: 53/840\n",
      "Batch loss: 0.6635856628417969 batch: 54/840\n",
      "Batch loss: 0.5914559960365295 batch: 55/840\n",
      "Batch loss: 0.599764883518219 batch: 56/840\n",
      "Batch loss: 0.5914096236228943 batch: 57/840\n",
      "Batch loss: 0.502642035484314 batch: 58/840\n",
      "Batch loss: 0.6232933402061462 batch: 59/840\n",
      "Batch loss: 0.528312623500824 batch: 60/840\n",
      "Batch loss: 0.7223654389381409 batch: 61/840\n",
      "Batch loss: 0.605108380317688 batch: 62/840\n",
      "Batch loss: 0.6581575870513916 batch: 63/840\n",
      "Batch loss: 0.7201799750328064 batch: 64/840\n",
      "Batch loss: 0.5627424716949463 batch: 65/840\n",
      "Batch loss: 0.5735978484153748 batch: 66/840\n",
      "Batch loss: 0.6637431383132935 batch: 67/840\n",
      "Batch loss: 0.6019957065582275 batch: 68/840\n",
      "Batch loss: 0.7184947729110718 batch: 69/840\n",
      "Batch loss: 0.5289148092269897 batch: 70/840\n",
      "Batch loss: 0.6767242550849915 batch: 71/840\n",
      "Batch loss: 0.6964318156242371 batch: 72/840\n",
      "Batch loss: 0.7423313856124878 batch: 73/840\n",
      "Batch loss: 0.5865061283111572 batch: 74/840\n",
      "Batch loss: 0.6549205780029297 batch: 75/840\n",
      "Batch loss: 0.4098970890045166 batch: 76/840\n",
      "Batch loss: 0.5664315223693848 batch: 77/840\n",
      "Batch loss: 0.7985435724258423 batch: 78/840\n",
      "Batch loss: 0.6084314584732056 batch: 79/840\n",
      "Batch loss: 0.6736888885498047 batch: 80/840\n",
      "Batch loss: 0.5519720911979675 batch: 81/840\n",
      "Batch loss: 0.5673079490661621 batch: 82/840\n",
      "Batch loss: 0.5437675714492798 batch: 83/840\n",
      "Batch loss: 0.6975885033607483 batch: 84/840\n",
      "Batch loss: 0.6439808011054993 batch: 85/840\n",
      "Batch loss: 0.9581282734870911 batch: 86/840\n",
      "Batch loss: 0.5046485662460327 batch: 87/840\n",
      "Batch loss: 0.40764090418815613 batch: 88/840\n",
      "Batch loss: 0.44796082377433777 batch: 89/840\n",
      "Batch loss: 0.5065372586250305 batch: 90/840\n",
      "Batch loss: 0.5739279389381409 batch: 91/840\n",
      "Batch loss: 0.6156134009361267 batch: 92/840\n",
      "Batch loss: 0.6284353733062744 batch: 93/840\n",
      "Batch loss: 0.47464805841445923 batch: 94/840\n",
      "Batch loss: 0.6532154679298401 batch: 95/840\n",
      "Batch loss: 0.5047198534011841 batch: 96/840\n",
      "Batch loss: 0.637610673904419 batch: 97/840\n",
      "Batch loss: 0.7737067937850952 batch: 98/840\n",
      "Batch loss: 0.5402426719665527 batch: 99/840\n",
      "Batch loss: 0.6700270771980286 batch: 100/840\n",
      "Batch loss: 0.5214547514915466 batch: 101/840\n",
      "Batch loss: 0.4305437207221985 batch: 102/840\n",
      "Batch loss: 0.5966559052467346 batch: 103/840\n",
      "Batch loss: 0.5246259570121765 batch: 104/840\n",
      "Batch loss: 0.5770728588104248 batch: 105/840\n",
      "Batch loss: 0.7206098437309265 batch: 106/840\n",
      "Batch loss: 0.5398903489112854 batch: 107/840\n",
      "Batch loss: 0.8196612000465393 batch: 108/840\n",
      "Batch loss: 0.5979653596878052 batch: 109/840\n",
      "Batch loss: 0.4936859607696533 batch: 110/840\n",
      "Batch loss: 0.5446556210517883 batch: 111/840\n",
      "Batch loss: 0.6814711093902588 batch: 112/840\n",
      "Batch loss: 0.6420598030090332 batch: 113/840\n",
      "Batch loss: 0.5506348013877869 batch: 114/840\n",
      "Batch loss: 0.6911467909812927 batch: 115/840\n",
      "Batch loss: 0.5991480350494385 batch: 116/840\n",
      "Batch loss: 0.5730461478233337 batch: 117/840\n",
      "Batch loss: 0.49594226479530334 batch: 118/840\n",
      "Batch loss: 0.732176661491394 batch: 119/840\n",
      "Batch loss: 0.5392971634864807 batch: 120/840\n",
      "Batch loss: 0.7084080576896667 batch: 121/840\n",
      "Batch loss: 0.7715887427330017 batch: 122/840\n",
      "Batch loss: 0.49259239435195923 batch: 123/840\n",
      "Batch loss: 0.5728116631507874 batch: 124/840\n",
      "Batch loss: 0.5490445494651794 batch: 125/840\n",
      "Batch loss: 0.5913078784942627 batch: 126/840\n",
      "Batch loss: 0.682318925857544 batch: 127/840\n",
      "Batch loss: 0.6761289834976196 batch: 128/840\n",
      "Batch loss: 0.6729873418807983 batch: 129/840\n",
      "Batch loss: 0.6343296766281128 batch: 130/840\n",
      "Batch loss: 0.6247191429138184 batch: 131/840\n",
      "Batch loss: 0.938626766204834 batch: 132/840\n",
      "Batch loss: 0.694658100605011 batch: 133/840\n",
      "Batch loss: 0.6043897867202759 batch: 134/840\n",
      "Batch loss: 0.5619468092918396 batch: 135/840\n",
      "Batch loss: 0.6880490779876709 batch: 136/840\n",
      "Batch loss: 0.5341567397117615 batch: 137/840\n",
      "Batch loss: 0.609834611415863 batch: 138/840\n",
      "Batch loss: 0.5895158052444458 batch: 139/840\n",
      "Batch loss: 0.7574436664581299 batch: 140/840\n",
      "Batch loss: 0.4511687755584717 batch: 141/840\n",
      "Batch loss: 0.5521855354309082 batch: 142/840\n",
      "Batch loss: 0.5262014865875244 batch: 143/840\n",
      "Batch loss: 0.6043587923049927 batch: 144/840\n",
      "Batch loss: 0.6950498819351196 batch: 145/840\n",
      "Batch loss: 0.6469001173973083 batch: 146/840\n",
      "Batch loss: 0.4858131408691406 batch: 147/840\n",
      "Batch loss: 0.7057952284812927 batch: 148/840\n",
      "Batch loss: 0.6889013051986694 batch: 149/840\n",
      "Batch loss: 0.6178810000419617 batch: 150/840\n",
      "Batch loss: 0.5708010196685791 batch: 151/840\n",
      "Batch loss: 0.5991243124008179 batch: 152/840\n",
      "Batch loss: 0.5511414408683777 batch: 153/840\n",
      "Batch loss: 0.737783670425415 batch: 154/840\n",
      "Batch loss: 0.5157290697097778 batch: 155/840\n",
      "Batch loss: 0.656087338924408 batch: 156/840\n",
      "Batch loss: 0.648597002029419 batch: 157/840\n",
      "Batch loss: 0.4791492223739624 batch: 158/840\n",
      "Batch loss: 0.47350776195526123 batch: 159/840\n",
      "Batch loss: 0.5580700039863586 batch: 160/840\n",
      "Batch loss: 0.6473807692527771 batch: 161/840\n",
      "Batch loss: 0.6486113667488098 batch: 162/840\n",
      "Batch loss: 0.7545526623725891 batch: 163/840\n",
      "Batch loss: 0.43263280391693115 batch: 164/840\n",
      "Batch loss: 0.6675505042076111 batch: 165/840\n",
      "Batch loss: 0.49276724457740784 batch: 166/840\n",
      "Batch loss: 0.6355986595153809 batch: 167/840\n",
      "Batch loss: 0.7043858170509338 batch: 168/840\n",
      "Batch loss: 0.48999109864234924 batch: 169/840\n",
      "Batch loss: 0.707563579082489 batch: 170/840\n",
      "Batch loss: 0.6087126731872559 batch: 171/840\n",
      "Batch loss: 0.7442494034767151 batch: 172/840\n",
      "Batch loss: 0.590583860874176 batch: 173/840\n",
      "Batch loss: 0.5996540188789368 batch: 174/840\n",
      "Batch loss: 0.6130767464637756 batch: 175/840\n",
      "Batch loss: 0.7061548829078674 batch: 176/840\n",
      "Batch loss: 0.5901138782501221 batch: 177/840\n",
      "Batch loss: 0.6515862345695496 batch: 178/840\n",
      "Batch loss: 0.6686532497406006 batch: 179/840\n",
      "Batch loss: 0.46786367893218994 batch: 180/840\n",
      "Batch loss: 0.525535523891449 batch: 181/840\n",
      "Batch loss: 0.5873168706893921 batch: 182/840\n",
      "Batch loss: 0.7538831830024719 batch: 183/840\n",
      "Batch loss: 0.4875122308731079 batch: 184/840\n",
      "Batch loss: 0.4216795265674591 batch: 185/840\n",
      "Batch loss: 0.5124651193618774 batch: 186/840\n",
      "Batch loss: 0.6106393933296204 batch: 187/840\n",
      "Batch loss: 0.5134781002998352 batch: 188/840\n",
      "Batch loss: 0.5842614769935608 batch: 189/840\n",
      "Batch loss: 0.7086212635040283 batch: 190/840\n",
      "Batch loss: 0.8658574819564819 batch: 191/840\n",
      "Batch loss: 0.4449169933795929 batch: 192/840\n",
      "Batch loss: 0.4480072259902954 batch: 193/840\n",
      "Batch loss: 0.4187833368778229 batch: 194/840\n",
      "Batch loss: 0.5582166314125061 batch: 195/840\n",
      "Batch loss: 0.730170488357544 batch: 196/840\n",
      "Batch loss: 0.6134916543960571 batch: 197/840\n",
      "Batch loss: 0.4929153323173523 batch: 198/840\n",
      "Batch loss: 0.5597830414772034 batch: 199/840\n",
      "Batch loss: 0.8042430877685547 batch: 200/840\n",
      "Batch loss: 0.5894637107849121 batch: 201/840\n",
      "Batch loss: 0.5423139333724976 batch: 202/840\n",
      "Batch loss: 0.5355672836303711 batch: 203/840\n",
      "Batch loss: 0.6681519150733948 batch: 204/840\n",
      "Batch loss: 0.7418822646141052 batch: 205/840\n",
      "Batch loss: 0.6681916117668152 batch: 206/840\n",
      "Batch loss: 0.6297212243080139 batch: 207/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5972554087638855 batch: 208/840\n",
      "Batch loss: 0.5354229807853699 batch: 209/840\n",
      "Batch loss: 0.6074554324150085 batch: 210/840\n",
      "Batch loss: 0.49014610052108765 batch: 211/840\n",
      "Batch loss: 0.5661182999610901 batch: 212/840\n",
      "Batch loss: 0.7376562356948853 batch: 213/840\n",
      "Batch loss: 0.6857926249504089 batch: 214/840\n",
      "Batch loss: 0.5795580148696899 batch: 215/840\n",
      "Batch loss: 0.6052008271217346 batch: 216/840\n",
      "Batch loss: 0.7540960907936096 batch: 217/840\n",
      "Batch loss: 0.6341010928153992 batch: 218/840\n",
      "Batch loss: 0.606706440448761 batch: 219/840\n",
      "Batch loss: 0.7404935956001282 batch: 220/840\n",
      "Batch loss: 0.502988874912262 batch: 221/840\n",
      "Batch loss: 0.7262333631515503 batch: 222/840\n",
      "Batch loss: 0.5107777118682861 batch: 223/840\n",
      "Batch loss: 0.7443498969078064 batch: 224/840\n",
      "Batch loss: 0.6478846073150635 batch: 225/840\n",
      "Batch loss: 0.7010459899902344 batch: 226/840\n",
      "Batch loss: 0.7102625966072083 batch: 227/840\n",
      "Batch loss: 0.5508115887641907 batch: 228/840\n",
      "Batch loss: 0.46950867772102356 batch: 229/840\n",
      "Batch loss: 0.5847160220146179 batch: 230/840\n",
      "Batch loss: 0.4987185597419739 batch: 231/840\n",
      "Batch loss: 0.5933867692947388 batch: 232/840\n",
      "Batch loss: 0.7380827069282532 batch: 233/840\n",
      "Batch loss: 0.560271680355072 batch: 234/840\n",
      "Batch loss: 0.555297315120697 batch: 235/840\n",
      "Batch loss: 0.6655780076980591 batch: 236/840\n",
      "Batch loss: 0.5263563394546509 batch: 237/840\n",
      "Batch loss: 0.6942012310028076 batch: 238/840\n",
      "Batch loss: 0.6066264510154724 batch: 239/840\n",
      "Batch loss: 0.5850980877876282 batch: 240/840\n",
      "Batch loss: 0.754574716091156 batch: 241/840\n",
      "Batch loss: 0.5950651168823242 batch: 242/840\n",
      "Batch loss: 0.5940989851951599 batch: 243/840\n",
      "Batch loss: 0.7496497631072998 batch: 244/840\n",
      "Batch loss: 0.5282310843467712 batch: 245/840\n",
      "Batch loss: 0.6469603776931763 batch: 246/840\n",
      "Batch loss: 0.691882312297821 batch: 247/840\n",
      "Batch loss: 0.7508229613304138 batch: 248/840\n",
      "Batch loss: 0.9411064982414246 batch: 249/840\n",
      "Batch loss: 0.5394818782806396 batch: 250/840\n",
      "Batch loss: 0.49266374111175537 batch: 251/840\n",
      "Batch loss: 0.5295614004135132 batch: 252/840\n",
      "Batch loss: 0.637263298034668 batch: 253/840\n",
      "Batch loss: 0.6818622350692749 batch: 254/840\n",
      "Batch loss: 0.619996964931488 batch: 255/840\n",
      "Batch loss: 0.6406422257423401 batch: 256/840\n",
      "Batch loss: 0.48577815294265747 batch: 257/840\n",
      "Batch loss: 0.7653711438179016 batch: 258/840\n",
      "Batch loss: 0.45616206526756287 batch: 259/840\n",
      "Batch loss: 0.49218448996543884 batch: 260/840\n",
      "Batch loss: 0.5245451331138611 batch: 261/840\n",
      "Batch loss: 0.3905567526817322 batch: 262/840\n",
      "Batch loss: 0.565608024597168 batch: 263/840\n",
      "Batch loss: 0.5377100110054016 batch: 264/840\n",
      "Batch loss: 0.5551292300224304 batch: 265/840\n",
      "Batch loss: 0.570193350315094 batch: 266/840\n",
      "Batch loss: 0.6620549559593201 batch: 267/840\n",
      "Batch loss: 0.5514234304428101 batch: 268/840\n",
      "Batch loss: 0.5548726320266724 batch: 269/840\n",
      "Batch loss: 0.5944116115570068 batch: 270/840\n",
      "Batch loss: 0.5051957964897156 batch: 271/840\n",
      "Batch loss: 0.7745897769927979 batch: 272/840\n",
      "Batch loss: 0.8320685029029846 batch: 273/840\n",
      "Batch loss: 0.6113373637199402 batch: 274/840\n",
      "Batch loss: 0.7782652378082275 batch: 275/840\n",
      "Batch loss: 0.5278048515319824 batch: 276/840\n",
      "Batch loss: 0.5943477153778076 batch: 277/840\n",
      "Batch loss: 0.8312634825706482 batch: 278/840\n",
      "Batch loss: 0.702339768409729 batch: 279/840\n",
      "Batch loss: 0.7254237532615662 batch: 280/840\n",
      "Batch loss: 0.5284101366996765 batch: 281/840\n",
      "Batch loss: 0.5144178867340088 batch: 282/840\n",
      "Batch loss: 0.6411024332046509 batch: 283/840\n",
      "Batch loss: 0.46735772490501404 batch: 284/840\n",
      "Batch loss: 0.5399102568626404 batch: 285/840\n",
      "Batch loss: 0.6189377307891846 batch: 286/840\n",
      "Batch loss: 0.4189673662185669 batch: 287/840\n",
      "Batch loss: 0.5018985271453857 batch: 288/840\n",
      "Batch loss: 0.7077987790107727 batch: 289/840\n",
      "Batch loss: 0.8372603058815002 batch: 290/840\n",
      "Batch loss: 0.6445760130882263 batch: 291/840\n",
      "Batch loss: 0.590519368648529 batch: 292/840\n",
      "Batch loss: 0.616605818271637 batch: 293/840\n",
      "Batch loss: 0.5924672484397888 batch: 294/840\n",
      "Batch loss: 0.5111612677574158 batch: 295/840\n",
      "Batch loss: 0.7204611897468567 batch: 296/840\n",
      "Batch loss: 0.5773171186447144 batch: 297/840\n",
      "Batch loss: 0.6784302592277527 batch: 298/840\n",
      "Batch loss: 0.5661265850067139 batch: 299/840\n",
      "Batch loss: 0.6837051510810852 batch: 300/840\n",
      "Batch loss: 0.7080201506614685 batch: 301/840\n",
      "Batch loss: 0.6039088368415833 batch: 302/840\n",
      "Batch loss: 0.640472948551178 batch: 303/840\n",
      "Batch loss: 0.49669745564460754 batch: 304/840\n",
      "Batch loss: 0.5121185779571533 batch: 305/840\n",
      "Batch loss: 0.6168881058692932 batch: 306/840\n",
      "Batch loss: 0.5741708874702454 batch: 307/840\n",
      "Batch loss: 0.7058767080307007 batch: 308/840\n",
      "Batch loss: 0.5556126236915588 batch: 309/840\n",
      "Batch loss: 0.8173702359199524 batch: 310/840\n",
      "Batch loss: 0.5519742369651794 batch: 311/840\n",
      "Batch loss: 0.6935550570487976 batch: 312/840\n",
      "Batch loss: 0.6891629099845886 batch: 313/840\n",
      "Batch loss: 0.433669775724411 batch: 314/840\n",
      "Batch loss: 0.5951154828071594 batch: 315/840\n",
      "Batch loss: 0.4526495635509491 batch: 316/840\n",
      "Batch loss: 0.6458932757377625 batch: 317/840\n",
      "Batch loss: 0.6349151134490967 batch: 318/840\n",
      "Batch loss: 0.7317905426025391 batch: 319/840\n",
      "Batch loss: 0.5578371286392212 batch: 320/840\n",
      "Batch loss: 0.5674157738685608 batch: 321/840\n",
      "Batch loss: 0.5948849320411682 batch: 322/840\n",
      "Batch loss: 0.7586277723312378 batch: 323/840\n",
      "Batch loss: 0.6106854677200317 batch: 324/840\n",
      "Batch loss: 0.49504268169403076 batch: 325/840\n",
      "Batch loss: 0.6422235369682312 batch: 326/840\n",
      "Batch loss: 0.5566940903663635 batch: 327/840\n",
      "Batch loss: 0.6617343425750732 batch: 328/840\n",
      "Batch loss: 0.7450305819511414 batch: 329/840\n",
      "Batch loss: 0.6127148270606995 batch: 330/840\n",
      "Batch loss: 0.6972867846488953 batch: 331/840\n",
      "Batch loss: 0.6597552299499512 batch: 332/840\n",
      "Batch loss: 0.5389736294746399 batch: 333/840\n",
      "Batch loss: 0.6399287581443787 batch: 334/840\n",
      "Batch loss: 0.5208539366722107 batch: 335/840\n",
      "Batch loss: 0.6037229299545288 batch: 336/840\n",
      "Batch loss: 0.7541481256484985 batch: 337/840\n",
      "Batch loss: 0.7181038856506348 batch: 338/840\n",
      "Batch loss: 0.5287973284721375 batch: 339/840\n",
      "Batch loss: 0.7857170104980469 batch: 340/840\n",
      "Batch loss: 0.577848494052887 batch: 341/840\n",
      "Batch loss: 0.42005324363708496 batch: 342/840\n",
      "Batch loss: 0.8149873614311218 batch: 343/840\n",
      "Batch loss: 0.5677974820137024 batch: 344/840\n",
      "Batch loss: 0.4748934805393219 batch: 345/840\n",
      "Batch loss: 0.6228579878807068 batch: 346/840\n",
      "Batch loss: 0.6173164248466492 batch: 347/840\n",
      "Batch loss: 0.5383410453796387 batch: 348/840\n",
      "Batch loss: 0.646533727645874 batch: 349/840\n",
      "Batch loss: 0.5483815670013428 batch: 350/840\n",
      "Batch loss: 0.7126424312591553 batch: 351/840\n",
      "Batch loss: 0.5823574662208557 batch: 352/840\n",
      "Batch loss: 0.6570212841033936 batch: 353/840\n",
      "Batch loss: 0.6606947183609009 batch: 354/840\n",
      "Batch loss: 0.5874472260475159 batch: 355/840\n",
      "Batch loss: 0.4488002061843872 batch: 356/840\n",
      "Batch loss: 0.6035298109054565 batch: 357/840\n",
      "Batch loss: 0.6421039700508118 batch: 358/840\n",
      "Batch loss: 0.5922119617462158 batch: 359/840\n",
      "Batch loss: 0.7852170467376709 batch: 360/840\n",
      "Batch loss: 0.7477223873138428 batch: 361/840\n",
      "Batch loss: 0.5271193981170654 batch: 362/840\n",
      "Batch loss: 0.6237422227859497 batch: 363/840\n",
      "Batch loss: 0.6315290331840515 batch: 364/840\n",
      "Batch loss: 0.48774388432502747 batch: 365/840\n",
      "Batch loss: 0.6339418292045593 batch: 366/840\n",
      "Batch loss: 0.42181068658828735 batch: 367/840\n",
      "Batch loss: 0.7011731863021851 batch: 368/840\n",
      "Batch loss: 0.5465574264526367 batch: 369/840\n",
      "Batch loss: 0.7798868417739868 batch: 370/840\n",
      "Batch loss: 0.6123000383377075 batch: 371/840\n",
      "Batch loss: 0.5477502942085266 batch: 372/840\n",
      "Batch loss: 0.5932220220565796 batch: 373/840\n",
      "Batch loss: 0.7840862274169922 batch: 374/840\n",
      "Batch loss: 0.5103150010108948 batch: 375/840\n",
      "Batch loss: 0.5269532203674316 batch: 376/840\n",
      "Batch loss: 0.6591143012046814 batch: 377/840\n",
      "Batch loss: 0.5308864712715149 batch: 378/840\n",
      "Batch loss: 0.5357276201248169 batch: 379/840\n",
      "Batch loss: 0.8561905026435852 batch: 380/840\n",
      "Batch loss: 0.7771059274673462 batch: 381/840\n",
      "Batch loss: 0.6093423962593079 batch: 382/840\n",
      "Batch loss: 0.6564793586730957 batch: 383/840\n",
      "Batch loss: 0.6064603924751282 batch: 384/840\n",
      "Batch loss: 0.6998807787895203 batch: 385/840\n",
      "Batch loss: 0.7041065096855164 batch: 386/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5868216156959534 batch: 387/840\n",
      "Batch loss: 0.7090156674385071 batch: 388/840\n",
      "Batch loss: 0.5429076552391052 batch: 389/840\n",
      "Batch loss: 0.8035603165626526 batch: 390/840\n",
      "Batch loss: 0.5088576674461365 batch: 391/840\n",
      "Batch loss: 0.6249111294746399 batch: 392/840\n",
      "Batch loss: 0.44435226917266846 batch: 393/840\n",
      "Batch loss: 0.7629169225692749 batch: 394/840\n",
      "Batch loss: 0.5455337762832642 batch: 395/840\n",
      "Batch loss: 0.6442421078681946 batch: 396/840\n",
      "Batch loss: 0.599124550819397 batch: 397/840\n",
      "Batch loss: 0.7209210395812988 batch: 398/840\n",
      "Batch loss: 0.501073956489563 batch: 399/840\n",
      "Batch loss: 0.590226948261261 batch: 400/840\n",
      "Batch loss: 0.7158079743385315 batch: 401/840\n",
      "Batch loss: 0.4878758192062378 batch: 402/840\n",
      "Batch loss: 0.6109613180160522 batch: 403/840\n",
      "Batch loss: 0.5311827063560486 batch: 404/840\n",
      "Batch loss: 0.5416290760040283 batch: 405/840\n",
      "Batch loss: 0.6825850605964661 batch: 406/840\n",
      "Batch loss: 0.541194498538971 batch: 407/840\n",
      "Batch loss: 0.6545137763023376 batch: 408/840\n",
      "Batch loss: 0.6991758942604065 batch: 409/840\n",
      "Batch loss: 0.6822594404220581 batch: 410/840\n",
      "Batch loss: 0.7268694043159485 batch: 411/840\n",
      "Batch loss: 0.736175537109375 batch: 412/840\n",
      "Batch loss: 0.6417230367660522 batch: 413/840\n",
      "Batch loss: 0.6251525282859802 batch: 414/840\n",
      "Batch loss: 0.9051380157470703 batch: 415/840\n",
      "Batch loss: 0.48210322856903076 batch: 416/840\n",
      "Batch loss: 0.5462442636489868 batch: 417/840\n",
      "Batch loss: 0.7458499670028687 batch: 418/840\n",
      "Batch loss: 0.5724313855171204 batch: 419/840\n",
      "Batch loss: 0.6577833294868469 batch: 420/840\n",
      "Batch loss: 0.4222729504108429 batch: 421/840\n",
      "Batch loss: 0.5383245944976807 batch: 422/840\n",
      "Batch loss: 0.6770138740539551 batch: 423/840\n",
      "Batch loss: 0.5836667418479919 batch: 424/840\n",
      "Batch loss: 0.6835038661956787 batch: 425/840\n",
      "Batch loss: 0.5870009064674377 batch: 426/840\n",
      "Batch loss: 0.6013579368591309 batch: 427/840\n",
      "Batch loss: 0.7368980646133423 batch: 428/840\n",
      "Batch loss: 0.5290585160255432 batch: 429/840\n",
      "Batch loss: 0.761981725692749 batch: 430/840\n",
      "Batch loss: 0.6069988012313843 batch: 431/840\n",
      "Batch loss: 0.5852348804473877 batch: 432/840\n",
      "Batch loss: 0.4155879616737366 batch: 433/840\n",
      "Batch loss: 0.5336276292800903 batch: 434/840\n",
      "Batch loss: 0.6642064452171326 batch: 435/840\n",
      "Batch loss: 0.5918504595756531 batch: 436/840\n",
      "Batch loss: 0.6727287173271179 batch: 437/840\n",
      "Batch loss: 0.3847738206386566 batch: 438/840\n",
      "Batch loss: 0.6346426010131836 batch: 439/840\n",
      "Batch loss: 0.6175182461738586 batch: 440/840\n",
      "Batch loss: 0.6510849595069885 batch: 441/840\n",
      "Batch loss: 0.5762301087379456 batch: 442/840\n",
      "Batch loss: 0.5770943760871887 batch: 443/840\n",
      "Batch loss: 0.6079592704772949 batch: 444/840\n",
      "Batch loss: 0.5525555610656738 batch: 445/840\n",
      "Batch loss: 0.6798714399337769 batch: 446/840\n",
      "Batch loss: 0.6728543639183044 batch: 447/840\n",
      "Batch loss: 0.684811532497406 batch: 448/840\n",
      "Batch loss: 0.4680596590042114 batch: 449/840\n",
      "Batch loss: 0.6048129200935364 batch: 450/840\n",
      "Batch loss: 0.5680590271949768 batch: 451/840\n",
      "Batch loss: 0.7046110033988953 batch: 452/840\n",
      "Batch loss: 0.6388681530952454 batch: 453/840\n",
      "Batch loss: 0.5592718720436096 batch: 454/840\n",
      "Batch loss: 0.5670613646507263 batch: 455/840\n",
      "Batch loss: 0.5061460137367249 batch: 456/840\n",
      "Batch loss: 0.5325531363487244 batch: 457/840\n",
      "Batch loss: 0.6074894666671753 batch: 458/840\n",
      "Batch loss: 0.5249370336532593 batch: 459/840\n",
      "Batch loss: 0.3869294822216034 batch: 460/840\n",
      "Batch loss: 0.7836107015609741 batch: 461/840\n",
      "Batch loss: 0.6272377967834473 batch: 462/840\n",
      "Batch loss: 0.7419019937515259 batch: 463/840\n",
      "Batch loss: 0.41248103976249695 batch: 464/840\n",
      "Batch loss: 0.8706890940666199 batch: 465/840\n",
      "Batch loss: 0.580897867679596 batch: 466/840\n",
      "Batch loss: 0.9329431653022766 batch: 467/840\n",
      "Batch loss: 0.5399682521820068 batch: 468/840\n",
      "Batch loss: 0.5695666074752808 batch: 469/840\n",
      "Batch loss: 0.5953652858734131 batch: 470/840\n",
      "Batch loss: 0.6248725056648254 batch: 471/840\n",
      "Batch loss: 0.8161422610282898 batch: 472/840\n",
      "Batch loss: 0.7097030878067017 batch: 473/840\n",
      "Batch loss: 0.5251952409744263 batch: 474/840\n",
      "Batch loss: 0.665160596370697 batch: 475/840\n",
      "Batch loss: 0.6198986172676086 batch: 476/840\n",
      "Batch loss: 0.42710068821907043 batch: 477/840\n",
      "Batch loss: 0.4687286615371704 batch: 478/840\n",
      "Batch loss: 0.49688920378685 batch: 479/840\n",
      "Batch loss: 0.6106258034706116 batch: 480/840\n",
      "Batch loss: 0.5854296684265137 batch: 481/840\n",
      "Batch loss: 0.6136499643325806 batch: 482/840\n",
      "Batch loss: 0.8226121664047241 batch: 483/840\n",
      "Batch loss: 0.514097273349762 batch: 484/840\n",
      "Batch loss: 0.49319398403167725 batch: 485/840\n",
      "Batch loss: 0.5686481595039368 batch: 486/840\n",
      "Batch loss: 0.4740261733531952 batch: 487/840\n",
      "Batch loss: 0.6147465109825134 batch: 488/840\n",
      "Batch loss: 0.5546398162841797 batch: 489/840\n",
      "Batch loss: 0.5924856662750244 batch: 490/840\n",
      "Batch loss: 0.37632256746292114 batch: 491/840\n",
      "Batch loss: 0.6550464630126953 batch: 492/840\n",
      "Batch loss: 0.6121532917022705 batch: 493/840\n",
      "Batch loss: 0.43152302503585815 batch: 494/840\n",
      "Batch loss: 0.8320987820625305 batch: 495/840\n",
      "Batch loss: 0.6156984567642212 batch: 496/840\n",
      "Batch loss: 0.6419810652732849 batch: 497/840\n",
      "Batch loss: 0.4369092583656311 batch: 498/840\n",
      "Batch loss: 0.6874517798423767 batch: 499/840\n",
      "Batch loss: 0.5703195929527283 batch: 500/840\n",
      "Batch loss: 0.5399722456932068 batch: 501/840\n",
      "Batch loss: 0.6542296409606934 batch: 502/840\n",
      "Batch loss: 0.4900375306606293 batch: 503/840\n",
      "Batch loss: 0.6445146799087524 batch: 504/840\n",
      "Batch loss: 0.6783971190452576 batch: 505/840\n",
      "Batch loss: 0.5311009883880615 batch: 506/840\n",
      "Batch loss: 0.6875464916229248 batch: 507/840\n",
      "Batch loss: 0.517839789390564 batch: 508/840\n",
      "Batch loss: 0.7074503302574158 batch: 509/840\n",
      "Batch loss: 0.5864798426628113 batch: 510/840\n",
      "Batch loss: 0.4723464846611023 batch: 511/840\n",
      "Batch loss: 0.6087992191314697 batch: 512/840\n",
      "Batch loss: 0.602628767490387 batch: 513/840\n",
      "Batch loss: 0.710349440574646 batch: 514/840\n",
      "Batch loss: 0.4516608715057373 batch: 515/840\n",
      "Batch loss: 0.7020012736320496 batch: 516/840\n",
      "Batch loss: 0.6140272617340088 batch: 517/840\n",
      "Batch loss: 0.6744216680526733 batch: 518/840\n",
      "Batch loss: 0.7878504991531372 batch: 519/840\n",
      "Batch loss: 0.6444138288497925 batch: 520/840\n",
      "Batch loss: 0.6895850300788879 batch: 521/840\n",
      "Batch loss: 0.591420590877533 batch: 522/840\n",
      "Batch loss: 0.4875677227973938 batch: 523/840\n",
      "Batch loss: 0.7214210033416748 batch: 524/840\n",
      "Batch loss: 0.5564210414886475 batch: 525/840\n",
      "Batch loss: 0.6900248527526855 batch: 526/840\n",
      "Batch loss: 0.680833637714386 batch: 527/840\n",
      "Batch loss: 0.6256269216537476 batch: 528/840\n",
      "Batch loss: 0.5154610872268677 batch: 529/840\n",
      "Batch loss: 0.5558111667633057 batch: 530/840\n",
      "Batch loss: 0.5310511589050293 batch: 531/840\n",
      "Batch loss: 0.4486702084541321 batch: 532/840\n",
      "Batch loss: 0.6728498935699463 batch: 533/840\n",
      "Batch loss: 0.6305618286132812 batch: 534/840\n",
      "Batch loss: 0.5931811332702637 batch: 535/840\n",
      "Batch loss: 0.6287310719490051 batch: 536/840\n",
      "Batch loss: 0.643649697303772 batch: 537/840\n",
      "Batch loss: 0.5053192973136902 batch: 538/840\n",
      "Batch loss: 0.609330415725708 batch: 539/840\n",
      "Batch loss: 0.7145398855209351 batch: 540/840\n",
      "Batch loss: 0.5520023107528687 batch: 541/840\n",
      "Batch loss: 0.6430734395980835 batch: 542/840\n",
      "Batch loss: 0.39857304096221924 batch: 543/840\n",
      "Batch loss: 0.6254681944847107 batch: 544/840\n",
      "Batch loss: 0.673715353012085 batch: 545/840\n",
      "Batch loss: 0.5421239137649536 batch: 546/840\n",
      "Batch loss: 0.5091658234596252 batch: 547/840\n",
      "Batch loss: 0.471024751663208 batch: 548/840\n",
      "Batch loss: 0.5010433793067932 batch: 549/840\n",
      "Batch loss: 0.5018201470375061 batch: 550/840\n",
      "Batch loss: 0.696021556854248 batch: 551/840\n",
      "Batch loss: 0.4263855814933777 batch: 552/840\n",
      "Batch loss: 0.6857188940048218 batch: 553/840\n",
      "Batch loss: 0.6829026937484741 batch: 554/840\n",
      "Batch loss: 0.6079176068305969 batch: 555/840\n",
      "Batch loss: 0.6269418001174927 batch: 556/840\n",
      "Batch loss: 0.5819898843765259 batch: 557/840\n",
      "Batch loss: 0.6505609154701233 batch: 558/840\n",
      "Batch loss: 0.5107420086860657 batch: 559/840\n",
      "Batch loss: 0.6197854280471802 batch: 560/840\n",
      "Batch loss: 0.6162340641021729 batch: 561/840\n",
      "Batch loss: 0.49941059947013855 batch: 562/840\n",
      "Batch loss: 0.4406820237636566 batch: 563/840\n",
      "Batch loss: 0.5787259936332703 batch: 564/840\n",
      "Batch loss: 0.7510669231414795 batch: 565/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6115856766700745 batch: 566/840\n",
      "Batch loss: 0.8163964152336121 batch: 567/840\n",
      "Batch loss: 0.5675003528594971 batch: 568/840\n",
      "Batch loss: 0.6457807421684265 batch: 569/840\n",
      "Batch loss: 0.5076826810836792 batch: 570/840\n",
      "Batch loss: 0.769838809967041 batch: 571/840\n",
      "Batch loss: 0.6338231563568115 batch: 572/840\n",
      "Batch loss: 0.6582450270652771 batch: 573/840\n",
      "Batch loss: 0.7656224966049194 batch: 574/840\n",
      "Batch loss: 0.5773225426673889 batch: 575/840\n",
      "Batch loss: 0.570605456829071 batch: 576/840\n",
      "Batch loss: 0.4971667230129242 batch: 577/840\n",
      "Batch loss: 0.681968629360199 batch: 578/840\n",
      "Batch loss: 0.583021342754364 batch: 579/840\n",
      "Batch loss: 0.8553261756896973 batch: 580/840\n",
      "Batch loss: 0.6238713264465332 batch: 581/840\n",
      "Batch loss: 0.7455951571464539 batch: 582/840\n",
      "Batch loss: 0.5325659513473511 batch: 583/840\n",
      "Batch loss: 0.7433427572250366 batch: 584/840\n",
      "Batch loss: 0.5405387282371521 batch: 585/840\n",
      "Batch loss: 0.596453070640564 batch: 586/840\n",
      "Batch loss: 0.6466764807701111 batch: 587/840\n",
      "Batch loss: 0.6038901805877686 batch: 588/840\n",
      "Batch loss: 0.6473907232284546 batch: 589/840\n",
      "Batch loss: 0.6382755041122437 batch: 590/840\n",
      "Batch loss: 0.35428521037101746 batch: 591/840\n",
      "Batch loss: 0.5107002258300781 batch: 592/840\n",
      "Batch loss: 0.7576454877853394 batch: 593/840\n",
      "Batch loss: 0.5961301922798157 batch: 594/840\n",
      "Batch loss: 0.3269703984260559 batch: 595/840\n",
      "Batch loss: 0.39877110719680786 batch: 596/840\n",
      "Batch loss: 0.573017418384552 batch: 597/840\n",
      "Batch loss: 0.44916820526123047 batch: 598/840\n",
      "Batch loss: 0.6934231519699097 batch: 599/840\n",
      "Batch loss: 0.6001988053321838 batch: 600/840\n",
      "Batch loss: 0.6716780066490173 batch: 601/840\n",
      "Batch loss: 0.47190016508102417 batch: 602/840\n",
      "Batch loss: 0.6075832843780518 batch: 603/840\n",
      "Batch loss: 0.6602815985679626 batch: 604/840\n",
      "Batch loss: 0.8008551597595215 batch: 605/840\n",
      "Batch loss: 0.8510645031929016 batch: 606/840\n",
      "Batch loss: 0.6786004900932312 batch: 607/840\n",
      "Batch loss: 0.5650021433830261 batch: 608/840\n",
      "Batch loss: 0.5849172472953796 batch: 609/840\n",
      "Batch loss: 0.6742687225341797 batch: 610/840\n",
      "Batch loss: 0.6694832444190979 batch: 611/840\n",
      "Batch loss: 0.710560142993927 batch: 612/840\n",
      "Batch loss: 0.691088080406189 batch: 613/840\n",
      "Batch loss: 0.5850109457969666 batch: 614/840\n",
      "Batch loss: 0.5308511257171631 batch: 615/840\n",
      "Batch loss: 0.47779038548469543 batch: 616/840\n",
      "Batch loss: 0.5442243218421936 batch: 617/840\n",
      "Batch loss: 0.6014792919158936 batch: 618/840\n",
      "Batch loss: 0.7267778515815735 batch: 619/840\n",
      "Batch loss: 0.5247629880905151 batch: 620/840\n",
      "Batch loss: 0.6035275459289551 batch: 621/840\n",
      "Batch loss: 0.6017756462097168 batch: 622/840\n",
      "Batch loss: 0.585462212562561 batch: 623/840\n",
      "Batch loss: 0.768311083316803 batch: 624/840\n",
      "Batch loss: 0.6503797769546509 batch: 625/840\n",
      "Batch loss: 0.6111810207366943 batch: 626/840\n",
      "Batch loss: 0.5570045113563538 batch: 627/840\n",
      "Batch loss: 0.5892650485038757 batch: 628/840\n",
      "Batch loss: 0.5993655920028687 batch: 629/840\n",
      "Batch loss: 0.53597092628479 batch: 630/840\n",
      "Batch loss: 0.6380742788314819 batch: 631/840\n",
      "Batch loss: 0.6569502353668213 batch: 632/840\n",
      "Batch loss: 0.4858400821685791 batch: 633/840\n",
      "Batch loss: 0.6662734746932983 batch: 634/840\n",
      "Batch loss: 0.5582931041717529 batch: 635/840\n",
      "Batch loss: 0.5196669101715088 batch: 636/840\n",
      "Batch loss: 0.5352943539619446 batch: 637/840\n",
      "Batch loss: 0.6525521874427795 batch: 638/840\n",
      "Batch loss: 0.6651278138160706 batch: 639/840\n",
      "Batch loss: 0.5796352028846741 batch: 640/840\n",
      "Batch loss: 0.7594344615936279 batch: 641/840\n",
      "Batch loss: 0.5963388681411743 batch: 642/840\n",
      "Batch loss: 0.8585491180419922 batch: 643/840\n",
      "Batch loss: 0.5682386159896851 batch: 644/840\n",
      "Batch loss: 0.5359650254249573 batch: 645/840\n",
      "Batch loss: 0.5690069794654846 batch: 646/840\n",
      "Batch loss: 0.6332535147666931 batch: 647/840\n",
      "Batch loss: 0.5998218059539795 batch: 648/840\n",
      "Batch loss: 0.601966142654419 batch: 649/840\n",
      "Batch loss: 0.5270782113075256 batch: 650/840\n",
      "Batch loss: 0.5941241979598999 batch: 651/840\n",
      "Batch loss: 0.7504913210868835 batch: 652/840\n",
      "Batch loss: 0.516394853591919 batch: 653/840\n",
      "Batch loss: 0.7891575694084167 batch: 654/840\n",
      "Batch loss: 0.6076741218566895 batch: 655/840\n",
      "Batch loss: 0.6209293603897095 batch: 656/840\n",
      "Batch loss: 0.5564825534820557 batch: 657/840\n",
      "Batch loss: 0.6336123943328857 batch: 658/840\n",
      "Batch loss: 0.6278128027915955 batch: 659/840\n",
      "Batch loss: 0.5989031791687012 batch: 660/840\n",
      "Batch loss: 0.7882199287414551 batch: 661/840\n",
      "Batch loss: 0.6101253032684326 batch: 662/840\n",
      "Batch loss: 0.6548088192939758 batch: 663/840\n",
      "Batch loss: 0.6437728404998779 batch: 664/840\n",
      "Batch loss: 0.6600924730300903 batch: 665/840\n",
      "Batch loss: 0.6254845261573792 batch: 666/840\n",
      "Batch loss: 0.6774287223815918 batch: 667/840\n",
      "Batch loss: 0.5305777192115784 batch: 668/840\n",
      "Batch loss: 0.6030817031860352 batch: 669/840\n",
      "Batch loss: 0.5947596430778503 batch: 670/840\n",
      "Batch loss: 0.6563836932182312 batch: 671/840\n",
      "Batch loss: 0.5778344869613647 batch: 672/840\n",
      "Batch loss: 0.7565372586250305 batch: 673/840\n",
      "Batch loss: 0.6704297661781311 batch: 674/840\n",
      "Batch loss: 0.6451169848442078 batch: 675/840\n",
      "Batch loss: 0.5306954383850098 batch: 676/840\n",
      "Batch loss: 0.6419148445129395 batch: 677/840\n",
      "Batch loss: 0.6799435615539551 batch: 678/840\n",
      "Batch loss: 0.8309096693992615 batch: 679/840\n",
      "Batch loss: 0.6684606671333313 batch: 680/840\n",
      "Batch loss: 0.6689434051513672 batch: 681/840\n",
      "Batch loss: 0.6420881152153015 batch: 682/840\n",
      "Batch loss: 0.553988516330719 batch: 683/840\n",
      "Batch loss: 0.8469083905220032 batch: 684/840\n",
      "Batch loss: 0.612356424331665 batch: 685/840\n",
      "Batch loss: 0.7734052538871765 batch: 686/840\n",
      "Batch loss: 0.690568745136261 batch: 687/840\n",
      "Batch loss: 0.623232364654541 batch: 688/840\n",
      "Batch loss: 0.5718336701393127 batch: 689/840\n",
      "Batch loss: 0.5374205112457275 batch: 690/840\n",
      "Batch loss: 0.6371331810951233 batch: 691/840\n",
      "Batch loss: 0.6357640624046326 batch: 692/840\n",
      "Batch loss: 0.6657404899597168 batch: 693/840\n",
      "Batch loss: 0.6935780048370361 batch: 694/840\n",
      "Batch loss: 0.6202605962753296 batch: 695/840\n",
      "Batch loss: 0.6267274022102356 batch: 696/840\n",
      "Batch loss: 0.5851894617080688 batch: 697/840\n",
      "Batch loss: 0.595486044883728 batch: 698/840\n",
      "Batch loss: 0.7068057060241699 batch: 699/840\n",
      "Batch loss: 0.6749449372291565 batch: 700/840\n",
      "Batch loss: 0.6868634223937988 batch: 701/840\n",
      "Batch loss: 0.6484792828559875 batch: 702/840\n",
      "Batch loss: 0.6994680762290955 batch: 703/840\n",
      "Batch loss: 0.7092059850692749 batch: 704/840\n",
      "Batch loss: 0.613995373249054 batch: 705/840\n",
      "Batch loss: 0.5951858162879944 batch: 706/840\n",
      "Batch loss: 0.6631699204444885 batch: 707/840\n",
      "Batch loss: 0.6219282150268555 batch: 708/840\n",
      "Batch loss: 0.7271666526794434 batch: 709/840\n",
      "Batch loss: 0.8156127333641052 batch: 710/840\n",
      "Batch loss: 0.44199156761169434 batch: 711/840\n",
      "Batch loss: 0.7835512757301331 batch: 712/840\n",
      "Batch loss: 0.5345831513404846 batch: 713/840\n",
      "Batch loss: 0.598852813243866 batch: 714/840\n",
      "Batch loss: 0.816577672958374 batch: 715/840\n",
      "Batch loss: 0.6559327840805054 batch: 716/840\n",
      "Batch loss: 0.762544572353363 batch: 717/840\n",
      "Batch loss: 0.5878655910491943 batch: 718/840\n",
      "Batch loss: 0.5472426414489746 batch: 719/840\n",
      "Batch loss: 0.5954152345657349 batch: 720/840\n",
      "Batch loss: 0.683834433555603 batch: 721/840\n",
      "Batch loss: 0.7764973640441895 batch: 722/840\n",
      "Batch loss: 0.5012838244438171 batch: 723/840\n",
      "Batch loss: 0.8202220797538757 batch: 724/840\n",
      "Batch loss: 0.7103267908096313 batch: 725/840\n",
      "Batch loss: 0.5698848366737366 batch: 726/840\n",
      "Batch loss: 0.6504359245300293 batch: 727/840\n",
      "Batch loss: 0.6917517185211182 batch: 728/840\n",
      "Batch loss: 0.6704090237617493 batch: 729/840\n",
      "Batch loss: 0.6986468434333801 batch: 730/840\n",
      "Batch loss: 0.500704288482666 batch: 731/840\n",
      "Batch loss: 0.7054010033607483 batch: 732/840\n",
      "Batch loss: 0.5713416934013367 batch: 733/840\n",
      "Batch loss: 0.5493916273117065 batch: 734/840\n",
      "Batch loss: 0.6803291440010071 batch: 735/840\n",
      "Batch loss: 0.5960351228713989 batch: 736/840\n",
      "Batch loss: 0.618638277053833 batch: 737/840\n",
      "Batch loss: 0.5143008828163147 batch: 738/840\n",
      "Batch loss: 0.6781225800514221 batch: 739/840\n",
      "Batch loss: 0.7644713521003723 batch: 740/840\n",
      "Batch loss: 0.49746426939964294 batch: 741/840\n",
      "Batch loss: 0.581377387046814 batch: 742/840\n",
      "Batch loss: 0.42611953616142273 batch: 743/840\n",
      "Batch loss: 0.5299518704414368 batch: 744/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5446745753288269 batch: 745/840\n",
      "Batch loss: 0.6182487607002258 batch: 746/840\n",
      "Batch loss: 0.7370836734771729 batch: 747/840\n",
      "Batch loss: 0.6698896884918213 batch: 748/840\n",
      "Batch loss: 0.542156457901001 batch: 749/840\n",
      "Batch loss: 0.4097430109977722 batch: 750/840\n",
      "Batch loss: 0.6536552906036377 batch: 751/840\n",
      "Batch loss: 0.8325798511505127 batch: 752/840\n",
      "Batch loss: 0.4543101489543915 batch: 753/840\n",
      "Batch loss: 0.6355700492858887 batch: 754/840\n",
      "Batch loss: 0.5765249729156494 batch: 755/840\n",
      "Batch loss: 0.554629385471344 batch: 756/840\n",
      "Batch loss: 0.7365243434906006 batch: 757/840\n",
      "Batch loss: 0.5000913739204407 batch: 758/840\n",
      "Batch loss: 0.5045077204704285 batch: 759/840\n",
      "Batch loss: 0.569338321685791 batch: 760/840\n",
      "Batch loss: 0.489003449678421 batch: 761/840\n",
      "Batch loss: 0.8078245520591736 batch: 762/840\n",
      "Batch loss: 0.4633783996105194 batch: 763/840\n",
      "Batch loss: 0.5938063263893127 batch: 764/840\n",
      "Batch loss: 0.47184890508651733 batch: 765/840\n",
      "Batch loss: 0.5797761678695679 batch: 766/840\n",
      "Batch loss: 0.6228775382041931 batch: 767/840\n",
      "Batch loss: 0.6489959955215454 batch: 768/840\n",
      "Batch loss: 0.593310534954071 batch: 769/840\n",
      "Batch loss: 0.6225935220718384 batch: 770/840\n",
      "Batch loss: 0.6114181876182556 batch: 771/840\n",
      "Batch loss: 0.4666973054409027 batch: 772/840\n",
      "Batch loss: 0.5232582092285156 batch: 773/840\n",
      "Batch loss: 0.4356083273887634 batch: 774/840\n",
      "Batch loss: 0.49974390864372253 batch: 775/840\n",
      "Batch loss: 0.4995076358318329 batch: 776/840\n",
      "Batch loss: 0.4266093969345093 batch: 777/840\n",
      "Batch loss: 0.4290367066860199 batch: 778/840\n",
      "Batch loss: 0.7406404614448547 batch: 779/840\n",
      "Batch loss: 0.5718177556991577 batch: 780/840\n",
      "Batch loss: 0.6707674264907837 batch: 781/840\n",
      "Batch loss: 0.5640389919281006 batch: 782/840\n",
      "Batch loss: 0.45383113622665405 batch: 783/840\n",
      "Batch loss: 0.7043517827987671 batch: 784/840\n",
      "Batch loss: 0.5343902111053467 batch: 785/840\n",
      "Batch loss: 0.7011523246765137 batch: 786/840\n",
      "Batch loss: 0.509574830532074 batch: 787/840\n",
      "Batch loss: 0.6524469256401062 batch: 788/840\n",
      "Batch loss: 0.7047359347343445 batch: 789/840\n",
      "Batch loss: 0.6330021619796753 batch: 790/840\n",
      "Batch loss: 0.6097828149795532 batch: 791/840\n",
      "Batch loss: 0.37941333651542664 batch: 792/840\n",
      "Batch loss: 0.5718061923980713 batch: 793/840\n",
      "Batch loss: 0.6272184252738953 batch: 794/840\n",
      "Batch loss: 0.6657535433769226 batch: 795/840\n",
      "Batch loss: 0.6577233672142029 batch: 796/840\n",
      "Batch loss: 0.6358896493911743 batch: 797/840\n",
      "Batch loss: 0.732444167137146 batch: 798/840\n",
      "Batch loss: 0.5902425050735474 batch: 799/840\n",
      "Batch loss: 0.5233396291732788 batch: 800/840\n",
      "Batch loss: 0.7103937268257141 batch: 801/840\n",
      "Batch loss: 0.4915095269680023 batch: 802/840\n",
      "Batch loss: 0.5313786864280701 batch: 803/840\n",
      "Batch loss: 0.7207344174385071 batch: 804/840\n",
      "Batch loss: 0.5200048089027405 batch: 805/840\n",
      "Batch loss: 0.7755113244056702 batch: 806/840\n",
      "Batch loss: 0.7053383588790894 batch: 807/840\n",
      "Batch loss: 0.6676795482635498 batch: 808/840\n",
      "Batch loss: 0.6103734374046326 batch: 809/840\n",
      "Batch loss: 0.5744026303291321 batch: 810/840\n",
      "Batch loss: 0.5325297117233276 batch: 811/840\n",
      "Batch loss: 0.5840043425559998 batch: 812/840\n",
      "Batch loss: 0.6014295816421509 batch: 813/840\n",
      "Batch loss: 0.6247525215148926 batch: 814/840\n",
      "Batch loss: 0.6849014163017273 batch: 815/840\n",
      "Batch loss: 0.6573808789253235 batch: 816/840\n",
      "Batch loss: 0.662889838218689 batch: 817/840\n",
      "Batch loss: 0.7417044639587402 batch: 818/840\n",
      "Batch loss: 0.5389643311500549 batch: 819/840\n",
      "Batch loss: 0.6523484587669373 batch: 820/840\n",
      "Batch loss: 0.6709242463111877 batch: 821/840\n",
      "Batch loss: 0.5825122594833374 batch: 822/840\n",
      "Batch loss: 0.6984928846359253 batch: 823/840\n",
      "Batch loss: 0.7298630475997925 batch: 824/840\n",
      "Batch loss: 0.6629869937896729 batch: 825/840\n",
      "Batch loss: 0.6550341844558716 batch: 826/840\n",
      "Batch loss: 0.5255725383758545 batch: 827/840\n",
      "Batch loss: 0.6470170617103577 batch: 828/840\n",
      "Batch loss: 0.57442706823349 batch: 829/840\n",
      "Batch loss: 0.7178085446357727 batch: 830/840\n",
      "Batch loss: 0.5599928498268127 batch: 831/840\n",
      "Batch loss: 0.7927252054214478 batch: 832/840\n",
      "Batch loss: 0.6887117028236389 batch: 833/840\n",
      "Batch loss: 0.57737135887146 batch: 834/840\n",
      "Batch loss: 0.511448085308075 batch: 835/840\n",
      "Batch loss: 0.4500316381454468 batch: 836/840\n",
      "Batch loss: 0.6513654589653015 batch: 837/840\n",
      "Batch loss: 0.7558839321136475 batch: 838/840\n",
      "Batch loss: 0.600027859210968 batch: 839/840\n",
      "Batch loss: 0.7432034015655518 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 15/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(torchModel.parameters(), lr=0.003)\n",
    "#optimizer = optim.Adam(torchModel.parameters(), lr=0.0015)\n",
    "optimizer = optim.Adam(torchModel.TorchModel.parameters(), lr=0.0015)\n",
    "\n",
    "epochs = 15\n",
    "steps = 0\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "torchModel.to(device)\n",
    "#optimizer.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f'Running epoch {e+1}/{epochs}')\n",
    "    for images, labels in mMiniBatcherTrain.getBatchIterator():\n",
    "        #print('Training batch...')\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        #print(f'labels.shape: {labels.shape}')\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        \n",
    "        #print('Running torch')\n",
    "        output = torchModel(images)\n",
    "        \n",
    "        #print(f'output.shape: {output.shape}')\n",
    "        #print('Calculating loss')\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        #print('Back prop.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        print(f'Batch loss: {batch_loss} {mMiniBatcherTrain.getBatchInfo()}')\n",
    "        running_loss += batch_loss\n",
    "        \n",
    "        del labels\n",
    "        del images\n",
    "        del output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "   \n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    print('Running evaluation loop...')\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        torchModel.eval()\n",
    "        for images, labels in mMiniBatcherTest.getBatchIterator():\n",
    "            #print('Validation batch...')\n",
    "            #labels = torch.from_numpy(labels.values).type(torch.FloatTensor)\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "            output = torchModel(images)\n",
    "            test_loss += criterion(output, labels).to('cpu') #Want the loss on CPU\n",
    "\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "            #print(top_p)\n",
    "            #top_p_target, top_class_target = labels.topk(1, dim=1)\n",
    "            #equals = top_class == top_class_target\n",
    "            equals = top_class == labels.view(top_class.shape)\n",
    "            accuracy += torch.sum(equals.type(torch.FloatTensor)).to('cpu')\n",
    "            print(mMiniBatcherTest.getBatchInfo())\n",
    "            \n",
    "            del labels\n",
    "            del images\n",
    "            del output\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() \n",
    "\n",
    "    torchModel.train()\n",
    "\n",
    "    train_losses.append(running_loss/len(mMiniBatcherTrain.X))\n",
    "    test_losses.append(test_loss/len(mMiniBatcherTest.X))\n",
    "\n",
    "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "          \"Training Loss: {:.5f}.. \".format(train_losses[-1]),\n",
    "          \"Test Loss: {:.5f}.. \".format(test_losses[-1]),\n",
    "          \"Test Accuracy: {:.5f}\".format(accuracy/len(mMiniBatcherTest.X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "humanitarian-designer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchNLP(\n",
       "  (Bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50325, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (TorchModel): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchModel.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "healthy-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_light_train, X_light_test, Y_light_train, Y_light_test = train_test_split(X_transformed, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sealed-matter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112000, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_light_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "intermediate-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "mMiniBatcherTrain = MiniBatcher(X_light_train, Y_light_train, batch_size=500)\n",
    "mMiniBatcherTest = MiniBatcher(X_light_test, Y_light_test, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "distinct-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, test_losses = [], []\n",
    "optimizer = optim.Adam(torchModelLight.TorchModel.parameters(), lr=0.0015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "imperial-passing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1/75\n",
      "Batch loss: 2.051389217376709 batch: 1/224\n",
      "Batch loss: 1.5002692937850952 batch: 2/224\n",
      "Batch loss: 1.3102997541427612 batch: 3/224\n",
      "Batch loss: 1.2626101970672607 batch: 4/224\n",
      "Batch loss: 1.2128772735595703 batch: 5/224\n",
      "Batch loss: 1.1883563995361328 batch: 6/224\n",
      "Batch loss: 1.0900918245315552 batch: 7/224\n",
      "Batch loss: 1.100092887878418 batch: 8/224\n",
      "Batch loss: 1.073155403137207 batch: 9/224\n",
      "Batch loss: 0.9463008046150208 batch: 10/224\n",
      "Batch loss: 0.9881950616836548 batch: 11/224\n",
      "Batch loss: 0.936345100402832 batch: 12/224\n",
      "Batch loss: 0.9024549722671509 batch: 13/224\n",
      "Batch loss: 0.9313780069351196 batch: 14/224\n",
      "Batch loss: 0.9049537777900696 batch: 15/224\n",
      "Batch loss: 1.058172345161438 batch: 16/224\n",
      "Batch loss: 0.8997771143913269 batch: 17/224\n",
      "Batch loss: 0.9437781572341919 batch: 18/224\n",
      "Batch loss: 0.8697070479393005 batch: 19/224\n",
      "Batch loss: 0.8494426012039185 batch: 20/224\n",
      "Batch loss: 0.9026504755020142 batch: 21/224\n",
      "Batch loss: 0.8266898393630981 batch: 22/224\n",
      "Batch loss: 0.9140889048576355 batch: 23/224\n",
      "Batch loss: 0.8625554442405701 batch: 24/224\n",
      "Batch loss: 0.8174707889556885 batch: 25/224\n",
      "Batch loss: 0.7958838939666748 batch: 26/224\n",
      "Batch loss: 0.9041134715080261 batch: 27/224\n",
      "Batch loss: 0.842427134513855 batch: 28/224\n",
      "Batch loss: 0.8349437713623047 batch: 29/224\n",
      "Batch loss: 0.8137657642364502 batch: 30/224\n",
      "Batch loss: 0.8130100965499878 batch: 31/224\n",
      "Batch loss: 0.8460472822189331 batch: 32/224\n",
      "Batch loss: 0.7621215581893921 batch: 33/224\n",
      "Batch loss: 0.8246039152145386 batch: 34/224\n",
      "Batch loss: 0.8218666911125183 batch: 35/224\n",
      "Batch loss: 0.8822758793830872 batch: 36/224\n",
      "Batch loss: 0.7825161814689636 batch: 37/224\n",
      "Batch loss: 0.7395969033241272 batch: 38/224\n",
      "Batch loss: 0.8128405809402466 batch: 39/224\n",
      "Batch loss: 0.7457241415977478 batch: 40/224\n",
      "Batch loss: 0.8832455277442932 batch: 41/224\n",
      "Batch loss: 0.7296960353851318 batch: 42/224\n",
      "Batch loss: 0.8630692362785339 batch: 43/224\n",
      "Batch loss: 0.7161561250686646 batch: 44/224\n",
      "Batch loss: 0.7371024489402771 batch: 45/224\n",
      "Batch loss: 0.8535147309303284 batch: 46/224\n",
      "Batch loss: 0.7805078625679016 batch: 47/224\n",
      "Batch loss: 0.7695302963256836 batch: 48/224\n",
      "Batch loss: 0.7613201141357422 batch: 49/224\n",
      "Batch loss: 0.7362508177757263 batch: 50/224\n",
      "Batch loss: 0.7253468036651611 batch: 51/224\n",
      "Batch loss: 0.6710477471351624 batch: 52/224\n",
      "Batch loss: 0.8600229024887085 batch: 53/224\n",
      "Batch loss: 0.7167454957962036 batch: 54/224\n",
      "Batch loss: 0.7069209218025208 batch: 55/224\n",
      "Batch loss: 0.6786784529685974 batch: 56/224\n",
      "Batch loss: 0.7179375886917114 batch: 57/224\n",
      "Batch loss: 0.7581527233123779 batch: 58/224\n",
      "Batch loss: 0.7428990006446838 batch: 59/224\n",
      "Batch loss: 0.7532960176467896 batch: 60/224\n",
      "Batch loss: 0.7467865943908691 batch: 61/224\n",
      "Batch loss: 0.6377713084220886 batch: 62/224\n",
      "Batch loss: 0.689509391784668 batch: 63/224\n",
      "Batch loss: 0.7443867921829224 batch: 64/224\n",
      "Batch loss: 0.7411324381828308 batch: 65/224\n",
      "Batch loss: 0.7090088725090027 batch: 66/224\n",
      "Batch loss: 0.6612797379493713 batch: 67/224\n",
      "Batch loss: 0.7615366578102112 batch: 68/224\n",
      "Batch loss: 0.7081378102302551 batch: 69/224\n",
      "Batch loss: 0.7980939149856567 batch: 70/224\n",
      "Batch loss: 0.6590852737426758 batch: 71/224\n",
      "Batch loss: 0.6484129428863525 batch: 72/224\n",
      "Batch loss: 0.7465053200721741 batch: 73/224\n",
      "Batch loss: 0.6848455667495728 batch: 74/224\n",
      "Batch loss: 0.7080488801002502 batch: 75/224\n",
      "Batch loss: 0.7016991376876831 batch: 76/224\n",
      "Batch loss: 0.6637151837348938 batch: 77/224\n",
      "Batch loss: 0.6547191143035889 batch: 78/224\n",
      "Batch loss: 0.6888176798820496 batch: 79/224\n",
      "Batch loss: 0.7581915855407715 batch: 80/224\n",
      "Batch loss: 0.7773349285125732 batch: 81/224\n",
      "Batch loss: 0.6763076186180115 batch: 82/224\n",
      "Batch loss: 0.7791423201560974 batch: 83/224\n",
      "Batch loss: 0.6776788234710693 batch: 84/224\n",
      "Batch loss: 0.7026448249816895 batch: 85/224\n",
      "Batch loss: 0.6712601184844971 batch: 86/224\n",
      "Batch loss: 0.7163977026939392 batch: 87/224\n",
      "Batch loss: 0.6694482564926147 batch: 88/224\n",
      "Batch loss: 0.6606312990188599 batch: 89/224\n",
      "Batch loss: 0.6815131902694702 batch: 90/224\n",
      "Batch loss: 0.6073213219642639 batch: 91/224\n",
      "Batch loss: 0.6732760071754456 batch: 92/224\n",
      "Batch loss: 0.6038950085639954 batch: 93/224\n",
      "Batch loss: 0.6540464162826538 batch: 94/224\n",
      "Batch loss: 0.6318058967590332 batch: 95/224\n",
      "Batch loss: 0.6688753366470337 batch: 96/224\n",
      "Batch loss: 0.6216245293617249 batch: 97/224\n",
      "Batch loss: 0.6082533597946167 batch: 98/224\n",
      "Batch loss: 0.7602621912956238 batch: 99/224\n",
      "Batch loss: 0.6640486717224121 batch: 100/224\n",
      "Batch loss: 0.682677149772644 batch: 101/224\n",
      "Batch loss: 0.6043474674224854 batch: 102/224\n",
      "Batch loss: 0.6393913626670837 batch: 103/224\n",
      "Batch loss: 0.6102749705314636 batch: 104/224\n",
      "Batch loss: 0.6493261456489563 batch: 105/224\n",
      "Batch loss: 0.6466190218925476 batch: 106/224\n",
      "Batch loss: 0.6145304441452026 batch: 107/224\n",
      "Batch loss: 0.6479306221008301 batch: 108/224\n",
      "Batch loss: 0.5959933400154114 batch: 109/224\n",
      "Batch loss: 0.6135138869285583 batch: 110/224\n",
      "Batch loss: 0.7408918738365173 batch: 111/224\n",
      "Batch loss: 0.5759739279747009 batch: 112/224\n",
      "Batch loss: 0.7079041004180908 batch: 113/224\n",
      "Batch loss: 0.6511693596839905 batch: 114/224\n",
      "Batch loss: 0.7686052322387695 batch: 115/224\n",
      "Batch loss: 0.5777552127838135 batch: 116/224\n",
      "Batch loss: 0.6491776704788208 batch: 117/224\n",
      "Batch loss: 0.620991051197052 batch: 118/224\n",
      "Batch loss: 0.6552988886833191 batch: 119/224\n",
      "Batch loss: 0.6084817051887512 batch: 120/224\n",
      "Batch loss: 0.6314381957054138 batch: 121/224\n",
      "Batch loss: 0.625390887260437 batch: 122/224\n",
      "Batch loss: 0.6070993542671204 batch: 123/224\n",
      "Batch loss: 0.5821665525436401 batch: 124/224\n",
      "Batch loss: 0.7099757790565491 batch: 125/224\n",
      "Batch loss: 0.6080737709999084 batch: 126/224\n",
      "Batch loss: 0.6281038522720337 batch: 127/224\n",
      "Batch loss: 0.6241399645805359 batch: 128/224\n",
      "Batch loss: 0.6248025298118591 batch: 129/224\n",
      "Batch loss: 0.6882136464118958 batch: 130/224\n",
      "Batch loss: 0.6188119053840637 batch: 131/224\n",
      "Batch loss: 0.5992018580436707 batch: 132/224\n",
      "Batch loss: 0.5905043482780457 batch: 133/224\n",
      "Batch loss: 0.6186363101005554 batch: 134/224\n",
      "Batch loss: 0.7162119150161743 batch: 135/224\n",
      "Batch loss: 0.6922503709793091 batch: 136/224\n",
      "Batch loss: 0.5820032358169556 batch: 137/224\n",
      "Batch loss: 0.6375352144241333 batch: 138/224\n",
      "Batch loss: 0.6566834449768066 batch: 139/224\n",
      "Batch loss: 0.6783004403114319 batch: 140/224\n",
      "Batch loss: 0.5113466382026672 batch: 141/224\n",
      "Batch loss: 0.6280938982963562 batch: 142/224\n",
      "Batch loss: 0.6319569945335388 batch: 143/224\n",
      "Batch loss: 0.6244553923606873 batch: 144/224\n",
      "Batch loss: 0.585187554359436 batch: 145/224\n",
      "Batch loss: 0.5852469205856323 batch: 146/224\n",
      "Batch loss: 0.6149492263793945 batch: 147/224\n",
      "Batch loss: 0.6128419637680054 batch: 148/224\n",
      "Batch loss: 0.6108561754226685 batch: 149/224\n",
      "Batch loss: 0.6428839564323425 batch: 150/224\n",
      "Batch loss: 0.6041656732559204 batch: 151/224\n",
      "Batch loss: 0.5819145441055298 batch: 152/224\n",
      "Batch loss: 0.6785731911659241 batch: 153/224\n",
      "Batch loss: 0.626785933971405 batch: 154/224\n",
      "Batch loss: 0.5868253111839294 batch: 155/224\n",
      "Batch loss: 0.6901644468307495 batch: 156/224\n",
      "Batch loss: 0.6356691122055054 batch: 157/224\n",
      "Batch loss: 0.6640803217887878 batch: 158/224\n",
      "Batch loss: 0.502220630645752 batch: 159/224\n",
      "Batch loss: 0.5615450143814087 batch: 160/224\n",
      "Batch loss: 0.5698418617248535 batch: 161/224\n",
      "Batch loss: 0.6019636392593384 batch: 162/224\n",
      "Batch loss: 0.6050254106521606 batch: 163/224\n",
      "Batch loss: 0.6044437885284424 batch: 164/224\n",
      "Batch loss: 0.6665240526199341 batch: 165/224\n",
      "Batch loss: 0.6266874074935913 batch: 166/224\n",
      "Batch loss: 0.5382184386253357 batch: 167/224\n",
      "Batch loss: 0.5398153066635132 batch: 168/224\n",
      "Batch loss: 0.5738928914070129 batch: 169/224\n",
      "Batch loss: 0.6184442043304443 batch: 170/224\n",
      "Batch loss: 0.5698205232620239 batch: 171/224\n",
      "Batch loss: 0.570152223110199 batch: 172/224\n",
      "Batch loss: 0.5706118941307068 batch: 173/224\n",
      "Batch loss: 0.5833704471588135 batch: 174/224\n",
      "Batch loss: 0.5630561709403992 batch: 175/224\n",
      "Batch loss: 0.5927358269691467 batch: 176/224\n",
      "Batch loss: 0.6576811075210571 batch: 177/224\n",
      "Batch loss: 0.5402266383171082 batch: 178/224\n",
      "Batch loss: 0.6200453639030457 batch: 179/224\n",
      "Batch loss: 0.4758261740207672 batch: 180/224\n",
      "Batch loss: 0.6356050968170166 batch: 181/224\n",
      "Batch loss: 0.5798385143280029 batch: 182/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.609939694404602 batch: 183/224\n",
      "Batch loss: 0.6053434610366821 batch: 184/224\n",
      "Batch loss: 0.6644452810287476 batch: 185/224\n",
      "Batch loss: 0.5441371202468872 batch: 186/224\n",
      "Batch loss: 0.5648946166038513 batch: 187/224\n",
      "Batch loss: 0.46447739005088806 batch: 188/224\n",
      "Batch loss: 0.6685130596160889 batch: 189/224\n",
      "Batch loss: 0.6238279938697815 batch: 190/224\n",
      "Batch loss: 0.5676591992378235 batch: 191/224\n",
      "Batch loss: 0.5636019110679626 batch: 192/224\n",
      "Batch loss: 0.5706005692481995 batch: 193/224\n",
      "Batch loss: 0.5478938221931458 batch: 194/224\n",
      "Batch loss: 0.6082051992416382 batch: 195/224\n",
      "Batch loss: 0.6068695187568665 batch: 196/224\n",
      "Batch loss: 0.5945420861244202 batch: 197/224\n",
      "Batch loss: 0.6216607093811035 batch: 198/224\n",
      "Batch loss: 0.5042544007301331 batch: 199/224\n",
      "Batch loss: 0.593712568283081 batch: 200/224\n",
      "Batch loss: 0.6263366341590881 batch: 201/224\n",
      "Batch loss: 0.6265589594841003 batch: 202/224\n",
      "Batch loss: 0.5583133101463318 batch: 203/224\n",
      "Batch loss: 0.5990321636199951 batch: 204/224\n",
      "Batch loss: 0.6696092486381531 batch: 205/224\n",
      "Batch loss: 0.516471266746521 batch: 206/224\n",
      "Batch loss: 0.6596874594688416 batch: 207/224\n",
      "Batch loss: 0.5414815545082092 batch: 208/224\n",
      "Batch loss: 0.5965801477432251 batch: 209/224\n",
      "Batch loss: 0.5631590485572815 batch: 210/224\n",
      "Batch loss: 0.5462316870689392 batch: 211/224\n",
      "Batch loss: 0.6040540337562561 batch: 212/224\n",
      "Batch loss: 0.6000827550888062 batch: 213/224\n",
      "Batch loss: 0.6233373880386353 batch: 214/224\n",
      "Batch loss: 0.661949634552002 batch: 215/224\n",
      "Batch loss: 0.5196688771247864 batch: 216/224\n",
      "Batch loss: 0.585342288017273 batch: 217/224\n",
      "Batch loss: 0.5318586826324463 batch: 218/224\n",
      "Batch loss: 0.5376924872398376 batch: 219/224\n",
      "Batch loss: 0.5178776979446411 batch: 220/224\n",
      "Batch loss: 0.6548195481300354 batch: 221/224\n",
      "Batch loss: 0.5589916110038757 batch: 222/224\n",
      "Batch loss: 0.5750566720962524 batch: 223/224\n",
      "Batch loss: 0.5878856182098389 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 1/75..  Training Loss: 0.00140..  Test Loss: 0.00107..  Test Accuracy: 0.79982\n",
      "Running epoch 2/75\n",
      "Batch loss: 0.5287756323814392 batch: 1/224\n",
      "Batch loss: 0.5784463882446289 batch: 2/224\n",
      "Batch loss: 0.553637683391571 batch: 3/224\n",
      "Batch loss: 0.6535735726356506 batch: 4/224\n",
      "Batch loss: 0.5522056818008423 batch: 5/224\n",
      "Batch loss: 0.6430449485778809 batch: 6/224\n",
      "Batch loss: 0.5585664510726929 batch: 7/224\n",
      "Batch loss: 0.611777126789093 batch: 8/224\n",
      "Batch loss: 0.5673601031303406 batch: 9/224\n",
      "Batch loss: 0.524017870426178 batch: 10/224\n",
      "Batch loss: 0.6351195573806763 batch: 11/224\n",
      "Batch loss: 0.5438750386238098 batch: 12/224\n",
      "Batch loss: 0.5143207907676697 batch: 13/224\n",
      "Batch loss: 0.5131789445877075 batch: 14/224\n",
      "Batch loss: 0.5667537450790405 batch: 15/224\n",
      "Batch loss: 0.7093127965927124 batch: 16/224\n",
      "Batch loss: 0.565949022769928 batch: 17/224\n",
      "Batch loss: 0.6181455850601196 batch: 18/224\n",
      "Batch loss: 0.5340468883514404 batch: 19/224\n",
      "Batch loss: 0.5747650861740112 batch: 20/224\n",
      "Batch loss: 0.6324162483215332 batch: 21/224\n",
      "Batch loss: 0.545400857925415 batch: 22/224\n",
      "Batch loss: 0.5911781787872314 batch: 23/224\n",
      "Batch loss: 0.5838026404380798 batch: 24/224\n",
      "Batch loss: 0.5315160751342773 batch: 25/224\n",
      "Batch loss: 0.49362826347351074 batch: 26/224\n",
      "Batch loss: 0.6224862933158875 batch: 27/224\n",
      "Batch loss: 0.5992262959480286 batch: 28/224\n",
      "Batch loss: 0.5828828811645508 batch: 29/224\n",
      "Batch loss: 0.5453312397003174 batch: 30/224\n",
      "Batch loss: 0.5840011239051819 batch: 31/224\n",
      "Batch loss: 0.640006959438324 batch: 32/224\n",
      "Batch loss: 0.5405642986297607 batch: 33/224\n",
      "Batch loss: 0.5724897980690002 batch: 34/224\n",
      "Batch loss: 0.5805549025535583 batch: 35/224\n",
      "Batch loss: 0.6790781617164612 batch: 36/224\n",
      "Batch loss: 0.5722220540046692 batch: 37/224\n",
      "Batch loss: 0.5395727753639221 batch: 38/224\n",
      "Batch loss: 0.5765970945358276 batch: 39/224\n",
      "Batch loss: 0.545881986618042 batch: 40/224\n",
      "Batch loss: 0.587360680103302 batch: 41/224\n",
      "Batch loss: 0.5225881338119507 batch: 42/224\n",
      "Batch loss: 0.6268653273582458 batch: 43/224\n",
      "Batch loss: 0.5211797952651978 batch: 44/224\n",
      "Batch loss: 0.5392202734947205 batch: 45/224\n",
      "Batch loss: 0.6058559417724609 batch: 46/224\n",
      "Batch loss: 0.5845069289207458 batch: 47/224\n",
      "Batch loss: 0.5876854658126831 batch: 48/224\n",
      "Batch loss: 0.5562032461166382 batch: 49/224\n",
      "Batch loss: 0.5642699003219604 batch: 50/224\n",
      "Batch loss: 0.5771054625511169 batch: 51/224\n",
      "Batch loss: 0.46713384985923767 batch: 52/224\n",
      "Batch loss: 0.6554558277130127 batch: 53/224\n",
      "Batch loss: 0.5252694487571716 batch: 54/224\n",
      "Batch loss: 0.5110821723937988 batch: 55/224\n",
      "Batch loss: 0.5410409569740295 batch: 56/224\n",
      "Batch loss: 0.5994138121604919 batch: 57/224\n",
      "Batch loss: 0.5918172597885132 batch: 58/224\n",
      "Batch loss: 0.5825263857841492 batch: 59/224\n",
      "Batch loss: 0.5862975120544434 batch: 60/224\n",
      "Batch loss: 0.6186376214027405 batch: 61/224\n",
      "Batch loss: 0.529409646987915 batch: 62/224\n",
      "Batch loss: 0.5278285145759583 batch: 63/224\n",
      "Batch loss: 0.6142792701721191 batch: 64/224\n",
      "Batch loss: 0.5576638579368591 batch: 65/224\n",
      "Batch loss: 0.5582119822502136 batch: 66/224\n",
      "Batch loss: 0.49725666642189026 batch: 67/224\n",
      "Batch loss: 0.5894097685813904 batch: 68/224\n",
      "Batch loss: 0.5580742955207825 batch: 69/224\n",
      "Batch loss: 0.5836153626441956 batch: 70/224\n",
      "Batch loss: 0.5245317816734314 batch: 71/224\n",
      "Batch loss: 0.4601016640663147 batch: 72/224\n",
      "Batch loss: 0.5996107459068298 batch: 73/224\n",
      "Batch loss: 0.4933219850063324 batch: 74/224\n",
      "Batch loss: 0.5730646252632141 batch: 75/224\n",
      "Batch loss: 0.5669876933097839 batch: 76/224\n",
      "Batch loss: 0.5259438753128052 batch: 77/224\n",
      "Batch loss: 0.5412284135818481 batch: 78/224\n",
      "Batch loss: 0.5668132901191711 batch: 79/224\n",
      "Batch loss: 0.6280785799026489 batch: 80/224\n",
      "Batch loss: 0.6567454934120178 batch: 81/224\n",
      "Batch loss: 0.5359753966331482 batch: 82/224\n",
      "Batch loss: 0.6727325320243835 batch: 83/224\n",
      "Batch loss: 0.5392944812774658 batch: 84/224\n",
      "Batch loss: 0.5689653754234314 batch: 85/224\n",
      "Batch loss: 0.5212569236755371 batch: 86/224\n",
      "Batch loss: 0.5737513303756714 batch: 87/224\n",
      "Batch loss: 0.5233051180839539 batch: 88/224\n",
      "Batch loss: 0.5314733386039734 batch: 89/224\n",
      "Batch loss: 0.5663308501243591 batch: 90/224\n",
      "Batch loss: 0.5394090414047241 batch: 91/224\n",
      "Batch loss: 0.5374979376792908 batch: 92/224\n",
      "Batch loss: 0.48405569791793823 batch: 93/224\n",
      "Batch loss: 0.5184770226478577 batch: 94/224\n",
      "Batch loss: 0.4919221103191376 batch: 95/224\n",
      "Batch loss: 0.5341865420341492 batch: 96/224\n",
      "Batch loss: 0.5181136131286621 batch: 97/224\n",
      "Batch loss: 0.4984973073005676 batch: 98/224\n",
      "Batch loss: 0.5805134773254395 batch: 99/224\n",
      "Batch loss: 0.5302266478538513 batch: 100/224\n",
      "Batch loss: 0.6003223061561584 batch: 101/224\n",
      "Batch loss: 0.48553138971328735 batch: 102/224\n",
      "Batch loss: 0.5524566769599915 batch: 103/224\n",
      "Batch loss: 0.50828617811203 batch: 104/224\n",
      "Batch loss: 0.4844972789287567 batch: 105/224\n",
      "Batch loss: 0.5650585293769836 batch: 106/224\n",
      "Batch loss: 0.5017538666725159 batch: 107/224\n",
      "Batch loss: 0.542445719242096 batch: 108/224\n",
      "Batch loss: 0.5356810688972473 batch: 109/224\n",
      "Batch loss: 0.5483977794647217 batch: 110/224\n",
      "Batch loss: 0.6137405037879944 batch: 111/224\n",
      "Batch loss: 0.5058677792549133 batch: 112/224\n",
      "Batch loss: 0.5879542231559753 batch: 113/224\n",
      "Batch loss: 0.5935932397842407 batch: 114/224\n",
      "Batch loss: 0.6472758054733276 batch: 115/224\n",
      "Batch loss: 0.48795250058174133 batch: 116/224\n",
      "Batch loss: 0.5227125287055969 batch: 117/224\n",
      "Batch loss: 0.5477668046951294 batch: 118/224\n",
      "Batch loss: 0.5466973185539246 batch: 119/224\n",
      "Batch loss: 0.5482267737388611 batch: 120/224\n",
      "Batch loss: 0.537352442741394 batch: 121/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5589286684989929 batch: 122/224\n",
      "Batch loss: 0.514904797077179 batch: 123/224\n",
      "Batch loss: 0.5158848166465759 batch: 124/224\n",
      "Batch loss: 0.5736355781555176 batch: 125/224\n",
      "Batch loss: 0.5238995552062988 batch: 126/224\n",
      "Batch loss: 0.5342587828636169 batch: 127/224\n",
      "Batch loss: 0.5148451924324036 batch: 128/224\n",
      "Batch loss: 0.5376291275024414 batch: 129/224\n",
      "Batch loss: 0.5565242171287537 batch: 130/224\n",
      "Batch loss: 0.5237817168235779 batch: 131/224\n",
      "Batch loss: 0.4785759747028351 batch: 132/224\n",
      "Batch loss: 0.49033281207084656 batch: 133/224\n",
      "Batch loss: 0.527341365814209 batch: 134/224\n",
      "Batch loss: 0.6019658446311951 batch: 135/224\n",
      "Batch loss: 0.6132474541664124 batch: 136/224\n",
      "Batch loss: 0.4702299237251282 batch: 137/224\n",
      "Batch loss: 0.5434388518333435 batch: 138/224\n",
      "Batch loss: 0.5645616054534912 batch: 139/224\n",
      "Batch loss: 0.5405524969100952 batch: 140/224\n",
      "Batch loss: 0.45308762788772583 batch: 141/224\n",
      "Batch loss: 0.5286273956298828 batch: 142/224\n",
      "Batch loss: 0.5432398319244385 batch: 143/224\n",
      "Batch loss: 0.5385037660598755 batch: 144/224\n",
      "Batch loss: 0.4967938959598541 batch: 145/224\n",
      "Batch loss: 0.5615674257278442 batch: 146/224\n",
      "Batch loss: 0.4957742691040039 batch: 147/224\n",
      "Batch loss: 0.5425868034362793 batch: 148/224\n",
      "Batch loss: 0.5634666085243225 batch: 149/224\n",
      "Batch loss: 0.5619582533836365 batch: 150/224\n",
      "Batch loss: 0.5208700299263 batch: 151/224\n",
      "Batch loss: 0.49212154746055603 batch: 152/224\n",
      "Batch loss: 0.5728163719177246 batch: 153/224\n",
      "Batch loss: 0.5462185144424438 batch: 154/224\n",
      "Batch loss: 0.46185943484306335 batch: 155/224\n",
      "Batch loss: 0.5964682698249817 batch: 156/224\n",
      "Batch loss: 0.5371717214584351 batch: 157/224\n",
      "Batch loss: 0.5918250679969788 batch: 158/224\n",
      "Batch loss: 0.4400950074195862 batch: 159/224\n",
      "Batch loss: 0.5186790227890015 batch: 160/224\n",
      "Batch loss: 0.5125001668930054 batch: 161/224\n",
      "Batch loss: 0.5266177654266357 batch: 162/224\n",
      "Batch loss: 0.5227147936820984 batch: 163/224\n",
      "Batch loss: 0.5051183700561523 batch: 164/224\n",
      "Batch loss: 0.5776519775390625 batch: 165/224\n",
      "Batch loss: 0.5793477892875671 batch: 166/224\n",
      "Batch loss: 0.4738078713417053 batch: 167/224\n",
      "Batch loss: 0.43706315755844116 batch: 168/224\n",
      "Batch loss: 0.4937570095062256 batch: 169/224\n",
      "Batch loss: 0.5158948302268982 batch: 170/224\n",
      "Batch loss: 0.460685133934021 batch: 171/224\n",
      "Batch loss: 0.4935128688812256 batch: 172/224\n",
      "Batch loss: 0.5297952890396118 batch: 173/224\n",
      "Batch loss: 0.5241928100585938 batch: 174/224\n",
      "Batch loss: 0.5350023508071899 batch: 175/224\n",
      "Batch loss: 0.5004597306251526 batch: 176/224\n",
      "Batch loss: 0.564699113368988 batch: 177/224\n",
      "Batch loss: 0.4503447711467743 batch: 178/224\n",
      "Batch loss: 0.5385112762451172 batch: 179/224\n",
      "Batch loss: 0.43656298518180847 batch: 180/224\n",
      "Batch loss: 0.5449244379997253 batch: 181/224\n",
      "Batch loss: 0.4869237542152405 batch: 182/224\n",
      "Batch loss: 0.5133607387542725 batch: 183/224\n",
      "Batch loss: 0.5072519779205322 batch: 184/224\n",
      "Batch loss: 0.5970207452774048 batch: 185/224\n",
      "Batch loss: 0.4875025451183319 batch: 186/224\n",
      "Batch loss: 0.48259472846984863 batch: 187/224\n",
      "Batch loss: 0.39987653493881226 batch: 188/224\n",
      "Batch loss: 0.5614601969718933 batch: 189/224\n",
      "Batch loss: 0.5348501801490784 batch: 190/224\n",
      "Batch loss: 0.49487847089767456 batch: 191/224\n",
      "Batch loss: 0.5176463723182678 batch: 192/224\n",
      "Batch loss: 0.5486448407173157 batch: 193/224\n",
      "Batch loss: 0.5187755823135376 batch: 194/224\n",
      "Batch loss: 0.5122992992401123 batch: 195/224\n",
      "Batch loss: 0.5513727068901062 batch: 196/224\n",
      "Batch loss: 0.5131986141204834 batch: 197/224\n",
      "Batch loss: 0.5637433528900146 batch: 198/224\n",
      "Batch loss: 0.4461667835712433 batch: 199/224\n",
      "Batch loss: 0.5275814533233643 batch: 200/224\n",
      "Batch loss: 0.5401714444160461 batch: 201/224\n",
      "Batch loss: 0.5345868468284607 batch: 202/224\n",
      "Batch loss: 0.4917881488800049 batch: 203/224\n",
      "Batch loss: 0.5148470997810364 batch: 204/224\n",
      "Batch loss: 0.5752453207969666 batch: 205/224\n",
      "Batch loss: 0.46476829051971436 batch: 206/224\n",
      "Batch loss: 0.5889155268669128 batch: 207/224\n",
      "Batch loss: 0.48805925250053406 batch: 208/224\n",
      "Batch loss: 0.5234386920928955 batch: 209/224\n",
      "Batch loss: 0.5112627744674683 batch: 210/224\n",
      "Batch loss: 0.470365434885025 batch: 211/224\n",
      "Batch loss: 0.5158987045288086 batch: 212/224\n",
      "Batch loss: 0.5434185862541199 batch: 213/224\n",
      "Batch loss: 0.5558587312698364 batch: 214/224\n",
      "Batch loss: 0.5975444912910461 batch: 215/224\n",
      "Batch loss: 0.4844478964805603 batch: 216/224\n",
      "Batch loss: 0.5038900375366211 batch: 217/224\n",
      "Batch loss: 0.5059676170349121 batch: 218/224\n",
      "Batch loss: 0.46498775482177734 batch: 219/224\n",
      "Batch loss: 0.43692272901535034 batch: 220/224\n",
      "Batch loss: 0.5829792618751526 batch: 221/224\n",
      "Batch loss: 0.48152515292167664 batch: 222/224\n",
      "Batch loss: 0.4714359641075134 batch: 223/224\n",
      "Batch loss: 0.4954119920730591 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 2/75..  Training Loss: 0.00108..  Test Loss: 0.00098..  Test Accuracy: 0.81932\n",
      "Running epoch 3/75\n",
      "Batch loss: 0.473173588514328 batch: 1/224\n",
      "Batch loss: 0.5043092370033264 batch: 2/224\n",
      "Batch loss: 0.46639174222946167 batch: 3/224\n",
      "Batch loss: 0.5670951008796692 batch: 4/224\n",
      "Batch loss: 0.4979827404022217 batch: 5/224\n",
      "Batch loss: 0.5860770344734192 batch: 6/224\n",
      "Batch loss: 0.4901632070541382 batch: 7/224\n",
      "Batch loss: 0.5627723932266235 batch: 8/224\n",
      "Batch loss: 0.4993269741535187 batch: 9/224\n",
      "Batch loss: 0.498744398355484 batch: 10/224\n",
      "Batch loss: 0.5786643624305725 batch: 11/224\n",
      "Batch loss: 0.4767248332500458 batch: 12/224\n",
      "Batch loss: 0.4480748474597931 batch: 13/224\n",
      "Batch loss: 0.45375460386276245 batch: 14/224\n",
      "Batch loss: 0.5079845786094666 batch: 15/224\n",
      "Batch loss: 0.6249567270278931 batch: 16/224\n",
      "Batch loss: 0.4870434105396271 batch: 17/224\n",
      "Batch loss: 0.5852073431015015 batch: 18/224\n",
      "Batch loss: 0.48601874709129333 batch: 19/224\n",
      "Batch loss: 0.5169572234153748 batch: 20/224\n",
      "Batch loss: 0.5491896271705627 batch: 21/224\n",
      "Batch loss: 0.4802308678627014 batch: 22/224\n",
      "Batch loss: 0.5101423859596252 batch: 23/224\n",
      "Batch loss: 0.5328480005264282 batch: 24/224\n",
      "Batch loss: 0.4749334454536438 batch: 25/224\n",
      "Batch loss: 0.4323633015155792 batch: 26/224\n",
      "Batch loss: 0.5062937140464783 batch: 27/224\n",
      "Batch loss: 0.5125457048416138 batch: 28/224\n",
      "Batch loss: 0.5321366786956787 batch: 29/224\n",
      "Batch loss: 0.4762912690639496 batch: 30/224\n",
      "Batch loss: 0.55363929271698 batch: 31/224\n",
      "Batch loss: 0.5617046356201172 batch: 32/224\n",
      "Batch loss: 0.49472281336784363 batch: 33/224\n",
      "Batch loss: 0.4773385226726532 batch: 34/224\n",
      "Batch loss: 0.5173004269599915 batch: 35/224\n",
      "Batch loss: 0.5859910845756531 batch: 36/224\n",
      "Batch loss: 0.5295122861862183 batch: 37/224\n",
      "Batch loss: 0.49253717064857483 batch: 38/224\n",
      "Batch loss: 0.5148537158966064 batch: 39/224\n",
      "Batch loss: 0.5350475311279297 batch: 40/224\n",
      "Batch loss: 0.5422041416168213 batch: 41/224\n",
      "Batch loss: 0.4915824234485626 batch: 42/224\n",
      "Batch loss: 0.5846542119979858 batch: 43/224\n",
      "Batch loss: 0.45699289441108704 batch: 44/224\n",
      "Batch loss: 0.4784531593322754 batch: 45/224\n",
      "Batch loss: 0.5823000073432922 batch: 46/224\n",
      "Batch loss: 0.5271852612495422 batch: 47/224\n",
      "Batch loss: 0.5170612931251526 batch: 48/224\n",
      "Batch loss: 0.4774632453918457 batch: 49/224\n",
      "Batch loss: 0.4801231622695923 batch: 50/224\n",
      "Batch loss: 0.5031440258026123 batch: 51/224\n",
      "Batch loss: 0.4205006957054138 batch: 52/224\n",
      "Batch loss: 0.5823226571083069 batch: 53/224\n",
      "Batch loss: 0.47170916199684143 batch: 54/224\n",
      "Batch loss: 0.4775856137275696 batch: 55/224\n",
      "Batch loss: 0.501918613910675 batch: 56/224\n",
      "Batch loss: 0.5597723126411438 batch: 57/224\n",
      "Batch loss: 0.5208597183227539 batch: 58/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5095574259757996 batch: 59/224\n",
      "Batch loss: 0.5429641008377075 batch: 60/224\n",
      "Batch loss: 0.5352919697761536 batch: 61/224\n",
      "Batch loss: 0.5022321939468384 batch: 62/224\n",
      "Batch loss: 0.5203932523727417 batch: 63/224\n",
      "Batch loss: 0.5443901419639587 batch: 64/224\n",
      "Batch loss: 0.5103943943977356 batch: 65/224\n",
      "Batch loss: 0.4937814772129059 batch: 66/224\n",
      "Batch loss: 0.4703431725502014 batch: 67/224\n",
      "Batch loss: 0.5143647789955139 batch: 68/224\n",
      "Batch loss: 0.5403704047203064 batch: 69/224\n",
      "Batch loss: 0.5079476237297058 batch: 70/224\n",
      "Batch loss: 0.4803677201271057 batch: 71/224\n",
      "Batch loss: 0.4165387749671936 batch: 72/224\n",
      "Batch loss: 0.5569448471069336 batch: 73/224\n",
      "Batch loss: 0.475454181432724 batch: 74/224\n",
      "Batch loss: 0.5567184686660767 batch: 75/224\n",
      "Batch loss: 0.5290900468826294 batch: 76/224\n",
      "Batch loss: 0.4769805371761322 batch: 77/224\n",
      "Batch loss: 0.50425124168396 batch: 78/224\n",
      "Batch loss: 0.4868847131729126 batch: 79/224\n",
      "Batch loss: 0.5463533997535706 batch: 80/224\n",
      "Batch loss: 0.5871570110321045 batch: 81/224\n",
      "Batch loss: 0.5470458269119263 batch: 82/224\n",
      "Batch loss: 0.5875881910324097 batch: 83/224\n",
      "Batch loss: 0.4997429847717285 batch: 84/224\n",
      "Batch loss: 0.5142197012901306 batch: 85/224\n",
      "Batch loss: 0.4622037708759308 batch: 86/224\n",
      "Batch loss: 0.49989184737205505 batch: 87/224\n",
      "Batch loss: 0.5066516995429993 batch: 88/224\n",
      "Batch loss: 0.4798685312271118 batch: 89/224\n",
      "Batch loss: 0.5076907873153687 batch: 90/224\n",
      "Batch loss: 0.45574209094047546 batch: 91/224\n",
      "Batch loss: 0.48982349038124084 batch: 92/224\n",
      "Batch loss: 0.42461711168289185 batch: 93/224\n",
      "Batch loss: 0.49628886580467224 batch: 94/224\n",
      "Batch loss: 0.4629829227924347 batch: 95/224\n",
      "Batch loss: 0.5101528167724609 batch: 96/224\n",
      "Batch loss: 0.4522401988506317 batch: 97/224\n",
      "Batch loss: 0.43706464767456055 batch: 98/224\n",
      "Batch loss: 0.5369852781295776 batch: 99/224\n",
      "Batch loss: 0.4985789656639099 batch: 100/224\n",
      "Batch loss: 0.5226744413375854 batch: 101/224\n",
      "Batch loss: 0.4628502428531647 batch: 102/224\n",
      "Batch loss: 0.5163872241973877 batch: 103/224\n",
      "Batch loss: 0.4453182816505432 batch: 104/224\n",
      "Batch loss: 0.4329807758331299 batch: 105/224\n",
      "Batch loss: 0.5270912051200867 batch: 106/224\n",
      "Batch loss: 0.47767889499664307 batch: 107/224\n",
      "Batch loss: 0.5249489545822144 batch: 108/224\n",
      "Batch loss: 0.4973391890525818 batch: 109/224\n",
      "Batch loss: 0.498178094625473 batch: 110/224\n",
      "Batch loss: 0.5844618082046509 batch: 111/224\n",
      "Batch loss: 0.46031710505485535 batch: 112/224\n",
      "Batch loss: 0.5458191633224487 batch: 113/224\n",
      "Batch loss: 0.49296244978904724 batch: 114/224\n",
      "Batch loss: 0.5626817941665649 batch: 115/224\n",
      "Batch loss: 0.4290042817592621 batch: 116/224\n",
      "Batch loss: 0.4820282757282257 batch: 117/224\n",
      "Batch loss: 0.5055426955223083 batch: 118/224\n",
      "Batch loss: 0.48929575085639954 batch: 119/224\n",
      "Batch loss: 0.4874451756477356 batch: 120/224\n",
      "Batch loss: 0.49163055419921875 batch: 121/224\n",
      "Batch loss: 0.499509334564209 batch: 122/224\n",
      "Batch loss: 0.46958574652671814 batch: 123/224\n",
      "Batch loss: 0.4564904272556305 batch: 124/224\n",
      "Batch loss: 0.5327587723731995 batch: 125/224\n",
      "Batch loss: 0.5075864791870117 batch: 126/224\n",
      "Batch loss: 0.5038484334945679 batch: 127/224\n",
      "Batch loss: 0.4336773455142975 batch: 128/224\n",
      "Batch loss: 0.5027627348899841 batch: 129/224\n",
      "Batch loss: 0.5130565762519836 batch: 130/224\n",
      "Batch loss: 0.4573233425617218 batch: 131/224\n",
      "Batch loss: 0.4603855609893799 batch: 132/224\n",
      "Batch loss: 0.4421079754829407 batch: 133/224\n",
      "Batch loss: 0.4564969539642334 batch: 134/224\n",
      "Batch loss: 0.5614902377128601 batch: 135/224\n",
      "Batch loss: 0.5175142884254456 batch: 136/224\n",
      "Batch loss: 0.41569599509239197 batch: 137/224\n",
      "Batch loss: 0.500023603439331 batch: 138/224\n",
      "Batch loss: 0.4939640164375305 batch: 139/224\n",
      "Batch loss: 0.5540677905082703 batch: 140/224\n",
      "Batch loss: 0.41744163632392883 batch: 141/224\n",
      "Batch loss: 0.444472074508667 batch: 142/224\n",
      "Batch loss: 0.4902847707271576 batch: 143/224\n",
      "Batch loss: 0.5017924308776855 batch: 144/224\n",
      "Batch loss: 0.46264368295669556 batch: 145/224\n",
      "Batch loss: 0.5186168551445007 batch: 146/224\n",
      "Batch loss: 0.4619198739528656 batch: 147/224\n",
      "Batch loss: 0.46886250376701355 batch: 148/224\n",
      "Batch loss: 0.4790816605091095 batch: 149/224\n",
      "Batch loss: 0.49677687883377075 batch: 150/224\n",
      "Batch loss: 0.48081091046333313 batch: 151/224\n",
      "Batch loss: 0.45789816975593567 batch: 152/224\n",
      "Batch loss: 0.5036677718162537 batch: 153/224\n",
      "Batch loss: 0.49995750188827515 batch: 154/224\n",
      "Batch loss: 0.44188714027404785 batch: 155/224\n",
      "Batch loss: 0.5290071964263916 batch: 156/224\n",
      "Batch loss: 0.5001005530357361 batch: 157/224\n",
      "Batch loss: 0.5406272411346436 batch: 158/224\n",
      "Batch loss: 0.4334820806980133 batch: 159/224\n",
      "Batch loss: 0.4659799337387085 batch: 160/224\n",
      "Batch loss: 0.4534303843975067 batch: 161/224\n",
      "Batch loss: 0.4583338499069214 batch: 162/224\n",
      "Batch loss: 0.47303423285484314 batch: 163/224\n",
      "Batch loss: 0.4880524277687073 batch: 164/224\n",
      "Batch loss: 0.4989790618419647 batch: 165/224\n",
      "Batch loss: 0.5057986378669739 batch: 166/224\n",
      "Batch loss: 0.4145221412181854 batch: 167/224\n",
      "Batch loss: 0.3982033133506775 batch: 168/224\n",
      "Batch loss: 0.4418433606624603 batch: 169/224\n",
      "Batch loss: 0.48220592737197876 batch: 170/224\n",
      "Batch loss: 0.4221392273902893 batch: 171/224\n",
      "Batch loss: 0.4455154240131378 batch: 172/224\n",
      "Batch loss: 0.4538637399673462 batch: 173/224\n",
      "Batch loss: 0.4532132148742676 batch: 174/224\n",
      "Batch loss: 0.4633793830871582 batch: 175/224\n",
      "Batch loss: 0.4300065040588379 batch: 176/224\n",
      "Batch loss: 0.515000581741333 batch: 177/224\n",
      "Batch loss: 0.44018927216529846 batch: 178/224\n",
      "Batch loss: 0.5036997199058533 batch: 179/224\n",
      "Batch loss: 0.3954012393951416 batch: 180/224\n",
      "Batch loss: 0.5203655958175659 batch: 181/224\n",
      "Batch loss: 0.4837397038936615 batch: 182/224\n",
      "Batch loss: 0.5054293870925903 batch: 183/224\n",
      "Batch loss: 0.45821139216423035 batch: 184/224\n",
      "Batch loss: 0.5448195338249207 batch: 185/224\n",
      "Batch loss: 0.43182459473609924 batch: 186/224\n",
      "Batch loss: 0.4441380798816681 batch: 187/224\n",
      "Batch loss: 0.33402010798454285 batch: 188/224\n",
      "Batch loss: 0.5399274826049805 batch: 189/224\n",
      "Batch loss: 0.49654093384742737 batch: 190/224\n",
      "Batch loss: 0.4848185181617737 batch: 191/224\n",
      "Batch loss: 0.47364288568496704 batch: 192/224\n",
      "Batch loss: 0.4942872226238251 batch: 193/224\n",
      "Batch loss: 0.4473413825035095 batch: 194/224\n",
      "Batch loss: 0.5145921111106873 batch: 195/224\n",
      "Batch loss: 0.5001479387283325 batch: 196/224\n",
      "Batch loss: 0.4657062292098999 batch: 197/224\n",
      "Batch loss: 0.47453877329826355 batch: 198/224\n",
      "Batch loss: 0.43117886781692505 batch: 199/224\n",
      "Batch loss: 0.5240556597709656 batch: 200/224\n",
      "Batch loss: 0.4739469587802887 batch: 201/224\n",
      "Batch loss: 0.5289297103881836 batch: 202/224\n",
      "Batch loss: 0.4499077796936035 batch: 203/224\n",
      "Batch loss: 0.4718959629535675 batch: 204/224\n",
      "Batch loss: 0.5322781205177307 batch: 205/224\n",
      "Batch loss: 0.4585283100605011 batch: 206/224\n",
      "Batch loss: 0.5483553409576416 batch: 207/224\n",
      "Batch loss: 0.43828460574150085 batch: 208/224\n",
      "Batch loss: 0.481778621673584 batch: 209/224\n",
      "Batch loss: 0.4814499616622925 batch: 210/224\n",
      "Batch loss: 0.45707353949546814 batch: 211/224\n",
      "Batch loss: 0.4667406678199768 batch: 212/224\n",
      "Batch loss: 0.5050623416900635 batch: 213/224\n",
      "Batch loss: 0.5017023086547852 batch: 214/224\n",
      "Batch loss: 0.5133650302886963 batch: 215/224\n",
      "Batch loss: 0.4733341634273529 batch: 216/224\n",
      "Batch loss: 0.489221453666687 batch: 217/224\n",
      "Batch loss: 0.45874038338661194 batch: 218/224\n",
      "Batch loss: 0.4129848778247833 batch: 219/224\n",
      "Batch loss: 0.39792948961257935 batch: 220/224\n",
      "Batch loss: 0.5075005292892456 batch: 221/224\n",
      "Batch loss: 0.47664618492126465 batch: 222/224\n",
      "Batch loss: 0.4188876748085022 batch: 223/224\n",
      "Batch loss: 0.45749038457870483 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 3/75..  Training Loss: 0.00099..  Test Loss: 0.00093..  Test Accuracy: 0.83011\n",
      "Running epoch 4/75\n",
      "Batch loss: 0.4237085282802582 batch: 1/224\n",
      "Batch loss: 0.4630293548107147 batch: 2/224\n",
      "Batch loss: 0.44055137038230896 batch: 3/224\n",
      "Batch loss: 0.4997287094593048 batch: 4/224\n",
      "Batch loss: 0.4676056504249573 batch: 5/224\n",
      "Batch loss: 0.5622755289077759 batch: 6/224\n",
      "Batch loss: 0.447068452835083 batch: 7/224\n",
      "Batch loss: 0.48818033933639526 batch: 8/224\n",
      "Batch loss: 0.464382529258728 batch: 9/224\n",
      "Batch loss: 0.44056183099746704 batch: 10/224\n",
      "Batch loss: 0.5262156128883362 batch: 11/224\n",
      "Batch loss: 0.4543817639350891 batch: 12/224\n",
      "Batch loss: 0.40474754571914673 batch: 13/224\n",
      "Batch loss: 0.4282059371471405 batch: 14/224\n",
      "Batch loss: 0.47732704877853394 batch: 15/224\n",
      "Batch loss: 0.5817759037017822 batch: 16/224\n",
      "Batch loss: 0.4567321538925171 batch: 17/224\n",
      "Batch loss: 0.5436912775039673 batch: 18/224\n",
      "Batch loss: 0.4477956295013428 batch: 19/224\n",
      "Batch loss: 0.45210742950439453 batch: 20/224\n",
      "Batch loss: 0.5160548686981201 batch: 21/224\n",
      "Batch loss: 0.42389369010925293 batch: 22/224\n",
      "Batch loss: 0.48572468757629395 batch: 23/224\n",
      "Batch loss: 0.5002158880233765 batch: 24/224\n",
      "Batch loss: 0.42733433842658997 batch: 25/224\n",
      "Batch loss: 0.42803463339805603 batch: 26/224\n",
      "Batch loss: 0.4662034809589386 batch: 27/224\n",
      "Batch loss: 0.486522376537323 batch: 28/224\n",
      "Batch loss: 0.5042691230773926 batch: 29/224\n",
      "Batch loss: 0.4474296569824219 batch: 30/224\n",
      "Batch loss: 0.46858495473861694 batch: 31/224\n",
      "Batch loss: 0.5204545259475708 batch: 32/224\n",
      "Batch loss: 0.42292580008506775 batch: 33/224\n",
      "Batch loss: 0.4425097703933716 batch: 34/224\n",
      "Batch loss: 0.4559992849826813 batch: 35/224\n",
      "Batch loss: 0.591990053653717 batch: 36/224\n",
      "Batch loss: 0.47198599576950073 batch: 37/224\n",
      "Batch loss: 0.4630230963230133 batch: 38/224\n",
      "Batch loss: 0.4700448215007782 batch: 39/224\n",
      "Batch loss: 0.4888194799423218 batch: 40/224\n",
      "Batch loss: 0.49752917885780334 batch: 41/224\n",
      "Batch loss: 0.45395627617836 batch: 42/224\n",
      "Batch loss: 0.5197172164916992 batch: 43/224\n",
      "Batch loss: 0.4424732029438019 batch: 44/224\n",
      "Batch loss: 0.44973644614219666 batch: 45/224\n",
      "Batch loss: 0.5166947841644287 batch: 46/224\n",
      "Batch loss: 0.47617223858833313 batch: 47/224\n",
      "Batch loss: 0.4682537317276001 batch: 48/224\n",
      "Batch loss: 0.4216230511665344 batch: 49/224\n",
      "Batch loss: 0.4591395854949951 batch: 50/224\n",
      "Batch loss: 0.4678439795970917 batch: 51/224\n",
      "Batch loss: 0.3905014097690582 batch: 52/224\n",
      "Batch loss: 0.5039955377578735 batch: 53/224\n",
      "Batch loss: 0.42325088381767273 batch: 54/224\n",
      "Batch loss: 0.42096608877182007 batch: 55/224\n",
      "Batch loss: 0.41451168060302734 batch: 56/224\n",
      "Batch loss: 0.5315093398094177 batch: 57/224\n",
      "Batch loss: 0.4984866976737976 batch: 58/224\n",
      "Batch loss: 0.44199615716934204 batch: 59/224\n",
      "Batch loss: 0.5197200775146484 batch: 60/224\n",
      "Batch loss: 0.48729193210601807 batch: 61/224\n",
      "Batch loss: 0.4472642242908478 batch: 62/224\n",
      "Batch loss: 0.4553194046020508 batch: 63/224\n",
      "Batch loss: 0.49358147382736206 batch: 64/224\n",
      "Batch loss: 0.4775092303752899 batch: 65/224\n",
      "Batch loss: 0.4309808313846588 batch: 66/224\n",
      "Batch loss: 0.41084206104278564 batch: 67/224\n",
      "Batch loss: 0.5058929324150085 batch: 68/224\n",
      "Batch loss: 0.4767991304397583 batch: 69/224\n",
      "Batch loss: 0.476325660943985 batch: 70/224\n",
      "Batch loss: 0.44400879740715027 batch: 71/224\n",
      "Batch loss: 0.3537195920944214 batch: 72/224\n",
      "Batch loss: 0.5138450264930725 batch: 73/224\n",
      "Batch loss: 0.43479520082473755 batch: 74/224\n",
      "Batch loss: 0.4727323055267334 batch: 75/224\n",
      "Batch loss: 0.4861499071121216 batch: 76/224\n",
      "Batch loss: 0.439043790102005 batch: 77/224\n",
      "Batch loss: 0.47574231028556824 batch: 78/224\n",
      "Batch loss: 0.4421401619911194 batch: 79/224\n",
      "Batch loss: 0.5201995968818665 batch: 80/224\n",
      "Batch loss: 0.5572345852851868 batch: 81/224\n",
      "Batch loss: 0.5081138014793396 batch: 82/224\n",
      "Batch loss: 0.5371806621551514 batch: 83/224\n",
      "Batch loss: 0.44022074341773987 batch: 84/224\n",
      "Batch loss: 0.4540683627128601 batch: 85/224\n",
      "Batch loss: 0.4393077790737152 batch: 86/224\n",
      "Batch loss: 0.4931700825691223 batch: 87/224\n",
      "Batch loss: 0.44977861642837524 batch: 88/224\n",
      "Batch loss: 0.42868295311927795 batch: 89/224\n",
      "Batch loss: 0.4745614230632782 batch: 90/224\n",
      "Batch loss: 0.4568316340446472 batch: 91/224\n",
      "Batch loss: 0.47227713465690613 batch: 92/224\n",
      "Batch loss: 0.4071846604347229 batch: 93/224\n",
      "Batch loss: 0.4330180585384369 batch: 94/224\n",
      "Batch loss: 0.41246649622917175 batch: 95/224\n",
      "Batch loss: 0.4677261710166931 batch: 96/224\n",
      "Batch loss: 0.4245643615722656 batch: 97/224\n",
      "Batch loss: 0.387297123670578 batch: 98/224\n",
      "Batch loss: 0.502538800239563 batch: 99/224\n",
      "Batch loss: 0.43384474515914917 batch: 100/224\n",
      "Batch loss: 0.49082204699516296 batch: 101/224\n",
      "Batch loss: 0.44162002205848694 batch: 102/224\n",
      "Batch loss: 0.45206505060195923 batch: 103/224\n",
      "Batch loss: 0.39361247420310974 batch: 104/224\n",
      "Batch loss: 0.41165000200271606 batch: 105/224\n",
      "Batch loss: 0.4471747577190399 batch: 106/224\n",
      "Batch loss: 0.40179258584976196 batch: 107/224\n",
      "Batch loss: 0.4992992579936981 batch: 108/224\n",
      "Batch loss: 0.4361848533153534 batch: 109/224\n",
      "Batch loss: 0.47635355591773987 batch: 110/224\n",
      "Batch loss: 0.5612303614616394 batch: 111/224\n",
      "Batch loss: 0.4050663113594055 batch: 112/224\n",
      "Batch loss: 0.5137434005737305 batch: 113/224\n",
      "Batch loss: 0.5026994943618774 batch: 114/224\n",
      "Batch loss: 0.5279378294944763 batch: 115/224\n",
      "Batch loss: 0.4089410901069641 batch: 116/224\n",
      "Batch loss: 0.39396896958351135 batch: 117/224\n",
      "Batch loss: 0.4605981409549713 batch: 118/224\n",
      "Batch loss: 0.4359743893146515 batch: 119/224\n",
      "Batch loss: 0.4892112612724304 batch: 120/224\n",
      "Batch loss: 0.47898396849632263 batch: 121/224\n",
      "Batch loss: 0.4763150215148926 batch: 122/224\n",
      "Batch loss: 0.4523236155509949 batch: 123/224\n",
      "Batch loss: 0.42121753096580505 batch: 124/224\n",
      "Batch loss: 0.48078373074531555 batch: 125/224\n",
      "Batch loss: 0.4595673084259033 batch: 126/224\n",
      "Batch loss: 0.4655803442001343 batch: 127/224\n",
      "Batch loss: 0.44064658880233765 batch: 128/224\n",
      "Batch loss: 0.4623905122280121 batch: 129/224\n",
      "Batch loss: 0.47199127078056335 batch: 130/224\n",
      "Batch loss: 0.4152715504169464 batch: 131/224\n",
      "Batch loss: 0.4228527247905731 batch: 132/224\n",
      "Batch loss: 0.40454772114753723 batch: 133/224\n",
      "Batch loss: 0.4361250400543213 batch: 134/224\n",
      "Batch loss: 0.49179574847221375 batch: 135/224\n",
      "Batch loss: 0.482095330953598 batch: 136/224\n",
      "Batch loss: 0.3931674659252167 batch: 137/224\n",
      "Batch loss: 0.4782283902168274 batch: 138/224\n",
      "Batch loss: 0.4440799951553345 batch: 139/224\n",
      "Batch loss: 0.5103409886360168 batch: 140/224\n",
      "Batch loss: 0.39558207988739014 batch: 141/224\n",
      "Batch loss: 0.46536025404930115 batch: 142/224\n",
      "Batch loss: 0.4639527201652527 batch: 143/224\n",
      "Batch loss: 0.4437157213687897 batch: 144/224\n",
      "Batch loss: 0.4404025375843048 batch: 145/224\n",
      "Batch loss: 0.45415106415748596 batch: 146/224\n",
      "Batch loss: 0.4444740414619446 batch: 147/224\n",
      "Batch loss: 0.44570618867874146 batch: 148/224\n",
      "Batch loss: 0.4603746831417084 batch: 149/224\n",
      "Batch loss: 0.45320090651512146 batch: 150/224\n",
      "Batch loss: 0.4437004327774048 batch: 151/224\n",
      "Batch loss: 0.4193635582923889 batch: 152/224\n",
      "Batch loss: 0.493356317281723 batch: 153/224\n",
      "Batch loss: 0.4925505518913269 batch: 154/224\n",
      "Batch loss: 0.42373520135879517 batch: 155/224\n",
      "Batch loss: 0.5206881165504456 batch: 156/224\n",
      "Batch loss: 0.4524613618850708 batch: 157/224\n",
      "Batch loss: 0.5028498768806458 batch: 158/224\n",
      "Batch loss: 0.3878289461135864 batch: 159/224\n",
      "Batch loss: 0.4292830228805542 batch: 160/224\n",
      "Batch loss: 0.44212013483047485 batch: 161/224\n",
      "Batch loss: 0.43794316053390503 batch: 162/224\n",
      "Batch loss: 0.41022756695747375 batch: 163/224\n",
      "Batch loss: 0.435024231672287 batch: 164/224\n",
      "Batch loss: 0.46535414457321167 batch: 165/224\n",
      "Batch loss: 0.4505484700202942 batch: 166/224\n",
      "Batch loss: 0.38204705715179443 batch: 167/224\n",
      "Batch loss: 0.3450092673301697 batch: 168/224\n",
      "Batch loss: 0.42843472957611084 batch: 169/224\n",
      "Batch loss: 0.44758519530296326 batch: 170/224\n",
      "Batch loss: 0.4064340591430664 batch: 171/224\n",
      "Batch loss: 0.4168816804885864 batch: 172/224\n",
      "Batch loss: 0.4498516619205475 batch: 173/224\n",
      "Batch loss: 0.4462016522884369 batch: 174/224\n",
      "Batch loss: 0.4037790298461914 batch: 175/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.42429885268211365 batch: 176/224\n",
      "Batch loss: 0.4816114902496338 batch: 177/224\n",
      "Batch loss: 0.37901246547698975 batch: 178/224\n",
      "Batch loss: 0.4615442752838135 batch: 179/224\n",
      "Batch loss: 0.3879757821559906 batch: 180/224\n",
      "Batch loss: 0.5026776790618896 batch: 181/224\n",
      "Batch loss: 0.4620586335659027 batch: 182/224\n",
      "Batch loss: 0.4573133587837219 batch: 183/224\n",
      "Batch loss: 0.4011075794696808 batch: 184/224\n",
      "Batch loss: 0.5155763626098633 batch: 185/224\n",
      "Batch loss: 0.4121538996696472 batch: 186/224\n",
      "Batch loss: 0.4377525746822357 batch: 187/224\n",
      "Batch loss: 0.32270655035972595 batch: 188/224\n",
      "Batch loss: 0.48356351256370544 batch: 189/224\n",
      "Batch loss: 0.4732690453529358 batch: 190/224\n",
      "Batch loss: 0.40683647990226746 batch: 191/224\n",
      "Batch loss: 0.4654726982116699 batch: 192/224\n",
      "Batch loss: 0.45737236738204956 batch: 193/224\n",
      "Batch loss: 0.41821545362472534 batch: 194/224\n",
      "Batch loss: 0.46257373690605164 batch: 195/224\n",
      "Batch loss: 0.4833627939224243 batch: 196/224\n",
      "Batch loss: 0.4373364746570587 batch: 197/224\n",
      "Batch loss: 0.435869038105011 batch: 198/224\n",
      "Batch loss: 0.3928413689136505 batch: 199/224\n",
      "Batch loss: 0.4480576515197754 batch: 200/224\n",
      "Batch loss: 0.46325039863586426 batch: 201/224\n",
      "Batch loss: 0.45763394236564636 batch: 202/224\n",
      "Batch loss: 0.4347788691520691 batch: 203/224\n",
      "Batch loss: 0.40874525904655457 batch: 204/224\n",
      "Batch loss: 0.48133420944213867 batch: 205/224\n",
      "Batch loss: 0.4105764329433441 batch: 206/224\n",
      "Batch loss: 0.4940650165081024 batch: 207/224\n",
      "Batch loss: 0.3976578414440155 batch: 208/224\n",
      "Batch loss: 0.4119017422199249 batch: 209/224\n",
      "Batch loss: 0.40264755487442017 batch: 210/224\n",
      "Batch loss: 0.39822232723236084 batch: 211/224\n",
      "Batch loss: 0.4386690855026245 batch: 212/224\n",
      "Batch loss: 0.44913092255592346 batch: 213/224\n",
      "Batch loss: 0.4483763873577118 batch: 214/224\n",
      "Batch loss: 0.4736882150173187 batch: 215/224\n",
      "Batch loss: 0.4272906184196472 batch: 216/224\n",
      "Batch loss: 0.41600123047828674 batch: 217/224\n",
      "Batch loss: 0.40425950288772583 batch: 218/224\n",
      "Batch loss: 0.4271177053451538 batch: 219/224\n",
      "Batch loss: 0.371261864900589 batch: 220/224\n",
      "Batch loss: 0.5087942481040955 batch: 221/224\n",
      "Batch loss: 0.4161851704120636 batch: 222/224\n",
      "Batch loss: 0.4195200204849243 batch: 223/224\n",
      "Batch loss: 0.397415429353714 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 4/75..  Training Loss: 0.00091..  Test Loss: 0.00085..  Test Accuracy: 0.84771\n",
      "Running epoch 5/75\n",
      "Batch loss: 0.4041956663131714 batch: 1/224\n",
      "Batch loss: 0.4074147045612335 batch: 2/224\n",
      "Batch loss: 0.4059681296348572 batch: 3/224\n",
      "Batch loss: 0.48752662539482117 batch: 4/224\n",
      "Batch loss: 0.4513518810272217 batch: 5/224\n",
      "Batch loss: 0.5407286286354065 batch: 6/224\n",
      "Batch loss: 0.4369191527366638 batch: 7/224\n",
      "Batch loss: 0.43402203917503357 batch: 8/224\n",
      "Batch loss: 0.38379085063934326 batch: 9/224\n",
      "Batch loss: 0.37620365619659424 batch: 10/224\n",
      "Batch loss: 0.5038164258003235 batch: 11/224\n",
      "Batch loss: 0.40071240067481995 batch: 12/224\n",
      "Batch loss: 0.3685876429080963 batch: 13/224\n",
      "Batch loss: 0.3991315960884094 batch: 14/224\n",
      "Batch loss: 0.43336331844329834 batch: 15/224\n",
      "Batch loss: 0.561690628528595 batch: 16/224\n",
      "Batch loss: 0.4143747389316559 batch: 17/224\n",
      "Batch loss: 0.5068907141685486 batch: 18/224\n",
      "Batch loss: 0.4174635112285614 batch: 19/224\n",
      "Batch loss: 0.4203857481479645 batch: 20/224\n",
      "Batch loss: 0.4885118305683136 batch: 21/224\n",
      "Batch loss: 0.39889946579933167 batch: 22/224\n",
      "Batch loss: 0.4428042471408844 batch: 23/224\n",
      "Batch loss: 0.48799586296081543 batch: 24/224\n",
      "Batch loss: 0.39346256852149963 batch: 25/224\n",
      "Batch loss: 0.3778112232685089 batch: 26/224\n",
      "Batch loss: 0.435646653175354 batch: 27/224\n",
      "Batch loss: 0.40676331520080566 batch: 28/224\n",
      "Batch loss: 0.46556389331817627 batch: 29/224\n",
      "Batch loss: 0.4185371696949005 batch: 30/224\n",
      "Batch loss: 0.45211493968963623 batch: 31/224\n",
      "Batch loss: 0.5089445114135742 batch: 32/224\n",
      "Batch loss: 0.4449862539768219 batch: 33/224\n",
      "Batch loss: 0.421932190656662 batch: 34/224\n",
      "Batch loss: 0.41992780566215515 batch: 35/224\n",
      "Batch loss: 0.5099871158599854 batch: 36/224\n",
      "Batch loss: 0.4382363259792328 batch: 37/224\n",
      "Batch loss: 0.40972304344177246 batch: 38/224\n",
      "Batch loss: 0.4424547851085663 batch: 39/224\n",
      "Batch loss: 0.4237369894981384 batch: 40/224\n",
      "Batch loss: 0.4671087861061096 batch: 41/224\n",
      "Batch loss: 0.4126357436180115 batch: 42/224\n",
      "Batch loss: 0.48233693838119507 batch: 43/224\n",
      "Batch loss: 0.3657156825065613 batch: 44/224\n",
      "Batch loss: 0.4105089604854584 batch: 45/224\n",
      "Batch loss: 0.48869287967681885 batch: 46/224\n",
      "Batch loss: 0.46906381845474243 batch: 47/224\n",
      "Batch loss: 0.4162570536136627 batch: 48/224\n",
      "Batch loss: 0.3936779499053955 batch: 49/224\n",
      "Batch loss: 0.39083123207092285 batch: 50/224\n",
      "Batch loss: 0.41252872347831726 batch: 51/224\n",
      "Batch loss: 0.3626994490623474 batch: 52/224\n",
      "Batch loss: 0.5014010071754456 batch: 53/224\n",
      "Batch loss: 0.418699711561203 batch: 54/224\n",
      "Batch loss: 0.3900798261165619 batch: 55/224\n",
      "Batch loss: 0.3853296637535095 batch: 56/224\n",
      "Batch loss: 0.5003236532211304 batch: 57/224\n",
      "Batch loss: 0.4580414295196533 batch: 58/224\n",
      "Batch loss: 0.4373650848865509 batch: 59/224\n",
      "Batch loss: 0.47603222727775574 batch: 60/224\n",
      "Batch loss: 0.48632723093032837 batch: 61/224\n",
      "Batch loss: 0.40907683968544006 batch: 62/224\n",
      "Batch loss: 0.4182758927345276 batch: 63/224\n",
      "Batch loss: 0.46903499960899353 batch: 64/224\n",
      "Batch loss: 0.4030686914920807 batch: 65/224\n",
      "Batch loss: 0.4365488886833191 batch: 66/224\n",
      "Batch loss: 0.39606818556785583 batch: 67/224\n",
      "Batch loss: 0.46919897198677063 batch: 68/224\n",
      "Batch loss: 0.45076537132263184 batch: 69/224\n",
      "Batch loss: 0.4349100887775421 batch: 70/224\n",
      "Batch loss: 0.41485199332237244 batch: 71/224\n",
      "Batch loss: 0.3472505211830139 batch: 72/224\n",
      "Batch loss: 0.4963313043117523 batch: 73/224\n",
      "Batch loss: 0.41098877787590027 batch: 74/224\n",
      "Batch loss: 0.4538626968860626 batch: 75/224\n",
      "Batch loss: 0.4694770872592926 batch: 76/224\n",
      "Batch loss: 0.419188529253006 batch: 77/224\n",
      "Batch loss: 0.4396804869174957 batch: 78/224\n",
      "Batch loss: 0.4565437436103821 batch: 79/224\n",
      "Batch loss: 0.4360203444957733 batch: 80/224\n",
      "Batch loss: 0.573704719543457 batch: 81/224\n",
      "Batch loss: 0.4587187170982361 batch: 82/224\n",
      "Batch loss: 0.48894843459129333 batch: 83/224\n",
      "Batch loss: 0.40119802951812744 batch: 84/224\n",
      "Batch loss: 0.4101642966270447 batch: 85/224\n",
      "Batch loss: 0.42136356234550476 batch: 86/224\n",
      "Batch loss: 0.4683968126773834 batch: 87/224\n",
      "Batch loss: 0.41530945897102356 batch: 88/224\n",
      "Batch loss: 0.41393616795539856 batch: 89/224\n",
      "Batch loss: 0.43431463837623596 batch: 90/224\n",
      "Batch loss: 0.42922091484069824 batch: 91/224\n",
      "Batch loss: 0.41380512714385986 batch: 92/224\n",
      "Batch loss: 0.36874523758888245 batch: 93/224\n",
      "Batch loss: 0.4517349898815155 batch: 94/224\n",
      "Batch loss: 0.39160144329071045 batch: 95/224\n",
      "Batch loss: 0.39646676182746887 batch: 96/224\n",
      "Batch loss: 0.41954511404037476 batch: 97/224\n",
      "Batch loss: 0.3550342321395874 batch: 98/224\n",
      "Batch loss: 0.4706409275531769 batch: 99/224\n",
      "Batch loss: 0.4275660514831543 batch: 100/224\n",
      "Batch loss: 0.47055584192276 batch: 101/224\n",
      "Batch loss: 0.4061284065246582 batch: 102/224\n",
      "Batch loss: 0.43121424317359924 batch: 103/224\n",
      "Batch loss: 0.3749125599861145 batch: 104/224\n",
      "Batch loss: 0.36963123083114624 batch: 105/224\n",
      "Batch loss: 0.4529320299625397 batch: 106/224\n",
      "Batch loss: 0.37666669487953186 batch: 107/224\n",
      "Batch loss: 0.43023303151130676 batch: 108/224\n",
      "Batch loss: 0.4340141713619232 batch: 109/224\n",
      "Batch loss: 0.4364685118198395 batch: 110/224\n",
      "Batch loss: 0.4683394134044647 batch: 111/224\n",
      "Batch loss: 0.3713909387588501 batch: 112/224\n",
      "Batch loss: 0.4787002205848694 batch: 113/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.39368921518325806 batch: 114/224\n",
      "Batch loss: 0.48174285888671875 batch: 115/224\n",
      "Batch loss: 0.3795621693134308 batch: 116/224\n",
      "Batch loss: 0.38192155957221985 batch: 117/224\n",
      "Batch loss: 0.44627469778060913 batch: 118/224\n",
      "Batch loss: 0.4034266471862793 batch: 119/224\n",
      "Batch loss: 0.45843881368637085 batch: 120/224\n",
      "Batch loss: 0.45732852816581726 batch: 121/224\n",
      "Batch loss: 0.4445878863334656 batch: 122/224\n",
      "Batch loss: 0.40803781151771545 batch: 123/224\n",
      "Batch loss: 0.4122887849807739 batch: 124/224\n",
      "Batch loss: 0.42643213272094727 batch: 125/224\n",
      "Batch loss: 0.43778762221336365 batch: 126/224\n",
      "Batch loss: 0.4423856735229492 batch: 127/224\n",
      "Batch loss: 0.39779388904571533 batch: 128/224\n",
      "Batch loss: 0.44058528542518616 batch: 129/224\n",
      "Batch loss: 0.4042496085166931 batch: 130/224\n",
      "Batch loss: 0.416612446308136 batch: 131/224\n",
      "Batch loss: 0.35732823610305786 batch: 132/224\n",
      "Batch loss: 0.3883267939090729 batch: 133/224\n",
      "Batch loss: 0.4254691004753113 batch: 134/224\n",
      "Batch loss: 0.5118933916091919 batch: 135/224\n",
      "Batch loss: 0.443349689245224 batch: 136/224\n",
      "Batch loss: 0.38150686025619507 batch: 137/224\n",
      "Batch loss: 0.432941198348999 batch: 138/224\n",
      "Batch loss: 0.4427310824394226 batch: 139/224\n",
      "Batch loss: 0.4573163390159607 batch: 140/224\n",
      "Batch loss: 0.3720936179161072 batch: 141/224\n",
      "Batch loss: 0.41401344537734985 batch: 142/224\n",
      "Batch loss: 0.44287803769111633 batch: 143/224\n",
      "Batch loss: 0.44793936610221863 batch: 144/224\n",
      "Batch loss: 0.4276658296585083 batch: 145/224\n",
      "Batch loss: 0.41019102931022644 batch: 146/224\n",
      "Batch loss: 0.4332992434501648 batch: 147/224\n",
      "Batch loss: 0.39427101612091064 batch: 148/224\n",
      "Batch loss: 0.44610172510147095 batch: 149/224\n",
      "Batch loss: 0.4391114413738251 batch: 150/224\n",
      "Batch loss: 0.4287102520465851 batch: 151/224\n",
      "Batch loss: 0.38016563653945923 batch: 152/224\n",
      "Batch loss: 0.4412247836589813 batch: 153/224\n",
      "Batch loss: 0.46851587295532227 batch: 154/224\n",
      "Batch loss: 0.37569817900657654 batch: 155/224\n",
      "Batch loss: 0.4648992121219635 batch: 156/224\n",
      "Batch loss: 0.4191684424877167 batch: 157/224\n",
      "Batch loss: 0.4628593623638153 batch: 158/224\n",
      "Batch loss: 0.35774239897727966 batch: 159/224\n",
      "Batch loss: 0.417477548122406 batch: 160/224\n",
      "Batch loss: 0.38382405042648315 batch: 161/224\n",
      "Batch loss: 0.4076399505138397 batch: 162/224\n",
      "Batch loss: 0.406122624874115 batch: 163/224\n",
      "Batch loss: 0.4072853922843933 batch: 164/224\n",
      "Batch loss: 0.4625980854034424 batch: 165/224\n",
      "Batch loss: 0.40481191873550415 batch: 166/224\n",
      "Batch loss: 0.373970091342926 batch: 167/224\n",
      "Batch loss: 0.35477811098098755 batch: 168/224\n",
      "Batch loss: 0.40506136417388916 batch: 169/224\n",
      "Batch loss: 0.42356398701667786 batch: 170/224\n",
      "Batch loss: 0.364010214805603 batch: 171/224\n",
      "Batch loss: 0.37378108501434326 batch: 172/224\n",
      "Batch loss: 0.39317587018013 batch: 173/224\n",
      "Batch loss: 0.4126853048801422 batch: 174/224\n",
      "Batch loss: 0.42068684101104736 batch: 175/224\n",
      "Batch loss: 0.40707600116729736 batch: 176/224\n",
      "Batch loss: 0.4224158823490143 batch: 177/224\n",
      "Batch loss: 0.3777244985103607 batch: 178/224\n",
      "Batch loss: 0.4642525613307953 batch: 179/224\n",
      "Batch loss: 0.3449186384677887 batch: 180/224\n",
      "Batch loss: 0.4442514479160309 batch: 181/224\n",
      "Batch loss: 0.4107241928577423 batch: 182/224\n",
      "Batch loss: 0.41955238580703735 batch: 183/224\n",
      "Batch loss: 0.38873541355133057 batch: 184/224\n",
      "Batch loss: 0.4926527738571167 batch: 185/224\n",
      "Batch loss: 0.36593562364578247 batch: 186/224\n",
      "Batch loss: 0.3826276659965515 batch: 187/224\n",
      "Batch loss: 0.2922203540802002 batch: 188/224\n",
      "Batch loss: 0.44334468245506287 batch: 189/224\n",
      "Batch loss: 0.4409923255443573 batch: 190/224\n",
      "Batch loss: 0.3811911940574646 batch: 191/224\n",
      "Batch loss: 0.4377431273460388 batch: 192/224\n",
      "Batch loss: 0.40975630283355713 batch: 193/224\n",
      "Batch loss: 0.4233877658843994 batch: 194/224\n",
      "Batch loss: 0.43756434321403503 batch: 195/224\n",
      "Batch loss: 0.43442872166633606 batch: 196/224\n",
      "Batch loss: 0.3977307677268982 batch: 197/224\n",
      "Batch loss: 0.4413769245147705 batch: 198/224\n",
      "Batch loss: 0.36663496494293213 batch: 199/224\n",
      "Batch loss: 0.4539056718349457 batch: 200/224\n",
      "Batch loss: 0.39076074957847595 batch: 201/224\n",
      "Batch loss: 0.44943857192993164 batch: 202/224\n",
      "Batch loss: 0.4239102900028229 batch: 203/224\n",
      "Batch loss: 0.3579525649547577 batch: 204/224\n",
      "Batch loss: 0.43763646483421326 batch: 205/224\n",
      "Batch loss: 0.360105037689209 batch: 206/224\n",
      "Batch loss: 0.4860413074493408 batch: 207/224\n",
      "Batch loss: 0.37444475293159485 batch: 208/224\n",
      "Batch loss: 0.4248020350933075 batch: 209/224\n",
      "Batch loss: 0.4259273409843445 batch: 210/224\n",
      "Batch loss: 0.3740798532962799 batch: 211/224\n",
      "Batch loss: 0.42099395394325256 batch: 212/224\n",
      "Batch loss: 0.47637391090393066 batch: 213/224\n",
      "Batch loss: 0.4483164846897125 batch: 214/224\n",
      "Batch loss: 0.41475218534469604 batch: 215/224\n",
      "Batch loss: 0.398710161447525 batch: 216/224\n",
      "Batch loss: 0.43951740860939026 batch: 217/224\n",
      "Batch loss: 0.388958215713501 batch: 218/224\n",
      "Batch loss: 0.35761547088623047 batch: 219/224\n",
      "Batch loss: 0.36217138171195984 batch: 220/224\n",
      "Batch loss: 0.44632720947265625 batch: 221/224\n",
      "Batch loss: 0.409941166639328 batch: 222/224\n",
      "Batch loss: 0.34073197841644287 batch: 223/224\n",
      "Batch loss: 0.3722233474254608 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 5/75..  Training Loss: 0.00085..  Test Loss: 0.00082..  Test Accuracy: 0.85254\n",
      "Running epoch 6/75\n",
      "Batch loss: 0.3731641471385956 batch: 1/224\n",
      "Batch loss: 0.36269402503967285 batch: 2/224\n",
      "Batch loss: 0.39421871304512024 batch: 3/224\n",
      "Batch loss: 0.44538164138793945 batch: 4/224\n",
      "Batch loss: 0.38794538378715515 batch: 5/224\n",
      "Batch loss: 0.4867973029613495 batch: 6/224\n",
      "Batch loss: 0.4058080017566681 batch: 7/224\n",
      "Batch loss: 0.4263036847114563 batch: 8/224\n",
      "Batch loss: 0.34798696637153625 batch: 9/224\n",
      "Batch loss: 0.4121273159980774 batch: 10/224\n",
      "Batch loss: 0.48797374963760376 batch: 11/224\n",
      "Batch loss: 0.3978009819984436 batch: 12/224\n",
      "Batch loss: 0.3636073172092438 batch: 13/224\n",
      "Batch loss: 0.377521812915802 batch: 14/224\n",
      "Batch loss: 0.39001214504241943 batch: 15/224\n",
      "Batch loss: 0.5145193934440613 batch: 16/224\n",
      "Batch loss: 0.403793603181839 batch: 17/224\n",
      "Batch loss: 0.45787671208381653 batch: 18/224\n",
      "Batch loss: 0.3664999008178711 batch: 19/224\n",
      "Batch loss: 0.397062748670578 batch: 20/224\n",
      "Batch loss: 0.4772311747074127 batch: 21/224\n",
      "Batch loss: 0.356067955493927 batch: 22/224\n",
      "Batch loss: 0.42607277631759644 batch: 23/224\n",
      "Batch loss: 0.45181339979171753 batch: 24/224\n",
      "Batch loss: 0.3680271506309509 batch: 25/224\n",
      "Batch loss: 0.33595073223114014 batch: 26/224\n",
      "Batch loss: 0.4227537512779236 batch: 27/224\n",
      "Batch loss: 0.40218427777290344 batch: 28/224\n",
      "Batch loss: 0.4453442692756653 batch: 29/224\n",
      "Batch loss: 0.38567760586738586 batch: 30/224\n",
      "Batch loss: 0.44203415513038635 batch: 31/224\n",
      "Batch loss: 0.4415583908557892 batch: 32/224\n",
      "Batch loss: 0.41203629970550537 batch: 33/224\n",
      "Batch loss: 0.4092528820037842 batch: 34/224\n",
      "Batch loss: 0.4150873124599457 batch: 35/224\n",
      "Batch loss: 0.48236560821533203 batch: 36/224\n",
      "Batch loss: 0.40672728419303894 batch: 37/224\n",
      "Batch loss: 0.3884265422821045 batch: 38/224\n",
      "Batch loss: 0.4036163091659546 batch: 39/224\n",
      "Batch loss: 0.41490259766578674 batch: 40/224\n",
      "Batch loss: 0.4247497618198395 batch: 41/224\n",
      "Batch loss: 0.37917909026145935 batch: 42/224\n",
      "Batch loss: 0.45531097054481506 batch: 43/224\n",
      "Batch loss: 0.35651084780693054 batch: 44/224\n",
      "Batch loss: 0.3623826205730438 batch: 45/224\n",
      "Batch loss: 0.48555806279182434 batch: 46/224\n",
      "Batch loss: 0.4136289358139038 batch: 47/224\n",
      "Batch loss: 0.3694830536842346 batch: 48/224\n",
      "Batch loss: 0.36852404475212097 batch: 49/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.3664517104625702 batch: 50/224\n",
      "Batch loss: 0.41264960169792175 batch: 51/224\n",
      "Batch loss: 0.3232639729976654 batch: 52/224\n",
      "Batch loss: 0.43372708559036255 batch: 53/224\n",
      "Batch loss: 0.3494653105735779 batch: 54/224\n",
      "Batch loss: 0.3671596944332123 batch: 55/224\n",
      "Batch loss: 0.3696686327457428 batch: 56/224\n",
      "Batch loss: 0.460529088973999 batch: 57/224\n",
      "Batch loss: 0.39535412192344666 batch: 58/224\n",
      "Batch loss: 0.3894205391407013 batch: 59/224\n",
      "Batch loss: 0.47491905093193054 batch: 60/224\n",
      "Batch loss: 0.43006911873817444 batch: 61/224\n",
      "Batch loss: 0.3775545358657837 batch: 62/224\n",
      "Batch loss: 0.39252033829689026 batch: 63/224\n",
      "Batch loss: 0.4134763181209564 batch: 64/224\n",
      "Batch loss: 0.36628130078315735 batch: 65/224\n",
      "Batch loss: 0.38239923119544983 batch: 66/224\n",
      "Batch loss: 0.3882645070552826 batch: 67/224\n",
      "Batch loss: 0.4454536437988281 batch: 68/224\n",
      "Batch loss: 0.42552649974823 batch: 69/224\n",
      "Batch loss: 0.4244391918182373 batch: 70/224\n",
      "Batch loss: 0.3946574032306671 batch: 71/224\n",
      "Batch loss: 0.30586153268814087 batch: 72/224\n",
      "Batch loss: 0.42846259474754333 batch: 73/224\n",
      "Batch loss: 0.405728280544281 batch: 74/224\n",
      "Batch loss: 0.4205441176891327 batch: 75/224\n",
      "Batch loss: 0.4241798520088196 batch: 76/224\n",
      "Batch loss: 0.3527173697948456 batch: 77/224\n",
      "Batch loss: 0.4048902690410614 batch: 78/224\n",
      "Batch loss: 0.4232648015022278 batch: 79/224\n",
      "Batch loss: 0.4241564869880676 batch: 80/224\n",
      "Batch loss: 0.47260791063308716 batch: 81/224\n",
      "Batch loss: 0.4509187340736389 batch: 82/224\n",
      "Batch loss: 0.4395746886730194 batch: 83/224\n",
      "Batch loss: 0.3766084909439087 batch: 84/224\n",
      "Batch loss: 0.40011337399482727 batch: 85/224\n",
      "Batch loss: 0.37334102392196655 batch: 86/224\n",
      "Batch loss: 0.3711235523223877 batch: 87/224\n",
      "Batch loss: 0.4137011170387268 batch: 88/224\n",
      "Batch loss: 0.3950810432434082 batch: 89/224\n",
      "Batch loss: 0.4137119948863983 batch: 90/224\n",
      "Batch loss: 0.4060356616973877 batch: 91/224\n",
      "Batch loss: 0.41892868280410767 batch: 92/224\n",
      "Batch loss: 0.3446516990661621 batch: 93/224\n",
      "Batch loss: 0.4222806990146637 batch: 94/224\n",
      "Batch loss: 0.36481255292892456 batch: 95/224\n",
      "Batch loss: 0.3745722472667694 batch: 96/224\n",
      "Batch loss: 0.402601420879364 batch: 97/224\n",
      "Batch loss: 0.3215981423854828 batch: 98/224\n",
      "Batch loss: 0.4460691213607788 batch: 99/224\n",
      "Batch loss: 0.36587971448898315 batch: 100/224\n",
      "Batch loss: 0.42555850744247437 batch: 101/224\n",
      "Batch loss: 0.3795393705368042 batch: 102/224\n",
      "Batch loss: 0.40204957127571106 batch: 103/224\n",
      "Batch loss: 0.3516501784324646 batch: 104/224\n",
      "Batch loss: 0.34459662437438965 batch: 105/224\n",
      "Batch loss: 0.40806758403778076 batch: 106/224\n",
      "Batch loss: 0.3496919870376587 batch: 107/224\n",
      "Batch loss: 0.41574734449386597 batch: 108/224\n",
      "Batch loss: 0.3820212483406067 batch: 109/224\n",
      "Batch loss: 0.4007594883441925 batch: 110/224\n",
      "Batch loss: 0.4499441981315613 batch: 111/224\n",
      "Batch loss: 0.3885398805141449 batch: 112/224\n",
      "Batch loss: 0.434535413980484 batch: 113/224\n",
      "Batch loss: 0.3960883915424347 batch: 114/224\n",
      "Batch loss: 0.4328826665878296 batch: 115/224\n",
      "Batch loss: 0.36109763383865356 batch: 116/224\n",
      "Batch loss: 0.3508133292198181 batch: 117/224\n",
      "Batch loss: 0.4136847257614136 batch: 118/224\n",
      "Batch loss: 0.37860992550849915 batch: 119/224\n",
      "Batch loss: 0.46502459049224854 batch: 120/224\n",
      "Batch loss: 0.3999764323234558 batch: 121/224\n",
      "Batch loss: 0.41130530834198 batch: 122/224\n",
      "Batch loss: 0.382622092962265 batch: 123/224\n",
      "Batch loss: 0.4083620607852936 batch: 124/224\n",
      "Batch loss: 0.41748031973838806 batch: 125/224\n",
      "Batch loss: 0.38997915387153625 batch: 126/224\n",
      "Batch loss: 0.41244035959243774 batch: 127/224\n",
      "Batch loss: 0.3822479844093323 batch: 128/224\n",
      "Batch loss: 0.41214922070503235 batch: 129/224\n",
      "Batch loss: 0.40904563665390015 batch: 130/224\n",
      "Batch loss: 0.36946672201156616 batch: 131/224\n",
      "Batch loss: 0.3736943304538727 batch: 132/224\n",
      "Batch loss: 0.3995515704154968 batch: 133/224\n",
      "Batch loss: 0.39951691031455994 batch: 134/224\n",
      "Batch loss: 0.43554162979125977 batch: 135/224\n",
      "Batch loss: 0.4198151230812073 batch: 136/224\n",
      "Batch loss: 0.3882805109024048 batch: 137/224\n",
      "Batch loss: 0.40364837646484375 batch: 138/224\n",
      "Batch loss: 0.409402996301651 batch: 139/224\n",
      "Batch loss: 0.43322592973709106 batch: 140/224\n",
      "Batch loss: 0.3384040892124176 batch: 141/224\n",
      "Batch loss: 0.37473881244659424 batch: 142/224\n",
      "Batch loss: 0.4039083421230316 batch: 143/224\n",
      "Batch loss: 0.4058581292629242 batch: 144/224\n",
      "Batch loss: 0.38568922877311707 batch: 145/224\n",
      "Batch loss: 0.42272424697875977 batch: 146/224\n",
      "Batch loss: 0.3720601201057434 batch: 147/224\n",
      "Batch loss: 0.3692103922367096 batch: 148/224\n",
      "Batch loss: 0.42728355526924133 batch: 149/224\n",
      "Batch loss: 0.43209484219551086 batch: 150/224\n",
      "Batch loss: 0.4081939160823822 batch: 151/224\n",
      "Batch loss: 0.3724161982536316 batch: 152/224\n",
      "Batch loss: 0.41762569546699524 batch: 153/224\n",
      "Batch loss: 0.47163474559783936 batch: 154/224\n",
      "Batch loss: 0.3769567608833313 batch: 155/224\n",
      "Batch loss: 0.3933316171169281 batch: 156/224\n",
      "Batch loss: 0.4275817275047302 batch: 157/224\n",
      "Batch loss: 0.4242081940174103 batch: 158/224\n",
      "Batch loss: 0.3616480529308319 batch: 159/224\n",
      "Batch loss: 0.3954816460609436 batch: 160/224\n",
      "Batch loss: 0.37383708357810974 batch: 161/224\n",
      "Batch loss: 0.3897210657596588 batch: 162/224\n",
      "Batch loss: 0.3861808180809021 batch: 163/224\n",
      "Batch loss: 0.3846307694911957 batch: 164/224\n",
      "Batch loss: 0.3963809907436371 batch: 165/224\n",
      "Batch loss: 0.388304203748703 batch: 166/224\n",
      "Batch loss: 0.3120593726634979 batch: 167/224\n",
      "Batch loss: 0.3278313875198364 batch: 168/224\n",
      "Batch loss: 0.3714945316314697 batch: 169/224\n",
      "Batch loss: 0.3836291432380676 batch: 170/224\n",
      "Batch loss: 0.33326488733291626 batch: 171/224\n",
      "Batch loss: 0.34861624240875244 batch: 172/224\n",
      "Batch loss: 0.3932408094406128 batch: 173/224\n",
      "Batch loss: 0.3923172652721405 batch: 174/224\n",
      "Batch loss: 0.3791590631008148 batch: 175/224\n",
      "Batch loss: 0.36993783712387085 batch: 176/224\n",
      "Batch loss: 0.4311550259590149 batch: 177/224\n",
      "Batch loss: 0.3261910378932953 batch: 178/224\n",
      "Batch loss: 0.43165314197540283 batch: 179/224\n",
      "Batch loss: 0.34315013885498047 batch: 180/224\n",
      "Batch loss: 0.47319042682647705 batch: 181/224\n",
      "Batch loss: 0.3951148986816406 batch: 182/224\n",
      "Batch loss: 0.4228247404098511 batch: 183/224\n",
      "Batch loss: 0.32697492837905884 batch: 184/224\n",
      "Batch loss: 0.45324790477752686 batch: 185/224\n",
      "Batch loss: 0.3445781171321869 batch: 186/224\n",
      "Batch loss: 0.3678306043148041 batch: 187/224\n",
      "Batch loss: 0.277811199426651 batch: 188/224\n",
      "Batch loss: 0.4264928698539734 batch: 189/224\n",
      "Batch loss: 0.4078066647052765 batch: 190/224\n",
      "Batch loss: 0.4053973853588104 batch: 191/224\n",
      "Batch loss: 0.4007866382598877 batch: 192/224\n",
      "Batch loss: 0.3866702616214752 batch: 193/224\n",
      "Batch loss: 0.3798426389694214 batch: 194/224\n",
      "Batch loss: 0.4086899757385254 batch: 195/224\n",
      "Batch loss: 0.4430321455001831 batch: 196/224\n",
      "Batch loss: 0.41346997022628784 batch: 197/224\n",
      "Batch loss: 0.4047316610813141 batch: 198/224\n",
      "Batch loss: 0.3303139805793762 batch: 199/224\n",
      "Batch loss: 0.3991835117340088 batch: 200/224\n",
      "Batch loss: 0.3832520842552185 batch: 201/224\n",
      "Batch loss: 0.40651780366897583 batch: 202/224\n",
      "Batch loss: 0.3727979063987732 batch: 203/224\n",
      "Batch loss: 0.3910573720932007 batch: 204/224\n",
      "Batch loss: 0.41405725479125977 batch: 205/224\n",
      "Batch loss: 0.340007483959198 batch: 206/224\n",
      "Batch loss: 0.4259048402309418 batch: 207/224\n",
      "Batch loss: 0.3478824198246002 batch: 208/224\n",
      "Batch loss: 0.38274532556533813 batch: 209/224\n",
      "Batch loss: 0.37098339200019836 batch: 210/224\n",
      "Batch loss: 0.35534799098968506 batch: 211/224\n",
      "Batch loss: 0.40583792328834534 batch: 212/224\n",
      "Batch loss: 0.42451515793800354 batch: 213/224\n",
      "Batch loss: 0.3902236819267273 batch: 214/224\n",
      "Batch loss: 0.44852879643440247 batch: 215/224\n",
      "Batch loss: 0.3803981840610504 batch: 216/224\n",
      "Batch loss: 0.4023786187171936 batch: 217/224\n",
      "Batch loss: 0.3614780604839325 batch: 218/224\n",
      "Batch loss: 0.3734910488128662 batch: 219/224\n",
      "Batch loss: 0.3545073866844177 batch: 220/224\n",
      "Batch loss: 0.42801880836486816 batch: 221/224\n",
      "Batch loss: 0.3847368657588959 batch: 222/224\n",
      "Batch loss: 0.3482944369316101 batch: 223/224\n",
      "Batch loss: 0.3575037121772766 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 6/75..  Training Loss: 0.00079..  Test Loss: 0.00078..  Test Accuracy: 0.86068\n",
      "Running epoch 7/75\n",
      "Batch loss: 0.32893693447113037 batch: 1/224\n",
      "Batch loss: 0.3734044134616852 batch: 2/224\n",
      "Batch loss: 0.3540605306625366 batch: 3/224\n",
      "Batch loss: 0.41198721528053284 batch: 4/224\n",
      "Batch loss: 0.3614331781864166 batch: 5/224\n",
      "Batch loss: 0.44898521900177 batch: 6/224\n",
      "Batch loss: 0.3565686047077179 batch: 7/224\n",
      "Batch loss: 0.40924468636512756 batch: 8/224\n",
      "Batch loss: 0.33065265417099 batch: 9/224\n",
      "Batch loss: 0.3675522804260254 batch: 10/224\n",
      "Batch loss: 0.4247841238975525 batch: 11/224\n",
      "Batch loss: 0.4039866328239441 batch: 12/224\n",
      "Batch loss: 0.30939987301826477 batch: 13/224\n",
      "Batch loss: 0.3241652846336365 batch: 14/224\n",
      "Batch loss: 0.38690292835235596 batch: 15/224\n",
      "Batch loss: 0.4773314893245697 batch: 16/224\n",
      "Batch loss: 0.3665210008621216 batch: 17/224\n",
      "Batch loss: 0.4527953565120697 batch: 18/224\n",
      "Batch loss: 0.3650228977203369 batch: 19/224\n",
      "Batch loss: 0.3796681761741638 batch: 20/224\n",
      "Batch loss: 0.4458552300930023 batch: 21/224\n",
      "Batch loss: 0.34634754061698914 batch: 22/224\n",
      "Batch loss: 0.4003770649433136 batch: 23/224\n",
      "Batch loss: 0.4138447046279907 batch: 24/224\n",
      "Batch loss: 0.37272727489471436 batch: 25/224\n",
      "Batch loss: 0.3414565324783325 batch: 26/224\n",
      "Batch loss: 0.3852146565914154 batch: 27/224\n",
      "Batch loss: 0.3624822199344635 batch: 28/224\n",
      "Batch loss: 0.4017117917537689 batch: 29/224\n",
      "Batch loss: 0.3728167414665222 batch: 30/224\n",
      "Batch loss: 0.3933430016040802 batch: 31/224\n",
      "Batch loss: 0.442911833524704 batch: 32/224\n",
      "Batch loss: 0.3918101489543915 batch: 33/224\n",
      "Batch loss: 0.34871628880500793 batch: 34/224\n",
      "Batch loss: 0.37484386563301086 batch: 35/224\n",
      "Batch loss: 0.44210612773895264 batch: 36/224\n",
      "Batch loss: 0.39588287472724915 batch: 37/224\n",
      "Batch loss: 0.37288185954093933 batch: 38/224\n",
      "Batch loss: 0.3959469497203827 batch: 39/224\n",
      "Batch loss: 0.4177229106426239 batch: 40/224\n",
      "Batch loss: 0.443466454744339 batch: 41/224\n",
      "Batch loss: 0.390461802482605 batch: 42/224\n",
      "Batch loss: 0.4179803133010864 batch: 43/224\n",
      "Batch loss: 0.3332250118255615 batch: 44/224\n",
      "Batch loss: 0.37153470516204834 batch: 45/224\n",
      "Batch loss: 0.4673318564891815 batch: 46/224\n",
      "Batch loss: 0.4036579728126526 batch: 47/224\n",
      "Batch loss: 0.3985365927219391 batch: 48/224\n",
      "Batch loss: 0.31405532360076904 batch: 49/224\n",
      "Batch loss: 0.38436126708984375 batch: 50/224\n",
      "Batch loss: 0.3630138337612152 batch: 51/224\n",
      "Batch loss: 0.30103200674057007 batch: 52/224\n",
      "Batch loss: 0.4652126133441925 batch: 53/224\n",
      "Batch loss: 0.36977460980415344 batch: 54/224\n",
      "Batch loss: 0.351194828748703 batch: 55/224\n",
      "Batch loss: 0.39240217208862305 batch: 56/224\n",
      "Batch loss: 0.45148134231567383 batch: 57/224\n",
      "Batch loss: 0.426072895526886 batch: 58/224\n",
      "Batch loss: 0.3707442283630371 batch: 59/224\n",
      "Batch loss: 0.4526731073856354 batch: 60/224\n",
      "Batch loss: 0.3881920278072357 batch: 61/224\n",
      "Batch loss: 0.37456268072128296 batch: 62/224\n",
      "Batch loss: 0.3989904522895813 batch: 63/224\n",
      "Batch loss: 0.4187900424003601 batch: 64/224\n",
      "Batch loss: 0.37647560238838196 batch: 65/224\n",
      "Batch loss: 0.3557036221027374 batch: 66/224\n",
      "Batch loss: 0.36809638142585754 batch: 67/224\n",
      "Batch loss: 0.43317827582359314 batch: 68/224\n",
      "Batch loss: 0.4271275997161865 batch: 69/224\n",
      "Batch loss: 0.3860585689544678 batch: 70/224\n",
      "Batch loss: 0.3474264442920685 batch: 71/224\n",
      "Batch loss: 0.2738054096698761 batch: 72/224\n",
      "Batch loss: 0.4261854588985443 batch: 73/224\n",
      "Batch loss: 0.360115647315979 batch: 74/224\n",
      "Batch loss: 0.4034588634967804 batch: 75/224\n",
      "Batch loss: 0.41685986518859863 batch: 76/224\n",
      "Batch loss: 0.39653655886650085 batch: 77/224\n",
      "Batch loss: 0.37027567625045776 batch: 78/224\n",
      "Batch loss: 0.41870570182800293 batch: 79/224\n",
      "Batch loss: 0.40435314178466797 batch: 80/224\n",
      "Batch loss: 0.47527384757995605 batch: 81/224\n",
      "Batch loss: 0.41213831305503845 batch: 82/224\n",
      "Batch loss: 0.4130606949329376 batch: 83/224\n",
      "Batch loss: 0.357387900352478 batch: 84/224\n",
      "Batch loss: 0.39322006702423096 batch: 85/224\n",
      "Batch loss: 0.3793317973613739 batch: 86/224\n",
      "Batch loss: 0.3898458778858185 batch: 87/224\n",
      "Batch loss: 0.4010440409183502 batch: 88/224\n",
      "Batch loss: 0.3672759234905243 batch: 89/224\n",
      "Batch loss: 0.4061274528503418 batch: 90/224\n",
      "Batch loss: 0.3815527558326721 batch: 91/224\n",
      "Batch loss: 0.3778340816497803 batch: 92/224\n",
      "Batch loss: 0.3293914198875427 batch: 93/224\n",
      "Batch loss: 0.39940574765205383 batch: 94/224\n",
      "Batch loss: 0.3392067849636078 batch: 95/224\n",
      "Batch loss: 0.37428998947143555 batch: 96/224\n",
      "Batch loss: 0.3411499261856079 batch: 97/224\n",
      "Batch loss: 0.2892964482307434 batch: 98/224\n",
      "Batch loss: 0.4185333847999573 batch: 99/224\n",
      "Batch loss: 0.33229437470436096 batch: 100/224\n",
      "Batch loss: 0.39622005820274353 batch: 101/224\n",
      "Batch loss: 0.4013659358024597 batch: 102/224\n",
      "Batch loss: 0.3748563528060913 batch: 103/224\n",
      "Batch loss: 0.3160605728626251 batch: 104/224\n",
      "Batch loss: 0.3415886163711548 batch: 105/224\n",
      "Batch loss: 0.40995123982429504 batch: 106/224\n",
      "Batch loss: 0.33501961827278137 batch: 107/224\n",
      "Batch loss: 0.40948185324668884 batch: 108/224\n",
      "Batch loss: 0.3628220856189728 batch: 109/224\n",
      "Batch loss: 0.39222949743270874 batch: 110/224\n",
      "Batch loss: 0.42207103967666626 batch: 111/224\n",
      "Batch loss: 0.3741770088672638 batch: 112/224\n",
      "Batch loss: 0.3949827253818512 batch: 113/224\n",
      "Batch loss: 0.3777768313884735 batch: 114/224\n",
      "Batch loss: 0.4129764437675476 batch: 115/224\n",
      "Batch loss: 0.3288182020187378 batch: 116/224\n",
      "Batch loss: 0.31399673223495483 batch: 117/224\n",
      "Batch loss: 0.4344543218612671 batch: 118/224\n",
      "Batch loss: 0.3388974368572235 batch: 119/224\n",
      "Batch loss: 0.41252416372299194 batch: 120/224\n",
      "Batch loss: 0.3761601448059082 batch: 121/224\n",
      "Batch loss: 0.36938172578811646 batch: 122/224\n",
      "Batch loss: 0.364288866519928 batch: 123/224\n",
      "Batch loss: 0.37031927704811096 batch: 124/224\n",
      "Batch loss: 0.3805956244468689 batch: 125/224\n",
      "Batch loss: 0.345003217458725 batch: 126/224\n",
      "Batch loss: 0.37013211846351624 batch: 127/224\n",
      "Batch loss: 0.3601842224597931 batch: 128/224\n",
      "Batch loss: 0.37520483136177063 batch: 129/224\n",
      "Batch loss: 0.3818027079105377 batch: 130/224\n",
      "Batch loss: 0.3521566689014435 batch: 131/224\n",
      "Batch loss: 0.3313559889793396 batch: 132/224\n",
      "Batch loss: 0.3436283469200134 batch: 133/224\n",
      "Batch loss: 0.36049750447273254 batch: 134/224\n",
      "Batch loss: 0.4479560852050781 batch: 135/224\n",
      "Batch loss: 0.39074957370758057 batch: 136/224\n",
      "Batch loss: 0.3285810649394989 batch: 137/224\n",
      "Batch loss: 0.38734501600265503 batch: 138/224\n",
      "Batch loss: 0.39092424511909485 batch: 139/224\n",
      "Batch loss: 0.43200984597206116 batch: 140/224\n",
      "Batch loss: 0.3199637830257416 batch: 141/224\n",
      "Batch loss: 0.34277239441871643 batch: 142/224\n",
      "Batch loss: 0.3597450256347656 batch: 143/224\n",
      "Batch loss: 0.3639124035835266 batch: 144/224\n",
      "Batch loss: 0.3628426492214203 batch: 145/224\n",
      "Batch loss: 0.388245165348053 batch: 146/224\n",
      "Batch loss: 0.36345553398132324 batch: 147/224\n",
      "Batch loss: 0.3481192886829376 batch: 148/224\n",
      "Batch loss: 0.3954951763153076 batch: 149/224\n",
      "Batch loss: 0.40878424048423767 batch: 150/224\n",
      "Batch loss: 0.3906816244125366 batch: 151/224\n",
      "Batch loss: 0.3278990685939789 batch: 152/224\n",
      "Batch loss: 0.3784380853176117 batch: 153/224\n",
      "Batch loss: 0.4195740818977356 batch: 154/224\n",
      "Batch loss: 0.3441257178783417 batch: 155/224\n",
      "Batch loss: 0.3792136311531067 batch: 156/224\n",
      "Batch loss: 0.3558572828769684 batch: 157/224\n",
      "Batch loss: 0.438504695892334 batch: 158/224\n",
      "Batch loss: 0.32311493158340454 batch: 159/224\n",
      "Batch loss: 0.38693684339523315 batch: 160/224\n",
      "Batch loss: 0.3419574499130249 batch: 161/224\n",
      "Batch loss: 0.37797674536705017 batch: 162/224\n",
      "Batch loss: 0.3610346019268036 batch: 163/224\n",
      "Batch loss: 0.34206289052963257 batch: 164/224\n",
      "Batch loss: 0.40217965841293335 batch: 165/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.38084423542022705 batch: 166/224\n",
      "Batch loss: 0.3110886812210083 batch: 167/224\n",
      "Batch loss: 0.31350651383399963 batch: 168/224\n",
      "Batch loss: 0.3850771486759186 batch: 169/224\n",
      "Batch loss: 0.35687124729156494 batch: 170/224\n",
      "Batch loss: 0.3516024053096771 batch: 171/224\n",
      "Batch loss: 0.35179486870765686 batch: 172/224\n",
      "Batch loss: 0.3602185845375061 batch: 173/224\n",
      "Batch loss: 0.37482577562332153 batch: 174/224\n",
      "Batch loss: 0.39816951751708984 batch: 175/224\n",
      "Batch loss: 0.35847586393356323 batch: 176/224\n",
      "Batch loss: 0.41026225686073303 batch: 177/224\n",
      "Batch loss: 0.3161337673664093 batch: 178/224\n",
      "Batch loss: 0.38128912448883057 batch: 179/224\n",
      "Batch loss: 0.3252645432949066 batch: 180/224\n",
      "Batch loss: 0.44079577922821045 batch: 181/224\n",
      "Batch loss: 0.347291499376297 batch: 182/224\n",
      "Batch loss: 0.3922438323497772 batch: 183/224\n",
      "Batch loss: 0.3575045168399811 batch: 184/224\n",
      "Batch loss: 0.4189414978027344 batch: 185/224\n",
      "Batch loss: 0.3145492970943451 batch: 186/224\n",
      "Batch loss: 0.36895275115966797 batch: 187/224\n",
      "Batch loss: 0.26367974281311035 batch: 188/224\n",
      "Batch loss: 0.40879830718040466 batch: 189/224\n",
      "Batch loss: 0.3925732672214508 batch: 190/224\n",
      "Batch loss: 0.36898273229599 batch: 191/224\n",
      "Batch loss: 0.38182199001312256 batch: 192/224\n",
      "Batch loss: 0.3484204113483429 batch: 193/224\n",
      "Batch loss: 0.3589329421520233 batch: 194/224\n",
      "Batch loss: 0.4057621359825134 batch: 195/224\n",
      "Batch loss: 0.41353023052215576 batch: 196/224\n",
      "Batch loss: 0.3525593876838684 batch: 197/224\n",
      "Batch loss: 0.39908215403556824 batch: 198/224\n",
      "Batch loss: 0.33635225892066956 batch: 199/224\n",
      "Batch loss: 0.41008663177490234 batch: 200/224\n",
      "Batch loss: 0.3825068473815918 batch: 201/224\n",
      "Batch loss: 0.3935336172580719 batch: 202/224\n",
      "Batch loss: 0.37125176191329956 batch: 203/224\n",
      "Batch loss: 0.35834985971450806 batch: 204/224\n",
      "Batch loss: 0.40284913778305054 batch: 205/224\n",
      "Batch loss: 0.3527115285396576 batch: 206/224\n",
      "Batch loss: 0.43309447169303894 batch: 207/224\n",
      "Batch loss: 0.323729544878006 batch: 208/224\n",
      "Batch loss: 0.403023898601532 batch: 209/224\n",
      "Batch loss: 0.3369057774543762 batch: 210/224\n",
      "Batch loss: 0.3353864550590515 batch: 211/224\n",
      "Batch loss: 0.37878912687301636 batch: 212/224\n",
      "Batch loss: 0.4126286506652832 batch: 213/224\n",
      "Batch loss: 0.39554452896118164 batch: 214/224\n",
      "Batch loss: 0.3902694284915924 batch: 215/224\n",
      "Batch loss: 0.3634847402572632 batch: 216/224\n",
      "Batch loss: 0.3891875147819519 batch: 217/224\n",
      "Batch loss: 0.3942551016807556 batch: 218/224\n",
      "Batch loss: 0.34880301356315613 batch: 219/224\n",
      "Batch loss: 0.30529308319091797 batch: 220/224\n",
      "Batch loss: 0.40912044048309326 batch: 221/224\n",
      "Batch loss: 0.33707869052886963 batch: 222/224\n",
      "Batch loss: 0.340713769197464 batch: 223/224\n",
      "Batch loss: 0.3539564907550812 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 7/75..  Training Loss: 0.00076..  Test Loss: 0.00074..  Test Accuracy: 0.86657\n",
      "Running epoch 8/75\n",
      "Batch loss: 0.3322301208972931 batch: 1/224\n",
      "Batch loss: 0.34618014097213745 batch: 2/224\n",
      "Batch loss: 0.34619811177253723 batch: 3/224\n",
      "Batch loss: 0.4179286062717438 batch: 4/224\n",
      "Batch loss: 0.34870442748069763 batch: 5/224\n",
      "Batch loss: 0.4102809429168701 batch: 6/224\n",
      "Batch loss: 0.335088849067688 batch: 7/224\n",
      "Batch loss: 0.3894078731536865 batch: 8/224\n",
      "Batch loss: 0.3094644546508789 batch: 9/224\n",
      "Batch loss: 0.3715953230857849 batch: 10/224\n",
      "Batch loss: 0.4180395305156708 batch: 11/224\n",
      "Batch loss: 0.36898544430732727 batch: 12/224\n",
      "Batch loss: 0.3227536678314209 batch: 13/224\n",
      "Batch loss: 0.3461798429489136 batch: 14/224\n",
      "Batch loss: 0.35342177748680115 batch: 15/224\n",
      "Batch loss: 0.51520174741745 batch: 16/224\n",
      "Batch loss: 0.35053643584251404 batch: 17/224\n",
      "Batch loss: 0.42370784282684326 batch: 18/224\n",
      "Batch loss: 0.33929792046546936 batch: 19/224\n",
      "Batch loss: 0.3529272675514221 batch: 20/224\n",
      "Batch loss: 0.4362999498844147 batch: 21/224\n",
      "Batch loss: 0.3127477765083313 batch: 22/224\n",
      "Batch loss: 0.4019167125225067 batch: 23/224\n",
      "Batch loss: 0.3892451822757721 batch: 24/224\n",
      "Batch loss: 0.32299289107322693 batch: 25/224\n",
      "Batch loss: 0.2952542006969452 batch: 26/224\n",
      "Batch loss: 0.35556158423423767 batch: 27/224\n",
      "Batch loss: 0.36317962408065796 batch: 28/224\n",
      "Batch loss: 0.3728000521659851 batch: 29/224\n",
      "Batch loss: 0.3573647141456604 batch: 30/224\n",
      "Batch loss: 0.3663664162158966 batch: 31/224\n",
      "Batch loss: 0.38531285524368286 batch: 32/224\n",
      "Batch loss: 0.33380642533302307 batch: 33/224\n",
      "Batch loss: 0.33246785402297974 batch: 34/224\n",
      "Batch loss: 0.36790478229522705 batch: 35/224\n",
      "Batch loss: 0.43386921286582947 batch: 36/224\n",
      "Batch loss: 0.36309558153152466 batch: 37/224\n",
      "Batch loss: 0.37049564719200134 batch: 38/224\n",
      "Batch loss: 0.34777605533599854 batch: 39/224\n",
      "Batch loss: 0.3586098849773407 batch: 40/224\n",
      "Batch loss: 0.40235137939453125 batch: 41/224\n",
      "Batch loss: 0.35708844661712646 batch: 42/224\n",
      "Batch loss: 0.40171390771865845 batch: 43/224\n",
      "Batch loss: 0.3203997015953064 batch: 44/224\n",
      "Batch loss: 0.34968337416648865 batch: 45/224\n",
      "Batch loss: 0.45196622610092163 batch: 46/224\n",
      "Batch loss: 0.36989399790763855 batch: 47/224\n",
      "Batch loss: 0.3438238799571991 batch: 48/224\n",
      "Batch loss: 0.3011165261268616 batch: 49/224\n",
      "Batch loss: 0.3424461781978607 batch: 50/224\n",
      "Batch loss: 0.33486610651016235 batch: 51/224\n",
      "Batch loss: 0.32035714387893677 batch: 52/224\n",
      "Batch loss: 0.4289097189903259 batch: 53/224\n",
      "Batch loss: 0.31084054708480835 batch: 54/224\n",
      "Batch loss: 0.3627067804336548 batch: 55/224\n",
      "Batch loss: 0.32529646158218384 batch: 56/224\n",
      "Batch loss: 0.4110284745693207 batch: 57/224\n",
      "Batch loss: 0.40550950169563293 batch: 58/224\n",
      "Batch loss: 0.3545871675014496 batch: 59/224\n",
      "Batch loss: 0.4173530042171478 batch: 60/224\n",
      "Batch loss: 0.4164876639842987 batch: 61/224\n",
      "Batch loss: 0.3260329067707062 batch: 62/224\n",
      "Batch loss: 0.3423563241958618 batch: 63/224\n",
      "Batch loss: 0.39246121048927307 batch: 64/224\n",
      "Batch loss: 0.38364100456237793 batch: 65/224\n",
      "Batch loss: 0.38011884689331055 batch: 66/224\n",
      "Batch loss: 0.3416610062122345 batch: 67/224\n",
      "Batch loss: 0.37677663564682007 batch: 68/224\n",
      "Batch loss: 0.3893872797489166 batch: 69/224\n",
      "Batch loss: 0.3952851891517639 batch: 70/224\n",
      "Batch loss: 0.32148221135139465 batch: 71/224\n",
      "Batch loss: 0.2613615095615387 batch: 72/224\n",
      "Batch loss: 0.41415300965309143 batch: 73/224\n",
      "Batch loss: 0.3991553485393524 batch: 74/224\n",
      "Batch loss: 0.37489089369773865 batch: 75/224\n",
      "Batch loss: 0.3800009787082672 batch: 76/224\n",
      "Batch loss: 0.32264775037765503 batch: 77/224\n",
      "Batch loss: 0.3857978582382202 batch: 78/224\n",
      "Batch loss: 0.404634952545166 batch: 79/224\n",
      "Batch loss: 0.38119906187057495 batch: 80/224\n",
      "Batch loss: 0.40542688965797424 batch: 81/224\n",
      "Batch loss: 0.38165223598480225 batch: 82/224\n",
      "Batch loss: 0.3810238242149353 batch: 83/224\n",
      "Batch loss: 0.34475067257881165 batch: 84/224\n",
      "Batch loss: 0.3689426779747009 batch: 85/224\n",
      "Batch loss: 0.34781619906425476 batch: 86/224\n",
      "Batch loss: 0.38227325677871704 batch: 87/224\n",
      "Batch loss: 0.3630872666835785 batch: 88/224\n",
      "Batch loss: 0.34077906608581543 batch: 89/224\n",
      "Batch loss: 0.35879623889923096 batch: 90/224\n",
      "Batch loss: 0.37579846382141113 batch: 91/224\n",
      "Batch loss: 0.3727223575115204 batch: 92/224\n",
      "Batch loss: 0.32770273089408875 batch: 93/224\n",
      "Batch loss: 0.3768623173236847 batch: 94/224\n",
      "Batch loss: 0.31760141253471375 batch: 95/224\n",
      "Batch loss: 0.3528250455856323 batch: 96/224\n",
      "Batch loss: 0.3268381953239441 batch: 97/224\n",
      "Batch loss: 0.31236323714256287 batch: 98/224\n",
      "Batch loss: 0.380160927772522 batch: 99/224\n",
      "Batch loss: 0.3445321023464203 batch: 100/224\n",
      "Batch loss: 0.40655583143234253 batch: 101/224\n",
      "Batch loss: 0.33530232310295105 batch: 102/224\n",
      "Batch loss: 0.36559340357780457 batch: 103/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.3083213269710541 batch: 104/224\n",
      "Batch loss: 0.3311077058315277 batch: 105/224\n",
      "Batch loss: 0.3371976613998413 batch: 106/224\n",
      "Batch loss: 0.3176942765712738 batch: 107/224\n",
      "Batch loss: 0.37091755867004395 batch: 108/224\n",
      "Batch loss: 0.3453049957752228 batch: 109/224\n",
      "Batch loss: 0.3410201966762543 batch: 110/224\n",
      "Batch loss: 0.3807843029499054 batch: 111/224\n",
      "Batch loss: 0.3469850420951843 batch: 112/224\n",
      "Batch loss: 0.41006922721862793 batch: 113/224\n",
      "Batch loss: 0.35051077604293823 batch: 114/224\n",
      "Batch loss: 0.3707248568534851 batch: 115/224\n",
      "Batch loss: 0.3461156487464905 batch: 116/224\n",
      "Batch loss: 0.3278913199901581 batch: 117/224\n",
      "Batch loss: 0.39890027046203613 batch: 118/224\n",
      "Batch loss: 0.32126933336257935 batch: 119/224\n",
      "Batch loss: 0.36476922035217285 batch: 120/224\n",
      "Batch loss: 0.37898245453834534 batch: 121/224\n",
      "Batch loss: 0.3869783282279968 batch: 122/224\n",
      "Batch loss: 0.33912503719329834 batch: 123/224\n",
      "Batch loss: 0.33500903844833374 batch: 124/224\n",
      "Batch loss: 0.3480253517627716 batch: 125/224\n",
      "Batch loss: 0.36452579498291016 batch: 126/224\n",
      "Batch loss: 0.35502129793167114 batch: 127/224\n",
      "Batch loss: 0.362978994846344 batch: 128/224\n",
      "Batch loss: 0.37972816824913025 batch: 129/224\n",
      "Batch loss: 0.37137553095817566 batch: 130/224\n",
      "Batch loss: 0.3194708824157715 batch: 131/224\n",
      "Batch loss: 0.3254331946372986 batch: 132/224\n",
      "Batch loss: 0.3818667531013489 batch: 133/224\n",
      "Batch loss: 0.334598571062088 batch: 134/224\n",
      "Batch loss: 0.42614611983299255 batch: 135/224\n",
      "Batch loss: 0.3924292027950287 batch: 136/224\n",
      "Batch loss: 0.3335826098918915 batch: 137/224\n",
      "Batch loss: 0.38078609108924866 batch: 138/224\n",
      "Batch loss: 0.3620641231536865 batch: 139/224\n",
      "Batch loss: 0.38642677664756775 batch: 140/224\n",
      "Batch loss: 0.28170356154441833 batch: 141/224\n",
      "Batch loss: 0.32396766543388367 batch: 142/224\n",
      "Batch loss: 0.36932283639907837 batch: 143/224\n",
      "Batch loss: 0.4022330939769745 batch: 144/224\n",
      "Batch loss: 0.35588735342025757 batch: 145/224\n",
      "Batch loss: 0.3730534315109253 batch: 146/224\n",
      "Batch loss: 0.3506831228733063 batch: 147/224\n",
      "Batch loss: 0.31404078006744385 batch: 148/224\n",
      "Batch loss: 0.3549022972583771 batch: 149/224\n",
      "Batch loss: 0.36312973499298096 batch: 150/224\n",
      "Batch loss: 0.3773925006389618 batch: 151/224\n",
      "Batch loss: 0.3616861402988434 batch: 152/224\n",
      "Batch loss: 0.38276737928390503 batch: 153/224\n",
      "Batch loss: 0.43161147832870483 batch: 154/224\n",
      "Batch loss: 0.32498836517333984 batch: 155/224\n",
      "Batch loss: 0.38452503085136414 batch: 156/224\n",
      "Batch loss: 0.37848302721977234 batch: 157/224\n",
      "Batch loss: 0.41827625036239624 batch: 158/224\n",
      "Batch loss: 0.30726930499076843 batch: 159/224\n",
      "Batch loss: 0.36900973320007324 batch: 160/224\n",
      "Batch loss: 0.2974015772342682 batch: 161/224\n",
      "Batch loss: 0.3704586327075958 batch: 162/224\n",
      "Batch loss: 0.36095166206359863 batch: 163/224\n",
      "Batch loss: 0.3456552028656006 batch: 164/224\n",
      "Batch loss: 0.3815881013870239 batch: 165/224\n",
      "Batch loss: 0.3357529044151306 batch: 166/224\n",
      "Batch loss: 0.32427552342414856 batch: 167/224\n",
      "Batch loss: 0.28720417618751526 batch: 168/224\n",
      "Batch loss: 0.34169599413871765 batch: 169/224\n",
      "Batch loss: 0.33901798725128174 batch: 170/224\n",
      "Batch loss: 0.32974037528038025 batch: 171/224\n",
      "Batch loss: 0.3652282953262329 batch: 172/224\n",
      "Batch loss: 0.3893338739871979 batch: 173/224\n",
      "Batch loss: 0.3627977669239044 batch: 174/224\n",
      "Batch loss: 0.33933308720588684 batch: 175/224\n",
      "Batch loss: 0.3386680483818054 batch: 176/224\n",
      "Batch loss: 0.3742397725582123 batch: 177/224\n",
      "Batch loss: 0.3185853660106659 batch: 178/224\n",
      "Batch loss: 0.37168559432029724 batch: 179/224\n",
      "Batch loss: 0.340202271938324 batch: 180/224\n",
      "Batch loss: 0.4194236993789673 batch: 181/224\n",
      "Batch loss: 0.3632243871688843 batch: 182/224\n",
      "Batch loss: 0.4354265332221985 batch: 183/224\n",
      "Batch loss: 0.3439377248287201 batch: 184/224\n",
      "Batch loss: 0.4172249436378479 batch: 185/224\n",
      "Batch loss: 0.3123548924922943 batch: 186/224\n",
      "Batch loss: 0.3355858623981476 batch: 187/224\n",
      "Batch loss: 0.27374017238616943 batch: 188/224\n",
      "Batch loss: 0.3987177908420563 batch: 189/224\n",
      "Batch loss: 0.37760215997695923 batch: 190/224\n",
      "Batch loss: 0.35866156220436096 batch: 191/224\n",
      "Batch loss: 0.37590447068214417 batch: 192/224\n",
      "Batch loss: 0.3561761975288391 batch: 193/224\n",
      "Batch loss: 0.3650738298892975 batch: 194/224\n",
      "Batch loss: 0.40379926562309265 batch: 195/224\n",
      "Batch loss: 0.39280036091804504 batch: 196/224\n",
      "Batch loss: 0.36483708024024963 batch: 197/224\n",
      "Batch loss: 0.3610748052597046 batch: 198/224\n",
      "Batch loss: 0.30668172240257263 batch: 199/224\n",
      "Batch loss: 0.38715195655822754 batch: 200/224\n",
      "Batch loss: 0.3715064227581024 batch: 201/224\n",
      "Batch loss: 0.3912622034549713 batch: 202/224\n",
      "Batch loss: 0.33576154708862305 batch: 203/224\n",
      "Batch loss: 0.3213094472885132 batch: 204/224\n",
      "Batch loss: 0.36947378516197205 batch: 205/224\n",
      "Batch loss: 0.32968372106552124 batch: 206/224\n",
      "Batch loss: 0.3867909014225006 batch: 207/224\n",
      "Batch loss: 0.3293721377849579 batch: 208/224\n",
      "Batch loss: 0.33446961641311646 batch: 209/224\n",
      "Batch loss: 0.3539852797985077 batch: 210/224\n",
      "Batch loss: 0.34645384550094604 batch: 211/224\n",
      "Batch loss: 0.3696121871471405 batch: 212/224\n",
      "Batch loss: 0.35170215368270874 batch: 213/224\n",
      "Batch loss: 0.3749018609523773 batch: 214/224\n",
      "Batch loss: 0.39317891001701355 batch: 215/224\n",
      "Batch loss: 0.3797016441822052 batch: 216/224\n",
      "Batch loss: 0.3700349032878876 batch: 217/224\n",
      "Batch loss: 0.32327091693878174 batch: 218/224\n",
      "Batch loss: 0.3031051456928253 batch: 219/224\n",
      "Batch loss: 0.287354439496994 batch: 220/224\n",
      "Batch loss: 0.3978379964828491 batch: 221/224\n",
      "Batch loss: 0.30365124344825745 batch: 222/224\n",
      "Batch loss: 0.3166385889053345 batch: 223/224\n",
      "Batch loss: 0.32160791754722595 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 8/75..  Training Loss: 0.00072..  Test Loss: 0.00072..  Test Accuracy: 0.87189\n",
      "Running epoch 9/75\n",
      "Batch loss: 0.3028401732444763 batch: 1/224\n",
      "Batch loss: 0.3446361720561981 batch: 2/224\n",
      "Batch loss: 0.32829970121383667 batch: 3/224\n",
      "Batch loss: 0.38386768102645874 batch: 4/224\n",
      "Batch loss: 0.35217025876045227 batch: 5/224\n",
      "Batch loss: 0.4199031889438629 batch: 6/224\n",
      "Batch loss: 0.32704347372055054 batch: 7/224\n",
      "Batch loss: 0.3784833252429962 batch: 8/224\n",
      "Batch loss: 0.2884151339530945 batch: 9/224\n",
      "Batch loss: 0.3262570798397064 batch: 10/224\n",
      "Batch loss: 0.4116170406341553 batch: 11/224\n",
      "Batch loss: 0.327173113822937 batch: 12/224\n",
      "Batch loss: 0.2732260823249817 batch: 13/224\n",
      "Batch loss: 0.30617839097976685 batch: 14/224\n",
      "Batch loss: 0.3350614905357361 batch: 15/224\n",
      "Batch loss: 0.45843592286109924 batch: 16/224\n",
      "Batch loss: 0.3445826470851898 batch: 17/224\n",
      "Batch loss: 0.3900582790374756 batch: 18/224\n",
      "Batch loss: 0.30436697602272034 batch: 19/224\n",
      "Batch loss: 0.29372668266296387 batch: 20/224\n",
      "Batch loss: 0.3928034007549286 batch: 21/224\n",
      "Batch loss: 0.32769909501075745 batch: 22/224\n",
      "Batch loss: 0.4081708788871765 batch: 23/224\n",
      "Batch loss: 0.3784223198890686 batch: 24/224\n",
      "Batch loss: 0.30711597204208374 batch: 25/224\n",
      "Batch loss: 0.29402923583984375 batch: 26/224\n",
      "Batch loss: 0.34599077701568604 batch: 27/224\n",
      "Batch loss: 0.3472411632537842 batch: 28/224\n",
      "Batch loss: 0.34521615505218506 batch: 29/224\n",
      "Batch loss: 0.318868488073349 batch: 30/224\n",
      "Batch loss: 0.31715840101242065 batch: 31/224\n",
      "Batch loss: 0.4032401144504547 batch: 32/224\n",
      "Batch loss: 0.3214315176010132 batch: 33/224\n",
      "Batch loss: 0.3318997323513031 batch: 34/224\n",
      "Batch loss: 0.3284987807273865 batch: 35/224\n",
      "Batch loss: 0.3839047849178314 batch: 36/224\n",
      "Batch loss: 0.3590528070926666 batch: 37/224\n",
      "Batch loss: 0.34548431634902954 batch: 38/224\n",
      "Batch loss: 0.32722851634025574 batch: 39/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.3608580529689789 batch: 40/224\n",
      "Batch loss: 0.3801504075527191 batch: 41/224\n",
      "Batch loss: 0.3611850440502167 batch: 42/224\n",
      "Batch loss: 0.37704703211784363 batch: 43/224\n",
      "Batch loss: 0.3093070685863495 batch: 44/224\n",
      "Batch loss: 0.34085437655448914 batch: 45/224\n",
      "Batch loss: 0.41032129526138306 batch: 46/224\n",
      "Batch loss: 0.346940279006958 batch: 47/224\n",
      "Batch loss: 0.3620174825191498 batch: 48/224\n",
      "Batch loss: 0.3115389049053192 batch: 49/224\n",
      "Batch loss: 0.3299591541290283 batch: 50/224\n",
      "Batch loss: 0.3022816479206085 batch: 51/224\n",
      "Batch loss: 0.28311288356781006 batch: 52/224\n",
      "Batch loss: 0.39118102192878723 batch: 53/224\n",
      "Batch loss: 0.33570343255996704 batch: 54/224\n",
      "Batch loss: 0.3239630460739136 batch: 55/224\n",
      "Batch loss: 0.3045752942562103 batch: 56/224\n",
      "Batch loss: 0.4109703600406647 batch: 57/224\n",
      "Batch loss: 0.3845982849597931 batch: 58/224\n",
      "Batch loss: 0.3668096959590912 batch: 59/224\n",
      "Batch loss: 0.3963017463684082 batch: 60/224\n",
      "Batch loss: 0.3854560852050781 batch: 61/224\n",
      "Batch loss: 0.31708019971847534 batch: 62/224\n",
      "Batch loss: 0.3534104526042938 batch: 63/224\n",
      "Batch loss: 0.35322079062461853 batch: 64/224\n",
      "Batch loss: 0.3450435698032379 batch: 65/224\n",
      "Batch loss: 0.38059961795806885 batch: 66/224\n",
      "Batch loss: 0.33283594250679016 batch: 67/224\n",
      "Batch loss: 0.33984866738319397 batch: 68/224\n",
      "Batch loss: 0.36224812269210815 batch: 69/224\n",
      "Batch loss: 0.3374025523662567 batch: 70/224\n",
      "Batch loss: 0.34088486433029175 batch: 71/224\n",
      "Batch loss: 0.24371524155139923 batch: 72/224\n",
      "Batch loss: 0.38969600200653076 batch: 73/224\n",
      "Batch loss: 0.3735150098800659 batch: 74/224\n",
      "Batch loss: 0.356244295835495 batch: 75/224\n",
      "Batch loss: 0.3891586363315582 batch: 76/224\n",
      "Batch loss: 0.32087987661361694 batch: 77/224\n",
      "Batch loss: 0.364153116941452 batch: 78/224\n",
      "Batch loss: 0.34887781739234924 batch: 79/224\n",
      "Batch loss: 0.3775639533996582 batch: 80/224\n",
      "Batch loss: 0.42552489042282104 batch: 81/224\n",
      "Batch loss: 0.4134768843650818 batch: 82/224\n",
      "Batch loss: 0.35600554943084717 batch: 83/224\n",
      "Batch loss: 0.3043927550315857 batch: 84/224\n",
      "Batch loss: 0.36675354838371277 batch: 85/224\n",
      "Batch loss: 0.3250259757041931 batch: 86/224\n",
      "Batch loss: 0.3434859812259674 batch: 87/224\n",
      "Batch loss: 0.34189391136169434 batch: 88/224\n",
      "Batch loss: 0.3407261073589325 batch: 89/224\n",
      "Batch loss: 0.36842092871665955 batch: 90/224\n",
      "Batch loss: 0.37591296434402466 batch: 91/224\n",
      "Batch loss: 0.350115567445755 batch: 92/224\n",
      "Batch loss: 0.28284573554992676 batch: 93/224\n",
      "Batch loss: 0.34347155690193176 batch: 94/224\n",
      "Batch loss: 0.30206984281539917 batch: 95/224\n",
      "Batch loss: 0.31324732303619385 batch: 96/224\n",
      "Batch loss: 0.3335517942905426 batch: 97/224\n",
      "Batch loss: 0.27829113602638245 batch: 98/224\n",
      "Batch loss: 0.36891797184944153 batch: 99/224\n",
      "Batch loss: 0.34728971123695374 batch: 100/224\n",
      "Batch loss: 0.3703353703022003 batch: 101/224\n",
      "Batch loss: 0.32943418622016907 batch: 102/224\n",
      "Batch loss: 0.34225183725357056 batch: 103/224\n",
      "Batch loss: 0.30424368381500244 batch: 104/224\n",
      "Batch loss: 0.30226847529411316 batch: 105/224\n",
      "Batch loss: 0.34700828790664673 batch: 106/224\n",
      "Batch loss: 0.2991597056388855 batch: 107/224\n",
      "Batch loss: 0.3430894911289215 batch: 108/224\n",
      "Batch loss: 0.3097512722015381 batch: 109/224\n",
      "Batch loss: 0.3136821389198303 batch: 110/224\n",
      "Batch loss: 0.38071295619010925 batch: 111/224\n",
      "Batch loss: 0.3090918958187103 batch: 112/224\n",
      "Batch loss: 0.3950798809528351 batch: 113/224\n",
      "Batch loss: 0.3429349958896637 batch: 114/224\n",
      "Batch loss: 0.34151291847229004 batch: 115/224\n",
      "Batch loss: 0.3072957694530487 batch: 116/224\n",
      "Batch loss: 0.29702892899513245 batch: 117/224\n",
      "Batch loss: 0.3633682131767273 batch: 118/224\n",
      "Batch loss: 0.30428797006607056 batch: 119/224\n",
      "Batch loss: 0.33836397528648376 batch: 120/224\n",
      "Batch loss: 0.33874279260635376 batch: 121/224\n",
      "Batch loss: 0.37673771381378174 batch: 122/224\n",
      "Batch loss: 0.3497071862220764 batch: 123/224\n",
      "Batch loss: 0.33461761474609375 batch: 124/224\n",
      "Batch loss: 0.3457334339618683 batch: 125/224\n",
      "Batch loss: 0.34922125935554504 batch: 126/224\n",
      "Batch loss: 0.3563663959503174 batch: 127/224\n",
      "Batch loss: 0.3182721734046936 batch: 128/224\n",
      "Batch loss: 0.37681570649147034 batch: 129/224\n",
      "Batch loss: 0.34168753027915955 batch: 130/224\n",
      "Batch loss: 0.3167162239551544 batch: 131/224\n",
      "Batch loss: 0.3334237039089203 batch: 132/224\n",
      "Batch loss: 0.3286176323890686 batch: 133/224\n",
      "Batch loss: 0.30587059259414673 batch: 134/224\n",
      "Batch loss: 0.3571726083755493 batch: 135/224\n",
      "Batch loss: 0.36233747005462646 batch: 136/224\n",
      "Batch loss: 0.3091415464878082 batch: 137/224\n",
      "Batch loss: 0.3815358579158783 batch: 138/224\n",
      "Batch loss: 0.3258006274700165 batch: 139/224\n",
      "Batch loss: 0.3756136894226074 batch: 140/224\n",
      "Batch loss: 0.2855055630207062 batch: 141/224\n",
      "Batch loss: 0.316804975271225 batch: 142/224\n",
      "Batch loss: 0.3378828465938568 batch: 143/224\n",
      "Batch loss: 0.3300638794898987 batch: 144/224\n",
      "Batch loss: 0.3291983902454376 batch: 145/224\n",
      "Batch loss: 0.3696460425853729 batch: 146/224\n",
      "Batch loss: 0.34563523530960083 batch: 147/224\n",
      "Batch loss: 0.3254106342792511 batch: 148/224\n",
      "Batch loss: 0.3337751030921936 batch: 149/224\n",
      "Batch loss: 0.3307325541973114 batch: 150/224\n",
      "Batch loss: 0.3755896985530853 batch: 151/224\n",
      "Batch loss: 0.33054810762405396 batch: 152/224\n",
      "Batch loss: 0.3371433913707733 batch: 153/224\n",
      "Batch loss: 0.3850272297859192 batch: 154/224\n",
      "Batch loss: 0.32627761363983154 batch: 155/224\n",
      "Batch loss: 0.3692857027053833 batch: 156/224\n",
      "Batch loss: 0.3337411880493164 batch: 157/224\n",
      "Batch loss: 0.40310773253440857 batch: 158/224\n",
      "Batch loss: 0.26019296050071716 batch: 159/224\n",
      "Batch loss: 0.3157152831554413 batch: 160/224\n",
      "Batch loss: 0.2959965467453003 batch: 161/224\n",
      "Batch loss: 0.30752143263816833 batch: 162/224\n",
      "Batch loss: 0.33475565910339355 batch: 163/224\n",
      "Batch loss: 0.31231436133384705 batch: 164/224\n",
      "Batch loss: 0.38621532917022705 batch: 165/224\n",
      "Batch loss: 0.30696192383766174 batch: 166/224\n",
      "Batch loss: 0.27489417791366577 batch: 167/224\n",
      "Batch loss: 0.28711840510368347 batch: 168/224\n",
      "Batch loss: 0.3493533730506897 batch: 169/224\n",
      "Batch loss: 0.34335407614707947 batch: 170/224\n",
      "Batch loss: 0.304023414850235 batch: 171/224\n",
      "Batch loss: 0.32718828320503235 batch: 172/224\n",
      "Batch loss: 0.3263656795024872 batch: 173/224\n",
      "Batch loss: 0.32763415575027466 batch: 174/224\n",
      "Batch loss: 0.3072255551815033 batch: 175/224\n",
      "Batch loss: 0.29436784982681274 batch: 176/224\n",
      "Batch loss: 0.37253811955451965 batch: 177/224\n",
      "Batch loss: 0.29589375853538513 batch: 178/224\n",
      "Batch loss: 0.3457038104534149 batch: 179/224\n",
      "Batch loss: 0.30359068512916565 batch: 180/224\n",
      "Batch loss: 0.38449999690055847 batch: 181/224\n",
      "Batch loss: 0.3327331840991974 batch: 182/224\n",
      "Batch loss: 0.3914433717727661 batch: 183/224\n",
      "Batch loss: 0.3258172869682312 batch: 184/224\n",
      "Batch loss: 0.4163304567337036 batch: 185/224\n",
      "Batch loss: 0.3224983215332031 batch: 186/224\n",
      "Batch loss: 0.32746362686157227 batch: 187/224\n",
      "Batch loss: 0.21898530423641205 batch: 188/224\n",
      "Batch loss: 0.3504200279712677 batch: 189/224\n",
      "Batch loss: 0.38960781693458557 batch: 190/224\n",
      "Batch loss: 0.33486083149909973 batch: 191/224\n",
      "Batch loss: 0.38786783814430237 batch: 192/224\n",
      "Batch loss: 0.3431885242462158 batch: 193/224\n",
      "Batch loss: 0.32790783047676086 batch: 194/224\n",
      "Batch loss: 0.3447597622871399 batch: 195/224\n",
      "Batch loss: 0.3588365614414215 batch: 196/224\n",
      "Batch loss: 0.33492231369018555 batch: 197/224\n",
      "Batch loss: 0.3861003816127777 batch: 198/224\n",
      "Batch loss: 0.33330920338630676 batch: 199/224\n",
      "Batch loss: 0.3410208821296692 batch: 200/224\n",
      "Batch loss: 0.3831353187561035 batch: 201/224\n",
      "Batch loss: 0.3659335672855377 batch: 202/224\n",
      "Batch loss: 0.3584771156311035 batch: 203/224\n",
      "Batch loss: 0.30642661452293396 batch: 204/224\n",
      "Batch loss: 0.38068968057632446 batch: 205/224\n",
      "Batch loss: 0.30424928665161133 batch: 206/224\n",
      "Batch loss: 0.3974418640136719 batch: 207/224\n",
      "Batch loss: 0.28264591097831726 batch: 208/224\n",
      "Batch loss: 0.36139968037605286 batch: 209/224\n",
      "Batch loss: 0.36124908924102783 batch: 210/224\n",
      "Batch loss: 0.3088075816631317 batch: 211/224\n",
      "Batch loss: 0.35878926515579224 batch: 212/224\n",
      "Batch loss: 0.35694098472595215 batch: 213/224\n",
      "Batch loss: 0.3510482907295227 batch: 214/224\n",
      "Batch loss: 0.3738223910331726 batch: 215/224\n",
      "Batch loss: 0.3468134105205536 batch: 216/224\n",
      "Batch loss: 0.36037707328796387 batch: 217/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.3413775563240051 batch: 218/224\n",
      "Batch loss: 0.3185665011405945 batch: 219/224\n",
      "Batch loss: 0.300079345703125 batch: 220/224\n",
      "Batch loss: 0.3869471848011017 batch: 221/224\n",
      "Batch loss: 0.3141701817512512 batch: 222/224\n",
      "Batch loss: 0.3196716606616974 batch: 223/224\n",
      "Batch loss: 0.2950212061405182 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 9/75..  Training Loss: 0.00068..  Test Loss: 0.00070..  Test Accuracy: 0.87311\n",
      "Running epoch 10/75\n",
      "Batch loss: 0.2880702316761017 batch: 1/224\n",
      "Batch loss: 0.33450740575790405 batch: 2/224\n",
      "Batch loss: 0.3018481433391571 batch: 3/224\n",
      "Batch loss: 0.3775111138820648 batch: 4/224\n",
      "Batch loss: 0.31194132566452026 batch: 5/224\n",
      "Batch loss: 0.38017433881759644 batch: 6/224\n",
      "Batch loss: 0.31006133556365967 batch: 7/224\n",
      "Batch loss: 0.33474040031433105 batch: 8/224\n",
      "Batch loss: 0.2793961465358734 batch: 9/224\n",
      "Batch loss: 0.3379878103733063 batch: 10/224\n",
      "Batch loss: 0.3656948208808899 batch: 11/224\n",
      "Batch loss: 0.3459714651107788 batch: 12/224\n",
      "Batch loss: 0.2716127336025238 batch: 13/224\n",
      "Batch loss: 0.2973666787147522 batch: 14/224\n",
      "Batch loss: 0.32840779423713684 batch: 15/224\n",
      "Batch loss: 0.41612184047698975 batch: 16/224\n",
      "Batch loss: 0.2971118688583374 batch: 17/224\n",
      "Batch loss: 0.4003114402294159 batch: 18/224\n",
      "Batch loss: 0.30634617805480957 batch: 19/224\n",
      "Batch loss: 0.3043242394924164 batch: 20/224\n",
      "Batch loss: 0.34792450070381165 batch: 21/224\n",
      "Batch loss: 0.307796448469162 batch: 22/224\n",
      "Batch loss: 0.36517876386642456 batch: 23/224\n",
      "Batch loss: 0.34833958745002747 batch: 24/224\n",
      "Batch loss: 0.2584267258644104 batch: 25/224\n",
      "Batch loss: 0.2759380638599396 batch: 26/224\n",
      "Batch loss: 0.389891117811203 batch: 27/224\n",
      "Batch loss: 0.31728631258010864 batch: 28/224\n",
      "Batch loss: 0.39196017384529114 batch: 29/224\n",
      "Batch loss: 0.3280969560146332 batch: 30/224\n",
      "Batch loss: 0.31631943583488464 batch: 31/224\n",
      "Batch loss: 0.36102133989334106 batch: 32/224\n",
      "Batch loss: 0.30506616830825806 batch: 33/224\n",
      "Batch loss: 0.3117530941963196 batch: 34/224\n",
      "Batch loss: 0.29112884402275085 batch: 35/224\n",
      "Batch loss: 0.357347309589386 batch: 36/224\n",
      "Batch loss: 0.3432682156562805 batch: 37/224\n",
      "Batch loss: 0.33957138657569885 batch: 38/224\n",
      "Batch loss: 0.33567535877227783 batch: 39/224\n",
      "Batch loss: 0.363866925239563 batch: 40/224\n",
      "Batch loss: 0.36201050877571106 batch: 41/224\n",
      "Batch loss: 0.3593158423900604 batch: 42/224\n",
      "Batch loss: 0.332550048828125 batch: 43/224\n",
      "Batch loss: 0.2622799277305603 batch: 44/224\n",
      "Batch loss: 0.27354681491851807 batch: 45/224\n",
      "Batch loss: 0.397459477186203 batch: 46/224\n",
      "Batch loss: 0.3346079885959625 batch: 47/224\n",
      "Batch loss: 0.319078266620636 batch: 48/224\n",
      "Batch loss: 0.27737462520599365 batch: 49/224\n",
      "Batch loss: 0.3273563086986542 batch: 50/224\n",
      "Batch loss: 0.30464842915534973 batch: 51/224\n",
      "Batch loss: 0.2886725962162018 batch: 52/224\n",
      "Batch loss: 0.3701910376548767 batch: 53/224\n",
      "Batch loss: 0.294800728559494 batch: 54/224\n",
      "Batch loss: 0.3150158226490021 batch: 55/224\n",
      "Batch loss: 0.3221483826637268 batch: 56/224\n",
      "Batch loss: 0.39294061064720154 batch: 57/224\n",
      "Batch loss: 0.3407014012336731 batch: 58/224\n",
      "Batch loss: 0.3290759027004242 batch: 59/224\n",
      "Batch loss: 0.39941349625587463 batch: 60/224\n",
      "Batch loss: 0.3095477521419525 batch: 61/224\n",
      "Batch loss: 0.3015194237232208 batch: 62/224\n",
      "Batch loss: 0.34248894453048706 batch: 63/224\n",
      "Batch loss: 0.37958237528800964 batch: 64/224\n",
      "Batch loss: 0.34967443346977234 batch: 65/224\n",
      "Batch loss: 0.35932666063308716 batch: 66/224\n",
      "Batch loss: 0.3145750164985657 batch: 67/224\n",
      "Batch loss: 0.34610581398010254 batch: 68/224\n",
      "Batch loss: 0.35288548469543457 batch: 69/224\n",
      "Batch loss: 0.3416215479373932 batch: 70/224\n",
      "Batch loss: 0.2859407067298889 batch: 71/224\n",
      "Batch loss: 0.23776505887508392 batch: 72/224\n",
      "Batch loss: 0.35340848565101624 batch: 73/224\n",
      "Batch loss: 0.353303998708725 batch: 74/224\n",
      "Batch loss: 0.3367035686969757 batch: 75/224\n",
      "Batch loss: 0.3577367663383484 batch: 76/224\n",
      "Batch loss: 0.33265408873558044 batch: 77/224\n",
      "Batch loss: 0.3402358889579773 batch: 78/224\n",
      "Batch loss: 0.3898816704750061 batch: 79/224\n",
      "Batch loss: 0.3261873424053192 batch: 80/224\n",
      "Batch loss: 0.39054635167121887 batch: 81/224\n",
      "Batch loss: 0.371808260679245 batch: 82/224\n",
      "Batch loss: 0.33809569478034973 batch: 83/224\n",
      "Batch loss: 0.29661160707473755 batch: 84/224\n",
      "Batch loss: 0.30698952078819275 batch: 85/224\n",
      "Batch loss: 0.3474964201450348 batch: 86/224\n",
      "Batch loss: 0.3175865113735199 batch: 87/224\n",
      "Batch loss: 0.33795687556266785 batch: 88/224\n",
      "Batch loss: 0.3391525447368622 batch: 89/224\n",
      "Batch loss: 0.35692527890205383 batch: 90/224\n",
      "Batch loss: 0.329200804233551 batch: 91/224\n",
      "Batch loss: 0.3573794662952423 batch: 92/224\n",
      "Batch loss: 0.3015331029891968 batch: 93/224\n",
      "Batch loss: 0.32958275079727173 batch: 94/224\n",
      "Batch loss: 0.2883303165435791 batch: 95/224\n",
      "Batch loss: 0.31367823481559753 batch: 96/224\n",
      "Batch loss: 0.30664804577827454 batch: 97/224\n",
      "Batch loss: 0.28907155990600586 batch: 98/224\n",
      "Batch loss: 0.3645770847797394 batch: 99/224\n",
      "Batch loss: 0.31381309032440186 batch: 100/224\n",
      "Batch loss: 0.3560965657234192 batch: 101/224\n",
      "Batch loss: 0.31430119276046753 batch: 102/224\n",
      "Batch loss: 0.37163496017456055 batch: 103/224\n",
      "Batch loss: 0.28527915477752686 batch: 104/224\n",
      "Batch loss: 0.3100524842739105 batch: 105/224\n",
      "Batch loss: 0.3116525411605835 batch: 106/224\n",
      "Batch loss: 0.2726234197616577 batch: 107/224\n",
      "Batch loss: 0.3265342116355896 batch: 108/224\n",
      "Batch loss: 0.3351273536682129 batch: 109/224\n",
      "Batch loss: 0.3321329951286316 batch: 110/224\n",
      "Batch loss: 0.34759363532066345 batch: 111/224\n",
      "Batch loss: 0.2985246479511261 batch: 112/224\n",
      "Batch loss: 0.35972970724105835 batch: 113/224\n",
      "Batch loss: 0.33337730169296265 batch: 114/224\n",
      "Batch loss: 0.3554815948009491 batch: 115/224\n",
      "Batch loss: 0.3027670681476593 batch: 116/224\n",
      "Batch loss: 0.3260844647884369 batch: 117/224\n",
      "Batch loss: 0.3800491392612457 batch: 118/224\n",
      "Batch loss: 0.31547459959983826 batch: 119/224\n",
      "Batch loss: 0.3588491380214691 batch: 120/224\n",
      "Batch loss: 0.2913578748703003 batch: 121/224\n",
      "Batch loss: 0.3660723865032196 batch: 122/224\n",
      "Batch loss: 0.3207117021083832 batch: 123/224\n",
      "Batch loss: 0.3534401059150696 batch: 124/224\n",
      "Batch loss: 0.3206482529640198 batch: 125/224\n",
      "Batch loss: 0.32580363750457764 batch: 126/224\n",
      "Batch loss: 0.3431069850921631 batch: 127/224\n",
      "Batch loss: 0.3016990125179291 batch: 128/224\n",
      "Batch loss: 0.34734272956848145 batch: 129/224\n",
      "Batch loss: 0.3596417307853699 batch: 130/224\n",
      "Batch loss: 0.2939748167991638 batch: 131/224\n",
      "Batch loss: 0.29773572087287903 batch: 132/224\n",
      "Batch loss: 0.3144109845161438 batch: 133/224\n",
      "Batch loss: 0.31438690423965454 batch: 134/224\n",
      "Batch loss: 0.3728814423084259 batch: 135/224\n",
      "Batch loss: 0.33123651146888733 batch: 136/224\n",
      "Batch loss: 0.2884458601474762 batch: 137/224\n",
      "Batch loss: 0.3114106357097626 batch: 138/224\n",
      "Batch loss: 0.35438886284828186 batch: 139/224\n",
      "Batch loss: 0.41441503167152405 batch: 140/224\n",
      "Batch loss: 0.28732869029045105 batch: 141/224\n",
      "Batch loss: 0.33275893330574036 batch: 142/224\n",
      "Batch loss: 0.3042091131210327 batch: 143/224\n",
      "Batch loss: 0.33675724267959595 batch: 144/224\n",
      "Batch loss: 0.3494652211666107 batch: 145/224\n",
      "Batch loss: 0.36958783864974976 batch: 146/224\n",
      "Batch loss: 0.3139673173427582 batch: 147/224\n",
      "Batch loss: 0.28608348965644836 batch: 148/224\n",
      "Batch loss: 0.35894641280174255 batch: 149/224\n",
      "Batch loss: 0.35504716634750366 batch: 150/224\n",
      "Batch loss: 0.33821600675582886 batch: 151/224\n",
      "Batch loss: 0.28027257323265076 batch: 152/224\n",
      "Batch loss: 0.3608277142047882 batch: 153/224\n",
      "Batch loss: 0.36808308959007263 batch: 154/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.3126835823059082 batch: 155/224\n",
      "Batch loss: 0.30781087279319763 batch: 156/224\n",
      "Batch loss: 0.33518850803375244 batch: 157/224\n",
      "Batch loss: 0.37348443269729614 batch: 158/224\n",
      "Batch loss: 0.2512665390968323 batch: 159/224\n",
      "Batch loss: 0.30034759640693665 batch: 160/224\n",
      "Batch loss: 0.29028233885765076 batch: 161/224\n",
      "Batch loss: 0.3323058485984802 batch: 162/224\n",
      "Batch loss: 0.28858014941215515 batch: 163/224\n",
      "Batch loss: 0.32448068261146545 batch: 164/224\n",
      "Batch loss: 0.3710296154022217 batch: 165/224\n",
      "Batch loss: 0.32053083181381226 batch: 166/224\n",
      "Batch loss: 0.26008713245391846 batch: 167/224\n",
      "Batch loss: 0.2785772979259491 batch: 168/224\n",
      "Batch loss: 0.33201366662979126 batch: 169/224\n",
      "Batch loss: 0.302150160074234 batch: 170/224\n",
      "Batch loss: 0.29408445954322815 batch: 171/224\n",
      "Batch loss: 0.3018672466278076 batch: 172/224\n",
      "Batch loss: 0.33116698265075684 batch: 173/224\n",
      "Batch loss: 0.3110015094280243 batch: 174/224\n",
      "Batch loss: 0.31888267397880554 batch: 175/224\n",
      "Batch loss: 0.3171220123767853 batch: 176/224\n",
      "Batch loss: 0.32209959626197815 batch: 177/224\n",
      "Batch loss: 0.29997292160987854 batch: 178/224\n",
      "Batch loss: 0.3465867340564728 batch: 179/224\n",
      "Batch loss: 0.28782030940055847 batch: 180/224\n",
      "Batch loss: 0.37717482447624207 batch: 181/224\n",
      "Batch loss: 0.3445393145084381 batch: 182/224\n",
      "Batch loss: 0.35816362500190735 batch: 183/224\n",
      "Batch loss: 0.3248428702354431 batch: 184/224\n",
      "Batch loss: 0.374593049287796 batch: 185/224\n",
      "Batch loss: 0.2894590497016907 batch: 186/224\n",
      "Batch loss: 0.3158370852470398 batch: 187/224\n",
      "Batch loss: 0.26785919070243835 batch: 188/224\n",
      "Batch loss: 0.34497424960136414 batch: 189/224\n",
      "Batch loss: 0.3306896388530731 batch: 190/224\n",
      "Batch loss: 0.3227550685405731 batch: 191/224\n",
      "Batch loss: 0.36400043964385986 batch: 192/224\n",
      "Batch loss: 0.3561362624168396 batch: 193/224\n",
      "Batch loss: 0.33392295241355896 batch: 194/224\n",
      "Batch loss: 0.35800227522850037 batch: 195/224\n",
      "Batch loss: 0.34847429394721985 batch: 196/224\n",
      "Batch loss: 0.30668818950653076 batch: 197/224\n",
      "Batch loss: 0.32940688729286194 batch: 198/224\n",
      "Batch loss: 0.298472136259079 batch: 199/224\n",
      "Batch loss: 0.3411185145378113 batch: 200/224\n",
      "Batch loss: 0.35517749190330505 batch: 201/224\n",
      "Batch loss: 0.3462984561920166 batch: 202/224\n",
      "Batch loss: 0.3617185950279236 batch: 203/224\n",
      "Batch loss: 0.30993857979774475 batch: 204/224\n",
      "Batch loss: 0.3324058949947357 batch: 205/224\n",
      "Batch loss: 0.2837231457233429 batch: 206/224\n",
      "Batch loss: 0.344013512134552 batch: 207/224\n",
      "Batch loss: 0.29880866408348083 batch: 208/224\n",
      "Batch loss: 0.3212292790412903 batch: 209/224\n",
      "Batch loss: 0.32354822754859924 batch: 210/224\n",
      "Batch loss: 0.29531341791152954 batch: 211/224\n",
      "Batch loss: 0.3627551794052124 batch: 212/224\n",
      "Batch loss: 0.33312350511550903 batch: 213/224\n",
      "Batch loss: 0.3589436709880829 batch: 214/224\n",
      "Batch loss: 0.3393471837043762 batch: 215/224\n",
      "Batch loss: 0.2870270609855652 batch: 216/224\n",
      "Batch loss: 0.3329697549343109 batch: 217/224\n",
      "Batch loss: 0.324567586183548 batch: 218/224\n",
      "Batch loss: 0.299137681722641 batch: 219/224\n",
      "Batch loss: 0.27408310770988464 batch: 220/224\n",
      "Batch loss: 0.34840118885040283 batch: 221/224\n",
      "Batch loss: 0.3244827389717102 batch: 222/224\n",
      "Batch loss: 0.30127188563346863 batch: 223/224\n",
      "Batch loss: 0.292007178068161 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 10/75..  Training Loss: 0.00066..  Test Loss: 0.00069..  Test Accuracy: 0.87650\n",
      "Running epoch 11/75\n",
      "Batch loss: 0.2803966999053955 batch: 1/224\n",
      "Batch loss: 0.3143322765827179 batch: 2/224\n",
      "Batch loss: 0.3006622791290283 batch: 3/224\n",
      "Batch loss: 0.34564122557640076 batch: 4/224\n",
      "Batch loss: 0.3397132456302643 batch: 5/224\n",
      "Batch loss: 0.3836602568626404 batch: 6/224\n",
      "Batch loss: 0.30703413486480713 batch: 7/224\n",
      "Batch loss: 0.3435552418231964 batch: 8/224\n",
      "Batch loss: 0.26315462589263916 batch: 9/224\n",
      "Batch loss: 0.32164666056632996 batch: 10/224\n",
      "Batch loss: 0.3710556924343109 batch: 11/224\n",
      "Batch loss: 0.3298509418964386 batch: 12/224\n",
      "Batch loss: 0.2580359876155853 batch: 13/224\n",
      "Batch loss: 0.29382258653640747 batch: 14/224\n",
      "Batch loss: 0.2942800223827362 batch: 15/224\n",
      "Batch loss: 0.39120081067085266 batch: 16/224\n",
      "Batch loss: 0.31170347332954407 batch: 17/224\n",
      "Batch loss: 0.37422508001327515 batch: 18/224\n",
      "Batch loss: 0.2827966809272766 batch: 19/224\n",
      "Batch loss: 0.2927687168121338 batch: 20/224\n",
      "Batch loss: 0.362763911485672 batch: 21/224\n",
      "Batch loss: 0.30549338459968567 batch: 22/224\n",
      "Batch loss: 0.3850405216217041 batch: 23/224\n",
      "Batch loss: 0.35147127509117126 batch: 24/224\n",
      "Batch loss: 0.2703458070755005 batch: 25/224\n",
      "Batch loss: 0.2757551670074463 batch: 26/224\n",
      "Batch loss: 0.3140701353549957 batch: 27/224\n",
      "Batch loss: 0.31555119156837463 batch: 28/224\n",
      "Batch loss: 0.32175832986831665 batch: 29/224\n",
      "Batch loss: 0.31089144945144653 batch: 30/224\n",
      "Batch loss: 0.27928149700164795 batch: 31/224\n",
      "Batch loss: 0.3844347894191742 batch: 32/224\n",
      "Batch loss: 0.3046848773956299 batch: 33/224\n",
      "Batch loss: 0.3084523677825928 batch: 34/224\n",
      "Batch loss: 0.3030742406845093 batch: 35/224\n",
      "Batch loss: 0.34026622772216797 batch: 36/224\n",
      "Batch loss: 0.30049797892570496 batch: 37/224\n",
      "Batch loss: 0.3234926760196686 batch: 38/224\n",
      "Batch loss: 0.30410030484199524 batch: 39/224\n",
      "Batch loss: 0.30959028005599976 batch: 40/224\n",
      "Batch loss: 0.3850337862968445 batch: 41/224\n",
      "Batch loss: 0.3257170021533966 batch: 42/224\n",
      "Batch loss: 0.33781367540359497 batch: 43/224\n",
      "Batch loss: 0.2637385427951813 batch: 44/224\n",
      "Batch loss: 0.2983148396015167 batch: 45/224\n",
      "Batch loss: 0.3727198839187622 batch: 46/224\n",
      "Batch loss: 0.3285059630870819 batch: 47/224\n",
      "Batch loss: 0.28902095556259155 batch: 48/224\n",
      "Batch loss: 0.26700419187545776 batch: 49/224\n",
      "Batch loss: 0.32693785429000854 batch: 50/224\n",
      "Batch loss: 0.32758310437202454 batch: 51/224\n",
      "Batch loss: 0.2919740676879883 batch: 52/224\n",
      "Batch loss: 0.3638499677181244 batch: 53/224\n",
      "Batch loss: 0.3196053206920624 batch: 54/224\n",
      "Batch loss: 0.2997884750366211 batch: 55/224\n",
      "Batch loss: 0.301700621843338 batch: 56/224\n",
      "Batch loss: 0.38554590940475464 batch: 57/224\n",
      "Batch loss: 0.3106471002101898 batch: 58/224\n",
      "Batch loss: 0.30206555128097534 batch: 59/224\n",
      "Batch loss: 0.3946259021759033 batch: 60/224\n",
      "Batch loss: 0.3111764192581177 batch: 61/224\n",
      "Batch loss: 0.2827781140804291 batch: 62/224\n",
      "Batch loss: 0.30207791924476624 batch: 63/224\n",
      "Batch loss: 0.36704662442207336 batch: 64/224\n",
      "Batch loss: 0.3304067552089691 batch: 65/224\n",
      "Batch loss: 0.331166535615921 batch: 66/224\n",
      "Batch loss: 0.29425784945487976 batch: 67/224\n",
      "Batch loss: 0.33895084261894226 batch: 68/224\n",
      "Batch loss: 0.3625759482383728 batch: 69/224\n",
      "Batch loss: 0.33145537972450256 batch: 70/224\n",
      "Batch loss: 0.2881007194519043 batch: 71/224\n",
      "Batch loss: 0.2108118087053299 batch: 72/224\n",
      "Batch loss: 0.3655988276004791 batch: 73/224\n",
      "Batch loss: 0.30428966879844666 batch: 74/224\n",
      "Batch loss: 0.3351796269416809 batch: 75/224\n",
      "Batch loss: 0.3518192172050476 batch: 76/224\n",
      "Batch loss: 0.2998904287815094 batch: 77/224\n",
      "Batch loss: 0.3139905035495758 batch: 78/224\n",
      "Batch loss: 0.3571154773235321 batch: 79/224\n",
      "Batch loss: 0.32033979892730713 batch: 80/224\n",
      "Batch loss: 0.4142090380191803 batch: 81/224\n",
      "Batch loss: 0.34163549542427063 batch: 82/224\n",
      "Batch loss: 0.35376375913619995 batch: 83/224\n",
      "Batch loss: 0.28207817673683167 batch: 84/224\n",
      "Batch loss: 0.3200484812259674 batch: 85/224\n",
      "Batch loss: 0.3181993365287781 batch: 86/224\n",
      "Batch loss: 0.3239288926124573 batch: 87/224\n",
      "Batch loss: 0.34065359830856323 batch: 88/224\n",
      "Batch loss: 0.3130735754966736 batch: 89/224\n",
      "Batch loss: 0.3784359097480774 batch: 90/224\n",
      "Batch loss: 0.31352707743644714 batch: 91/224\n",
      "Batch loss: 0.32198044657707214 batch: 92/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.2956802546977997 batch: 93/224\n",
      "Batch loss: 0.35647261142730713 batch: 94/224\n",
      "Batch loss: 0.29006943106651306 batch: 95/224\n",
      "Batch loss: 0.2781488001346588 batch: 96/224\n",
      "Batch loss: 0.31332021951675415 batch: 97/224\n",
      "Batch loss: 0.26318687200546265 batch: 98/224\n",
      "Batch loss: 0.3419337868690491 batch: 99/224\n",
      "Batch loss: 0.2768629193305969 batch: 100/224\n",
      "Batch loss: 0.357038676738739 batch: 101/224\n",
      "Batch loss: 0.32646292448043823 batch: 102/224\n",
      "Batch loss: 0.34421756863594055 batch: 103/224\n",
      "Batch loss: 0.30341726541519165 batch: 104/224\n",
      "Batch loss: 0.265485554933548 batch: 105/224\n",
      "Batch loss: 0.32504263520240784 batch: 106/224\n",
      "Batch loss: 0.28395316004753113 batch: 107/224\n",
      "Batch loss: 0.350388765335083 batch: 108/224\n",
      "Batch loss: 0.2978094518184662 batch: 109/224\n",
      "Batch loss: 0.29770880937576294 batch: 110/224\n",
      "Batch loss: 0.33740857243537903 batch: 111/224\n",
      "Batch loss: 0.3041297495365143 batch: 112/224\n",
      "Batch loss: 0.3574393093585968 batch: 113/224\n",
      "Batch loss: 0.288038432598114 batch: 114/224\n",
      "Batch loss: 0.29761090874671936 batch: 115/224\n",
      "Batch loss: 0.29027995467185974 batch: 116/224\n",
      "Batch loss: 0.27636510133743286 batch: 117/224\n",
      "Batch loss: 0.3489041328430176 batch: 118/224\n",
      "Batch loss: 0.32235342264175415 batch: 119/224\n",
      "Batch loss: 0.3216628134250641 batch: 120/224\n",
      "Batch loss: 0.28070035576820374 batch: 121/224\n",
      "Batch loss: 0.30692625045776367 batch: 122/224\n",
      "Batch loss: 0.29660168290138245 batch: 123/224\n",
      "Batch loss: 0.301667183637619 batch: 124/224\n",
      "Batch loss: 0.3267020285129547 batch: 125/224\n",
      "Batch loss: 0.36176666617393494 batch: 126/224\n",
      "Batch loss: 0.34116509556770325 batch: 127/224\n",
      "Batch loss: 0.3082004189491272 batch: 128/224\n",
      "Batch loss: 0.32239994406700134 batch: 129/224\n",
      "Batch loss: 0.29879212379455566 batch: 130/224\n",
      "Batch loss: 0.2584099769592285 batch: 131/224\n",
      "Batch loss: 0.28452247381210327 batch: 132/224\n",
      "Batch loss: 0.297745943069458 batch: 133/224\n",
      "Batch loss: 0.2820683717727661 batch: 134/224\n",
      "Batch loss: 0.4161362051963806 batch: 135/224\n",
      "Batch loss: 0.34239622950553894 batch: 136/224\n",
      "Batch loss: 0.28952670097351074 batch: 137/224\n",
      "Batch loss: 0.3024776577949524 batch: 138/224\n",
      "Batch loss: 0.35151758790016174 batch: 139/224\n",
      "Batch loss: 0.39267924427986145 batch: 140/224\n",
      "Batch loss: 0.25135698914527893 batch: 141/224\n",
      "Batch loss: 0.2995718717575073 batch: 142/224\n",
      "Batch loss: 0.28226935863494873 batch: 143/224\n",
      "Batch loss: 0.30216291546821594 batch: 144/224\n",
      "Batch loss: 0.3063712418079376 batch: 145/224\n",
      "Batch loss: 0.3409710228443146 batch: 146/224\n",
      "Batch loss: 0.3087276518344879 batch: 147/224\n",
      "Batch loss: 0.29011788964271545 batch: 148/224\n",
      "Batch loss: 0.324960857629776 batch: 149/224\n",
      "Batch loss: 0.32395467162132263 batch: 150/224\n",
      "Batch loss: 0.3248838186264038 batch: 151/224\n",
      "Batch loss: 0.2730761170387268 batch: 152/224\n",
      "Batch loss: 0.34817731380462646 batch: 153/224\n",
      "Batch loss: 0.3617759048938751 batch: 154/224\n",
      "Batch loss: 0.28546008467674255 batch: 155/224\n",
      "Batch loss: 0.29409047961235046 batch: 156/224\n",
      "Batch loss: 0.34843942523002625 batch: 157/224\n",
      "Batch loss: 0.36352768540382385 batch: 158/224\n",
      "Batch loss: 0.28194230794906616 batch: 159/224\n",
      "Batch loss: 0.3086668848991394 batch: 160/224\n",
      "Batch loss: 0.2890540063381195 batch: 161/224\n",
      "Batch loss: 0.3004298210144043 batch: 162/224\n",
      "Batch loss: 0.3019351363182068 batch: 163/224\n",
      "Batch loss: 0.29761508107185364 batch: 164/224\n",
      "Batch loss: 0.3579697608947754 batch: 165/224\n",
      "Batch loss: 0.29955998063087463 batch: 166/224\n",
      "Batch loss: 0.3024774193763733 batch: 167/224\n",
      "Batch loss: 0.25053319334983826 batch: 168/224\n",
      "Batch loss: 0.2940944731235504 batch: 169/224\n",
      "Batch loss: 0.2879989743232727 batch: 170/224\n",
      "Batch loss: 0.2851335406303406 batch: 171/224\n",
      "Batch loss: 0.3078056275844574 batch: 172/224\n",
      "Batch loss: 0.2938842177391052 batch: 173/224\n",
      "Batch loss: 0.30393826961517334 batch: 174/224\n",
      "Batch loss: 0.29909297823905945 batch: 175/224\n",
      "Batch loss: 0.3175179958343506 batch: 176/224\n",
      "Batch loss: 0.33457818627357483 batch: 177/224\n",
      "Batch loss: 0.29062604904174805 batch: 178/224\n",
      "Batch loss: 0.3181234300136566 batch: 179/224\n",
      "Batch loss: 0.2688418924808502 batch: 180/224\n",
      "Batch loss: 0.35510504245758057 batch: 181/224\n",
      "Batch loss: 0.3197759985923767 batch: 182/224\n",
      "Batch loss: 0.3594985902309418 batch: 183/224\n",
      "Batch loss: 0.2746439278125763 batch: 184/224\n",
      "Batch loss: 0.39622652530670166 batch: 185/224\n",
      "Batch loss: 0.3151477873325348 batch: 186/224\n",
      "Batch loss: 0.31304019689559937 batch: 187/224\n",
      "Batch loss: 0.24077796936035156 batch: 188/224\n",
      "Batch loss: 0.34908533096313477 batch: 189/224\n",
      "Batch loss: 0.30656352639198303 batch: 190/224\n",
      "Batch loss: 0.3283288776874542 batch: 191/224\n",
      "Batch loss: 0.35909804701805115 batch: 192/224\n",
      "Batch loss: 0.34400567412376404 batch: 193/224\n",
      "Batch loss: 0.333364874124527 batch: 194/224\n",
      "Batch loss: 0.3325015604496002 batch: 195/224\n",
      "Batch loss: 0.3425767719745636 batch: 196/224\n",
      "Batch loss: 0.2807500958442688 batch: 197/224\n",
      "Batch loss: 0.3279409110546112 batch: 198/224\n",
      "Batch loss: 0.3098689913749695 batch: 199/224\n",
      "Batch loss: 0.3254091143608093 batch: 200/224\n",
      "Batch loss: 0.31044113636016846 batch: 201/224\n",
      "Batch loss: 0.35499250888824463 batch: 202/224\n",
      "Batch loss: 0.32384148240089417 batch: 203/224\n",
      "Batch loss: 0.3068719506263733 batch: 204/224\n",
      "Batch loss: 0.3443123400211334 batch: 205/224\n",
      "Batch loss: 0.30706819891929626 batch: 206/224\n",
      "Batch loss: 0.3090989887714386 batch: 207/224\n",
      "Batch loss: 0.24982526898384094 batch: 208/224\n",
      "Batch loss: 0.32000935077667236 batch: 209/224\n",
      "Batch loss: 0.2770809233188629 batch: 210/224\n",
      "Batch loss: 0.2932489812374115 batch: 211/224\n",
      "Batch loss: 0.34017693996429443 batch: 212/224\n",
      "Batch loss: 0.32681408524513245 batch: 213/224\n",
      "Batch loss: 0.344368040561676 batch: 214/224\n",
      "Batch loss: 0.3332557678222656 batch: 215/224\n",
      "Batch loss: 0.3064756989479065 batch: 216/224\n",
      "Batch loss: 0.3389141857624054 batch: 217/224\n",
      "Batch loss: 0.28929781913757324 batch: 218/224\n",
      "Batch loss: 0.2480475902557373 batch: 219/224\n",
      "Batch loss: 0.25968223810195923 batch: 220/224\n",
      "Batch loss: 0.3776426315307617 batch: 221/224\n",
      "Batch loss: 0.2994178831577301 batch: 222/224\n",
      "Batch loss: 0.2884228825569153 batch: 223/224\n",
      "Batch loss: 0.2605533003807068 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 11/75..  Training Loss: 0.00063..  Test Loss: 0.00067..  Test Accuracy: 0.87679\n",
      "Running epoch 12/75\n",
      "Batch loss: 0.25711676478385925 batch: 1/224\n",
      "Batch loss: 0.3178633153438568 batch: 2/224\n",
      "Batch loss: 0.27733179926872253 batch: 3/224\n",
      "Batch loss: 0.3246738910675049 batch: 4/224\n",
      "Batch loss: 0.30149537324905396 batch: 5/224\n",
      "Batch loss: 0.4073762595653534 batch: 6/224\n",
      "Batch loss: 0.2816343605518341 batch: 7/224\n",
      "Batch loss: 0.32654714584350586 batch: 8/224\n",
      "Batch loss: 0.27098244428634644 batch: 9/224\n",
      "Batch loss: 0.28081902861595154 batch: 10/224\n",
      "Batch loss: 0.35414576530456543 batch: 11/224\n",
      "Batch loss: 0.30628177523612976 batch: 12/224\n",
      "Batch loss: 0.25782573223114014 batch: 13/224\n",
      "Batch loss: 0.2585885226726532 batch: 14/224\n",
      "Batch loss: 0.3134930729866028 batch: 15/224\n",
      "Batch loss: 0.41902586817741394 batch: 16/224\n",
      "Batch loss: 0.3138625919818878 batch: 17/224\n",
      "Batch loss: 0.3430159389972687 batch: 18/224\n",
      "Batch loss: 0.2655194401741028 batch: 19/224\n",
      "Batch loss: 0.27002543210983276 batch: 20/224\n",
      "Batch loss: 0.33632901310920715 batch: 21/224\n",
      "Batch loss: 0.2840307950973511 batch: 22/224\n",
      "Batch loss: 0.35004761815071106 batch: 23/224\n",
      "Batch loss: 0.3418615162372589 batch: 24/224\n",
      "Batch loss: 0.23769961297512054 batch: 25/224\n",
      "Batch loss: 0.243048757314682 batch: 26/224\n",
      "Batch loss: 0.3111710846424103 batch: 27/224\n",
      "Batch loss: 0.3114365339279175 batch: 28/224\n",
      "Batch loss: 0.334525465965271 batch: 29/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.29073140025138855 batch: 30/224\n",
      "Batch loss: 0.2909201979637146 batch: 31/224\n",
      "Batch loss: 0.34678149223327637 batch: 32/224\n",
      "Batch loss: 0.29541730880737305 batch: 33/224\n",
      "Batch loss: 0.30485662817955017 batch: 34/224\n",
      "Batch loss: 0.30898940563201904 batch: 35/224\n",
      "Batch loss: 0.3323677182197571 batch: 36/224\n",
      "Batch loss: 0.2930701971054077 batch: 37/224\n",
      "Batch loss: 0.2851681113243103 batch: 38/224\n",
      "Batch loss: 0.28728529810905457 batch: 39/224\n",
      "Batch loss: 0.3214578926563263 batch: 40/224\n",
      "Batch loss: 0.3364712595939636 batch: 41/224\n",
      "Batch loss: 0.3171972632408142 batch: 42/224\n",
      "Batch loss: 0.34606117010116577 batch: 43/224\n",
      "Batch loss: 0.26837921142578125 batch: 44/224\n",
      "Batch loss: 0.2796383798122406 batch: 45/224\n",
      "Batch loss: 0.37527501583099365 batch: 46/224\n",
      "Batch loss: 0.3368837535381317 batch: 47/224\n",
      "Batch loss: 0.29929396510124207 batch: 48/224\n",
      "Batch loss: 0.2662300765514374 batch: 49/224\n",
      "Batch loss: 0.30237776041030884 batch: 50/224\n",
      "Batch loss: 0.28140491247177124 batch: 51/224\n",
      "Batch loss: 0.27945077419281006 batch: 52/224\n",
      "Batch loss: 0.346041202545166 batch: 53/224\n",
      "Batch loss: 0.2614734470844269 batch: 54/224\n",
      "Batch loss: 0.280392587184906 batch: 55/224\n",
      "Batch loss: 0.27633652091026306 batch: 56/224\n",
      "Batch loss: 0.3594021499156952 batch: 57/224\n",
      "Batch loss: 0.2930138111114502 batch: 58/224\n",
      "Batch loss: 0.3174605965614319 batch: 59/224\n",
      "Batch loss: 0.3495512902736664 batch: 60/224\n",
      "Batch loss: 0.32092317938804626 batch: 61/224\n",
      "Batch loss: 0.3114142119884491 batch: 62/224\n",
      "Batch loss: 0.29116910696029663 batch: 63/224\n",
      "Batch loss: 0.34020110964775085 batch: 64/224\n",
      "Batch loss: 0.2887658178806305 batch: 65/224\n",
      "Batch loss: 0.336116224527359 batch: 66/224\n",
      "Batch loss: 0.31567803025245667 batch: 67/224\n",
      "Batch loss: 0.32458025217056274 batch: 68/224\n",
      "Batch loss: 0.3114190399646759 batch: 69/224\n",
      "Batch loss: 0.30826082825660706 batch: 70/224\n",
      "Batch loss: 0.3197638988494873 batch: 71/224\n",
      "Batch loss: 0.24280540645122528 batch: 72/224\n",
      "Batch loss: 0.3549041748046875 batch: 73/224\n",
      "Batch loss: 0.30924975872039795 batch: 74/224\n",
      "Batch loss: 0.32431551814079285 batch: 75/224\n",
      "Batch loss: 0.35352012515068054 batch: 76/224\n",
      "Batch loss: 0.26653313636779785 batch: 77/224\n",
      "Batch loss: 0.30833330750465393 batch: 78/224\n",
      "Batch loss: 0.31814783811569214 batch: 79/224\n",
      "Batch loss: 0.3247234523296356 batch: 80/224\n",
      "Batch loss: 0.4315060079097748 batch: 81/224\n",
      "Batch loss: 0.34981492161750793 batch: 82/224\n",
      "Batch loss: 0.293675035238266 batch: 83/224\n",
      "Batch loss: 0.26931315660476685 batch: 84/224\n",
      "Batch loss: 0.3134613037109375 batch: 85/224\n",
      "Batch loss: 0.31523397564888 batch: 86/224\n",
      "Batch loss: 0.32315322756767273 batch: 87/224\n",
      "Batch loss: 0.31992024183273315 batch: 88/224\n",
      "Batch loss: 0.30149486660957336 batch: 89/224\n",
      "Batch loss: 0.34952008724212646 batch: 90/224\n",
      "Batch loss: 0.28914669156074524 batch: 91/224\n",
      "Batch loss: 0.32123997807502747 batch: 92/224\n",
      "Batch loss: 0.25239700078964233 batch: 93/224\n",
      "Batch loss: 0.33207300305366516 batch: 94/224\n",
      "Batch loss: 0.27795374393463135 batch: 95/224\n",
      "Batch loss: 0.2696774899959564 batch: 96/224\n",
      "Batch loss: 0.27007442712783813 batch: 97/224\n",
      "Batch loss: 0.24652522802352905 batch: 98/224\n",
      "Batch loss: 0.3551494777202606 batch: 99/224\n",
      "Batch loss: 0.30406439304351807 batch: 100/224\n",
      "Batch loss: 0.32087427377700806 batch: 101/224\n",
      "Batch loss: 0.30094993114471436 batch: 102/224\n",
      "Batch loss: 0.2958070635795593 batch: 103/224\n",
      "Batch loss: 0.28146830201148987 batch: 104/224\n",
      "Batch loss: 0.27023938298225403 batch: 105/224\n",
      "Batch loss: 0.27871036529541016 batch: 106/224\n",
      "Batch loss: 0.2668329179286957 batch: 107/224\n",
      "Batch loss: 0.28615880012512207 batch: 108/224\n",
      "Batch loss: 0.2823026776313782 batch: 109/224\n",
      "Batch loss: 0.29634302854537964 batch: 110/224\n",
      "Batch loss: 0.32715705037117004 batch: 111/224\n",
      "Batch loss: 0.2843136191368103 batch: 112/224\n",
      "Batch loss: 0.33131781220436096 batch: 113/224\n",
      "Batch loss: 0.2838224470615387 batch: 114/224\n",
      "Batch loss: 0.2954587936401367 batch: 115/224\n",
      "Batch loss: 0.2749439477920532 batch: 116/224\n",
      "Batch loss: 0.3032976984977722 batch: 117/224\n",
      "Batch loss: 0.33777329325675964 batch: 118/224\n",
      "Batch loss: 0.28604790568351746 batch: 119/224\n",
      "Batch loss: 0.32257089018821716 batch: 120/224\n",
      "Batch loss: 0.30469343066215515 batch: 121/224\n",
      "Batch loss: 0.3133954107761383 batch: 122/224\n",
      "Batch loss: 0.2664502263069153 batch: 123/224\n",
      "Batch loss: 0.3113217055797577 batch: 124/224\n",
      "Batch loss: 0.28260737657546997 batch: 125/224\n",
      "Batch loss: 0.280225932598114 batch: 126/224\n",
      "Batch loss: 0.3288610875606537 batch: 127/224\n",
      "Batch loss: 0.2834368050098419 batch: 128/224\n",
      "Batch loss: 0.32654502987861633 batch: 129/224\n",
      "Batch loss: 0.32123979926109314 batch: 130/224\n",
      "Batch loss: 0.2625117599964142 batch: 131/224\n",
      "Batch loss: 0.28554829955101013 batch: 132/224\n",
      "Batch loss: 0.3065659999847412 batch: 133/224\n",
      "Batch loss: 0.27558910846710205 batch: 134/224\n",
      "Batch loss: 0.3405910134315491 batch: 135/224\n",
      "Batch loss: 0.3482556939125061 batch: 136/224\n",
      "Batch loss: 0.27698034048080444 batch: 137/224\n",
      "Batch loss: 0.3418196141719818 batch: 138/224\n",
      "Batch loss: 0.3334224820137024 batch: 139/224\n",
      "Batch loss: 0.35744982957839966 batch: 140/224\n",
      "Batch loss: 0.22692245244979858 batch: 141/224\n",
      "Batch loss: 0.30124056339263916 batch: 142/224\n",
      "Batch loss: 0.29830119013786316 batch: 143/224\n",
      "Batch loss: 0.34233227372169495 batch: 144/224\n",
      "Batch loss: 0.26244696974754333 batch: 145/224\n",
      "Batch loss: 0.3066997826099396 batch: 146/224\n",
      "Batch loss: 0.27282679080963135 batch: 147/224\n",
      "Batch loss: 0.2828187346458435 batch: 148/224\n",
      "Batch loss: 0.31021547317504883 batch: 149/224\n",
      "Batch loss: 0.3456052243709564 batch: 150/224\n",
      "Batch loss: 0.32739487290382385 batch: 151/224\n",
      "Batch loss: 0.2696634531021118 batch: 152/224\n",
      "Batch loss: 0.32392770051956177 batch: 153/224\n",
      "Batch loss: 0.37322792410850525 batch: 154/224\n",
      "Batch loss: 0.28183019161224365 batch: 155/224\n",
      "Batch loss: 0.29886895418167114 batch: 156/224\n",
      "Batch loss: 0.32394787669181824 batch: 157/224\n",
      "Batch loss: 0.38184404373168945 batch: 158/224\n",
      "Batch loss: 0.257489413022995 batch: 159/224\n",
      "Batch loss: 0.33785364031791687 batch: 160/224\n",
      "Batch loss: 0.2487093061208725 batch: 161/224\n",
      "Batch loss: 0.2759259045124054 batch: 162/224\n",
      "Batch loss: 0.2679501175880432 batch: 163/224\n",
      "Batch loss: 0.3058294355869293 batch: 164/224\n",
      "Batch loss: 0.35025379061698914 batch: 165/224\n",
      "Batch loss: 0.3075278103351593 batch: 166/224\n",
      "Batch loss: 0.26344457268714905 batch: 167/224\n",
      "Batch loss: 0.23520608246326447 batch: 168/224\n",
      "Batch loss: 0.29560524225234985 batch: 169/224\n",
      "Batch loss: 0.2977890074253082 batch: 170/224\n",
      "Batch loss: 0.2786422669887543 batch: 171/224\n",
      "Batch loss: 0.2843276858329773 batch: 172/224\n",
      "Batch loss: 0.30073708295822144 batch: 173/224\n",
      "Batch loss: 0.2949705421924591 batch: 174/224\n",
      "Batch loss: 0.2772797644138336 batch: 175/224\n",
      "Batch loss: 0.28938028216362 batch: 176/224\n",
      "Batch loss: 0.35254719853401184 batch: 177/224\n",
      "Batch loss: 0.2711836099624634 batch: 178/224\n",
      "Batch loss: 0.32922831177711487 batch: 179/224\n",
      "Batch loss: 0.24160096049308777 batch: 180/224\n",
      "Batch loss: 0.3432103991508484 batch: 181/224\n",
      "Batch loss: 0.3009083867073059 batch: 182/224\n",
      "Batch loss: 0.37897101044654846 batch: 183/224\n",
      "Batch loss: 0.3073805868625641 batch: 184/224\n",
      "Batch loss: 0.3549439013004303 batch: 185/224\n",
      "Batch loss: 0.2745162546634674 batch: 186/224\n",
      "Batch loss: 0.2946903109550476 batch: 187/224\n",
      "Batch loss: 0.21085385978221893 batch: 188/224\n",
      "Batch loss: 0.31415602564811707 batch: 189/224\n",
      "Batch loss: 0.3233799934387207 batch: 190/224\n",
      "Batch loss: 0.29527661204338074 batch: 191/224\n",
      "Batch loss: 0.34419235587120056 batch: 192/224\n",
      "Batch loss: 0.30898186564445496 batch: 193/224\n",
      "Batch loss: 0.29779499769210815 batch: 194/224\n",
      "Batch loss: 0.36540883779525757 batch: 195/224\n",
      "Batch loss: 0.3506413996219635 batch: 196/224\n",
      "Batch loss: 0.30507901310920715 batch: 197/224\n",
      "Batch loss: 0.31205499172210693 batch: 198/224\n",
      "Batch loss: 0.27224767208099365 batch: 199/224\n",
      "Batch loss: 0.35564887523651123 batch: 200/224\n",
      "Batch loss: 0.35757318139076233 batch: 201/224\n",
      "Batch loss: 0.3152538537979126 batch: 202/224\n",
      "Batch loss: 0.34340575337409973 batch: 203/224\n",
      "Batch loss: 0.2680954933166504 batch: 204/224\n",
      "Batch loss: 0.3032887876033783 batch: 205/224\n",
      "Batch loss: 0.3145957589149475 batch: 206/224\n",
      "Batch loss: 0.335721492767334 batch: 207/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.2852320671081543 batch: 208/224\n",
      "Batch loss: 0.2842003405094147 batch: 209/224\n",
      "Batch loss: 0.2936710715293884 batch: 210/224\n",
      "Batch loss: 0.266394704580307 batch: 211/224\n",
      "Batch loss: 0.302903950214386 batch: 212/224\n",
      "Batch loss: 0.3240087926387787 batch: 213/224\n",
      "Batch loss: 0.36661508679389954 batch: 214/224\n",
      "Batch loss: 0.31586650013923645 batch: 215/224\n",
      "Batch loss: 0.29888808727264404 batch: 216/224\n",
      "Batch loss: 0.3215794861316681 batch: 217/224\n",
      "Batch loss: 0.30473044514656067 batch: 218/224\n",
      "Batch loss: 0.291775643825531 batch: 219/224\n",
      "Batch loss: 0.24675136804580688 batch: 220/224\n",
      "Batch loss: 0.3474564850330353 batch: 221/224\n",
      "Batch loss: 0.2930055260658264 batch: 222/224\n",
      "Batch loss: 0.2548026740550995 batch: 223/224\n",
      "Batch loss: 0.27138185501098633 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 12/75..  Training Loss: 0.00061..  Test Loss: 0.00068..  Test Accuracy: 0.87729\n",
      "Running epoch 13/75\n",
      "Batch loss: 0.2698740065097809 batch: 1/224\n",
      "Batch loss: 0.29053375124931335 batch: 2/224\n",
      "Batch loss: 0.30233287811279297 batch: 3/224\n",
      "Batch loss: 0.3319794833660126 batch: 4/224\n",
      "Batch loss: 0.2953437268733978 batch: 5/224\n",
      "Batch loss: 0.3695962429046631 batch: 6/224\n",
      "Batch loss: 0.2925258278846741 batch: 7/224\n",
      "Batch loss: 0.3377971351146698 batch: 8/224\n",
      "Batch loss: 0.2558949589729309 batch: 9/224\n",
      "Batch loss: 0.2926885187625885 batch: 10/224\n",
      "Batch loss: 0.34378933906555176 batch: 11/224\n",
      "Batch loss: 0.3087073266506195 batch: 12/224\n",
      "Batch loss: 0.2500417232513428 batch: 13/224\n",
      "Batch loss: 0.289124459028244 batch: 14/224\n",
      "Batch loss: 0.28106001019477844 batch: 15/224\n",
      "Batch loss: 0.3858940005302429 batch: 16/224\n",
      "Batch loss: 0.2979966104030609 batch: 17/224\n",
      "Batch loss: 0.3556864857673645 batch: 18/224\n",
      "Batch loss: 0.2879522442817688 batch: 19/224\n",
      "Batch loss: 0.29856202006340027 batch: 20/224\n",
      "Batch loss: 0.3303338885307312 batch: 21/224\n",
      "Batch loss: 0.2956196963787079 batch: 22/224\n",
      "Batch loss: 0.3182135224342346 batch: 23/224\n",
      "Batch loss: 0.3561532497406006 batch: 24/224\n",
      "Batch loss: 0.27258822321891785 batch: 25/224\n",
      "Batch loss: 0.23886479437351227 batch: 26/224\n",
      "Batch loss: 0.28754836320877075 batch: 27/224\n",
      "Batch loss: 0.2716791331768036 batch: 28/224\n",
      "Batch loss: 0.3327035903930664 batch: 29/224\n",
      "Batch loss: 0.2837112247943878 batch: 30/224\n",
      "Batch loss: 0.2677338123321533 batch: 31/224\n",
      "Batch loss: 0.34722205996513367 batch: 32/224\n",
      "Batch loss: 0.274687260389328 batch: 33/224\n",
      "Batch loss: 0.2680247128009796 batch: 34/224\n",
      "Batch loss: 0.2940007448196411 batch: 35/224\n",
      "Batch loss: 0.33350059390068054 batch: 36/224\n",
      "Batch loss: 0.3065536618232727 batch: 37/224\n",
      "Batch loss: 0.31010621786117554 batch: 38/224\n",
      "Batch loss: 0.32513687014579773 batch: 39/224\n",
      "Batch loss: 0.2844603657722473 batch: 40/224\n",
      "Batch loss: 0.3157755434513092 batch: 41/224\n",
      "Batch loss: 0.3182442784309387 batch: 42/224\n",
      "Batch loss: 0.3239286243915558 batch: 43/224\n",
      "Batch loss: 0.24471142888069153 batch: 44/224\n",
      "Batch loss: 0.24482761323451996 batch: 45/224\n",
      "Batch loss: 0.3550432622432709 batch: 46/224\n",
      "Batch loss: 0.3092687129974365 batch: 47/224\n",
      "Batch loss: 0.3123435080051422 batch: 48/224\n",
      "Batch loss: 0.25525304675102234 batch: 49/224\n",
      "Batch loss: 0.28021007776260376 batch: 50/224\n",
      "Batch loss: 0.2740144729614258 batch: 51/224\n",
      "Batch loss: 0.26415368914604187 batch: 52/224\n",
      "Batch loss: 0.3468186855316162 batch: 53/224\n",
      "Batch loss: 0.2700505554676056 batch: 54/224\n",
      "Batch loss: 0.3002636730670929 batch: 55/224\n",
      "Batch loss: 0.2771424651145935 batch: 56/224\n",
      "Batch loss: 0.33554884791374207 batch: 57/224\n",
      "Batch loss: 0.32594814896583557 batch: 58/224\n",
      "Batch loss: 0.2777082920074463 batch: 59/224\n",
      "Batch loss: 0.3503786325454712 batch: 60/224\n",
      "Batch loss: 0.2839157283306122 batch: 61/224\n",
      "Batch loss: 0.2709346115589142 batch: 62/224\n",
      "Batch loss: 0.27701079845428467 batch: 63/224\n",
      "Batch loss: 0.2993832528591156 batch: 64/224\n",
      "Batch loss: 0.3001263439655304 batch: 65/224\n",
      "Batch loss: 0.31539565324783325 batch: 66/224\n",
      "Batch loss: 0.28769636154174805 batch: 67/224\n",
      "Batch loss: 0.2901698350906372 batch: 68/224\n",
      "Batch loss: 0.33454620838165283 batch: 69/224\n",
      "Batch loss: 0.26872169971466064 batch: 70/224\n",
      "Batch loss: 0.26057058572769165 batch: 71/224\n",
      "Batch loss: 0.1994272917509079 batch: 72/224\n",
      "Batch loss: 0.3257347345352173 batch: 73/224\n",
      "Batch loss: 0.3206416368484497 batch: 74/224\n",
      "Batch loss: 0.288947194814682 batch: 75/224\n",
      "Batch loss: 0.3461998403072357 batch: 76/224\n",
      "Batch loss: 0.30377471446990967 batch: 77/224\n",
      "Batch loss: 0.32550910115242004 batch: 78/224\n",
      "Batch loss: 0.2928708791732788 batch: 79/224\n",
      "Batch loss: 0.2942996919155121 batch: 80/224\n",
      "Batch loss: 0.36625468730926514 batch: 81/224\n",
      "Batch loss: 0.349607914686203 batch: 82/224\n",
      "Batch loss: 0.3155302405357361 batch: 83/224\n",
      "Batch loss: 0.2606939971446991 batch: 84/224\n",
      "Batch loss: 0.31449469923973083 batch: 85/224\n",
      "Batch loss: 0.2967569828033447 batch: 86/224\n",
      "Batch loss: 0.2707154452800751 batch: 87/224\n",
      "Batch loss: 0.34673458337783813 batch: 88/224\n",
      "Batch loss: 0.28435614705085754 batch: 89/224\n",
      "Batch loss: 0.35308003425598145 batch: 90/224\n",
      "Batch loss: 0.2955240309238434 batch: 91/224\n",
      "Batch loss: 0.2943297326564789 batch: 92/224\n",
      "Batch loss: 0.28102758526802063 batch: 93/224\n",
      "Batch loss: 0.31652411818504333 batch: 94/224\n",
      "Batch loss: 0.25603538751602173 batch: 95/224\n",
      "Batch loss: 0.263171911239624 batch: 96/224\n",
      "Batch loss: 0.28864893317222595 batch: 97/224\n",
      "Batch loss: 0.24896858632564545 batch: 98/224\n",
      "Batch loss: 0.3163052797317505 batch: 99/224\n",
      "Batch loss: 0.3266727924346924 batch: 100/224\n",
      "Batch loss: 0.3485582768917084 batch: 101/224\n",
      "Batch loss: 0.28693851828575134 batch: 102/224\n",
      "Batch loss: 0.30378350615501404 batch: 103/224\n",
      "Batch loss: 0.2869736850261688 batch: 104/224\n",
      "Batch loss: 0.2542628347873688 batch: 105/224\n",
      "Batch loss: 0.26917412877082825 batch: 106/224\n",
      "Batch loss: 0.26524150371551514 batch: 107/224\n",
      "Batch loss: 0.3095952570438385 batch: 108/224\n",
      "Batch loss: 0.2457781583070755 batch: 109/224\n",
      "Batch loss: 0.28378918766975403 batch: 110/224\n",
      "Batch loss: 0.30053412914276123 batch: 111/224\n",
      "Batch loss: 0.2622913420200348 batch: 112/224\n",
      "Batch loss: 0.3102424442768097 batch: 113/224\n",
      "Batch loss: 0.26501408219337463 batch: 114/224\n",
      "Batch loss: 0.29929864406585693 batch: 115/224\n",
      "Batch loss: 0.2590583860874176 batch: 116/224\n",
      "Batch loss: 0.26212167739868164 batch: 117/224\n",
      "Batch loss: 0.29374396800994873 batch: 118/224\n",
      "Batch loss: 0.2712060809135437 batch: 119/224\n",
      "Batch loss: 0.326734334230423 batch: 120/224\n",
      "Batch loss: 0.2931397557258606 batch: 121/224\n",
      "Batch loss: 0.31705960631370544 batch: 122/224\n",
      "Batch loss: 0.25222742557525635 batch: 123/224\n",
      "Batch loss: 0.28291961550712585 batch: 124/224\n",
      "Batch loss: 0.29887720942497253 batch: 125/224\n",
      "Batch loss: 0.28950148820877075 batch: 126/224\n",
      "Batch loss: 0.33301451802253723 batch: 127/224\n",
      "Batch loss: 0.29331332445144653 batch: 128/224\n",
      "Batch loss: 0.32140135765075684 batch: 129/224\n",
      "Batch loss: 0.3062600791454315 batch: 130/224\n",
      "Batch loss: 0.22742991149425507 batch: 131/224\n",
      "Batch loss: 0.25630056858062744 batch: 132/224\n",
      "Batch loss: 0.2564503848552704 batch: 133/224\n",
      "Batch loss: 0.2822621166706085 batch: 134/224\n",
      "Batch loss: 0.3182952404022217 batch: 135/224\n",
      "Batch loss: 0.36176005005836487 batch: 136/224\n",
      "Batch loss: 0.2636529207229614 batch: 137/224\n",
      "Batch loss: 0.3030991852283478 batch: 138/224\n",
      "Batch loss: 0.32710012793540955 batch: 139/224\n",
      "Batch loss: 0.35332658886909485 batch: 140/224\n",
      "Batch loss: 0.24042931199073792 batch: 141/224\n",
      "Batch loss: 0.27547577023506165 batch: 142/224\n",
      "Batch loss: 0.2799026072025299 batch: 143/224\n",
      "Batch loss: 0.2925819158554077 batch: 144/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.26534023880958557 batch: 145/224\n",
      "Batch loss: 0.30061498284339905 batch: 146/224\n",
      "Batch loss: 0.2949475646018982 batch: 147/224\n",
      "Batch loss: 0.3004094660282135 batch: 148/224\n",
      "Batch loss: 0.31097012758255005 batch: 149/224\n",
      "Batch loss: 0.32280901074409485 batch: 150/224\n",
      "Batch loss: 0.28862717747688293 batch: 151/224\n",
      "Batch loss: 0.26792433857917786 batch: 152/224\n",
      "Batch loss: 0.31050002574920654 batch: 153/224\n",
      "Batch loss: 0.343972772359848 batch: 154/224\n",
      "Batch loss: 0.28935426473617554 batch: 155/224\n",
      "Batch loss: 0.2704980969429016 batch: 156/224\n",
      "Batch loss: 0.34905946254730225 batch: 157/224\n",
      "Batch loss: 0.35696539282798767 batch: 158/224\n",
      "Batch loss: 0.2506287693977356 batch: 159/224\n",
      "Batch loss: 0.3081510365009308 batch: 160/224\n",
      "Batch loss: 0.27162328362464905 batch: 161/224\n",
      "Batch loss: 0.26594865322113037 batch: 162/224\n",
      "Batch loss: 0.2812190353870392 batch: 163/224\n",
      "Batch loss: 0.2510685324668884 batch: 164/224\n",
      "Batch loss: 0.3494689166545868 batch: 165/224\n",
      "Batch loss: 0.28849464654922485 batch: 166/224\n",
      "Batch loss: 0.23444347083568573 batch: 167/224\n",
      "Batch loss: 0.24068063497543335 batch: 168/224\n",
      "Batch loss: 0.30903899669647217 batch: 169/224\n",
      "Batch loss: 0.27395835518836975 batch: 170/224\n",
      "Batch loss: 0.2475987672805786 batch: 171/224\n",
      "Batch loss: 0.2907436192035675 batch: 172/224\n",
      "Batch loss: 0.3074658215045929 batch: 173/224\n",
      "Batch loss: 0.275154709815979 batch: 174/224\n",
      "Batch loss: 0.2621018886566162 batch: 175/224\n",
      "Batch loss: 0.2669268548488617 batch: 176/224\n",
      "Batch loss: 0.3215341567993164 batch: 177/224\n",
      "Batch loss: 0.2746942639350891 batch: 178/224\n",
      "Batch loss: 0.33296364545822144 batch: 179/224\n",
      "Batch loss: 0.23788514733314514 batch: 180/224\n",
      "Batch loss: 0.31739652156829834 batch: 181/224\n",
      "Batch loss: 0.2736228406429291 batch: 182/224\n",
      "Batch loss: 0.3454814851284027 batch: 183/224\n",
      "Batch loss: 0.2869721055030823 batch: 184/224\n",
      "Batch loss: 0.3546642065048218 batch: 185/224\n",
      "Batch loss: 0.2516460418701172 batch: 186/224\n",
      "Batch loss: 0.2598890960216522 batch: 187/224\n",
      "Batch loss: 0.22347427904605865 batch: 188/224\n",
      "Batch loss: 0.28122854232788086 batch: 189/224\n",
      "Batch loss: 0.315854012966156 batch: 190/224\n",
      "Batch loss: 0.26714953780174255 batch: 191/224\n",
      "Batch loss: 0.3398146331310272 batch: 192/224\n",
      "Batch loss: 0.2958643436431885 batch: 193/224\n",
      "Batch loss: 0.31156352162361145 batch: 194/224\n",
      "Batch loss: 0.3235824406147003 batch: 195/224\n",
      "Batch loss: 0.3270755112171173 batch: 196/224\n",
      "Batch loss: 0.29030102491378784 batch: 197/224\n",
      "Batch loss: 0.30949294567108154 batch: 198/224\n",
      "Batch loss: 0.26850876212120056 batch: 199/224\n",
      "Batch loss: 0.3234509229660034 batch: 200/224\n",
      "Batch loss: 0.319216787815094 batch: 201/224\n",
      "Batch loss: 0.29390352964401245 batch: 202/224\n",
      "Batch loss: 0.3324880301952362 batch: 203/224\n",
      "Batch loss: 0.27680113911628723 batch: 204/224\n",
      "Batch loss: 0.32938140630722046 batch: 205/224\n",
      "Batch loss: 0.27708473801612854 batch: 206/224\n",
      "Batch loss: 0.3302364945411682 batch: 207/224\n",
      "Batch loss: 0.22211037576198578 batch: 208/224\n",
      "Batch loss: 0.2767236828804016 batch: 209/224\n",
      "Batch loss: 0.2861478626728058 batch: 210/224\n",
      "Batch loss: 0.26272696256637573 batch: 211/224\n",
      "Batch loss: 0.3141968250274658 batch: 212/224\n",
      "Batch loss: 0.31426581740379333 batch: 213/224\n",
      "Batch loss: 0.3543147146701813 batch: 214/224\n",
      "Batch loss: 0.34101706743240356 batch: 215/224\n",
      "Batch loss: 0.31209614872932434 batch: 216/224\n",
      "Batch loss: 0.3301684856414795 batch: 217/224\n",
      "Batch loss: 0.30111125111579895 batch: 218/224\n",
      "Batch loss: 0.2565910518169403 batch: 219/224\n",
      "Batch loss: 0.25380513072013855 batch: 220/224\n",
      "Batch loss: 0.33922410011291504 batch: 221/224\n",
      "Batch loss: 0.2751297950744629 batch: 222/224\n",
      "Batch loss: 0.27234479784965515 batch: 223/224\n",
      "Batch loss: 0.25751110911369324 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 13/75..  Training Loss: 0.00059..  Test Loss: 0.00066..  Test Accuracy: 0.88096\n",
      "Running epoch 14/75\n",
      "Batch loss: 0.24959132075309753 batch: 1/224\n",
      "Batch loss: 0.2768685519695282 batch: 2/224\n",
      "Batch loss: 0.28220391273498535 batch: 3/224\n",
      "Batch loss: 0.3049278259277344 batch: 4/224\n",
      "Batch loss: 0.300201416015625 batch: 5/224\n",
      "Batch loss: 0.3366008400917053 batch: 6/224\n",
      "Batch loss: 0.2771437466144562 batch: 7/224\n",
      "Batch loss: 0.3253105878829956 batch: 8/224\n",
      "Batch loss: 0.23836649954319 batch: 9/224\n",
      "Batch loss: 0.2648904323577881 batch: 10/224\n",
      "Batch loss: 0.3190101683139801 batch: 11/224\n",
      "Batch loss: 0.27948200702667236 batch: 12/224\n",
      "Batch loss: 0.23935136198997498 batch: 13/224\n",
      "Batch loss: 0.2552339434623718 batch: 14/224\n",
      "Batch loss: 0.29748138785362244 batch: 15/224\n",
      "Batch loss: 0.4148784279823303 batch: 16/224\n",
      "Batch loss: 0.2788223922252655 batch: 17/224\n",
      "Batch loss: 0.3143512010574341 batch: 18/224\n",
      "Batch loss: 0.2606777548789978 batch: 19/224\n",
      "Batch loss: 0.2472943365573883 batch: 20/224\n",
      "Batch loss: 0.291408509016037 batch: 21/224\n",
      "Batch loss: 0.27977582812309265 batch: 22/224\n",
      "Batch loss: 0.28124213218688965 batch: 23/224\n",
      "Batch loss: 0.3316389322280884 batch: 24/224\n",
      "Batch loss: 0.23524390161037445 batch: 25/224\n",
      "Batch loss: 0.21585476398468018 batch: 26/224\n",
      "Batch loss: 0.2760595977306366 batch: 27/224\n",
      "Batch loss: 0.27402520179748535 batch: 28/224\n",
      "Batch loss: 0.31873705983161926 batch: 29/224\n",
      "Batch loss: 0.261743426322937 batch: 30/224\n",
      "Batch loss: 0.273240327835083 batch: 31/224\n",
      "Batch loss: 0.3240087330341339 batch: 32/224\n",
      "Batch loss: 0.28803351521492004 batch: 33/224\n",
      "Batch loss: 0.23988474905490875 batch: 34/224\n",
      "Batch loss: 0.2720222771167755 batch: 35/224\n",
      "Batch loss: 0.28687044978141785 batch: 36/224\n",
      "Batch loss: 0.2688581347465515 batch: 37/224\n",
      "Batch loss: 0.2904052138328552 batch: 38/224\n",
      "Batch loss: 0.31345221400260925 batch: 39/224\n",
      "Batch loss: 0.2605041563510895 batch: 40/224\n",
      "Batch loss: 0.3410022556781769 batch: 41/224\n",
      "Batch loss: 0.28469669818878174 batch: 42/224\n",
      "Batch loss: 0.3145771920681 batch: 43/224\n",
      "Batch loss: 0.2256738692522049 batch: 44/224\n",
      "Batch loss: 0.22770382463932037 batch: 45/224\n",
      "Batch loss: 0.3616296648979187 batch: 46/224\n",
      "Batch loss: 0.29150596261024475 batch: 47/224\n",
      "Batch loss: 0.29997897148132324 batch: 48/224\n",
      "Batch loss: 0.22842992842197418 batch: 49/224\n",
      "Batch loss: 0.2897108495235443 batch: 50/224\n",
      "Batch loss: 0.23562756180763245 batch: 51/224\n",
      "Batch loss: 0.24967122077941895 batch: 52/224\n",
      "Batch loss: 0.32242703437805176 batch: 53/224\n",
      "Batch loss: 0.2593907117843628 batch: 54/224\n",
      "Batch loss: 0.2583043873310089 batch: 55/224\n",
      "Batch loss: 0.2565237283706665 batch: 56/224\n",
      "Batch loss: 0.3564387857913971 batch: 57/224\n",
      "Batch loss: 0.2538090944290161 batch: 58/224\n",
      "Batch loss: 0.27644863724708557 batch: 59/224\n",
      "Batch loss: 0.34202855825424194 batch: 60/224\n",
      "Batch loss: 0.3025912344455719 batch: 61/224\n",
      "Batch loss: 0.26856061816215515 batch: 62/224\n",
      "Batch loss: 0.26543620228767395 batch: 63/224\n",
      "Batch loss: 0.2968365550041199 batch: 64/224\n",
      "Batch loss: 0.29007142782211304 batch: 65/224\n",
      "Batch loss: 0.2994295060634613 batch: 66/224\n",
      "Batch loss: 0.27270960807800293 batch: 67/224\n",
      "Batch loss: 0.29619571566581726 batch: 68/224\n",
      "Batch loss: 0.3242277503013611 batch: 69/224\n",
      "Batch loss: 0.30817365646362305 batch: 70/224\n",
      "Batch loss: 0.2613527774810791 batch: 71/224\n",
      "Batch loss: 0.21822267770767212 batch: 72/224\n",
      "Batch loss: 0.33942240476608276 batch: 73/224\n",
      "Batch loss: 0.2885114550590515 batch: 74/224\n",
      "Batch loss: 0.28991833329200745 batch: 75/224\n",
      "Batch loss: 0.3245124816894531 batch: 76/224\n",
      "Batch loss: 0.2722380757331848 batch: 77/224\n",
      "Batch loss: 0.2999873459339142 batch: 78/224\n",
      "Batch loss: 0.28438234329223633 batch: 79/224\n",
      "Batch loss: 0.2868933081626892 batch: 80/224\n",
      "Batch loss: 0.35568106174468994 batch: 81/224\n",
      "Batch loss: 0.30766457319259644 batch: 82/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.30233410000801086 batch: 83/224\n",
      "Batch loss: 0.23070895671844482 batch: 84/224\n",
      "Batch loss: 0.26278719305992126 batch: 85/224\n",
      "Batch loss: 0.280801922082901 batch: 86/224\n",
      "Batch loss: 0.2790251672267914 batch: 87/224\n",
      "Batch loss: 0.33472439646720886 batch: 88/224\n",
      "Batch loss: 0.26881396770477295 batch: 89/224\n",
      "Batch loss: 0.2739103138446808 batch: 90/224\n",
      "Batch loss: 0.2832368016242981 batch: 91/224\n",
      "Batch loss: 0.2838502824306488 batch: 92/224\n",
      "Batch loss: 0.24578116834163666 batch: 93/224\n",
      "Batch loss: 0.30300384759902954 batch: 94/224\n",
      "Batch loss: 0.2598344385623932 batch: 95/224\n",
      "Batch loss: 0.24744181334972382 batch: 96/224\n",
      "Batch loss: 0.277120441198349 batch: 97/224\n",
      "Batch loss: 0.2245694249868393 batch: 98/224\n",
      "Batch loss: 0.30264103412628174 batch: 99/224\n",
      "Batch loss: 0.2525767385959625 batch: 100/224\n",
      "Batch loss: 0.31229662895202637 batch: 101/224\n",
      "Batch loss: 0.2711906433105469 batch: 102/224\n",
      "Batch loss: 0.27837666869163513 batch: 103/224\n",
      "Batch loss: 0.24527926743030548 batch: 104/224\n",
      "Batch loss: 0.21457530558109283 batch: 105/224\n",
      "Batch loss: 0.26758846640586853 batch: 106/224\n",
      "Batch loss: 0.25655847787857056 batch: 107/224\n",
      "Batch loss: 0.2669532597064972 batch: 108/224\n",
      "Batch loss: 0.24456219375133514 batch: 109/224\n",
      "Batch loss: 0.2422972172498703 batch: 110/224\n",
      "Batch loss: 0.2872716188430786 batch: 111/224\n",
      "Batch loss: 0.26453059911727905 batch: 112/224\n",
      "Batch loss: 0.3244357407093048 batch: 113/224\n",
      "Batch loss: 0.22267548739910126 batch: 114/224\n",
      "Batch loss: 0.24546796083450317 batch: 115/224\n",
      "Batch loss: 0.26004713773727417 batch: 116/224\n",
      "Batch loss: 0.2580536901950836 batch: 117/224\n",
      "Batch loss: 0.3219347894191742 batch: 118/224\n",
      "Batch loss: 0.26150742173194885 batch: 119/224\n",
      "Batch loss: 0.30901575088500977 batch: 120/224\n",
      "Batch loss: 0.27030307054519653 batch: 121/224\n",
      "Batch loss: 0.2800189256668091 batch: 122/224\n",
      "Batch loss: 0.24656334519386292 batch: 123/224\n",
      "Batch loss: 0.27694252133369446 batch: 124/224\n",
      "Batch loss: 0.26628297567367554 batch: 125/224\n",
      "Batch loss: 0.2558034658432007 batch: 126/224\n",
      "Batch loss: 0.2690838575363159 batch: 127/224\n",
      "Batch loss: 0.25540977716445923 batch: 128/224\n",
      "Batch loss: 0.29673898220062256 batch: 129/224\n",
      "Batch loss: 0.3288695812225342 batch: 130/224\n",
      "Batch loss: 0.24110688269138336 batch: 131/224\n",
      "Batch loss: 0.2402084469795227 batch: 132/224\n",
      "Batch loss: 0.3033061921596527 batch: 133/224\n",
      "Batch loss: 0.2597469389438629 batch: 134/224\n",
      "Batch loss: 0.3409784734249115 batch: 135/224\n",
      "Batch loss: 0.29534590244293213 batch: 136/224\n",
      "Batch loss: 0.2536988854408264 batch: 137/224\n",
      "Batch loss: 0.32155030965805054 batch: 138/224\n",
      "Batch loss: 0.3214278221130371 batch: 139/224\n",
      "Batch loss: 0.3405078649520874 batch: 140/224\n",
      "Batch loss: 0.2124543935060501 batch: 141/224\n",
      "Batch loss: 0.27008911967277527 batch: 142/224\n",
      "Batch loss: 0.2626870274543762 batch: 143/224\n",
      "Batch loss: 0.3034917712211609 batch: 144/224\n",
      "Batch loss: 0.23913194239139557 batch: 145/224\n",
      "Batch loss: 0.28885725140571594 batch: 146/224\n",
      "Batch loss: 0.25985124707221985 batch: 147/224\n",
      "Batch loss: 0.2554318904876709 batch: 148/224\n",
      "Batch loss: 0.2926257848739624 batch: 149/224\n",
      "Batch loss: 0.2869289815425873 batch: 150/224\n",
      "Batch loss: 0.2946835458278656 batch: 151/224\n",
      "Batch loss: 0.25036728382110596 batch: 152/224\n",
      "Batch loss: 0.31145116686820984 batch: 153/224\n",
      "Batch loss: 0.31882336735725403 batch: 154/224\n",
      "Batch loss: 0.25955018401145935 batch: 155/224\n",
      "Batch loss: 0.28901347517967224 batch: 156/224\n",
      "Batch loss: 0.28641682863235474 batch: 157/224\n",
      "Batch loss: 0.3557042181491852 batch: 158/224\n",
      "Batch loss: 0.2244369387626648 batch: 159/224\n",
      "Batch loss: 0.2785596251487732 batch: 160/224\n",
      "Batch loss: 0.25193411111831665 batch: 161/224\n",
      "Batch loss: 0.26930418610572815 batch: 162/224\n",
      "Batch loss: 0.2602980136871338 batch: 163/224\n",
      "Batch loss: 0.3037814795970917 batch: 164/224\n",
      "Batch loss: 0.3281280994415283 batch: 165/224\n",
      "Batch loss: 0.2588834762573242 batch: 166/224\n",
      "Batch loss: 0.2473839521408081 batch: 167/224\n",
      "Batch loss: 0.24618810415267944 batch: 168/224\n",
      "Batch loss: 0.2992127239704132 batch: 169/224\n",
      "Batch loss: 0.25718656182289124 batch: 170/224\n",
      "Batch loss: 0.24857047200202942 batch: 171/224\n",
      "Batch loss: 0.27785295248031616 batch: 172/224\n",
      "Batch loss: 0.29204440116882324 batch: 173/224\n",
      "Batch loss: 0.2626381516456604 batch: 174/224\n",
      "Batch loss: 0.2873487174510956 batch: 175/224\n",
      "Batch loss: 0.27932441234588623 batch: 176/224\n",
      "Batch loss: 0.3208465576171875 batch: 177/224\n",
      "Batch loss: 0.2705686390399933 batch: 178/224\n",
      "Batch loss: 0.3248690068721771 batch: 179/224\n",
      "Batch loss: 0.2185494303703308 batch: 180/224\n",
      "Batch loss: 0.3109183609485626 batch: 181/224\n",
      "Batch loss: 0.2833404839038849 batch: 182/224\n",
      "Batch loss: 0.30354487895965576 batch: 183/224\n",
      "Batch loss: 0.26209595799446106 batch: 184/224\n",
      "Batch loss: 0.3087090849876404 batch: 185/224\n",
      "Batch loss: 0.24988339841365814 batch: 186/224\n",
      "Batch loss: 0.26787421107292175 batch: 187/224\n",
      "Batch loss: 0.2238260954618454 batch: 188/224\n",
      "Batch loss: 0.31754469871520996 batch: 189/224\n",
      "Batch loss: 0.2743275761604309 batch: 190/224\n",
      "Batch loss: 0.2636922597885132 batch: 191/224\n",
      "Batch loss: 0.3515182137489319 batch: 192/224\n",
      "Batch loss: 0.27283138036727905 batch: 193/224\n",
      "Batch loss: 0.2732950448989868 batch: 194/224\n",
      "Batch loss: 0.34778422117233276 batch: 195/224\n",
      "Batch loss: 0.32532942295074463 batch: 196/224\n",
      "Batch loss: 0.2919679582118988 batch: 197/224\n",
      "Batch loss: 0.27103129029273987 batch: 198/224\n",
      "Batch loss: 0.292603462934494 batch: 199/224\n",
      "Batch loss: 0.29550084471702576 batch: 200/224\n",
      "Batch loss: 0.30809199810028076 batch: 201/224\n",
      "Batch loss: 0.30792826414108276 batch: 202/224\n",
      "Batch loss: 0.29029762744903564 batch: 203/224\n",
      "Batch loss: 0.2729153335094452 batch: 204/224\n",
      "Batch loss: 0.3033747375011444 batch: 205/224\n",
      "Batch loss: 0.2351769357919693 batch: 206/224\n",
      "Batch loss: 0.3354548215866089 batch: 207/224\n",
      "Batch loss: 0.21022334694862366 batch: 208/224\n",
      "Batch loss: 0.26333773136138916 batch: 209/224\n",
      "Batch loss: 0.2832045257091522 batch: 210/224\n",
      "Batch loss: 0.25499778985977173 batch: 211/224\n",
      "Batch loss: 0.2782324254512787 batch: 212/224\n",
      "Batch loss: 0.29660865664482117 batch: 213/224\n",
      "Batch loss: 0.31054234504699707 batch: 214/224\n",
      "Batch loss: 0.2900327444076538 batch: 215/224\n",
      "Batch loss: 0.26723864674568176 batch: 216/224\n",
      "Batch loss: 0.2784564793109894 batch: 217/224\n",
      "Batch loss: 0.27606895565986633 batch: 218/224\n",
      "Batch loss: 0.21971546113491058 batch: 219/224\n",
      "Batch loss: 0.22773361206054688 batch: 220/224\n",
      "Batch loss: 0.346640944480896 batch: 221/224\n",
      "Batch loss: 0.2782667875289917 batch: 222/224\n",
      "Batch loss: 0.2291889488697052 batch: 223/224\n",
      "Batch loss: 0.2545902729034424 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 14/75..  Training Loss: 0.00056..  Test Loss: 0.00065..  Test Accuracy: 0.88407\n",
      "Running epoch 15/75\n",
      "Batch loss: 0.2536286413669586 batch: 1/224\n",
      "Batch loss: 0.26036691665649414 batch: 2/224\n",
      "Batch loss: 0.25840461254119873 batch: 3/224\n",
      "Batch loss: 0.2968936264514923 batch: 4/224\n",
      "Batch loss: 0.2604435682296753 batch: 5/224\n",
      "Batch loss: 0.32150858640670776 batch: 6/224\n",
      "Batch loss: 0.27512744069099426 batch: 7/224\n",
      "Batch loss: 0.3086284399032593 batch: 8/224\n",
      "Batch loss: 0.23904284834861755 batch: 9/224\n",
      "Batch loss: 0.2741832435131073 batch: 10/224\n",
      "Batch loss: 0.35090869665145874 batch: 11/224\n",
      "Batch loss: 0.26894161105155945 batch: 12/224\n",
      "Batch loss: 0.22384898364543915 batch: 13/224\n",
      "Batch loss: 0.263846755027771 batch: 14/224\n",
      "Batch loss: 0.3161681294441223 batch: 15/224\n",
      "Batch loss: 0.3488294184207916 batch: 16/224\n",
      "Batch loss: 0.2519267201423645 batch: 17/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.33112478256225586 batch: 18/224\n",
      "Batch loss: 0.2398572564125061 batch: 19/224\n",
      "Batch loss: 0.26717016100883484 batch: 20/224\n",
      "Batch loss: 0.3314230144023895 batch: 21/224\n",
      "Batch loss: 0.2480493187904358 batch: 22/224\n",
      "Batch loss: 0.30541348457336426 batch: 23/224\n",
      "Batch loss: 0.3112318813800812 batch: 24/224\n",
      "Batch loss: 0.26960164308547974 batch: 25/224\n",
      "Batch loss: 0.2140723019838333 batch: 26/224\n",
      "Batch loss: 0.30246660113334656 batch: 27/224\n",
      "Batch loss: 0.26067453622817993 batch: 28/224\n",
      "Batch loss: 0.29706066846847534 batch: 29/224\n",
      "Batch loss: 0.27173346281051636 batch: 30/224\n",
      "Batch loss: 0.2512189745903015 batch: 31/224\n",
      "Batch loss: 0.29195478558540344 batch: 32/224\n",
      "Batch loss: 0.29357072710990906 batch: 33/224\n",
      "Batch loss: 0.27677950263023376 batch: 34/224\n",
      "Batch loss: 0.25307515263557434 batch: 35/224\n",
      "Batch loss: 0.322815865278244 batch: 36/224\n",
      "Batch loss: 0.2776950001716614 batch: 37/224\n",
      "Batch loss: 0.3139491379261017 batch: 38/224\n",
      "Batch loss: 0.2626268267631531 batch: 39/224\n",
      "Batch loss: 0.2929539382457733 batch: 40/224\n",
      "Batch loss: 0.31821832060813904 batch: 41/224\n",
      "Batch loss: 0.258259117603302 batch: 42/224\n",
      "Batch loss: 0.29651927947998047 batch: 43/224\n",
      "Batch loss: 0.24191132187843323 batch: 44/224\n",
      "Batch loss: 0.2514280676841736 batch: 45/224\n",
      "Batch loss: 0.30140891671180725 batch: 46/224\n",
      "Batch loss: 0.2940724492073059 batch: 47/224\n",
      "Batch loss: 0.24282698333263397 batch: 48/224\n",
      "Batch loss: 0.240144282579422 batch: 49/224\n",
      "Batch loss: 0.2688665986061096 batch: 50/224\n",
      "Batch loss: 0.2549661695957184 batch: 51/224\n",
      "Batch loss: 0.29100221395492554 batch: 52/224\n",
      "Batch loss: 0.3010168969631195 batch: 53/224\n",
      "Batch loss: 0.2410227209329605 batch: 54/224\n",
      "Batch loss: 0.26932281255722046 batch: 55/224\n",
      "Batch loss: 0.24597468972206116 batch: 56/224\n",
      "Batch loss: 0.31310611963272095 batch: 57/224\n",
      "Batch loss: 0.2703854441642761 batch: 58/224\n",
      "Batch loss: 0.22806629538536072 batch: 59/224\n",
      "Batch loss: 0.3309361934661865 batch: 60/224\n",
      "Batch loss: 0.3084350824356079 batch: 61/224\n",
      "Batch loss: 0.24470578134059906 batch: 62/224\n",
      "Batch loss: 0.2744976580142975 batch: 63/224\n",
      "Batch loss: 0.2734544575214386 batch: 64/224\n",
      "Batch loss: 0.30337780714035034 batch: 65/224\n",
      "Batch loss: 0.3126108646392822 batch: 66/224\n",
      "Batch loss: 0.2740260362625122 batch: 67/224\n",
      "Batch loss: 0.25869685411453247 batch: 68/224\n",
      "Batch loss: 0.2915537655353546 batch: 69/224\n",
      "Batch loss: 0.2509862780570984 batch: 70/224\n",
      "Batch loss: 0.26495760679244995 batch: 71/224\n",
      "Batch loss: 0.2143402099609375 batch: 72/224\n",
      "Batch loss: 0.33565619587898254 batch: 73/224\n",
      "Batch loss: 0.2898849844932556 batch: 74/224\n",
      "Batch loss: 0.26740172505378723 batch: 75/224\n",
      "Batch loss: 0.31036245822906494 batch: 76/224\n",
      "Batch loss: 0.26917997002601624 batch: 77/224\n",
      "Batch loss: 0.29932790994644165 batch: 78/224\n",
      "Batch loss: 0.2771328091621399 batch: 79/224\n",
      "Batch loss: 0.24863800406455994 batch: 80/224\n",
      "Batch loss: 0.3412293791770935 batch: 81/224\n",
      "Batch loss: 0.3352096974849701 batch: 82/224\n",
      "Batch loss: 0.2573991119861603 batch: 83/224\n",
      "Batch loss: 0.27179425954818726 batch: 84/224\n",
      "Batch loss: 0.2650725543498993 batch: 85/224\n",
      "Batch loss: 0.2669852077960968 batch: 86/224\n",
      "Batch loss: 0.27865836024284363 batch: 87/224\n",
      "Batch loss: 0.32495033740997314 batch: 88/224\n",
      "Batch loss: 0.27913323044776917 batch: 89/224\n",
      "Batch loss: 0.3066858947277069 batch: 90/224\n",
      "Batch loss: 0.25996968150138855 batch: 91/224\n",
      "Batch loss: 0.2663736343383789 batch: 92/224\n",
      "Batch loss: 0.2542954385280609 batch: 93/224\n",
      "Batch loss: 0.29325446486473083 batch: 94/224\n",
      "Batch loss: 0.25751277804374695 batch: 95/224\n",
      "Batch loss: 0.2524479925632477 batch: 96/224\n",
      "Batch loss: 0.25643396377563477 batch: 97/224\n",
      "Batch loss: 0.22403202950954437 batch: 98/224\n",
      "Batch loss: 0.27938076853752136 batch: 99/224\n",
      "Batch loss: 0.23757898807525635 batch: 100/224\n",
      "Batch loss: 0.3232758045196533 batch: 101/224\n",
      "Batch loss: 0.23990747332572937 batch: 102/224\n",
      "Batch loss: 0.2857018709182739 batch: 103/224\n",
      "Batch loss: 0.2343841791152954 batch: 104/224\n",
      "Batch loss: 0.2355271577835083 batch: 105/224\n",
      "Batch loss: 0.2600845694541931 batch: 106/224\n",
      "Batch loss: 0.23234719038009644 batch: 107/224\n",
      "Batch loss: 0.2760298252105713 batch: 108/224\n",
      "Batch loss: 0.26211118698120117 batch: 109/224\n",
      "Batch loss: 0.28378140926361084 batch: 110/224\n",
      "Batch loss: 0.28750860691070557 batch: 111/224\n",
      "Batch loss: 0.23673312366008759 batch: 112/224\n",
      "Batch loss: 0.2921397387981415 batch: 113/224\n",
      "Batch loss: 0.215948686003685 batch: 114/224\n",
      "Batch loss: 0.22474098205566406 batch: 115/224\n",
      "Batch loss: 0.2680933177471161 batch: 116/224\n",
      "Batch loss: 0.2485102266073227 batch: 117/224\n",
      "Batch loss: 0.31047406792640686 batch: 118/224\n",
      "Batch loss: 0.23906703293323517 batch: 119/224\n",
      "Batch loss: 0.2974106967449188 batch: 120/224\n",
      "Batch loss: 0.2577586770057678 batch: 121/224\n",
      "Batch loss: 0.27469944953918457 batch: 122/224\n",
      "Batch loss: 0.18833591043949127 batch: 123/224\n",
      "Batch loss: 0.2794981002807617 batch: 124/224\n",
      "Batch loss: 0.26098567247390747 batch: 125/224\n",
      "Batch loss: 0.28932663798332214 batch: 126/224\n",
      "Batch loss: 0.269544780254364 batch: 127/224\n",
      "Batch loss: 0.25413161516189575 batch: 128/224\n",
      "Batch loss: 0.3045361340045929 batch: 129/224\n",
      "Batch loss: 0.3004102110862732 batch: 130/224\n",
      "Batch loss: 0.2486269623041153 batch: 131/224\n",
      "Batch loss: 0.2729397714138031 batch: 132/224\n",
      "Batch loss: 0.2763347029685974 batch: 133/224\n",
      "Batch loss: 0.26002928614616394 batch: 134/224\n",
      "Batch loss: 0.287742018699646 batch: 135/224\n",
      "Batch loss: 0.2769642472267151 batch: 136/224\n",
      "Batch loss: 0.26227089762687683 batch: 137/224\n",
      "Batch loss: 0.2877678871154785 batch: 138/224\n",
      "Batch loss: 0.26106709241867065 batch: 139/224\n",
      "Batch loss: 0.3169081509113312 batch: 140/224\n",
      "Batch loss: 0.21198630332946777 batch: 141/224\n",
      "Batch loss: 0.25736382603645325 batch: 142/224\n",
      "Batch loss: 0.2420484721660614 batch: 143/224\n",
      "Batch loss: 0.2860391139984131 batch: 144/224\n",
      "Batch loss: 0.24515600502490997 batch: 145/224\n",
      "Batch loss: 0.28862714767456055 batch: 146/224\n",
      "Batch loss: 0.2687445878982544 batch: 147/224\n",
      "Batch loss: 0.25397947430610657 batch: 148/224\n",
      "Batch loss: 0.29671308398246765 batch: 149/224\n",
      "Batch loss: 0.31592103838920593 batch: 150/224\n",
      "Batch loss: 0.300592303276062 batch: 151/224\n",
      "Batch loss: 0.2371014803647995 batch: 152/224\n",
      "Batch loss: 0.32439541816711426 batch: 153/224\n",
      "Batch loss: 0.3120162785053253 batch: 154/224\n",
      "Batch loss: 0.23764672875404358 batch: 155/224\n",
      "Batch loss: 0.296248197555542 batch: 156/224\n",
      "Batch loss: 0.304522305727005 batch: 157/224\n",
      "Batch loss: 0.35343506932258606 batch: 158/224\n",
      "Batch loss: 0.23058824241161346 batch: 159/224\n",
      "Batch loss: 0.2686638832092285 batch: 160/224\n",
      "Batch loss: 0.23502200841903687 batch: 161/224\n",
      "Batch loss: 0.262871116399765 batch: 162/224\n",
      "Batch loss: 0.2437181919813156 batch: 163/224\n",
      "Batch loss: 0.2698972821235657 batch: 164/224\n",
      "Batch loss: 0.31302905082702637 batch: 165/224\n",
      "Batch loss: 0.2622776925563812 batch: 166/224\n",
      "Batch loss: 0.2200545072555542 batch: 167/224\n",
      "Batch loss: 0.2143886536359787 batch: 168/224\n",
      "Batch loss: 0.28895923495292664 batch: 169/224\n",
      "Batch loss: 0.242313414812088 batch: 170/224\n",
      "Batch loss: 0.23718847334384918 batch: 171/224\n",
      "Batch loss: 0.24358311295509338 batch: 172/224\n",
      "Batch loss: 0.2692623436450958 batch: 173/224\n",
      "Batch loss: 0.2700667381286621 batch: 174/224\n",
      "Batch loss: 0.2538425922393799 batch: 175/224\n",
      "Batch loss: 0.2545126974582672 batch: 176/224\n",
      "Batch loss: 0.3025600016117096 batch: 177/224\n",
      "Batch loss: 0.27457672357559204 batch: 178/224\n",
      "Batch loss: 0.26301243901252747 batch: 179/224\n",
      "Batch loss: 0.20846228301525116 batch: 180/224\n",
      "Batch loss: 0.27976441383361816 batch: 181/224\n",
      "Batch loss: 0.28209638595581055 batch: 182/224\n",
      "Batch loss: 0.3309189975261688 batch: 183/224\n",
      "Batch loss: 0.23893775045871735 batch: 184/224\n",
      "Batch loss: 0.31769195199012756 batch: 185/224\n",
      "Batch loss: 0.22936749458312988 batch: 186/224\n",
      "Batch loss: 0.23028996586799622 batch: 187/224\n",
      "Batch loss: 0.20387685298919678 batch: 188/224\n",
      "Batch loss: 0.29256144165992737 batch: 189/224\n",
      "Batch loss: 0.31162792444229126 batch: 190/224\n",
      "Batch loss: 0.26068753004074097 batch: 191/224\n",
      "Batch loss: 0.3259892761707306 batch: 192/224\n",
      "Batch loss: 0.2834934890270233 batch: 193/224\n",
      "Batch loss: 0.2881360948085785 batch: 194/224\n",
      "Batch loss: 0.3427331745624542 batch: 195/224\n",
      "Batch loss: 0.3257671296596527 batch: 196/224\n",
      "Batch loss: 0.2985948920249939 batch: 197/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.24905599653720856 batch: 198/224\n",
      "Batch loss: 0.2600930333137512 batch: 199/224\n",
      "Batch loss: 0.29007554054260254 batch: 200/224\n",
      "Batch loss: 0.30278122425079346 batch: 201/224\n",
      "Batch loss: 0.29214927554130554 batch: 202/224\n",
      "Batch loss: 0.2749229669570923 batch: 203/224\n",
      "Batch loss: 0.24096108973026276 batch: 204/224\n",
      "Batch loss: 0.3231010437011719 batch: 205/224\n",
      "Batch loss: 0.24020302295684814 batch: 206/224\n",
      "Batch loss: 0.2939394414424896 batch: 207/224\n",
      "Batch loss: 0.2397698312997818 batch: 208/224\n",
      "Batch loss: 0.28373730182647705 batch: 209/224\n",
      "Batch loss: 0.2623060941696167 batch: 210/224\n",
      "Batch loss: 0.24434949457645416 batch: 211/224\n",
      "Batch loss: 0.27643871307373047 batch: 212/224\n",
      "Batch loss: 0.27386048436164856 batch: 213/224\n",
      "Batch loss: 0.31762459874153137 batch: 214/224\n",
      "Batch loss: 0.29917025566101074 batch: 215/224\n",
      "Batch loss: 0.28127554059028625 batch: 216/224\n",
      "Batch loss: 0.2737217843532562 batch: 217/224\n",
      "Batch loss: 0.2789155840873718 batch: 218/224\n",
      "Batch loss: 0.22401879727840424 batch: 219/224\n",
      "Batch loss: 0.2335556298494339 batch: 220/224\n",
      "Batch loss: 0.3156094253063202 batch: 221/224\n",
      "Batch loss: 0.27217844128608704 batch: 222/224\n",
      "Batch loss: 0.21872998774051666 batch: 223/224\n",
      "Batch loss: 0.24639873206615448 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 15/75..  Training Loss: 0.00055..  Test Loss: 0.00065..  Test Accuracy: 0.88114\n",
      "Running epoch 16/75\n",
      "Batch loss: 0.22520197927951813 batch: 1/224\n",
      "Batch loss: 0.2581540048122406 batch: 2/224\n",
      "Batch loss: 0.29355373978614807 batch: 3/224\n",
      "Batch loss: 0.3270878493785858 batch: 4/224\n",
      "Batch loss: 0.23726147413253784 batch: 5/224\n",
      "Batch loss: 0.33691564202308655 batch: 6/224\n",
      "Batch loss: 0.2595755457878113 batch: 7/224\n",
      "Batch loss: 0.2620628774166107 batch: 8/224\n",
      "Batch loss: 0.2248745858669281 batch: 9/224\n",
      "Batch loss: 0.25433534383773804 batch: 10/224\n",
      "Batch loss: 0.34352996945381165 batch: 11/224\n",
      "Batch loss: 0.2589068114757538 batch: 12/224\n",
      "Batch loss: 0.23263472318649292 batch: 13/224\n",
      "Batch loss: 0.24313782155513763 batch: 14/224\n",
      "Batch loss: 0.2685101330280304 batch: 15/224\n",
      "Batch loss: 0.34997835755348206 batch: 16/224\n",
      "Batch loss: 0.245205357670784 batch: 17/224\n",
      "Batch loss: 0.3072269856929779 batch: 18/224\n",
      "Batch loss: 0.22240383923053741 batch: 19/224\n",
      "Batch loss: 0.2757485508918762 batch: 20/224\n",
      "Batch loss: 0.31894969940185547 batch: 21/224\n",
      "Batch loss: 0.24823102355003357 batch: 22/224\n",
      "Batch loss: 0.29321783781051636 batch: 23/224\n",
      "Batch loss: 0.2832125425338745 batch: 24/224\n",
      "Batch loss: 0.251462459564209 batch: 25/224\n",
      "Batch loss: 0.23915456235408783 batch: 26/224\n",
      "Batch loss: 0.2651683986186981 batch: 27/224\n",
      "Batch loss: 0.27439212799072266 batch: 28/224\n",
      "Batch loss: 0.3167192339897156 batch: 29/224\n",
      "Batch loss: 0.2815284729003906 batch: 30/224\n",
      "Batch loss: 0.2628723680973053 batch: 31/224\n",
      "Batch loss: 0.2895355820655823 batch: 32/224\n",
      "Batch loss: 0.22782081365585327 batch: 33/224\n",
      "Batch loss: 0.24887476861476898 batch: 34/224\n",
      "Batch loss: 0.2887991666793823 batch: 35/224\n",
      "Batch loss: 0.3181563913822174 batch: 36/224\n",
      "Batch loss: 0.27988049387931824 batch: 37/224\n",
      "Batch loss: 0.29365265369415283 batch: 38/224\n",
      "Batch loss: 0.2603650987148285 batch: 39/224\n",
      "Batch loss: 0.26333141326904297 batch: 40/224\n",
      "Batch loss: 0.2901202440261841 batch: 41/224\n",
      "Batch loss: 0.24550887942314148 batch: 42/224\n",
      "Batch loss: 0.3032000660896301 batch: 43/224\n",
      "Batch loss: 0.2290198802947998 batch: 44/224\n",
      "Batch loss: 0.2697784900665283 batch: 45/224\n",
      "Batch loss: 0.3378216326236725 batch: 46/224\n",
      "Batch loss: 0.2892099916934967 batch: 47/224\n",
      "Batch loss: 0.2633652985095978 batch: 48/224\n",
      "Batch loss: 0.2522457540035248 batch: 49/224\n",
      "Batch loss: 0.257163941860199 batch: 50/224\n",
      "Batch loss: 0.2604879140853882 batch: 51/224\n",
      "Batch loss: 0.24464727938175201 batch: 52/224\n",
      "Batch loss: 0.31090331077575684 batch: 53/224\n",
      "Batch loss: 0.22474302351474762 batch: 54/224\n",
      "Batch loss: 0.25280046463012695 batch: 55/224\n",
      "Batch loss: 0.2686905860900879 batch: 56/224\n",
      "Batch loss: 0.29327625036239624 batch: 57/224\n",
      "Batch loss: 0.27645042538642883 batch: 58/224\n",
      "Batch loss: 0.23856918513774872 batch: 59/224\n",
      "Batch loss: 0.33579006791114807 batch: 60/224\n",
      "Batch loss: 0.3027668297290802 batch: 61/224\n",
      "Batch loss: 0.23409464955329895 batch: 62/224\n",
      "Batch loss: 0.29079797863960266 batch: 63/224\n",
      "Batch loss: 0.2972996234893799 batch: 64/224\n",
      "Batch loss: 0.23946477472782135 batch: 65/224\n",
      "Batch loss: 0.30635637044906616 batch: 66/224\n",
      "Batch loss: 0.2663653790950775 batch: 67/224\n",
      "Batch loss: 0.26443397998809814 batch: 68/224\n",
      "Batch loss: 0.2766725420951843 batch: 69/224\n",
      "Batch loss: 0.2533733546733856 batch: 70/224\n",
      "Batch loss: 0.24617595970630646 batch: 71/224\n",
      "Batch loss: 0.18870650231838226 batch: 72/224\n",
      "Batch loss: 0.30912190675735474 batch: 73/224\n",
      "Batch loss: 0.29711636900901794 batch: 74/224\n",
      "Batch loss: 0.2586899995803833 batch: 75/224\n",
      "Batch loss: 0.2763110399246216 batch: 76/224\n",
      "Batch loss: 0.25057947635650635 batch: 77/224\n",
      "Batch loss: 0.27164316177368164 batch: 78/224\n",
      "Batch loss: 0.2866566777229309 batch: 79/224\n",
      "Batch loss: 0.2577958106994629 batch: 80/224\n",
      "Batch loss: 0.36321282386779785 batch: 81/224\n",
      "Batch loss: 0.28603288531303406 batch: 82/224\n",
      "Batch loss: 0.2557847499847412 batch: 83/224\n",
      "Batch loss: 0.23439157009124756 batch: 84/224\n",
      "Batch loss: 0.2647133767604828 batch: 85/224\n",
      "Batch loss: 0.26549389958381653 batch: 86/224\n",
      "Batch loss: 0.28877347707748413 batch: 87/224\n",
      "Batch loss: 0.2672802805900574 batch: 88/224\n",
      "Batch loss: 0.24973483383655548 batch: 89/224\n",
      "Batch loss: 0.28920039534568787 batch: 90/224\n",
      "Batch loss: 0.23668940365314484 batch: 91/224\n",
      "Batch loss: 0.2817094922065735 batch: 92/224\n",
      "Batch loss: 0.2509770095348358 batch: 93/224\n",
      "Batch loss: 0.29839131236076355 batch: 94/224\n",
      "Batch loss: 0.2519846260547638 batch: 95/224\n",
      "Batch loss: 0.2609889507293701 batch: 96/224\n",
      "Batch loss: 0.2334442138671875 batch: 97/224\n",
      "Batch loss: 0.22970664501190186 batch: 98/224\n",
      "Batch loss: 0.27916035056114197 batch: 99/224\n",
      "Batch loss: 0.23123732209205627 batch: 100/224\n",
      "Batch loss: 0.2934713661670685 batch: 101/224\n",
      "Batch loss: 0.2516198754310608 batch: 102/224\n",
      "Batch loss: 0.27822569012641907 batch: 103/224\n",
      "Batch loss: 0.24585789442062378 batch: 104/224\n",
      "Batch loss: 0.23759204149246216 batch: 105/224\n",
      "Batch loss: 0.2545658349990845 batch: 106/224\n",
      "Batch loss: 0.2465720921754837 batch: 107/224\n",
      "Batch loss: 0.2836948037147522 batch: 108/224\n",
      "Batch loss: 0.2334049493074417 batch: 109/224\n",
      "Batch loss: 0.2328435182571411 batch: 110/224\n",
      "Batch loss: 0.27463605999946594 batch: 111/224\n",
      "Batch loss: 0.24852590262889862 batch: 112/224\n",
      "Batch loss: 0.30325207114219666 batch: 113/224\n",
      "Batch loss: 0.23639830946922302 batch: 114/224\n",
      "Batch loss: 0.2257484644651413 batch: 115/224\n",
      "Batch loss: 0.2375301569700241 batch: 116/224\n",
      "Batch loss: 0.21716032922267914 batch: 117/224\n",
      "Batch loss: 0.27757057547569275 batch: 118/224\n",
      "Batch loss: 0.24167117476463318 batch: 119/224\n",
      "Batch loss: 0.2795830965042114 batch: 120/224\n",
      "Batch loss: 0.23265042901039124 batch: 121/224\n",
      "Batch loss: 0.2755359411239624 batch: 122/224\n",
      "Batch loss: 0.2082279622554779 batch: 123/224\n",
      "Batch loss: 0.2499929815530777 batch: 124/224\n",
      "Batch loss: 0.24459435045719147 batch: 125/224\n",
      "Batch loss: 0.29276856780052185 batch: 126/224\n",
      "Batch loss: 0.28394967317581177 batch: 127/224\n",
      "Batch loss: 0.25791800022125244 batch: 128/224\n",
      "Batch loss: 0.2915111482143402 batch: 129/224\n",
      "Batch loss: 0.27972084283828735 batch: 130/224\n",
      "Batch loss: 0.2167644500732422 batch: 131/224\n",
      "Batch loss: 0.22979341447353363 batch: 132/224\n",
      "Batch loss: 0.2575477361679077 batch: 133/224\n",
      "Batch loss: 0.27109065651893616 batch: 134/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.30718815326690674 batch: 135/224\n",
      "Batch loss: 0.2969282567501068 batch: 136/224\n",
      "Batch loss: 0.26350802183151245 batch: 137/224\n",
      "Batch loss: 0.2553982436656952 batch: 138/224\n",
      "Batch loss: 0.25544577836990356 batch: 139/224\n",
      "Batch loss: 0.3269193470478058 batch: 140/224\n",
      "Batch loss: 0.19262097775936127 batch: 141/224\n",
      "Batch loss: 0.2566642165184021 batch: 142/224\n",
      "Batch loss: 0.24379000067710876 batch: 143/224\n",
      "Batch loss: 0.26889368891716003 batch: 144/224\n",
      "Batch loss: 0.2726178765296936 batch: 145/224\n",
      "Batch loss: 0.30568593740463257 batch: 146/224\n",
      "Batch loss: 0.23424944281578064 batch: 147/224\n",
      "Batch loss: 0.24714648723602295 batch: 148/224\n",
      "Batch loss: 0.2628975212574005 batch: 149/224\n",
      "Batch loss: 0.28692832589149475 batch: 150/224\n",
      "Batch loss: 0.26301315426826477 batch: 151/224\n",
      "Batch loss: 0.2485702931880951 batch: 152/224\n",
      "Batch loss: 0.28490063548088074 batch: 153/224\n",
      "Batch loss: 0.30078208446502686 batch: 154/224\n",
      "Batch loss: 0.2494553178548813 batch: 155/224\n",
      "Batch loss: 0.27758410573005676 batch: 156/224\n",
      "Batch loss: 0.2973223328590393 batch: 157/224\n",
      "Batch loss: 0.34147006273269653 batch: 158/224\n",
      "Batch loss: 0.20242100954055786 batch: 159/224\n",
      "Batch loss: 0.2604849934577942 batch: 160/224\n",
      "Batch loss: 0.20012570917606354 batch: 161/224\n",
      "Batch loss: 0.2488596886396408 batch: 162/224\n",
      "Batch loss: 0.2597864866256714 batch: 163/224\n",
      "Batch loss: 0.24415817856788635 batch: 164/224\n",
      "Batch loss: 0.2807011902332306 batch: 165/224\n",
      "Batch loss: 0.27202990651130676 batch: 166/224\n",
      "Batch loss: 0.20018857717514038 batch: 167/224\n",
      "Batch loss: 0.23785516619682312 batch: 168/224\n",
      "Batch loss: 0.2275318056344986 batch: 169/224\n",
      "Batch loss: 0.24530792236328125 batch: 170/224\n",
      "Batch loss: 0.250360906124115 batch: 171/224\n",
      "Batch loss: 0.2693935036659241 batch: 172/224\n",
      "Batch loss: 0.2924291491508484 batch: 173/224\n",
      "Batch loss: 0.23886741697788239 batch: 174/224\n",
      "Batch loss: 0.22112490236759186 batch: 175/224\n",
      "Batch loss: 0.26184698939323425 batch: 176/224\n",
      "Batch loss: 0.31482595205307007 batch: 177/224\n",
      "Batch loss: 0.2264331877231598 batch: 178/224\n",
      "Batch loss: 0.2624483108520508 batch: 179/224\n",
      "Batch loss: 0.21800610423088074 batch: 180/224\n",
      "Batch loss: 0.2957427203655243 batch: 181/224\n",
      "Batch loss: 0.2829902172088623 batch: 182/224\n",
      "Batch loss: 0.2650846540927887 batch: 183/224\n",
      "Batch loss: 0.23271651566028595 batch: 184/224\n",
      "Batch loss: 0.29838478565216064 batch: 185/224\n",
      "Batch loss: 0.2370854765176773 batch: 186/224\n",
      "Batch loss: 0.23519203066825867 batch: 187/224\n",
      "Batch loss: 0.2193671464920044 batch: 188/224\n",
      "Batch loss: 0.2904413640499115 batch: 189/224\n",
      "Batch loss: 0.2960336208343506 batch: 190/224\n",
      "Batch loss: 0.2701011002063751 batch: 191/224\n",
      "Batch loss: 0.2916851341724396 batch: 192/224\n",
      "Batch loss: 0.26634636521339417 batch: 193/224\n",
      "Batch loss: 0.2726610600948334 batch: 194/224\n",
      "Batch loss: 0.28097808361053467 batch: 195/224\n",
      "Batch loss: 0.28615280985832214 batch: 196/224\n",
      "Batch loss: 0.27960216999053955 batch: 197/224\n",
      "Batch loss: 0.2663518190383911 batch: 198/224\n",
      "Batch loss: 0.24316298961639404 batch: 199/224\n",
      "Batch loss: 0.2900116443634033 batch: 200/224\n",
      "Batch loss: 0.2939744293689728 batch: 201/224\n",
      "Batch loss: 0.2720905542373657 batch: 202/224\n",
      "Batch loss: 0.28781405091285706 batch: 203/224\n",
      "Batch loss: 0.2390599399805069 batch: 204/224\n",
      "Batch loss: 0.2651461064815521 batch: 205/224\n",
      "Batch loss: 0.23738828301429749 batch: 206/224\n",
      "Batch loss: 0.30768728256225586 batch: 207/224\n",
      "Batch loss: 0.25630927085876465 batch: 208/224\n",
      "Batch loss: 0.2740747630596161 batch: 209/224\n",
      "Batch loss: 0.25461098551750183 batch: 210/224\n",
      "Batch loss: 0.22757083177566528 batch: 211/224\n",
      "Batch loss: 0.2721032202243805 batch: 212/224\n",
      "Batch loss: 0.27264440059661865 batch: 213/224\n",
      "Batch loss: 0.30189552903175354 batch: 214/224\n",
      "Batch loss: 0.2685781717300415 batch: 215/224\n",
      "Batch loss: 0.23706571757793427 batch: 216/224\n",
      "Batch loss: 0.28911012411117554 batch: 217/224\n",
      "Batch loss: 0.2375916689634323 batch: 218/224\n",
      "Batch loss: 0.23227255046367645 batch: 219/224\n",
      "Batch loss: 0.20884093642234802 batch: 220/224\n",
      "Batch loss: 0.2990363538265228 batch: 221/224\n",
      "Batch loss: 0.2556053102016449 batch: 222/224\n",
      "Batch loss: 0.22686073184013367 batch: 223/224\n",
      "Batch loss: 0.24518783390522003 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 16/75..  Training Loss: 0.00053..  Test Loss: 0.00066..  Test Accuracy: 0.88125\n",
      "Running epoch 17/75\n",
      "Batch loss: 0.22142302989959717 batch: 1/224\n",
      "Batch loss: 0.2400580793619156 batch: 2/224\n",
      "Batch loss: 0.23473204672336578 batch: 3/224\n",
      "Batch loss: 0.2583547532558441 batch: 4/224\n",
      "Batch loss: 0.2649743854999542 batch: 5/224\n",
      "Batch loss: 0.30821192264556885 batch: 6/224\n",
      "Batch loss: 0.24139659106731415 batch: 7/224\n",
      "Batch loss: 0.27572041749954224 batch: 8/224\n",
      "Batch loss: 0.22315889596939087 batch: 9/224\n",
      "Batch loss: 0.24350689351558685 batch: 10/224\n",
      "Batch loss: 0.32224777340888977 batch: 11/224\n",
      "Batch loss: 0.24761517345905304 batch: 12/224\n",
      "Batch loss: 0.2040746510028839 batch: 13/224\n",
      "Batch loss: 0.26219111680984497 batch: 14/224\n",
      "Batch loss: 0.24085655808448792 batch: 15/224\n",
      "Batch loss: 0.35561996698379517 batch: 16/224\n",
      "Batch loss: 0.2288830578327179 batch: 17/224\n",
      "Batch loss: 0.31633755564689636 batch: 18/224\n",
      "Batch loss: 0.22032363712787628 batch: 19/224\n",
      "Batch loss: 0.2398727387189865 batch: 20/224\n",
      "Batch loss: 0.2886958122253418 batch: 21/224\n",
      "Batch loss: 0.22079645097255707 batch: 22/224\n",
      "Batch loss: 0.3143637180328369 batch: 23/224\n",
      "Batch loss: 0.29397881031036377 batch: 24/224\n",
      "Batch loss: 0.21439342200756073 batch: 25/224\n",
      "Batch loss: 0.20532988011837006 batch: 26/224\n",
      "Batch loss: 0.25694018602371216 batch: 27/224\n",
      "Batch loss: 0.2695469260215759 batch: 28/224\n",
      "Batch loss: 0.2832689583301544 batch: 29/224\n",
      "Batch loss: 0.2465238720178604 batch: 30/224\n",
      "Batch loss: 0.21379506587982178 batch: 31/224\n",
      "Batch loss: 0.26534706354141235 batch: 32/224\n",
      "Batch loss: 0.2654281258583069 batch: 33/224\n",
      "Batch loss: 0.23150435090065002 batch: 34/224\n",
      "Batch loss: 0.2818490266799927 batch: 35/224\n",
      "Batch loss: 0.2863449156284332 batch: 36/224\n",
      "Batch loss: 0.26757848262786865 batch: 37/224\n",
      "Batch loss: 0.2441900223493576 batch: 38/224\n",
      "Batch loss: 0.2596772015094757 batch: 39/224\n",
      "Batch loss: 0.2540835440158844 batch: 40/224\n",
      "Batch loss: 0.3115535080432892 batch: 41/224\n",
      "Batch loss: 0.2692882716655731 batch: 42/224\n",
      "Batch loss: 0.27110397815704346 batch: 43/224\n",
      "Batch loss: 0.2234034538269043 batch: 44/224\n",
      "Batch loss: 0.22668015956878662 batch: 45/224\n",
      "Batch loss: 0.35413748025894165 batch: 46/224\n",
      "Batch loss: 0.2669160068035126 batch: 47/224\n",
      "Batch loss: 0.296352356672287 batch: 48/224\n",
      "Batch loss: 0.21662960946559906 batch: 49/224\n",
      "Batch loss: 0.23714378476142883 batch: 50/224\n",
      "Batch loss: 0.2349122166633606 batch: 51/224\n",
      "Batch loss: 0.24689020216464996 batch: 52/224\n",
      "Batch loss: 0.2986820936203003 batch: 53/224\n",
      "Batch loss: 0.26536428928375244 batch: 54/224\n",
      "Batch loss: 0.2246907502412796 batch: 55/224\n",
      "Batch loss: 0.23693503439426422 batch: 56/224\n",
      "Batch loss: 0.31593790650367737 batch: 57/224\n",
      "Batch loss: 0.24424920976161957 batch: 58/224\n",
      "Batch loss: 0.22363395988941193 batch: 59/224\n",
      "Batch loss: 0.31124529242515564 batch: 60/224\n",
      "Batch loss: 0.26650020480155945 batch: 61/224\n",
      "Batch loss: 0.23644734919071198 batch: 62/224\n",
      "Batch loss: 0.2643534541130066 batch: 63/224\n",
      "Batch loss: 0.28889432549476624 batch: 64/224\n",
      "Batch loss: 0.26027387380599976 batch: 65/224\n",
      "Batch loss: 0.2949477434158325 batch: 66/224\n",
      "Batch loss: 0.24315626919269562 batch: 67/224\n",
      "Batch loss: 0.26932767033576965 batch: 68/224\n",
      "Batch loss: 0.3082951307296753 batch: 69/224\n",
      "Batch loss: 0.25366535782814026 batch: 70/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.2547236680984497 batch: 71/224\n",
      "Batch loss: 0.1813538521528244 batch: 72/224\n",
      "Batch loss: 0.3081943988800049 batch: 73/224\n",
      "Batch loss: 0.2501492500305176 batch: 74/224\n",
      "Batch loss: 0.26416468620300293 batch: 75/224\n",
      "Batch loss: 0.3046988546848297 batch: 76/224\n",
      "Batch loss: 0.25124287605285645 batch: 77/224\n",
      "Batch loss: 0.27756229043006897 batch: 78/224\n",
      "Batch loss: 0.3138044774532318 batch: 79/224\n",
      "Batch loss: 0.2403661459684372 batch: 80/224\n",
      "Batch loss: 0.35641276836395264 batch: 81/224\n",
      "Batch loss: 0.29585352540016174 batch: 82/224\n",
      "Batch loss: 0.28431451320648193 batch: 83/224\n",
      "Batch loss: 0.23953120410442352 batch: 84/224\n",
      "Batch loss: 0.24604521691799164 batch: 85/224\n",
      "Batch loss: 0.291057825088501 batch: 86/224\n",
      "Batch loss: 0.2679595649242401 batch: 87/224\n",
      "Batch loss: 0.2871382236480713 batch: 88/224\n",
      "Batch loss: 0.26616108417510986 batch: 89/224\n",
      "Batch loss: 0.3155551254749298 batch: 90/224\n",
      "Batch loss: 0.2579241991043091 batch: 91/224\n",
      "Batch loss: 0.274309903383255 batch: 92/224\n",
      "Batch loss: 0.22019213438034058 batch: 93/224\n",
      "Batch loss: 0.2680431008338928 batch: 94/224\n",
      "Batch loss: 0.24336735904216766 batch: 95/224\n",
      "Batch loss: 0.21614710986614227 batch: 96/224\n",
      "Batch loss: 0.23968996107578278 batch: 97/224\n",
      "Batch loss: 0.21915440261363983 batch: 98/224\n",
      "Batch loss: 0.2587786912918091 batch: 99/224\n",
      "Batch loss: 0.22573167085647583 batch: 100/224\n",
      "Batch loss: 0.29881492257118225 batch: 101/224\n",
      "Batch loss: 0.25053033232688904 batch: 102/224\n",
      "Batch loss: 0.252850204706192 batch: 103/224\n",
      "Batch loss: 0.21973590552806854 batch: 104/224\n",
      "Batch loss: 0.24656061828136444 batch: 105/224\n",
      "Batch loss: 0.28177693486213684 batch: 106/224\n",
      "Batch loss: 0.23441466689109802 batch: 107/224\n",
      "Batch loss: 0.2479998916387558 batch: 108/224\n",
      "Batch loss: 0.24660934507846832 batch: 109/224\n",
      "Batch loss: 0.23353369534015656 batch: 110/224\n",
      "Batch loss: 0.26544609665870667 batch: 111/224\n",
      "Batch loss: 0.23287972807884216 batch: 112/224\n",
      "Batch loss: 0.2809894382953644 batch: 113/224\n",
      "Batch loss: 0.22969931364059448 batch: 114/224\n",
      "Batch loss: 0.21392329037189484 batch: 115/224\n",
      "Batch loss: 0.24913743138313293 batch: 116/224\n",
      "Batch loss: 0.21960969269275665 batch: 117/224\n",
      "Batch loss: 0.2831762731075287 batch: 118/224\n",
      "Batch loss: 0.2325192242860794 batch: 119/224\n",
      "Batch loss: 0.2592540979385376 batch: 120/224\n",
      "Batch loss: 0.23980270326137543 batch: 121/224\n",
      "Batch loss: 0.2539675533771515 batch: 122/224\n",
      "Batch loss: 0.21091553568840027 batch: 123/224\n",
      "Batch loss: 0.2381654530763626 batch: 124/224\n",
      "Batch loss: 0.20774781703948975 batch: 125/224\n",
      "Batch loss: 0.2998177707195282 batch: 126/224\n",
      "Batch loss: 0.2889804244041443 batch: 127/224\n",
      "Batch loss: 0.24736949801445007 batch: 128/224\n",
      "Batch loss: 0.27614811062812805 batch: 129/224\n",
      "Batch loss: 0.26610198616981506 batch: 130/224\n",
      "Batch loss: 0.2088463306427002 batch: 131/224\n",
      "Batch loss: 0.23408649861812592 batch: 132/224\n",
      "Batch loss: 0.2482515424489975 batch: 133/224\n",
      "Batch loss: 0.24623030424118042 batch: 134/224\n",
      "Batch loss: 0.25654610991477966 batch: 135/224\n",
      "Batch loss: 0.3034876585006714 batch: 136/224\n",
      "Batch loss: 0.2305292785167694 batch: 137/224\n",
      "Batch loss: 0.23703652620315552 batch: 138/224\n",
      "Batch loss: 0.30461928248405457 batch: 139/224\n",
      "Batch loss: 0.2778562009334564 batch: 140/224\n",
      "Batch loss: 0.220494344830513 batch: 141/224\n",
      "Batch loss: 0.22555361688137054 batch: 142/224\n",
      "Batch loss: 0.22032827138900757 batch: 143/224\n",
      "Batch loss: 0.25097671151161194 batch: 144/224\n",
      "Batch loss: 0.254862517118454 batch: 145/224\n",
      "Batch loss: 0.2775523364543915 batch: 146/224\n",
      "Batch loss: 0.24446047842502594 batch: 147/224\n",
      "Batch loss: 0.2500208914279938 batch: 148/224\n",
      "Batch loss: 0.25534194707870483 batch: 149/224\n",
      "Batch loss: 0.27449578046798706 batch: 150/224\n",
      "Batch loss: 0.24262095987796783 batch: 151/224\n",
      "Batch loss: 0.2302030622959137 batch: 152/224\n",
      "Batch loss: 0.29602888226509094 batch: 153/224\n",
      "Batch loss: 0.2844308018684387 batch: 154/224\n",
      "Batch loss: 0.2395203411579132 batch: 155/224\n",
      "Batch loss: 0.26755398511886597 batch: 156/224\n",
      "Batch loss: 0.2973509132862091 batch: 157/224\n",
      "Batch loss: 0.3422680199146271 batch: 158/224\n",
      "Batch loss: 0.24289129674434662 batch: 159/224\n",
      "Batch loss: 0.254772424697876 batch: 160/224\n",
      "Batch loss: 0.2329149693250656 batch: 161/224\n",
      "Batch loss: 0.2728116512298584 batch: 162/224\n",
      "Batch loss: 0.22772106528282166 batch: 163/224\n",
      "Batch loss: 0.2565012276172638 batch: 164/224\n",
      "Batch loss: 0.30155983567237854 batch: 165/224\n",
      "Batch loss: 0.2583097219467163 batch: 166/224\n",
      "Batch loss: 0.2343660444021225 batch: 167/224\n",
      "Batch loss: 0.2157992422580719 batch: 168/224\n",
      "Batch loss: 0.2601473927497864 batch: 169/224\n",
      "Batch loss: 0.26491180062294006 batch: 170/224\n",
      "Batch loss: 0.22484363615512848 batch: 171/224\n",
      "Batch loss: 0.24402423202991486 batch: 172/224\n",
      "Batch loss: 0.2667560279369354 batch: 173/224\n",
      "Batch loss: 0.22107988595962524 batch: 174/224\n",
      "Batch loss: 0.2235240489244461 batch: 175/224\n",
      "Batch loss: 0.2237999141216278 batch: 176/224\n",
      "Batch loss: 0.2795811891555786 batch: 177/224\n",
      "Batch loss: 0.23369067907333374 batch: 178/224\n",
      "Batch loss: 0.28318721055984497 batch: 179/224\n",
      "Batch loss: 0.22310742735862732 batch: 180/224\n",
      "Batch loss: 0.2653152644634247 batch: 181/224\n",
      "Batch loss: 0.294756144285202 batch: 182/224\n",
      "Batch loss: 0.30059677362442017 batch: 183/224\n",
      "Batch loss: 0.23102790117263794 batch: 184/224\n",
      "Batch loss: 0.2802342474460602 batch: 185/224\n",
      "Batch loss: 0.22540467977523804 batch: 186/224\n",
      "Batch loss: 0.24642673134803772 batch: 187/224\n",
      "Batch loss: 0.19746480882167816 batch: 188/224\n",
      "Batch loss: 0.2481832653284073 batch: 189/224\n",
      "Batch loss: 0.2856133282184601 batch: 190/224\n",
      "Batch loss: 0.265919953584671 batch: 191/224\n",
      "Batch loss: 0.3104541003704071 batch: 192/224\n",
      "Batch loss: 0.2776244282722473 batch: 193/224\n",
      "Batch loss: 0.2749670743942261 batch: 194/224\n",
      "Batch loss: 0.30167141556739807 batch: 195/224\n",
      "Batch loss: 0.28819912672042847 batch: 196/224\n",
      "Batch loss: 0.24941319227218628 batch: 197/224\n",
      "Batch loss: 0.26725974678993225 batch: 198/224\n",
      "Batch loss: 0.2274315059185028 batch: 199/224\n",
      "Batch loss: 0.2636687159538269 batch: 200/224\n",
      "Batch loss: 0.25864675641059875 batch: 201/224\n",
      "Batch loss: 0.26204732060432434 batch: 202/224\n",
      "Batch loss: 0.25471460819244385 batch: 203/224\n",
      "Batch loss: 0.23949065804481506 batch: 204/224\n",
      "Batch loss: 0.2644614279270172 batch: 205/224\n",
      "Batch loss: 0.22314807772636414 batch: 206/224\n",
      "Batch loss: 0.24574615061283112 batch: 207/224\n",
      "Batch loss: 0.242721825838089 batch: 208/224\n",
      "Batch loss: 0.25893479585647583 batch: 209/224\n",
      "Batch loss: 0.2513331174850464 batch: 210/224\n",
      "Batch loss: 0.2374141812324524 batch: 211/224\n",
      "Batch loss: 0.28906959295272827 batch: 212/224\n",
      "Batch loss: 0.27597707509994507 batch: 213/224\n",
      "Batch loss: 0.2833147943019867 batch: 214/224\n",
      "Batch loss: 0.28240564465522766 batch: 215/224\n",
      "Batch loss: 0.25523173809051514 batch: 216/224\n",
      "Batch loss: 0.296639621257782 batch: 217/224\n",
      "Batch loss: 0.24723558127880096 batch: 218/224\n",
      "Batch loss: 0.2201785147190094 batch: 219/224\n",
      "Batch loss: 0.20446285605430603 batch: 220/224\n",
      "Batch loss: 0.287031888961792 batch: 221/224\n",
      "Batch loss: 0.24354881048202515 batch: 222/224\n",
      "Batch loss: 0.21028435230255127 batch: 223/224\n",
      "Batch loss: 0.22387579083442688 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 17/75..  Training Loss: 0.00052..  Test Loss: 0.00065..  Test Accuracy: 0.88439\n",
      "Running epoch 18/75\n",
      "Batch loss: 0.22115938365459442 batch: 1/224\n",
      "Batch loss: 0.24567097425460815 batch: 2/224\n",
      "Batch loss: 0.2076834738254547 batch: 3/224\n",
      "Batch loss: 0.2837876081466675 batch: 4/224\n",
      "Batch loss: 0.24429482221603394 batch: 5/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.2746429145336151 batch: 6/224\n",
      "Batch loss: 0.23994790017604828 batch: 7/224\n",
      "Batch loss: 0.25280246138572693 batch: 8/224\n",
      "Batch loss: 0.22026681900024414 batch: 9/224\n",
      "Batch loss: 0.2349146455526352 batch: 10/224\n",
      "Batch loss: 0.2936272919178009 batch: 11/224\n",
      "Batch loss: 0.20997467637062073 batch: 12/224\n",
      "Batch loss: 0.19934998452663422 batch: 13/224\n",
      "Batch loss: 0.2592780292034149 batch: 14/224\n",
      "Batch loss: 0.26323217153549194 batch: 15/224\n",
      "Batch loss: 0.3517647683620453 batch: 16/224\n",
      "Batch loss: 0.22189782559871674 batch: 17/224\n",
      "Batch loss: 0.30186161398887634 batch: 18/224\n",
      "Batch loss: 0.22487464547157288 batch: 19/224\n",
      "Batch loss: 0.2181241661310196 batch: 20/224\n",
      "Batch loss: 0.26926228404045105 batch: 21/224\n",
      "Batch loss: 0.23402775824069977 batch: 22/224\n",
      "Batch loss: 0.29183128476142883 batch: 23/224\n",
      "Batch loss: 0.2863917648792267 batch: 24/224\n",
      "Batch loss: 0.2179066687822342 batch: 25/224\n",
      "Batch loss: 0.2172187864780426 batch: 26/224\n",
      "Batch loss: 0.25610822439193726 batch: 27/224\n",
      "Batch loss: 0.24888817965984344 batch: 28/224\n",
      "Batch loss: 0.30172181129455566 batch: 29/224\n",
      "Batch loss: 0.24936416745185852 batch: 30/224\n",
      "Batch loss: 0.22898989915847778 batch: 31/224\n",
      "Batch loss: 0.28430527448654175 batch: 32/224\n",
      "Batch loss: 0.22729536890983582 batch: 33/224\n",
      "Batch loss: 0.27472230792045593 batch: 34/224\n",
      "Batch loss: 0.2589913606643677 batch: 35/224\n",
      "Batch loss: 0.2785051763057709 batch: 36/224\n",
      "Batch loss: 0.2428274005651474 batch: 37/224\n",
      "Batch loss: 0.24557679891586304 batch: 38/224\n",
      "Batch loss: 0.26181039214134216 batch: 39/224\n",
      "Batch loss: 0.23909853398799896 batch: 40/224\n",
      "Batch loss: 0.2624211013317108 batch: 41/224\n",
      "Batch loss: 0.23897992074489594 batch: 42/224\n",
      "Batch loss: 0.2528255879878998 batch: 43/224\n",
      "Batch loss: 0.21093930304050446 batch: 44/224\n",
      "Batch loss: 0.23411951959133148 batch: 45/224\n",
      "Batch loss: 0.3106746971607208 batch: 46/224\n",
      "Batch loss: 0.29367291927337646 batch: 47/224\n",
      "Batch loss: 0.2436148226261139 batch: 48/224\n",
      "Batch loss: 0.1939389705657959 batch: 49/224\n",
      "Batch loss: 0.23428331315517426 batch: 50/224\n",
      "Batch loss: 0.24035805463790894 batch: 51/224\n",
      "Batch loss: 0.2302779108285904 batch: 52/224\n",
      "Batch loss: 0.2802523672580719 batch: 53/224\n",
      "Batch loss: 0.2303355187177658 batch: 54/224\n",
      "Batch loss: 0.24700124561786652 batch: 55/224\n",
      "Batch loss: 0.226415753364563 batch: 56/224\n",
      "Batch loss: 0.2893703281879425 batch: 57/224\n",
      "Batch loss: 0.23321780562400818 batch: 58/224\n",
      "Batch loss: 0.2405349314212799 batch: 59/224\n",
      "Batch loss: 0.3225855529308319 batch: 60/224\n",
      "Batch loss: 0.2694193124771118 batch: 61/224\n",
      "Batch loss: 0.2604731321334839 batch: 62/224\n",
      "Batch loss: 0.237803652882576 batch: 63/224\n",
      "Batch loss: 0.2541136145591736 batch: 64/224\n",
      "Batch loss: 0.24791376292705536 batch: 65/224\n",
      "Batch loss: 0.27022188901901245 batch: 66/224\n",
      "Batch loss: 0.246209979057312 batch: 67/224\n",
      "Batch loss: 0.2538852393627167 batch: 68/224\n",
      "Batch loss: 0.2765944302082062 batch: 69/224\n",
      "Batch loss: 0.2254091054201126 batch: 70/224\n",
      "Batch loss: 0.21960219740867615 batch: 71/224\n",
      "Batch loss: 0.20013511180877686 batch: 72/224\n",
      "Batch loss: 0.28506845235824585 batch: 73/224\n",
      "Batch loss: 0.2532353699207306 batch: 74/224\n",
      "Batch loss: 0.2666243314743042 batch: 75/224\n",
      "Batch loss: 0.3187292814254761 batch: 76/224\n",
      "Batch loss: 0.2656690180301666 batch: 77/224\n",
      "Batch loss: 0.2653123438358307 batch: 78/224\n",
      "Batch loss: 0.29223281145095825 batch: 79/224\n",
      "Batch loss: 0.261868953704834 batch: 80/224\n",
      "Batch loss: 0.3081629276275635 batch: 81/224\n",
      "Batch loss: 0.27508625388145447 batch: 82/224\n",
      "Batch loss: 0.2536868751049042 batch: 83/224\n",
      "Batch loss: 0.1989496499300003 batch: 84/224\n",
      "Batch loss: 0.25289422273635864 batch: 85/224\n",
      "Batch loss: 0.24777181446552277 batch: 86/224\n",
      "Batch loss: 0.25911858677864075 batch: 87/224\n",
      "Batch loss: 0.2988026738166809 batch: 88/224\n",
      "Batch loss: 0.24679236114025116 batch: 89/224\n",
      "Batch loss: 0.2904101610183716 batch: 90/224\n",
      "Batch loss: 0.24577949941158295 batch: 91/224\n",
      "Batch loss: 0.26967334747314453 batch: 92/224\n",
      "Batch loss: 0.20385247468948364 batch: 93/224\n",
      "Batch loss: 0.2522478997707367 batch: 94/224\n",
      "Batch loss: 0.20609880983829498 batch: 95/224\n",
      "Batch loss: 0.24033422768115997 batch: 96/224\n",
      "Batch loss: 0.24444614350795746 batch: 97/224\n",
      "Batch loss: 0.18553727865219116 batch: 98/224\n",
      "Batch loss: 0.2624705731868744 batch: 99/224\n",
      "Batch loss: 0.22014915943145752 batch: 100/224\n",
      "Batch loss: 0.2842058539390564 batch: 101/224\n",
      "Batch loss: 0.2626003324985504 batch: 102/224\n",
      "Batch loss: 0.26486918330192566 batch: 103/224\n",
      "Batch loss: 0.2187301218509674 batch: 104/224\n",
      "Batch loss: 0.19682377576828003 batch: 105/224\n",
      "Batch loss: 0.2615932524204254 batch: 106/224\n",
      "Batch loss: 0.23014038801193237 batch: 107/224\n",
      "Batch loss: 0.23772400617599487 batch: 108/224\n",
      "Batch loss: 0.22849152982234955 batch: 109/224\n",
      "Batch loss: 0.23998457193374634 batch: 110/224\n",
      "Batch loss: 0.2581251561641693 batch: 111/224\n",
      "Batch loss: 0.2162330001592636 batch: 112/224\n",
      "Batch loss: 0.2821858525276184 batch: 113/224\n",
      "Batch loss: 0.21951918303966522 batch: 114/224\n",
      "Batch loss: 0.24009376764297485 batch: 115/224\n",
      "Batch loss: 0.2117004245519638 batch: 116/224\n",
      "Batch loss: 0.22120362520217896 batch: 117/224\n",
      "Batch loss: 0.27014338970184326 batch: 118/224\n",
      "Batch loss: 0.2625180184841156 batch: 119/224\n",
      "Batch loss: 0.26503676176071167 batch: 120/224\n",
      "Batch loss: 0.22404393553733826 batch: 121/224\n",
      "Batch loss: 0.24861972033977509 batch: 122/224\n",
      "Batch loss: 0.1836547553539276 batch: 123/224\n",
      "Batch loss: 0.24261213839054108 batch: 124/224\n",
      "Batch loss: 0.2321980595588684 batch: 125/224\n",
      "Batch loss: 0.2583989202976227 batch: 126/224\n",
      "Batch loss: 0.2498849332332611 batch: 127/224\n",
      "Batch loss: 0.21388126909732819 batch: 128/224\n",
      "Batch loss: 0.2701437473297119 batch: 129/224\n",
      "Batch loss: 0.2531236708164215 batch: 130/224\n",
      "Batch loss: 0.2171773761510849 batch: 131/224\n",
      "Batch loss: 0.2283044159412384 batch: 132/224\n",
      "Batch loss: 0.2721923291683197 batch: 133/224\n",
      "Batch loss: 0.23037973046302795 batch: 134/224\n",
      "Batch loss: 0.2673458456993103 batch: 135/224\n",
      "Batch loss: 0.2922503352165222 batch: 136/224\n",
      "Batch loss: 0.26194098591804504 batch: 137/224\n",
      "Batch loss: 0.2760797441005707 batch: 138/224\n",
      "Batch loss: 0.2681080996990204 batch: 139/224\n",
      "Batch loss: 0.3167722821235657 batch: 140/224\n",
      "Batch loss: 0.2004302442073822 batch: 141/224\n",
      "Batch loss: 0.2194981575012207 batch: 142/224\n",
      "Batch loss: 0.256165087223053 batch: 143/224\n",
      "Batch loss: 0.2548638880252838 batch: 144/224\n",
      "Batch loss: 0.22451993823051453 batch: 145/224\n",
      "Batch loss: 0.2915112376213074 batch: 146/224\n",
      "Batch loss: 0.2068837732076645 batch: 147/224\n",
      "Batch loss: 0.25589820742607117 batch: 148/224\n",
      "Batch loss: 0.2685777544975281 batch: 149/224\n",
      "Batch loss: 0.2881120443344116 batch: 150/224\n",
      "Batch loss: 0.23946614563465118 batch: 151/224\n",
      "Batch loss: 0.2411511093378067 batch: 152/224\n",
      "Batch loss: 0.27536800503730774 batch: 153/224\n",
      "Batch loss: 0.28290826082229614 batch: 154/224\n",
      "Batch loss: 0.23335713148117065 batch: 155/224\n",
      "Batch loss: 0.2184700071811676 batch: 156/224\n",
      "Batch loss: 0.22366319596767426 batch: 157/224\n",
      "Batch loss: 0.3423956632614136 batch: 158/224\n",
      "Batch loss: 0.1927885115146637 batch: 159/224\n",
      "Batch loss: 0.2571832537651062 batch: 160/224\n",
      "Batch loss: 0.19240175187587738 batch: 161/224\n",
      "Batch loss: 0.24413248896598816 batch: 162/224\n",
      "Batch loss: 0.21346351504325867 batch: 163/224\n",
      "Batch loss: 0.22446493804454803 batch: 164/224\n",
      "Batch loss: 0.2981894314289093 batch: 165/224\n",
      "Batch loss: 0.2159309834241867 batch: 166/224\n",
      "Batch loss: 0.21303889155387878 batch: 167/224\n",
      "Batch loss: 0.21011826395988464 batch: 168/224\n",
      "Batch loss: 0.2543216943740845 batch: 169/224\n",
      "Batch loss: 0.24396516382694244 batch: 170/224\n",
      "Batch loss: 0.20911499857902527 batch: 171/224\n",
      "Batch loss: 0.230697363615036 batch: 172/224\n",
      "Batch loss: 0.23436671495437622 batch: 173/224\n",
      "Batch loss: 0.244100421667099 batch: 174/224\n",
      "Batch loss: 0.20563536882400513 batch: 175/224\n",
      "Batch loss: 0.2590903341770172 batch: 176/224\n",
      "Batch loss: 0.24492698907852173 batch: 177/224\n",
      "Batch loss: 0.22527004778385162 batch: 178/224\n",
      "Batch loss: 0.27527734637260437 batch: 179/224\n",
      "Batch loss: 0.16166076064109802 batch: 180/224\n",
      "Batch loss: 0.25448235869407654 batch: 181/224\n",
      "Batch loss: 0.24614816904067993 batch: 182/224\n",
      "Batch loss: 0.2848054766654968 batch: 183/224\n",
      "Batch loss: 0.24434183537960052 batch: 184/224\n",
      "Batch loss: 0.27924367785453796 batch: 185/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.209682434797287 batch: 186/224\n",
      "Batch loss: 0.2376544177532196 batch: 187/224\n",
      "Batch loss: 0.2147752344608307 batch: 188/224\n",
      "Batch loss: 0.25310856103897095 batch: 189/224\n",
      "Batch loss: 0.2696669101715088 batch: 190/224\n",
      "Batch loss: 0.21474890410900116 batch: 191/224\n",
      "Batch loss: 0.30059629678726196 batch: 192/224\n",
      "Batch loss: 0.2555011510848999 batch: 193/224\n",
      "Batch loss: 0.2620560824871063 batch: 194/224\n",
      "Batch loss: 0.29585182666778564 batch: 195/224\n",
      "Batch loss: 0.2862544357776642 batch: 196/224\n",
      "Batch loss: 0.25186067819595337 batch: 197/224\n",
      "Batch loss: 0.24018046259880066 batch: 198/224\n",
      "Batch loss: 0.2495381385087967 batch: 199/224\n",
      "Batch loss: 0.2753344476222992 batch: 200/224\n",
      "Batch loss: 0.28121307492256165 batch: 201/224\n",
      "Batch loss: 0.24090327322483063 batch: 202/224\n",
      "Batch loss: 0.2527717351913452 batch: 203/224\n",
      "Batch loss: 0.21631939709186554 batch: 204/224\n",
      "Batch loss: 0.2577589750289917 batch: 205/224\n",
      "Batch loss: 0.22474893927574158 batch: 206/224\n",
      "Batch loss: 0.2665313184261322 batch: 207/224\n",
      "Batch loss: 0.22744697332382202 batch: 208/224\n",
      "Batch loss: 0.23270608484745026 batch: 209/224\n",
      "Batch loss: 0.24485322833061218 batch: 210/224\n",
      "Batch loss: 0.23775482177734375 batch: 211/224\n",
      "Batch loss: 0.2708424925804138 batch: 212/224\n",
      "Batch loss: 0.28734326362609863 batch: 213/224\n",
      "Batch loss: 0.27595025300979614 batch: 214/224\n",
      "Batch loss: 0.2915654182434082 batch: 215/224\n",
      "Batch loss: 0.24727286398410797 batch: 216/224\n",
      "Batch loss: 0.2758282721042633 batch: 217/224\n",
      "Batch loss: 0.23932969570159912 batch: 218/224\n",
      "Batch loss: 0.20694944262504578 batch: 219/224\n",
      "Batch loss: 0.22385269403457642 batch: 220/224\n",
      "Batch loss: 0.29946368932724 batch: 221/224\n",
      "Batch loss: 0.23365607857704163 batch: 222/224\n",
      "Batch loss: 0.19336019456386566 batch: 223/224\n",
      "Batch loss: 0.22349610924720764 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 18/75..  Training Loss: 0.00050..  Test Loss: 0.00066..  Test Accuracy: 0.88386\n",
      "Running epoch 19/75\n",
      "Batch loss: 0.20266655087471008 batch: 1/224\n",
      "Batch loss: 0.2329811155796051 batch: 2/224\n",
      "Batch loss: 0.22512952983379364 batch: 3/224\n",
      "Batch loss: 0.27491679787635803 batch: 4/224\n",
      "Batch loss: 0.22918373346328735 batch: 5/224\n",
      "Batch loss: 0.28056764602661133 batch: 6/224\n",
      "Batch loss: 0.22673091292381287 batch: 7/224\n",
      "Batch loss: 0.21761679649353027 batch: 8/224\n",
      "Batch loss: 0.18600434064865112 batch: 9/224\n",
      "Batch loss: 0.20502392947673798 batch: 10/224\n",
      "Batch loss: 0.2822330892086029 batch: 11/224\n",
      "Batch loss: 0.21261504292488098 batch: 12/224\n",
      "Batch loss: 0.16838696599006653 batch: 13/224\n",
      "Batch loss: 0.2216649204492569 batch: 14/224\n",
      "Batch loss: 0.25849488377571106 batch: 15/224\n",
      "Batch loss: 0.3123841881752014 batch: 16/224\n",
      "Batch loss: 0.22154660522937775 batch: 17/224\n",
      "Batch loss: 0.2794485092163086 batch: 18/224\n",
      "Batch loss: 0.21471169590950012 batch: 19/224\n",
      "Batch loss: 0.21750947833061218 batch: 20/224\n",
      "Batch loss: 0.26278236508369446 batch: 21/224\n",
      "Batch loss: 0.20780472457408905 batch: 22/224\n",
      "Batch loss: 0.23468272387981415 batch: 23/224\n",
      "Batch loss: 0.30501797795295715 batch: 24/224\n",
      "Batch loss: 0.19574689865112305 batch: 25/224\n",
      "Batch loss: 0.19759152829647064 batch: 26/224\n",
      "Batch loss: 0.22255082428455353 batch: 27/224\n",
      "Batch loss: 0.25587499141693115 batch: 28/224\n",
      "Batch loss: 0.24919338524341583 batch: 29/224\n",
      "Batch loss: 0.2257913053035736 batch: 30/224\n",
      "Batch loss: 0.20089979469776154 batch: 31/224\n",
      "Batch loss: 0.2535415589809418 batch: 32/224\n",
      "Batch loss: 0.23453041911125183 batch: 33/224\n",
      "Batch loss: 0.22734472155570984 batch: 34/224\n",
      "Batch loss: 0.23836906254291534 batch: 35/224\n",
      "Batch loss: 0.28284746408462524 batch: 36/224\n",
      "Batch loss: 0.22202324867248535 batch: 37/224\n",
      "Batch loss: 0.2662394344806671 batch: 38/224\n",
      "Batch loss: 0.2668735384941101 batch: 39/224\n",
      "Batch loss: 0.2575922906398773 batch: 40/224\n",
      "Batch loss: 0.26102641224861145 batch: 41/224\n",
      "Batch loss: 0.23004768788814545 batch: 42/224\n",
      "Batch loss: 0.2527787983417511 batch: 43/224\n",
      "Batch loss: 0.19895492494106293 batch: 44/224\n",
      "Batch loss: 0.1820410043001175 batch: 45/224\n",
      "Batch loss: 0.2941860854625702 batch: 46/224\n",
      "Batch loss: 0.251650333404541 batch: 47/224\n",
      "Batch loss: 0.2113094925880432 batch: 48/224\n",
      "Batch loss: 0.19331039488315582 batch: 49/224\n",
      "Batch loss: 0.2344440221786499 batch: 50/224\n",
      "Batch loss: 0.2159242480993271 batch: 51/224\n",
      "Batch loss: 0.23624631762504578 batch: 52/224\n",
      "Batch loss: 0.31305593252182007 batch: 53/224\n",
      "Batch loss: 0.18920832872390747 batch: 54/224\n",
      "Batch loss: 0.20874041318893433 batch: 55/224\n",
      "Batch loss: 0.24936318397521973 batch: 56/224\n",
      "Batch loss: 0.26201924681663513 batch: 57/224\n",
      "Batch loss: 0.2496812641620636 batch: 58/224\n",
      "Batch loss: 0.20168043673038483 batch: 59/224\n",
      "Batch loss: 0.28997328877449036 batch: 60/224\n",
      "Batch loss: 0.2860037684440613 batch: 61/224\n",
      "Batch loss: 0.19256484508514404 batch: 62/224\n",
      "Batch loss: 0.25284844636917114 batch: 63/224\n",
      "Batch loss: 0.276349276304245 batch: 64/224\n",
      "Batch loss: 0.2607875466346741 batch: 65/224\n",
      "Batch loss: 0.25485411286354065 batch: 66/224\n",
      "Batch loss: 0.23662711679935455 batch: 67/224\n",
      "Batch loss: 0.2639518976211548 batch: 68/224\n",
      "Batch loss: 0.29916152358055115 batch: 69/224\n",
      "Batch loss: 0.2531253397464752 batch: 70/224\n",
      "Batch loss: 0.21630121767520905 batch: 71/224\n",
      "Batch loss: 0.16124950349330902 batch: 72/224\n",
      "Batch loss: 0.2918158173561096 batch: 73/224\n",
      "Batch loss: 0.26256921887397766 batch: 74/224\n",
      "Batch loss: 0.2871059775352478 batch: 75/224\n",
      "Batch loss: 0.29691872000694275 batch: 76/224\n",
      "Batch loss: 0.23251914978027344 batch: 77/224\n",
      "Batch loss: 0.2539459466934204 batch: 78/224\n",
      "Batch loss: 0.2858811020851135 batch: 79/224\n",
      "Batch loss: 0.2310442328453064 batch: 80/224\n",
      "Batch loss: 0.3083511292934418 batch: 81/224\n",
      "Batch loss: 0.267825722694397 batch: 82/224\n",
      "Batch loss: 0.27841803431510925 batch: 83/224\n",
      "Batch loss: 0.22491171956062317 batch: 84/224\n",
      "Batch loss: 0.2746294438838959 batch: 85/224\n",
      "Batch loss: 0.24460609257221222 batch: 86/224\n",
      "Batch loss: 0.24904842674732208 batch: 87/224\n",
      "Batch loss: 0.2524466812610626 batch: 88/224\n",
      "Batch loss: 0.23357607424259186 batch: 89/224\n",
      "Batch loss: 0.29504185914993286 batch: 90/224\n",
      "Batch loss: 0.2287941575050354 batch: 91/224\n",
      "Batch loss: 0.25348329544067383 batch: 92/224\n",
      "Batch loss: 0.21818815171718597 batch: 93/224\n",
      "Batch loss: 0.2361111044883728 batch: 94/224\n",
      "Batch loss: 0.2059362679719925 batch: 95/224\n",
      "Batch loss: 0.2025110125541687 batch: 96/224\n",
      "Batch loss: 0.2555468678474426 batch: 97/224\n",
      "Batch loss: 0.17002402245998383 batch: 98/224\n",
      "Batch loss: 0.22942021489143372 batch: 99/224\n",
      "Batch loss: 0.19809232652187347 batch: 100/224\n",
      "Batch loss: 0.26864659786224365 batch: 101/224\n",
      "Batch loss: 0.23763467371463776 batch: 102/224\n",
      "Batch loss: 0.26224711537361145 batch: 103/224\n",
      "Batch loss: 0.23044903576374054 batch: 104/224\n",
      "Batch loss: 0.20721842348575592 batch: 105/224\n",
      "Batch loss: 0.23404426872730255 batch: 106/224\n",
      "Batch loss: 0.20085409283638 batch: 107/224\n",
      "Batch loss: 0.25850456953048706 batch: 108/224\n",
      "Batch loss: 0.22569787502288818 batch: 109/224\n",
      "Batch loss: 0.21528519690036774 batch: 110/224\n",
      "Batch loss: 0.25262123346328735 batch: 111/224\n",
      "Batch loss: 0.22129881381988525 batch: 112/224\n",
      "Batch loss: 0.28913483023643494 batch: 113/224\n",
      "Batch loss: 0.22955918312072754 batch: 114/224\n",
      "Batch loss: 0.23770341277122498 batch: 115/224\n",
      "Batch loss: 0.20418260991573334 batch: 116/224\n",
      "Batch loss: 0.21164068579673767 batch: 117/224\n",
      "Batch loss: 0.24847795069217682 batch: 118/224\n",
      "Batch loss: 0.23191873729228973 batch: 119/224\n",
      "Batch loss: 0.2582648992538452 batch: 120/224\n",
      "Batch loss: 0.19384396076202393 batch: 121/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.28139859437942505 batch: 122/224\n",
      "Batch loss: 0.2343870997428894 batch: 123/224\n",
      "Batch loss: 0.23880496621131897 batch: 124/224\n",
      "Batch loss: 0.22302597761154175 batch: 125/224\n",
      "Batch loss: 0.21855378150939941 batch: 126/224\n",
      "Batch loss: 0.26260414719581604 batch: 127/224\n",
      "Batch loss: 0.23506253957748413 batch: 128/224\n",
      "Batch loss: 0.2518000602722168 batch: 129/224\n",
      "Batch loss: 0.2790704369544983 batch: 130/224\n",
      "Batch loss: 0.21164929866790771 batch: 131/224\n",
      "Batch loss: 0.20949913561344147 batch: 132/224\n",
      "Batch loss: 0.22422939538955688 batch: 133/224\n",
      "Batch loss: 0.2322150617837906 batch: 134/224\n",
      "Batch loss: 0.29474061727523804 batch: 135/224\n",
      "Batch loss: 0.25127243995666504 batch: 136/224\n",
      "Batch loss: 0.24134990572929382 batch: 137/224\n",
      "Batch loss: 0.22888067364692688 batch: 138/224\n",
      "Batch loss: 0.23514722287654877 batch: 139/224\n",
      "Batch loss: 0.30215001106262207 batch: 140/224\n",
      "Batch loss: 0.15106554329395294 batch: 141/224\n",
      "Batch loss: 0.24238233268260956 batch: 142/224\n",
      "Batch loss: 0.22020813822746277 batch: 143/224\n",
      "Batch loss: 0.28192150592803955 batch: 144/224\n",
      "Batch loss: 0.22627229988574982 batch: 145/224\n",
      "Batch loss: 0.26846563816070557 batch: 146/224\n",
      "Batch loss: 0.19929537177085876 batch: 147/224\n",
      "Batch loss: 0.2740013003349304 batch: 148/224\n",
      "Batch loss: 0.26935532689094543 batch: 149/224\n",
      "Batch loss: 0.2721889615058899 batch: 150/224\n",
      "Batch loss: 0.23712287843227386 batch: 151/224\n",
      "Batch loss: 0.2604884207248688 batch: 152/224\n",
      "Batch loss: 0.25304844975471497 batch: 153/224\n",
      "Batch loss: 0.30055323243141174 batch: 154/224\n",
      "Batch loss: 0.23935160040855408 batch: 155/224\n",
      "Batch loss: 0.18914368748664856 batch: 156/224\n",
      "Batch loss: 0.3023030459880829 batch: 157/224\n",
      "Batch loss: 0.3399530053138733 batch: 158/224\n",
      "Batch loss: 0.2063952386379242 batch: 159/224\n",
      "Batch loss: 0.21466876566410065 batch: 160/224\n",
      "Batch loss: 0.21653904020786285 batch: 161/224\n",
      "Batch loss: 0.24144627153873444 batch: 162/224\n",
      "Batch loss: 0.21656207740306854 batch: 163/224\n",
      "Batch loss: 0.23342318832874298 batch: 164/224\n",
      "Batch loss: 0.306045800447464 batch: 165/224\n",
      "Batch loss: 0.22824618220329285 batch: 166/224\n",
      "Batch loss: 0.201532244682312 batch: 167/224\n",
      "Batch loss: 0.1862703263759613 batch: 168/224\n",
      "Batch loss: 0.26163479685783386 batch: 169/224\n",
      "Batch loss: 0.2527538537979126 batch: 170/224\n",
      "Batch loss: 0.2389935553073883 batch: 171/224\n",
      "Batch loss: 0.22375009953975677 batch: 172/224\n",
      "Batch loss: 0.25114190578460693 batch: 173/224\n",
      "Batch loss: 0.21150900423526764 batch: 174/224\n",
      "Batch loss: 0.19756822288036346 batch: 175/224\n",
      "Batch loss: 0.22830933332443237 batch: 176/224\n",
      "Batch loss: 0.22483818233013153 batch: 177/224\n",
      "Batch loss: 0.24507904052734375 batch: 178/224\n",
      "Batch loss: 0.256458044052124 batch: 179/224\n",
      "Batch loss: 0.213225856423378 batch: 180/224\n",
      "Batch loss: 0.27755728363990784 batch: 181/224\n",
      "Batch loss: 0.24498601257801056 batch: 182/224\n",
      "Batch loss: 0.2620021402835846 batch: 183/224\n",
      "Batch loss: 0.2457033097743988 batch: 184/224\n",
      "Batch loss: 0.24780628085136414 batch: 185/224\n",
      "Batch loss: 0.22385796904563904 batch: 186/224\n",
      "Batch loss: 0.2337009757757187 batch: 187/224\n",
      "Batch loss: 0.21650011837482452 batch: 188/224\n",
      "Batch loss: 0.27506110072135925 batch: 189/224\n",
      "Batch loss: 0.274770587682724 batch: 190/224\n",
      "Batch loss: 0.24259020388126373 batch: 191/224\n",
      "Batch loss: 0.2779276371002197 batch: 192/224\n",
      "Batch loss: 0.2351856380701065 batch: 193/224\n",
      "Batch loss: 0.2508394718170166 batch: 194/224\n",
      "Batch loss: 0.2717133164405823 batch: 195/224\n",
      "Batch loss: 0.2580931484699249 batch: 196/224\n",
      "Batch loss: 0.24965573847293854 batch: 197/224\n",
      "Batch loss: 0.25466322898864746 batch: 198/224\n",
      "Batch loss: 0.24934211373329163 batch: 199/224\n",
      "Batch loss: 0.25275376439094543 batch: 200/224\n",
      "Batch loss: 0.2667996287345886 batch: 201/224\n",
      "Batch loss: 0.2587270140647888 batch: 202/224\n",
      "Batch loss: 0.25965914130210876 batch: 203/224\n",
      "Batch loss: 0.2601335644721985 batch: 204/224\n",
      "Batch loss: 0.267160028219223 batch: 205/224\n",
      "Batch loss: 0.2459331601858139 batch: 206/224\n",
      "Batch loss: 0.2308545559644699 batch: 207/224\n",
      "Batch loss: 0.20848006010055542 batch: 208/224\n",
      "Batch loss: 0.23302319645881653 batch: 209/224\n",
      "Batch loss: 0.24639393389225006 batch: 210/224\n",
      "Batch loss: 0.24246753752231598 batch: 211/224\n",
      "Batch loss: 0.2729640603065491 batch: 212/224\n",
      "Batch loss: 0.2518470287322998 batch: 213/224\n",
      "Batch loss: 0.319906085729599 batch: 214/224\n",
      "Batch loss: 0.26799246668815613 batch: 215/224\n",
      "Batch loss: 0.2294733077287674 batch: 216/224\n",
      "Batch loss: 0.26331955194473267 batch: 217/224\n",
      "Batch loss: 0.2276657074689865 batch: 218/224\n",
      "Batch loss: 0.18741504848003387 batch: 219/224\n",
      "Batch loss: 0.19306321442127228 batch: 220/224\n",
      "Batch loss: 0.28004980087280273 batch: 221/224\n",
      "Batch loss: 0.22178462147712708 batch: 222/224\n",
      "Batch loss: 0.19187723100185394 batch: 223/224\n",
      "Batch loss: 0.2108374834060669 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 19/75..  Training Loss: 0.00048..  Test Loss: 0.00066..  Test Accuracy: 0.88421\n",
      "Running epoch 20/75\n",
      "Batch loss: 0.187889963388443 batch: 1/224\n",
      "Batch loss: 0.2292022705078125 batch: 2/224\n",
      "Batch loss: 0.1932729035615921 batch: 3/224\n",
      "Batch loss: 0.23886890709400177 batch: 4/224\n",
      "Batch loss: 0.23599080741405487 batch: 5/224\n",
      "Batch loss: 0.25586870312690735 batch: 6/224\n",
      "Batch loss: 0.20860174298286438 batch: 7/224\n",
      "Batch loss: 0.25321894884109497 batch: 8/224\n",
      "Batch loss: 0.1907561719417572 batch: 9/224\n",
      "Batch loss: 0.22587141394615173 batch: 10/224\n",
      "Batch loss: 0.2834717929363251 batch: 11/224\n",
      "Batch loss: 0.19152982532978058 batch: 12/224\n",
      "Batch loss: 0.17104224860668182 batch: 13/224\n",
      "Batch loss: 0.2076333612203598 batch: 14/224\n",
      "Batch loss: 0.23694205284118652 batch: 15/224\n",
      "Batch loss: 0.28957897424697876 batch: 16/224\n",
      "Batch loss: 0.216010183095932 batch: 17/224\n",
      "Batch loss: 0.24084432423114777 batch: 18/224\n",
      "Batch loss: 0.20710861682891846 batch: 19/224\n",
      "Batch loss: 0.20867550373077393 batch: 20/224\n",
      "Batch loss: 0.2607280910015106 batch: 21/224\n",
      "Batch loss: 0.22592344880104065 batch: 22/224\n",
      "Batch loss: 0.29265084862709045 batch: 23/224\n",
      "Batch loss: 0.30347496271133423 batch: 24/224\n",
      "Batch loss: 0.1956455558538437 batch: 25/224\n",
      "Batch loss: 0.19342012703418732 batch: 26/224\n",
      "Batch loss: 0.22432658076286316 batch: 27/224\n",
      "Batch loss: 0.23802144825458527 batch: 28/224\n",
      "Batch loss: 0.2576727867126465 batch: 29/224\n",
      "Batch loss: 0.23130223155021667 batch: 30/224\n",
      "Batch loss: 0.20357997715473175 batch: 31/224\n",
      "Batch loss: 0.24715366959571838 batch: 32/224\n",
      "Batch loss: 0.1990133672952652 batch: 33/224\n",
      "Batch loss: 0.22179226577281952 batch: 34/224\n",
      "Batch loss: 0.2407451719045639 batch: 35/224\n",
      "Batch loss: 0.22367286682128906 batch: 36/224\n",
      "Batch loss: 0.2469772845506668 batch: 37/224\n",
      "Batch loss: 0.26852947473526 batch: 38/224\n",
      "Batch loss: 0.237762451171875 batch: 39/224\n",
      "Batch loss: 0.23515558242797852 batch: 40/224\n",
      "Batch loss: 0.2641390264034271 batch: 41/224\n",
      "Batch loss: 0.20986449718475342 batch: 42/224\n",
      "Batch loss: 0.25439074635505676 batch: 43/224\n",
      "Batch loss: 0.20002786815166473 batch: 44/224\n",
      "Batch loss: 0.22995997965335846 batch: 45/224\n",
      "Batch loss: 0.2863346338272095 batch: 46/224\n",
      "Batch loss: 0.25766703486442566 batch: 47/224\n",
      "Batch loss: 0.25041112303733826 batch: 48/224\n",
      "Batch loss: 0.21565601229667664 batch: 49/224\n",
      "Batch loss: 0.19883780181407928 batch: 50/224\n",
      "Batch loss: 0.19276063144207 batch: 51/224\n",
      "Batch loss: 0.23142406344413757 batch: 52/224\n",
      "Batch loss: 0.3098680078983307 batch: 53/224\n",
      "Batch loss: 0.19834651052951813 batch: 54/224\n",
      "Batch loss: 0.22063381969928741 batch: 55/224\n",
      "Batch loss: 0.2187291830778122 batch: 56/224\n",
      "Batch loss: 0.2971865236759186 batch: 57/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.2514936625957489 batch: 58/224\n",
      "Batch loss: 0.21918703615665436 batch: 59/224\n",
      "Batch loss: 0.2629499137401581 batch: 60/224\n",
      "Batch loss: 0.2609691917896271 batch: 61/224\n",
      "Batch loss: 0.19587604701519012 batch: 62/224\n",
      "Batch loss: 0.25355958938598633 batch: 63/224\n",
      "Batch loss: 0.2681230902671814 batch: 64/224\n",
      "Batch loss: 0.25132235884666443 batch: 65/224\n",
      "Batch loss: 0.2515256702899933 batch: 66/224\n",
      "Batch loss: 0.23147444427013397 batch: 67/224\n",
      "Batch loss: 0.23575668036937714 batch: 68/224\n",
      "Batch loss: 0.2926945388317108 batch: 69/224\n",
      "Batch loss: 0.2516923248767853 batch: 70/224\n",
      "Batch loss: 0.21896837651729584 batch: 71/224\n",
      "Batch loss: 0.16409428417682648 batch: 72/224\n",
      "Batch loss: 0.287007600069046 batch: 73/224\n",
      "Batch loss: 0.24201756715774536 batch: 74/224\n",
      "Batch loss: 0.24439531564712524 batch: 75/224\n",
      "Batch loss: 0.31850728392601013 batch: 76/224\n",
      "Batch loss: 0.22781188786029816 batch: 77/224\n",
      "Batch loss: 0.23909682035446167 batch: 78/224\n",
      "Batch loss: 0.2329815775156021 batch: 79/224\n",
      "Batch loss: 0.236070454120636 batch: 80/224\n",
      "Batch loss: 0.30889156460762024 batch: 81/224\n",
      "Batch loss: 0.27919116616249084 batch: 82/224\n",
      "Batch loss: 0.21074320375919342 batch: 83/224\n",
      "Batch loss: 0.17788013815879822 batch: 84/224\n",
      "Batch loss: 0.2225659042596817 batch: 85/224\n",
      "Batch loss: 0.22428962588310242 batch: 86/224\n",
      "Batch loss: 0.26169073581695557 batch: 87/224\n",
      "Batch loss: 0.2516377866268158 batch: 88/224\n",
      "Batch loss: 0.23399955034255981 batch: 89/224\n",
      "Batch loss: 0.2973841726779938 batch: 90/224\n",
      "Batch loss: 0.2507406771183014 batch: 91/224\n",
      "Batch loss: 0.2497500628232956 batch: 92/224\n",
      "Batch loss: 0.21154004335403442 batch: 93/224\n",
      "Batch loss: 0.21500374376773834 batch: 94/224\n",
      "Batch loss: 0.19722171127796173 batch: 95/224\n",
      "Batch loss: 0.21965348720550537 batch: 96/224\n",
      "Batch loss: 0.2476026564836502 batch: 97/224\n",
      "Batch loss: 0.20168891549110413 batch: 98/224\n",
      "Batch loss: 0.2644597589969635 batch: 99/224\n",
      "Batch loss: 0.17788812518119812 batch: 100/224\n",
      "Batch loss: 0.2686820328235626 batch: 101/224\n",
      "Batch loss: 0.2358533889055252 batch: 102/224\n",
      "Batch loss: 0.24123048782348633 batch: 103/224\n",
      "Batch loss: 0.24594713747501373 batch: 104/224\n",
      "Batch loss: 0.16771873831748962 batch: 105/224\n",
      "Batch loss: 0.20138278603553772 batch: 106/224\n",
      "Batch loss: 0.18352657556533813 batch: 107/224\n",
      "Batch loss: 0.22110530734062195 batch: 108/224\n",
      "Batch loss: 0.19964075088500977 batch: 109/224\n",
      "Batch loss: 0.2348213493824005 batch: 110/224\n",
      "Batch loss: 0.22248682379722595 batch: 111/224\n",
      "Batch loss: 0.19470159709453583 batch: 112/224\n",
      "Batch loss: 0.27359285950660706 batch: 113/224\n",
      "Batch loss: 0.19620439410209656 batch: 114/224\n",
      "Batch loss: 0.1967836320400238 batch: 115/224\n",
      "Batch loss: 0.22349953651428223 batch: 116/224\n",
      "Batch loss: 0.2029094696044922 batch: 117/224\n",
      "Batch loss: 0.26192477345466614 batch: 118/224\n",
      "Batch loss: 0.23576286435127258 batch: 119/224\n",
      "Batch loss: 0.2492975890636444 batch: 120/224\n",
      "Batch loss: 0.21034754812717438 batch: 121/224\n",
      "Batch loss: 0.24966755509376526 batch: 122/224\n",
      "Batch loss: 0.18989236652851105 batch: 123/224\n",
      "Batch loss: 0.21280470490455627 batch: 124/224\n",
      "Batch loss: 0.22220055758953094 batch: 125/224\n",
      "Batch loss: 0.22089321911334991 batch: 126/224\n",
      "Batch loss: 0.28247955441474915 batch: 127/224\n",
      "Batch loss: 0.20219704508781433 batch: 128/224\n",
      "Batch loss: 0.2733055651187897 batch: 129/224\n",
      "Batch loss: 0.21751385927200317 batch: 130/224\n",
      "Batch loss: 0.1718531996011734 batch: 131/224\n",
      "Batch loss: 0.1951664537191391 batch: 132/224\n",
      "Batch loss: 0.2166135460138321 batch: 133/224\n",
      "Batch loss: 0.22509045898914337 batch: 134/224\n",
      "Batch loss: 0.2373472899198532 batch: 135/224\n",
      "Batch loss: 0.23249804973602295 batch: 136/224\n",
      "Batch loss: 0.2224670648574829 batch: 137/224\n",
      "Batch loss: 0.19543053209781647 batch: 138/224\n",
      "Batch loss: 0.24257969856262207 batch: 139/224\n",
      "Batch loss: 0.2932683825492859 batch: 140/224\n",
      "Batch loss: 0.18471959233283997 batch: 141/224\n",
      "Batch loss: 0.21832147240638733 batch: 142/224\n",
      "Batch loss: 0.23870278894901276 batch: 143/224\n",
      "Batch loss: 0.22163014113903046 batch: 144/224\n",
      "Batch loss: 0.20050117373466492 batch: 145/224\n",
      "Batch loss: 0.2679249346256256 batch: 146/224\n",
      "Batch loss: 0.20676499605178833 batch: 147/224\n",
      "Batch loss: 0.2479938417673111 batch: 148/224\n",
      "Batch loss: 0.26223060488700867 batch: 149/224\n",
      "Batch loss: 0.24944791197776794 batch: 150/224\n",
      "Batch loss: 0.21426936984062195 batch: 151/224\n",
      "Batch loss: 0.24154578149318695 batch: 152/224\n",
      "Batch loss: 0.2687568664550781 batch: 153/224\n",
      "Batch loss: 0.23903010785579681 batch: 154/224\n",
      "Batch loss: 0.24680519104003906 batch: 155/224\n",
      "Batch loss: 0.2370789349079132 batch: 156/224\n",
      "Batch loss: 0.25738558173179626 batch: 157/224\n",
      "Batch loss: 0.2992047965526581 batch: 158/224\n",
      "Batch loss: 0.21311120688915253 batch: 159/224\n",
      "Batch loss: 0.27183204889297485 batch: 160/224\n",
      "Batch loss: 0.18750301003456116 batch: 161/224\n",
      "Batch loss: 0.22580774128437042 batch: 162/224\n",
      "Batch loss: 0.2099004089832306 batch: 163/224\n",
      "Batch loss: 0.1955721229314804 batch: 164/224\n",
      "Batch loss: 0.2728801965713501 batch: 165/224\n",
      "Batch loss: 0.22979554533958435 batch: 166/224\n",
      "Batch loss: 0.16242916882038116 batch: 167/224\n",
      "Batch loss: 0.2106352150440216 batch: 168/224\n",
      "Batch loss: 0.23350638151168823 batch: 169/224\n",
      "Batch loss: 0.2346985936164856 batch: 170/224\n",
      "Batch loss: 0.22334307432174683 batch: 171/224\n",
      "Batch loss: 0.2202853262424469 batch: 172/224\n",
      "Batch loss: 0.23377065360546112 batch: 173/224\n",
      "Batch loss: 0.24496114253997803 batch: 174/224\n",
      "Batch loss: 0.23130588233470917 batch: 175/224\n",
      "Batch loss: 0.19898396730422974 batch: 176/224\n",
      "Batch loss: 0.2522944211959839 batch: 177/224\n",
      "Batch loss: 0.22194159030914307 batch: 178/224\n",
      "Batch loss: 0.2583739161491394 batch: 179/224\n",
      "Batch loss: 0.1694084256887436 batch: 180/224\n",
      "Batch loss: 0.2388746589422226 batch: 181/224\n",
      "Batch loss: 0.2745259404182434 batch: 182/224\n",
      "Batch loss: 0.2629283666610718 batch: 183/224\n",
      "Batch loss: 0.2126275897026062 batch: 184/224\n",
      "Batch loss: 0.2838245630264282 batch: 185/224\n",
      "Batch loss: 0.21048076450824738 batch: 186/224\n",
      "Batch loss: 0.24715901911258698 batch: 187/224\n",
      "Batch loss: 0.19552545249462128 batch: 188/224\n",
      "Batch loss: 0.22024738788604736 batch: 189/224\n",
      "Batch loss: 0.23858396708965302 batch: 190/224\n",
      "Batch loss: 0.21650978922843933 batch: 191/224\n",
      "Batch loss: 0.26465946435928345 batch: 192/224\n",
      "Batch loss: 0.25456467270851135 batch: 193/224\n",
      "Batch loss: 0.2683769762516022 batch: 194/224\n",
      "Batch loss: 0.26343631744384766 batch: 195/224\n",
      "Batch loss: 0.26392391324043274 batch: 196/224\n",
      "Batch loss: 0.2335646003484726 batch: 197/224\n",
      "Batch loss: 0.21814018487930298 batch: 198/224\n",
      "Batch loss: 0.20760373771190643 batch: 199/224\n",
      "Batch loss: 0.2692180573940277 batch: 200/224\n",
      "Batch loss: 0.24543647468090057 batch: 201/224\n",
      "Batch loss: 0.25413399934768677 batch: 202/224\n",
      "Batch loss: 0.26824289560317993 batch: 203/224\n",
      "Batch loss: 0.22029776871204376 batch: 204/224\n",
      "Batch loss: 0.2461775690317154 batch: 205/224\n",
      "Batch loss: 0.225187286734581 batch: 206/224\n",
      "Batch loss: 0.23934486508369446 batch: 207/224\n",
      "Batch loss: 0.19419175386428833 batch: 208/224\n",
      "Batch loss: 0.22522422671318054 batch: 209/224\n",
      "Batch loss: 0.2253417670726776 batch: 210/224\n",
      "Batch loss: 0.17979492247104645 batch: 211/224\n",
      "Batch loss: 0.25768136978149414 batch: 212/224\n",
      "Batch loss: 0.22803890705108643 batch: 213/224\n",
      "Batch loss: 0.26616862416267395 batch: 214/224\n",
      "Batch loss: 0.24369725584983826 batch: 215/224\n",
      "Batch loss: 0.22255480289459229 batch: 216/224\n",
      "Batch loss: 0.24914632737636566 batch: 217/224\n",
      "Batch loss: 0.21205878257751465 batch: 218/224\n",
      "Batch loss: 0.16150501370429993 batch: 219/224\n",
      "Batch loss: 0.19406074285507202 batch: 220/224\n",
      "Batch loss: 0.2684294581413269 batch: 221/224\n",
      "Batch loss: 0.22204580903053284 batch: 222/224\n",
      "Batch loss: 0.210362508893013 batch: 223/224\n",
      "Batch loss: 0.20192794501781464 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 20/75..  Training Loss: 0.00046..  Test Loss: 0.00066..  Test Accuracy: 0.88732\n",
      "Running epoch 21/75\n",
      "Batch loss: 0.18098333477973938 batch: 1/224\n",
      "Batch loss: 0.2196619212627411 batch: 2/224\n",
      "Batch loss: 0.21746519207954407 batch: 3/224\n",
      "Batch loss: 0.24042998254299164 batch: 4/224\n",
      "Batch loss: 0.21813562512397766 batch: 5/224\n",
      "Batch loss: 0.21257582306861877 batch: 6/224\n",
      "Batch loss: 0.20424297451972961 batch: 7/224\n",
      "Batch loss: 0.277749240398407 batch: 8/224\n",
      "Batch loss: 0.184175044298172 batch: 9/224\n",
      "Batch loss: 0.2381310611963272 batch: 10/224\n",
      "Batch loss: 0.25876981019973755 batch: 11/224\n",
      "Batch loss: 0.21856480836868286 batch: 12/224\n",
      "Batch loss: 0.19094620645046234 batch: 13/224\n",
      "Batch loss: 0.20332446694374084 batch: 14/224\n",
      "Batch loss: 0.21151019632816315 batch: 15/224\n",
      "Batch loss: 0.30822572112083435 batch: 16/224\n",
      "Batch loss: 0.21336716413497925 batch: 17/224\n",
      "Batch loss: 0.26347076892852783 batch: 18/224\n",
      "Batch loss: 0.1956513673067093 batch: 19/224\n",
      "Batch loss: 0.20465700328350067 batch: 20/224\n",
      "Batch loss: 0.2716352939605713 batch: 21/224\n",
      "Batch loss: 0.22134532034397125 batch: 22/224\n",
      "Batch loss: 0.22182847559452057 batch: 23/224\n",
      "Batch loss: 0.25318318605422974 batch: 24/224\n",
      "Batch loss: 0.18812143802642822 batch: 25/224\n",
      "Batch loss: 0.1888708919286728 batch: 26/224\n",
      "Batch loss: 0.22112254798412323 batch: 27/224\n",
      "Batch loss: 0.2245006114244461 batch: 28/224\n",
      "Batch loss: 0.24465486407279968 batch: 29/224\n",
      "Batch loss: 0.23513206839561462 batch: 30/224\n",
      "Batch loss: 0.24975676834583282 batch: 31/224\n",
      "Batch loss: 0.2914040684700012 batch: 32/224\n",
      "Batch loss: 0.2027815729379654 batch: 33/224\n",
      "Batch loss: 0.22829067707061768 batch: 34/224\n",
      "Batch loss: 0.2555137276649475 batch: 35/224\n",
      "Batch loss: 0.24628692865371704 batch: 36/224\n",
      "Batch loss: 0.22183053195476532 batch: 37/224\n",
      "Batch loss: 0.22046437859535217 batch: 38/224\n",
      "Batch loss: 0.22772188484668732 batch: 39/224\n",
      "Batch loss: 0.2150190770626068 batch: 40/224\n",
      "Batch loss: 0.26781415939331055 batch: 41/224\n",
      "Batch loss: 0.2365298718214035 batch: 42/224\n",
      "Batch loss: 0.2388903945684433 batch: 43/224\n",
      "Batch loss: 0.19114361703395844 batch: 44/224\n",
      "Batch loss: 0.18527193367481232 batch: 45/224\n",
      "Batch loss: 0.2362602949142456 batch: 46/224\n",
      "Batch loss: 0.23832803964614868 batch: 47/224\n",
      "Batch loss: 0.19834332168102264 batch: 48/224\n",
      "Batch loss: 0.1652202308177948 batch: 49/224\n",
      "Batch loss: 0.2244989275932312 batch: 50/224\n",
      "Batch loss: 0.2295747697353363 batch: 51/224\n",
      "Batch loss: 0.21208767592906952 batch: 52/224\n",
      "Batch loss: 0.28491824865341187 batch: 53/224\n",
      "Batch loss: 0.1713656187057495 batch: 54/224\n",
      "Batch loss: 0.19225630164146423 batch: 55/224\n",
      "Batch loss: 0.20798315107822418 batch: 56/224\n",
      "Batch loss: 0.2732585370540619 batch: 57/224\n",
      "Batch loss: 0.2097880244255066 batch: 58/224\n",
      "Batch loss: 0.22183063626289368 batch: 59/224\n",
      "Batch loss: 0.24995145201683044 batch: 60/224\n",
      "Batch loss: 0.21587027609348297 batch: 61/224\n",
      "Batch loss: 0.19879759848117828 batch: 62/224\n",
      "Batch loss: 0.23298054933547974 batch: 63/224\n",
      "Batch loss: 0.28453683853149414 batch: 64/224\n",
      "Batch loss: 0.23013173043727875 batch: 65/224\n",
      "Batch loss: 0.2339002788066864 batch: 66/224\n",
      "Batch loss: 0.21643468737602234 batch: 67/224\n",
      "Batch loss: 0.2362011820077896 batch: 68/224\n",
      "Batch loss: 0.263123482465744 batch: 69/224\n",
      "Batch loss: 0.22238041460514069 batch: 70/224\n",
      "Batch loss: 0.1916152536869049 batch: 71/224\n",
      "Batch loss: 0.17737334966659546 batch: 72/224\n",
      "Batch loss: 0.2664388418197632 batch: 73/224\n",
      "Batch loss: 0.2165377289056778 batch: 74/224\n",
      "Batch loss: 0.24240854382514954 batch: 75/224\n",
      "Batch loss: 0.28466710448265076 batch: 76/224\n",
      "Batch loss: 0.23212315142154694 batch: 77/224\n",
      "Batch loss: 0.24524030089378357 batch: 78/224\n",
      "Batch loss: 0.2632257044315338 batch: 79/224\n",
      "Batch loss: 0.252137154340744 batch: 80/224\n",
      "Batch loss: 0.26849284768104553 batch: 81/224\n",
      "Batch loss: 0.23187634348869324 batch: 82/224\n",
      "Batch loss: 0.23104524612426758 batch: 83/224\n",
      "Batch loss: 0.17470739781856537 batch: 84/224\n",
      "Batch loss: 0.23044529557228088 batch: 85/224\n",
      "Batch loss: 0.24036556482315063 batch: 86/224\n",
      "Batch loss: 0.24378980696201324 batch: 87/224\n",
      "Batch loss: 0.2606063783168793 batch: 88/224\n",
      "Batch loss: 0.2077074646949768 batch: 89/224\n",
      "Batch loss: 0.24158966541290283 batch: 90/224\n",
      "Batch loss: 0.20704662799835205 batch: 91/224\n",
      "Batch loss: 0.23854154348373413 batch: 92/224\n",
      "Batch loss: 0.21367670595645905 batch: 93/224\n",
      "Batch loss: 0.20888033509254456 batch: 94/224\n",
      "Batch loss: 0.18988044559955597 batch: 95/224\n",
      "Batch loss: 0.203846275806427 batch: 96/224\n",
      "Batch loss: 0.20661237835884094 batch: 97/224\n",
      "Batch loss: 0.16488449275493622 batch: 98/224\n",
      "Batch loss: 0.22988271713256836 batch: 99/224\n",
      "Batch loss: 0.21980535984039307 batch: 100/224\n",
      "Batch loss: 0.28491950035095215 batch: 101/224\n",
      "Batch loss: 0.23264670372009277 batch: 102/224\n",
      "Batch loss: 0.22011463344097137 batch: 103/224\n",
      "Batch loss: 0.205235555768013 batch: 104/224\n",
      "Batch loss: 0.15660697221755981 batch: 105/224\n",
      "Batch loss: 0.20818457007408142 batch: 106/224\n",
      "Batch loss: 0.16322411596775055 batch: 107/224\n",
      "Batch loss: 0.22624434530735016 batch: 108/224\n",
      "Batch loss: 0.2040935903787613 batch: 109/224\n",
      "Batch loss: 0.21302936971187592 batch: 110/224\n",
      "Batch loss: 0.2458132654428482 batch: 111/224\n",
      "Batch loss: 0.21553494036197662 batch: 112/224\n",
      "Batch loss: 0.2652255892753601 batch: 113/224\n",
      "Batch loss: 0.23291243612766266 batch: 114/224\n",
      "Batch loss: 0.21869951486587524 batch: 115/224\n",
      "Batch loss: 0.19830326735973358 batch: 116/224\n",
      "Batch loss: 0.20961196720600128 batch: 117/224\n",
      "Batch loss: 0.22825117409229279 batch: 118/224\n",
      "Batch loss: 0.232049822807312 batch: 119/224\n",
      "Batch loss: 0.22998358309268951 batch: 120/224\n",
      "Batch loss: 0.17180117964744568 batch: 121/224\n",
      "Batch loss: 0.2511994242668152 batch: 122/224\n",
      "Batch loss: 0.19034726917743683 batch: 123/224\n",
      "Batch loss: 0.1864166408777237 batch: 124/224\n",
      "Batch loss: 0.22296923398971558 batch: 125/224\n",
      "Batch loss: 0.23653560876846313 batch: 126/224\n",
      "Batch loss: 0.2502443492412567 batch: 127/224\n",
      "Batch loss: 0.20595940947532654 batch: 128/224\n",
      "Batch loss: 0.20922259986400604 batch: 129/224\n",
      "Batch loss: 0.23287968337535858 batch: 130/224\n",
      "Batch loss: 0.1625455915927887 batch: 131/224\n",
      "Batch loss: 0.19881321489810944 batch: 132/224\n",
      "Batch loss: 0.19457820057868958 batch: 133/224\n",
      "Batch loss: 0.2386007159948349 batch: 134/224\n",
      "Batch loss: 0.25544798374176025 batch: 135/224\n",
      "Batch loss: 0.2334185242652893 batch: 136/224\n",
      "Batch loss: 0.19997455179691315 batch: 137/224\n",
      "Batch loss: 0.2513677775859833 batch: 138/224\n",
      "Batch loss: 0.26259398460388184 batch: 139/224\n",
      "Batch loss: 0.2606087625026703 batch: 140/224\n",
      "Batch loss: 0.1589236706495285 batch: 141/224\n",
      "Batch loss: 0.20506326854228973 batch: 142/224\n",
      "Batch loss: 0.2252824753522873 batch: 143/224\n",
      "Batch loss: 0.22286632657051086 batch: 144/224\n",
      "Batch loss: 0.21223820745944977 batch: 145/224\n",
      "Batch loss: 0.2386343628168106 batch: 146/224\n",
      "Batch loss: 0.19223767518997192 batch: 147/224\n",
      "Batch loss: 0.20307007431983948 batch: 148/224\n",
      "Batch loss: 0.2395477443933487 batch: 149/224\n",
      "Batch loss: 0.25584742426872253 batch: 150/224\n",
      "Batch loss: 0.2072763293981552 batch: 151/224\n",
      "Batch loss: 0.23073890805244446 batch: 152/224\n",
      "Batch loss: 0.24102437496185303 batch: 153/224\n",
      "Batch loss: 0.2755052149295807 batch: 154/224\n",
      "Batch loss: 0.22587767243385315 batch: 155/224\n",
      "Batch loss: 0.20955714583396912 batch: 156/224\n",
      "Batch loss: 0.22726406157016754 batch: 157/224\n",
      "Batch loss: 0.26393139362335205 batch: 158/224\n",
      "Batch loss: 0.18042585253715515 batch: 159/224\n",
      "Batch loss: 0.24716611206531525 batch: 160/224\n",
      "Batch loss: 0.19607573747634888 batch: 161/224\n",
      "Batch loss: 0.23681102693080902 batch: 162/224\n",
      "Batch loss: 0.24880051612854004 batch: 163/224\n",
      "Batch loss: 0.21946817636489868 batch: 164/224\n",
      "Batch loss: 0.2971012592315674 batch: 165/224\n",
      "Batch loss: 0.21051949262619019 batch: 166/224\n",
      "Batch loss: 0.1947779655456543 batch: 167/224\n",
      "Batch loss: 0.17376306653022766 batch: 168/224\n",
      "Batch loss: 0.2232595682144165 batch: 169/224\n",
      "Batch loss: 0.21949458122253418 batch: 170/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.18925516307353973 batch: 171/224\n",
      "Batch loss: 0.22559340298175812 batch: 172/224\n",
      "Batch loss: 0.24177634716033936 batch: 173/224\n",
      "Batch loss: 0.2477124184370041 batch: 174/224\n",
      "Batch loss: 0.1981191486120224 batch: 175/224\n",
      "Batch loss: 0.19828473031520844 batch: 176/224\n",
      "Batch loss: 0.2436438798904419 batch: 177/224\n",
      "Batch loss: 0.22885653376579285 batch: 178/224\n",
      "Batch loss: 0.2429521232843399 batch: 179/224\n",
      "Batch loss: 0.19696316123008728 batch: 180/224\n",
      "Batch loss: 0.23899205029010773 batch: 181/224\n",
      "Batch loss: 0.28437912464141846 batch: 182/224\n",
      "Batch loss: 0.25548574328422546 batch: 183/224\n",
      "Batch loss: 0.21914176642894745 batch: 184/224\n",
      "Batch loss: 0.2608640491962433 batch: 185/224\n",
      "Batch loss: 0.20192202925682068 batch: 186/224\n",
      "Batch loss: 0.2337907999753952 batch: 187/224\n",
      "Batch loss: 0.1871570497751236 batch: 188/224\n",
      "Batch loss: 0.25336945056915283 batch: 189/224\n",
      "Batch loss: 0.25497767329216003 batch: 190/224\n",
      "Batch loss: 0.22234617173671722 batch: 191/224\n",
      "Batch loss: 0.24901649355888367 batch: 192/224\n",
      "Batch loss: 0.22906899452209473 batch: 193/224\n",
      "Batch loss: 0.22284774482250214 batch: 194/224\n",
      "Batch loss: 0.2616037428379059 batch: 195/224\n",
      "Batch loss: 0.2452656775712967 batch: 196/224\n",
      "Batch loss: 0.218357652425766 batch: 197/224\n",
      "Batch loss: 0.2297097146511078 batch: 198/224\n",
      "Batch loss: 0.21640394628047943 batch: 199/224\n",
      "Batch loss: 0.2218536138534546 batch: 200/224\n",
      "Batch loss: 0.2597290873527527 batch: 201/224\n",
      "Batch loss: 0.24162353575229645 batch: 202/224\n",
      "Batch loss: 0.2495567500591278 batch: 203/224\n",
      "Batch loss: 0.21991358697414398 batch: 204/224\n",
      "Batch loss: 0.22675266861915588 batch: 205/224\n",
      "Batch loss: 0.1997368335723877 batch: 206/224\n",
      "Batch loss: 0.24719703197479248 batch: 207/224\n",
      "Batch loss: 0.16997762024402618 batch: 208/224\n",
      "Batch loss: 0.2500792443752289 batch: 209/224\n",
      "Batch loss: 0.20945021510124207 batch: 210/224\n",
      "Batch loss: 0.2116641104221344 batch: 211/224\n",
      "Batch loss: 0.19916406273841858 batch: 212/224\n",
      "Batch loss: 0.24271169304847717 batch: 213/224\n",
      "Batch loss: 0.23405438661575317 batch: 214/224\n",
      "Batch loss: 0.2425507754087448 batch: 215/224\n",
      "Batch loss: 0.23296359181404114 batch: 216/224\n",
      "Batch loss: 0.22687537968158722 batch: 217/224\n",
      "Batch loss: 0.20782943069934845 batch: 218/224\n",
      "Batch loss: 0.17719866335391998 batch: 219/224\n",
      "Batch loss: 0.20460425317287445 batch: 220/224\n",
      "Batch loss: 0.2794804871082306 batch: 221/224\n",
      "Batch loss: 0.2282211035490036 batch: 222/224\n",
      "Batch loss: 0.21840670704841614 batch: 223/224\n",
      "Batch loss: 0.21222791075706482 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 21/75..  Training Loss: 0.00045..  Test Loss: 0.00067..  Test Accuracy: 0.88704\n",
      "Running epoch 22/75\n",
      "Batch loss: 0.1511482298374176 batch: 1/224\n",
      "Batch loss: 0.21599958837032318 batch: 2/224\n",
      "Batch loss: 0.19479632377624512 batch: 3/224\n",
      "Batch loss: 0.22552907466888428 batch: 4/224\n",
      "Batch loss: 0.21146003901958466 batch: 5/224\n",
      "Batch loss: 0.23223356902599335 batch: 6/224\n",
      "Batch loss: 0.17333075404167175 batch: 7/224\n",
      "Batch loss: 0.19781997799873352 batch: 8/224\n",
      "Batch loss: 0.2030714601278305 batch: 9/224\n",
      "Batch loss: 0.19306635856628418 batch: 10/224\n",
      "Batch loss: 0.2678114175796509 batch: 11/224\n",
      "Batch loss: 0.18893373012542725 batch: 12/224\n",
      "Batch loss: 0.16281558573246002 batch: 13/224\n",
      "Batch loss: 0.19266755878925323 batch: 14/224\n",
      "Batch loss: 0.22153453528881073 batch: 15/224\n",
      "Batch loss: 0.31996509432792664 batch: 16/224\n",
      "Batch loss: 0.18958428502082825 batch: 17/224\n",
      "Batch loss: 0.2746025025844574 batch: 18/224\n",
      "Batch loss: 0.21192075312137604 batch: 19/224\n",
      "Batch loss: 0.2248699963092804 batch: 20/224\n",
      "Batch loss: 0.2303611934185028 batch: 21/224\n",
      "Batch loss: 0.21099820733070374 batch: 22/224\n",
      "Batch loss: 0.2229764461517334 batch: 23/224\n",
      "Batch loss: 0.24251426756381989 batch: 24/224\n",
      "Batch loss: 0.19403032958507538 batch: 25/224\n",
      "Batch loss: 0.1509881615638733 batch: 26/224\n",
      "Batch loss: 0.23887978494167328 batch: 27/224\n",
      "Batch loss: 0.22651605308055878 batch: 28/224\n",
      "Batch loss: 0.2308061271905899 batch: 29/224\n",
      "Batch loss: 0.197181835770607 batch: 30/224\n",
      "Batch loss: 0.20147739350795746 batch: 31/224\n",
      "Batch loss: 0.2277011275291443 batch: 32/224\n",
      "Batch loss: 0.18073870241641998 batch: 33/224\n",
      "Batch loss: 0.201619952917099 batch: 34/224\n",
      "Batch loss: 0.20688867568969727 batch: 35/224\n",
      "Batch loss: 0.2514941394329071 batch: 36/224\n",
      "Batch loss: 0.21832570433616638 batch: 37/224\n",
      "Batch loss: 0.24661879241466522 batch: 38/224\n",
      "Batch loss: 0.27173951268196106 batch: 39/224\n",
      "Batch loss: 0.20520706474781036 batch: 40/224\n",
      "Batch loss: 0.22832506895065308 batch: 41/224\n",
      "Batch loss: 0.22360318899154663 batch: 42/224\n",
      "Batch loss: 0.22664214670658112 batch: 43/224\n",
      "Batch loss: 0.17528557777404785 batch: 44/224\n",
      "Batch loss: 0.19231240451335907 batch: 45/224\n",
      "Batch loss: 0.26676681637763977 batch: 46/224\n",
      "Batch loss: 0.23745942115783691 batch: 47/224\n",
      "Batch loss: 0.2364927977323532 batch: 48/224\n",
      "Batch loss: 0.17990590631961823 batch: 49/224\n",
      "Batch loss: 0.21357247233390808 batch: 50/224\n",
      "Batch loss: 0.22726550698280334 batch: 51/224\n",
      "Batch loss: 0.2196265012025833 batch: 52/224\n",
      "Batch loss: 0.2604071795940399 batch: 53/224\n",
      "Batch loss: 0.19938670098781586 batch: 54/224\n",
      "Batch loss: 0.1856916844844818 batch: 55/224\n",
      "Batch loss: 0.2004946768283844 batch: 56/224\n",
      "Batch loss: 0.23834063112735748 batch: 57/224\n",
      "Batch loss: 0.21679449081420898 batch: 58/224\n",
      "Batch loss: 0.20094546675682068 batch: 59/224\n",
      "Batch loss: 0.29497650265693665 batch: 60/224\n",
      "Batch loss: 0.22882510721683502 batch: 61/224\n",
      "Batch loss: 0.21981613337993622 batch: 62/224\n",
      "Batch loss: 0.20690946280956268 batch: 63/224\n",
      "Batch loss: 0.2270512729883194 batch: 64/224\n",
      "Batch loss: 0.22354018688201904 batch: 65/224\n",
      "Batch loss: 0.23067806661128998 batch: 66/224\n",
      "Batch loss: 0.20698867738246918 batch: 67/224\n",
      "Batch loss: 0.20531822741031647 batch: 68/224\n",
      "Batch loss: 0.23822590708732605 batch: 69/224\n",
      "Batch loss: 0.20598635077476501 batch: 70/224\n",
      "Batch loss: 0.16108083724975586 batch: 71/224\n",
      "Batch loss: 0.13616876304149628 batch: 72/224\n",
      "Batch loss: 0.2463054209947586 batch: 73/224\n",
      "Batch loss: 0.20731128752231598 batch: 74/224\n",
      "Batch loss: 0.22025322914123535 batch: 75/224\n",
      "Batch loss: 0.23072636127471924 batch: 76/224\n",
      "Batch loss: 0.23028548061847687 batch: 77/224\n",
      "Batch loss: 0.19526700675487518 batch: 78/224\n",
      "Batch loss: 0.24001502990722656 batch: 79/224\n",
      "Batch loss: 0.2672663629055023 batch: 80/224\n",
      "Batch loss: 0.2645215690135956 batch: 81/224\n",
      "Batch loss: 0.24109865725040436 batch: 82/224\n",
      "Batch loss: 0.2159249633550644 batch: 83/224\n",
      "Batch loss: 0.1597427874803543 batch: 84/224\n",
      "Batch loss: 0.23192574083805084 batch: 85/224\n",
      "Batch loss: 0.2262229025363922 batch: 86/224\n",
      "Batch loss: 0.24585184454917908 batch: 87/224\n",
      "Batch loss: 0.21166226267814636 batch: 88/224\n",
      "Batch loss: 0.22354553639888763 batch: 89/224\n",
      "Batch loss: 0.237857848405838 batch: 90/224\n",
      "Batch loss: 0.2207103818655014 batch: 91/224\n",
      "Batch loss: 0.27632972598075867 batch: 92/224\n",
      "Batch loss: 0.223871111869812 batch: 93/224\n",
      "Batch loss: 0.2336549311876297 batch: 94/224\n",
      "Batch loss: 0.23275479674339294 batch: 95/224\n",
      "Batch loss: 0.23178498446941376 batch: 96/224\n",
      "Batch loss: 0.18485595285892487 batch: 97/224\n",
      "Batch loss: 0.16846418380737305 batch: 98/224\n",
      "Batch loss: 0.25863033533096313 batch: 99/224\n",
      "Batch loss: 0.20432618260383606 batch: 100/224\n",
      "Batch loss: 0.2482193112373352 batch: 101/224\n",
      "Batch loss: 0.2202504426240921 batch: 102/224\n",
      "Batch loss: 0.2461017370223999 batch: 103/224\n",
      "Batch loss: 0.1983274519443512 batch: 104/224\n",
      "Batch loss: 0.2073763906955719 batch: 105/224\n",
      "Batch loss: 0.23812945187091827 batch: 106/224\n",
      "Batch loss: 0.17333832383155823 batch: 107/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.18686753511428833 batch: 108/224\n",
      "Batch loss: 0.1926957368850708 batch: 109/224\n",
      "Batch loss: 0.2126501053571701 batch: 110/224\n",
      "Batch loss: 0.2223348766565323 batch: 111/224\n",
      "Batch loss: 0.2006991058588028 batch: 112/224\n",
      "Batch loss: 0.23050373792648315 batch: 113/224\n",
      "Batch loss: 0.1866329461336136 batch: 114/224\n",
      "Batch loss: 0.18295559287071228 batch: 115/224\n",
      "Batch loss: 0.1893293857574463 batch: 116/224\n",
      "Batch loss: 0.1659853309392929 batch: 117/224\n",
      "Batch loss: 0.23470792174339294 batch: 118/224\n",
      "Batch loss: 0.18933917582035065 batch: 119/224\n",
      "Batch loss: 0.22715243697166443 batch: 120/224\n",
      "Batch loss: 0.1850651204586029 batch: 121/224\n",
      "Batch loss: 0.19686481356620789 batch: 122/224\n",
      "Batch loss: 0.23697228729724884 batch: 123/224\n",
      "Batch loss: 0.2008253037929535 batch: 124/224\n",
      "Batch loss: 0.17891527712345123 batch: 125/224\n",
      "Batch loss: 0.21923017501831055 batch: 126/224\n",
      "Batch loss: 0.22555741667747498 batch: 127/224\n",
      "Batch loss: 0.17678306996822357 batch: 128/224\n",
      "Batch loss: 0.22752995789051056 batch: 129/224\n",
      "Batch loss: 0.20832744240760803 batch: 130/224\n",
      "Batch loss: 0.1932310312986374 batch: 131/224\n",
      "Batch loss: 0.20076023042201996 batch: 132/224\n",
      "Batch loss: 0.1943400353193283 batch: 133/224\n",
      "Batch loss: 0.20425577461719513 batch: 134/224\n",
      "Batch loss: 0.23191876709461212 batch: 135/224\n",
      "Batch loss: 0.20205652713775635 batch: 136/224\n",
      "Batch loss: 0.2194695919752121 batch: 137/224\n",
      "Batch loss: 0.21000339090824127 batch: 138/224\n",
      "Batch loss: 0.2153085470199585 batch: 139/224\n",
      "Batch loss: 0.2816818356513977 batch: 140/224\n",
      "Batch loss: 0.17662404477596283 batch: 141/224\n",
      "Batch loss: 0.18568319082260132 batch: 142/224\n",
      "Batch loss: 0.19725599884986877 batch: 143/224\n",
      "Batch loss: 0.21937499940395355 batch: 144/224\n",
      "Batch loss: 0.24035604298114777 batch: 145/224\n",
      "Batch loss: 0.23695415258407593 batch: 146/224\n",
      "Batch loss: 0.20364336669445038 batch: 147/224\n",
      "Batch loss: 0.2018774449825287 batch: 148/224\n",
      "Batch loss: 0.22571991384029388 batch: 149/224\n",
      "Batch loss: 0.25665292143821716 batch: 150/224\n",
      "Batch loss: 0.21466641128063202 batch: 151/224\n",
      "Batch loss: 0.2531884014606476 batch: 152/224\n",
      "Batch loss: 0.2179732769727707 batch: 153/224\n",
      "Batch loss: 0.2514863610267639 batch: 154/224\n",
      "Batch loss: 0.2059093415737152 batch: 155/224\n",
      "Batch loss: 0.223728746175766 batch: 156/224\n",
      "Batch loss: 0.23284399509429932 batch: 157/224\n",
      "Batch loss: 0.28969961404800415 batch: 158/224\n",
      "Batch loss: 0.19282405078411102 batch: 159/224\n",
      "Batch loss: 0.22558702528476715 batch: 160/224\n",
      "Batch loss: 0.17160452902317047 batch: 161/224\n",
      "Batch loss: 0.21410086750984192 batch: 162/224\n",
      "Batch loss: 0.1637416034936905 batch: 163/224\n",
      "Batch loss: 0.20452743768692017 batch: 164/224\n",
      "Batch loss: 0.26855817437171936 batch: 165/224\n",
      "Batch loss: 0.18482738733291626 batch: 166/224\n",
      "Batch loss: 0.22129958868026733 batch: 167/224\n",
      "Batch loss: 0.20301233232021332 batch: 168/224\n",
      "Batch loss: 0.22636917233467102 batch: 169/224\n",
      "Batch loss: 0.23534433543682098 batch: 170/224\n",
      "Batch loss: 0.19226476550102234 batch: 171/224\n",
      "Batch loss: 0.22298602759838104 batch: 172/224\n",
      "Batch loss: 0.22072181105613708 batch: 173/224\n",
      "Batch loss: 0.18030719459056854 batch: 174/224\n",
      "Batch loss: 0.2004450112581253 batch: 175/224\n",
      "Batch loss: 0.17481926083564758 batch: 176/224\n",
      "Batch loss: 0.2352369725704193 batch: 177/224\n",
      "Batch loss: 0.21109075844287872 batch: 178/224\n",
      "Batch loss: 0.25702980160713196 batch: 179/224\n",
      "Batch loss: 0.18478749692440033 batch: 180/224\n",
      "Batch loss: 0.2465263456106186 batch: 181/224\n",
      "Batch loss: 0.25630414485931396 batch: 182/224\n",
      "Batch loss: 0.24951952695846558 batch: 183/224\n",
      "Batch loss: 0.18456260859966278 batch: 184/224\n",
      "Batch loss: 0.22855350375175476 batch: 185/224\n",
      "Batch loss: 0.16933338344097137 batch: 186/224\n",
      "Batch loss: 0.20220308005809784 batch: 187/224\n",
      "Batch loss: 0.1805652379989624 batch: 188/224\n",
      "Batch loss: 0.21890076994895935 batch: 189/224\n",
      "Batch loss: 0.2273411899805069 batch: 190/224\n",
      "Batch loss: 0.1889432966709137 batch: 191/224\n",
      "Batch loss: 0.2470356971025467 batch: 192/224\n",
      "Batch loss: 0.24124598503112793 batch: 193/224\n",
      "Batch loss: 0.2352548986673355 batch: 194/224\n",
      "Batch loss: 0.22286902368068695 batch: 195/224\n",
      "Batch loss: 0.260417103767395 batch: 196/224\n",
      "Batch loss: 0.25345927476882935 batch: 197/224\n",
      "Batch loss: 0.23549537360668182 batch: 198/224\n",
      "Batch loss: 0.21116647124290466 batch: 199/224\n",
      "Batch loss: 0.2367415428161621 batch: 200/224\n",
      "Batch loss: 0.24567893147468567 batch: 201/224\n",
      "Batch loss: 0.23847606778144836 batch: 202/224\n",
      "Batch loss: 0.221623495221138 batch: 203/224\n",
      "Batch loss: 0.211497962474823 batch: 204/224\n",
      "Batch loss: 0.261311411857605 batch: 205/224\n",
      "Batch loss: 0.2022477686405182 batch: 206/224\n",
      "Batch loss: 0.20871199667453766 batch: 207/224\n",
      "Batch loss: 0.20795992016792297 batch: 208/224\n",
      "Batch loss: 0.2066173106431961 batch: 209/224\n",
      "Batch loss: 0.18557590246200562 batch: 210/224\n",
      "Batch loss: 0.22428007423877716 batch: 211/224\n",
      "Batch loss: 0.2416372001171112 batch: 212/224\n",
      "Batch loss: 0.213174968957901 batch: 213/224\n",
      "Batch loss: 0.22875633835792542 batch: 214/224\n",
      "Batch loss: 0.21200183033943176 batch: 215/224\n",
      "Batch loss: 0.21227644383907318 batch: 216/224\n",
      "Batch loss: 0.2516557574272156 batch: 217/224\n",
      "Batch loss: 0.22910702228546143 batch: 218/224\n",
      "Batch loss: 0.19811061024665833 batch: 219/224\n",
      "Batch loss: 0.21424801647663116 batch: 220/224\n",
      "Batch loss: 0.29292815923690796 batch: 221/224\n",
      "Batch loss: 0.19956164062023163 batch: 222/224\n",
      "Batch loss: 0.1842859834432602 batch: 223/224\n",
      "Batch loss: 0.17449957132339478 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 22/75..  Training Loss: 0.00043..  Test Loss: 0.00068..  Test Accuracy: 0.88371\n",
      "Running epoch 23/75\n",
      "Batch loss: 0.18133032321929932 batch: 1/224\n",
      "Batch loss: 0.20597903430461884 batch: 2/224\n",
      "Batch loss: 0.18351112306118011 batch: 3/224\n",
      "Batch loss: 0.24787642061710358 batch: 4/224\n",
      "Batch loss: 0.2151569128036499 batch: 5/224\n",
      "Batch loss: 0.24989095330238342 batch: 6/224\n",
      "Batch loss: 0.19479762017726898 batch: 7/224\n",
      "Batch loss: 0.19388750195503235 batch: 8/224\n",
      "Batch loss: 0.20566093921661377 batch: 9/224\n",
      "Batch loss: 0.18782195448875427 batch: 10/224\n",
      "Batch loss: 0.24920223653316498 batch: 11/224\n",
      "Batch loss: 0.2030561864376068 batch: 12/224\n",
      "Batch loss: 0.1609184592962265 batch: 13/224\n",
      "Batch loss: 0.18713252246379852 batch: 14/224\n",
      "Batch loss: 0.20380531251430511 batch: 15/224\n",
      "Batch loss: 0.25989824533462524 batch: 16/224\n",
      "Batch loss: 0.19066467881202698 batch: 17/224\n",
      "Batch loss: 0.2410443127155304 batch: 18/224\n",
      "Batch loss: 0.17821459472179413 batch: 19/224\n",
      "Batch loss: 0.20183923840522766 batch: 20/224\n",
      "Batch loss: 0.20165754854679108 batch: 21/224\n",
      "Batch loss: 0.22039100527763367 batch: 22/224\n",
      "Batch loss: 0.22109787166118622 batch: 23/224\n",
      "Batch loss: 0.2503602206707001 batch: 24/224\n",
      "Batch loss: 0.1473270207643509 batch: 25/224\n",
      "Batch loss: 0.18002502620220184 batch: 26/224\n",
      "Batch loss: 0.1991257518529892 batch: 27/224\n",
      "Batch loss: 0.18350200355052948 batch: 28/224\n",
      "Batch loss: 0.22487568855285645 batch: 29/224\n",
      "Batch loss: 0.19521787762641907 batch: 30/224\n",
      "Batch loss: 0.19729776680469513 batch: 31/224\n",
      "Batch loss: 0.24084818363189697 batch: 32/224\n",
      "Batch loss: 0.16819468140602112 batch: 33/224\n",
      "Batch loss: 0.21828775107860565 batch: 34/224\n",
      "Batch loss: 0.24283908307552338 batch: 35/224\n",
      "Batch loss: 0.22392545640468597 batch: 36/224\n",
      "Batch loss: 0.21849118173122406 batch: 37/224\n",
      "Batch loss: 0.20375804603099823 batch: 38/224\n",
      "Batch loss: 0.24455277621746063 batch: 39/224\n",
      "Batch loss: 0.1875205785036087 batch: 40/224\n",
      "Batch loss: 0.26449477672576904 batch: 41/224\n",
      "Batch loss: 0.20719701051712036 batch: 42/224\n",
      "Batch loss: 0.21822570264339447 batch: 43/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.19775095582008362 batch: 44/224\n",
      "Batch loss: 0.17041867971420288 batch: 45/224\n",
      "Batch loss: 0.2679764926433563 batch: 46/224\n",
      "Batch loss: 0.2095799595117569 batch: 47/224\n",
      "Batch loss: 0.20057806372642517 batch: 48/224\n",
      "Batch loss: 0.21021904051303864 batch: 49/224\n",
      "Batch loss: 0.19992713630199432 batch: 50/224\n",
      "Batch loss: 0.1952270269393921 batch: 51/224\n",
      "Batch loss: 0.1852681189775467 batch: 52/224\n",
      "Batch loss: 0.23917265236377716 batch: 53/224\n",
      "Batch loss: 0.20123620331287384 batch: 54/224\n",
      "Batch loss: 0.2055051624774933 batch: 55/224\n",
      "Batch loss: 0.21633242070674896 batch: 56/224\n",
      "Batch loss: 0.2613159418106079 batch: 57/224\n",
      "Batch loss: 0.21086826920509338 batch: 58/224\n",
      "Batch loss: 0.24442407488822937 batch: 59/224\n",
      "Batch loss: 0.23786184191703796 batch: 60/224\n",
      "Batch loss: 0.22276252508163452 batch: 61/224\n",
      "Batch loss: 0.1966128796339035 batch: 62/224\n",
      "Batch loss: 0.19033318758010864 batch: 63/224\n",
      "Batch loss: 0.234676793217659 batch: 64/224\n",
      "Batch loss: 0.19894537329673767 batch: 65/224\n",
      "Batch loss: 0.24023832380771637 batch: 66/224\n",
      "Batch loss: 0.21632255613803864 batch: 67/224\n",
      "Batch loss: 0.21950368583202362 batch: 68/224\n",
      "Batch loss: 0.22692477703094482 batch: 69/224\n",
      "Batch loss: 0.18275965750217438 batch: 70/224\n",
      "Batch loss: 0.19023217260837555 batch: 71/224\n",
      "Batch loss: 0.13972927629947662 batch: 72/224\n",
      "Batch loss: 0.23677191138267517 batch: 73/224\n",
      "Batch loss: 0.2287190854549408 batch: 74/224\n",
      "Batch loss: 0.21703509986400604 batch: 75/224\n",
      "Batch loss: 0.2661024034023285 batch: 76/224\n",
      "Batch loss: 0.2343764305114746 batch: 77/224\n",
      "Batch loss: 0.20593389868736267 batch: 78/224\n",
      "Batch loss: 0.2208322137594223 batch: 79/224\n",
      "Batch loss: 0.1945429891347885 batch: 80/224\n",
      "Batch loss: 0.2888183295726776 batch: 81/224\n",
      "Batch loss: 0.2387833595275879 batch: 82/224\n",
      "Batch loss: 0.2249193638563156 batch: 83/224\n",
      "Batch loss: 0.16006222367286682 batch: 84/224\n",
      "Batch loss: 0.2169492244720459 batch: 85/224\n",
      "Batch loss: 0.23389719426631927 batch: 86/224\n",
      "Batch loss: 0.17207786440849304 batch: 87/224\n",
      "Batch loss: 0.24011269211769104 batch: 88/224\n",
      "Batch loss: 0.20566058158874512 batch: 89/224\n",
      "Batch loss: 0.26266247034072876 batch: 90/224\n",
      "Batch loss: 0.20053210854530334 batch: 91/224\n",
      "Batch loss: 0.21424657106399536 batch: 92/224\n",
      "Batch loss: 0.181616872549057 batch: 93/224\n",
      "Batch loss: 0.21773040294647217 batch: 94/224\n",
      "Batch loss: 0.16069233417510986 batch: 95/224\n",
      "Batch loss: 0.19830138981342316 batch: 96/224\n",
      "Batch loss: 0.24593256413936615 batch: 97/224\n",
      "Batch loss: 0.15737782418727875 batch: 98/224\n",
      "Batch loss: 0.22584466636180878 batch: 99/224\n",
      "Batch loss: 0.18624557554721832 batch: 100/224\n",
      "Batch loss: 0.22208596765995026 batch: 101/224\n",
      "Batch loss: 0.20655515789985657 batch: 102/224\n",
      "Batch loss: 0.21097810566425323 batch: 103/224\n",
      "Batch loss: 0.20733435451984406 batch: 104/224\n",
      "Batch loss: 0.20867043733596802 batch: 105/224\n",
      "Batch loss: 0.19320781528949738 batch: 106/224\n",
      "Batch loss: 0.1796078085899353 batch: 107/224\n",
      "Batch loss: 0.21202312409877777 batch: 108/224\n",
      "Batch loss: 0.2056739330291748 batch: 109/224\n",
      "Batch loss: 0.19664046168327332 batch: 110/224\n",
      "Batch loss: 0.23744367063045502 batch: 111/224\n",
      "Batch loss: 0.20122528076171875 batch: 112/224\n",
      "Batch loss: 0.23691798746585846 batch: 113/224\n",
      "Batch loss: 0.17746268212795258 batch: 114/224\n",
      "Batch loss: 0.18107138574123383 batch: 115/224\n",
      "Batch loss: 0.20908142626285553 batch: 116/224\n",
      "Batch loss: 0.19708998501300812 batch: 117/224\n",
      "Batch loss: 0.19926273822784424 batch: 118/224\n",
      "Batch loss: 0.19936439394950867 batch: 119/224\n",
      "Batch loss: 0.2632591128349304 batch: 120/224\n",
      "Batch loss: 0.20765992999076843 batch: 121/224\n",
      "Batch loss: 0.19389969110488892 batch: 122/224\n",
      "Batch loss: 0.1639784425497055 batch: 123/224\n",
      "Batch loss: 0.20491452515125275 batch: 124/224\n",
      "Batch loss: 0.1817600578069687 batch: 125/224\n",
      "Batch loss: 0.21821229159832 batch: 126/224\n",
      "Batch loss: 0.2309451550245285 batch: 127/224\n",
      "Batch loss: 0.1707538217306137 batch: 128/224\n",
      "Batch loss: 0.20957230031490326 batch: 129/224\n",
      "Batch loss: 0.23916791379451752 batch: 130/224\n",
      "Batch loss: 0.18873603641986847 batch: 131/224\n",
      "Batch loss: 0.18920551240444183 batch: 132/224\n",
      "Batch loss: 0.21416758000850677 batch: 133/224\n",
      "Batch loss: 0.2182360291481018 batch: 134/224\n",
      "Batch loss: 0.2226455807685852 batch: 135/224\n",
      "Batch loss: 0.21880003809928894 batch: 136/224\n",
      "Batch loss: 0.22285109758377075 batch: 137/224\n",
      "Batch loss: 0.18965977430343628 batch: 138/224\n",
      "Batch loss: 0.20097161829471588 batch: 139/224\n",
      "Batch loss: 0.2614040672779083 batch: 140/224\n",
      "Batch loss: 0.14132308959960938 batch: 141/224\n",
      "Batch loss: 0.18698136508464813 batch: 142/224\n",
      "Batch loss: 0.20111089944839478 batch: 143/224\n",
      "Batch loss: 0.213417649269104 batch: 144/224\n",
      "Batch loss: 0.2435317188501358 batch: 145/224\n",
      "Batch loss: 0.2564884424209595 batch: 146/224\n",
      "Batch loss: 0.18425007164478302 batch: 147/224\n",
      "Batch loss: 0.18037468194961548 batch: 148/224\n",
      "Batch loss: 0.2473859041929245 batch: 149/224\n",
      "Batch loss: 0.24715793132781982 batch: 150/224\n",
      "Batch loss: 0.18654204905033112 batch: 151/224\n",
      "Batch loss: 0.20225897431373596 batch: 152/224\n",
      "Batch loss: 0.24344518780708313 batch: 153/224\n",
      "Batch loss: 0.25427666306495667 batch: 154/224\n",
      "Batch loss: 0.18254217505455017 batch: 155/224\n",
      "Batch loss: 0.1956808716058731 batch: 156/224\n",
      "Batch loss: 0.23362217843532562 batch: 157/224\n",
      "Batch loss: 0.2920280396938324 batch: 158/224\n",
      "Batch loss: 0.20736704766750336 batch: 159/224\n",
      "Batch loss: 0.18485227227210999 batch: 160/224\n",
      "Batch loss: 0.18287751078605652 batch: 161/224\n",
      "Batch loss: 0.20388661324977875 batch: 162/224\n",
      "Batch loss: 0.21828410029411316 batch: 163/224\n",
      "Batch loss: 0.20246906578540802 batch: 164/224\n",
      "Batch loss: 0.27292051911354065 batch: 165/224\n",
      "Batch loss: 0.2095739096403122 batch: 166/224\n",
      "Batch loss: 0.18599173426628113 batch: 167/224\n",
      "Batch loss: 0.18237952888011932 batch: 168/224\n",
      "Batch loss: 0.2043745368719101 batch: 169/224\n",
      "Batch loss: 0.1983141005039215 batch: 170/224\n",
      "Batch loss: 0.1972656100988388 batch: 171/224\n",
      "Batch loss: 0.22651490569114685 batch: 172/224\n",
      "Batch loss: 0.19706420600414276 batch: 173/224\n",
      "Batch loss: 0.17740324139595032 batch: 174/224\n",
      "Batch loss: 0.16435711085796356 batch: 175/224\n",
      "Batch loss: 0.1985693722963333 batch: 176/224\n",
      "Batch loss: 0.22030271589756012 batch: 177/224\n",
      "Batch loss: 0.18040937185287476 batch: 178/224\n",
      "Batch loss: 0.24252007901668549 batch: 179/224\n",
      "Batch loss: 0.16797062754631042 batch: 180/224\n",
      "Batch loss: 0.2122858315706253 batch: 181/224\n",
      "Batch loss: 0.23796111345291138 batch: 182/224\n",
      "Batch loss: 0.2465999573469162 batch: 183/224\n",
      "Batch loss: 0.19425833225250244 batch: 184/224\n",
      "Batch loss: 0.22926495969295502 batch: 185/224\n",
      "Batch loss: 0.17726901173591614 batch: 186/224\n",
      "Batch loss: 0.2049318253993988 batch: 187/224\n",
      "Batch loss: 0.1747150719165802 batch: 188/224\n",
      "Batch loss: 0.22480739653110504 batch: 189/224\n",
      "Batch loss: 0.2119532972574234 batch: 190/224\n",
      "Batch loss: 0.20961639285087585 batch: 191/224\n",
      "Batch loss: 0.2199089378118515 batch: 192/224\n",
      "Batch loss: 0.19924473762512207 batch: 193/224\n",
      "Batch loss: 0.2325846403837204 batch: 194/224\n",
      "Batch loss: 0.22578909993171692 batch: 195/224\n",
      "Batch loss: 0.2265457957983017 batch: 196/224\n",
      "Batch loss: 0.18514344096183777 batch: 197/224\n",
      "Batch loss: 0.1897147297859192 batch: 198/224\n",
      "Batch loss: 0.2058051973581314 batch: 199/224\n",
      "Batch loss: 0.23139514029026031 batch: 200/224\n",
      "Batch loss: 0.23179231584072113 batch: 201/224\n",
      "Batch loss: 0.2377937138080597 batch: 202/224\n",
      "Batch loss: 0.16691625118255615 batch: 203/224\n",
      "Batch loss: 0.1899290382862091 batch: 204/224\n",
      "Batch loss: 0.19871126115322113 batch: 205/224\n",
      "Batch loss: 0.1908940225839615 batch: 206/224\n",
      "Batch loss: 0.17792470753192902 batch: 207/224\n",
      "Batch loss: 0.18000805377960205 batch: 208/224\n",
      "Batch loss: 0.2125454545021057 batch: 209/224\n",
      "Batch loss: 0.16730642318725586 batch: 210/224\n",
      "Batch loss: 0.1784738153219223 batch: 211/224\n",
      "Batch loss: 0.22006116807460785 batch: 212/224\n",
      "Batch loss: 0.20564277470111847 batch: 213/224\n",
      "Batch loss: 0.21616171300411224 batch: 214/224\n",
      "Batch loss: 0.259857714176178 batch: 215/224\n",
      "Batch loss: 0.1685418337583542 batch: 216/224\n",
      "Batch loss: 0.2162969708442688 batch: 217/224\n",
      "Batch loss: 0.1998143345117569 batch: 218/224\n",
      "Batch loss: 0.1668761819601059 batch: 219/224\n",
      "Batch loss: 0.16809199750423431 batch: 220/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.22441205382347107 batch: 221/224\n",
      "Batch loss: 0.19126267731189728 batch: 222/224\n",
      "Batch loss: 0.16594639420509338 batch: 223/224\n",
      "Batch loss: 0.1489129364490509 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 23/75..  Training Loss: 0.00042..  Test Loss: 0.00069..  Test Accuracy: 0.88743\n",
      "Running epoch 24/75\n",
      "Batch loss: 0.1618862897157669 batch: 1/224\n",
      "Batch loss: 0.2071155607700348 batch: 2/224\n",
      "Batch loss: 0.2092675119638443 batch: 3/224\n",
      "Batch loss: 0.22503778338432312 batch: 4/224\n",
      "Batch loss: 0.19648584723472595 batch: 5/224\n",
      "Batch loss: 0.21588300168514252 batch: 6/224\n",
      "Batch loss: 0.16501551866531372 batch: 7/224\n",
      "Batch loss: 0.17574526369571686 batch: 8/224\n",
      "Batch loss: 0.1516272872686386 batch: 9/224\n",
      "Batch loss: 0.18499863147735596 batch: 10/224\n",
      "Batch loss: 0.23618540167808533 batch: 11/224\n",
      "Batch loss: 0.16301149129867554 batch: 12/224\n",
      "Batch loss: 0.1801530122756958 batch: 13/224\n",
      "Batch loss: 0.134404718875885 batch: 14/224\n",
      "Batch loss: 0.191936656832695 batch: 15/224\n",
      "Batch loss: 0.22602613270282745 batch: 16/224\n",
      "Batch loss: 0.1894458681344986 batch: 17/224\n",
      "Batch loss: 0.22320862114429474 batch: 18/224\n",
      "Batch loss: 0.16834519803524017 batch: 19/224\n",
      "Batch loss: 0.21273937821388245 batch: 20/224\n",
      "Batch loss: 0.21808649599552155 batch: 21/224\n",
      "Batch loss: 0.20576314628124237 batch: 22/224\n",
      "Batch loss: 0.22994612157344818 batch: 23/224\n",
      "Batch loss: 0.2464136779308319 batch: 24/224\n",
      "Batch loss: 0.17911967635154724 batch: 25/224\n",
      "Batch loss: 0.17033599317073822 batch: 26/224\n",
      "Batch loss: 0.18111540377140045 batch: 27/224\n",
      "Batch loss: 0.21263360977172852 batch: 28/224\n",
      "Batch loss: 0.21423450112342834 batch: 29/224\n",
      "Batch loss: 0.20623616874217987 batch: 30/224\n",
      "Batch loss: 0.20458181202411652 batch: 31/224\n",
      "Batch loss: 0.2440262734889984 batch: 32/224\n",
      "Batch loss: 0.17961467802524567 batch: 33/224\n",
      "Batch loss: 0.20424342155456543 batch: 34/224\n",
      "Batch loss: 0.2534067928791046 batch: 35/224\n",
      "Batch loss: 0.22736452519893646 batch: 36/224\n",
      "Batch loss: 0.22073057293891907 batch: 37/224\n",
      "Batch loss: 0.1734849214553833 batch: 38/224\n",
      "Batch loss: 0.21677087247371674 batch: 39/224\n",
      "Batch loss: 0.1706390082836151 batch: 40/224\n",
      "Batch loss: 0.22778533399105072 batch: 41/224\n",
      "Batch loss: 0.19559580087661743 batch: 42/224\n",
      "Batch loss: 0.2176496833562851 batch: 43/224\n",
      "Batch loss: 0.1617351770401001 batch: 44/224\n",
      "Batch loss: 0.17294040322303772 batch: 45/224\n",
      "Batch loss: 0.23605777323246002 batch: 46/224\n",
      "Batch loss: 0.18428447842597961 batch: 47/224\n",
      "Batch loss: 0.19912110269069672 batch: 48/224\n",
      "Batch loss: 0.16326887905597687 batch: 49/224\n",
      "Batch loss: 0.17009063065052032 batch: 50/224\n",
      "Batch loss: 0.18212871253490448 batch: 51/224\n",
      "Batch loss: 0.1771894246339798 batch: 52/224\n",
      "Batch loss: 0.25431039929389954 batch: 53/224\n",
      "Batch loss: 0.15802954137325287 batch: 54/224\n",
      "Batch loss: 0.18012003600597382 batch: 55/224\n",
      "Batch loss: 0.21908605098724365 batch: 56/224\n",
      "Batch loss: 0.22729849815368652 batch: 57/224\n",
      "Batch loss: 0.1844615638256073 batch: 58/224\n",
      "Batch loss: 0.18089160323143005 batch: 59/224\n",
      "Batch loss: 0.2376868575811386 batch: 60/224\n",
      "Batch loss: 0.23047420382499695 batch: 61/224\n",
      "Batch loss: 0.17092491686344147 batch: 62/224\n",
      "Batch loss: 0.20855063199996948 batch: 63/224\n",
      "Batch loss: 0.24883462488651276 batch: 64/224\n",
      "Batch loss: 0.23269285261631012 batch: 65/224\n",
      "Batch loss: 0.2414906620979309 batch: 66/224\n",
      "Batch loss: 0.23130996525287628 batch: 67/224\n",
      "Batch loss: 0.2081269472837448 batch: 68/224\n",
      "Batch loss: 0.23648519814014435 batch: 69/224\n",
      "Batch loss: 0.21096865832805634 batch: 70/224\n",
      "Batch loss: 0.18321527540683746 batch: 71/224\n",
      "Batch loss: 0.14166192710399628 batch: 72/224\n",
      "Batch loss: 0.23112525045871735 batch: 73/224\n",
      "Batch loss: 0.19125957787036896 batch: 74/224\n",
      "Batch loss: 0.21714575588703156 batch: 75/224\n",
      "Batch loss: 0.22096171975135803 batch: 76/224\n",
      "Batch loss: 0.2088254988193512 batch: 77/224\n",
      "Batch loss: 0.20919197797775269 batch: 78/224\n",
      "Batch loss: 0.20786333084106445 batch: 79/224\n",
      "Batch loss: 0.19943158328533173 batch: 80/224\n",
      "Batch loss: 0.24791067838668823 batch: 81/224\n",
      "Batch loss: 0.21975579857826233 batch: 82/224\n",
      "Batch loss: 0.1922936737537384 batch: 83/224\n",
      "Batch loss: 0.16481320559978485 batch: 84/224\n",
      "Batch loss: 0.20649608969688416 batch: 85/224\n",
      "Batch loss: 0.219980850815773 batch: 86/224\n",
      "Batch loss: 0.2449740171432495 batch: 87/224\n",
      "Batch loss: 0.2245355248451233 batch: 88/224\n",
      "Batch loss: 0.23772163689136505 batch: 89/224\n",
      "Batch loss: 0.2411317080259323 batch: 90/224\n",
      "Batch loss: 0.21072806417942047 batch: 91/224\n",
      "Batch loss: 0.23011921346187592 batch: 92/224\n",
      "Batch loss: 0.17357583343982697 batch: 93/224\n",
      "Batch loss: 0.19436340034008026 batch: 94/224\n",
      "Batch loss: 0.1711907535791397 batch: 95/224\n",
      "Batch loss: 0.21071791648864746 batch: 96/224\n",
      "Batch loss: 0.17020347714424133 batch: 97/224\n",
      "Batch loss: 0.2006196826696396 batch: 98/224\n",
      "Batch loss: 0.22441764175891876 batch: 99/224\n",
      "Batch loss: 0.2181316614151001 batch: 100/224\n",
      "Batch loss: 0.25416913628578186 batch: 101/224\n",
      "Batch loss: 0.23030632734298706 batch: 102/224\n",
      "Batch loss: 0.20545199513435364 batch: 103/224\n",
      "Batch loss: 0.17813190817832947 batch: 104/224\n",
      "Batch loss: 0.15519335865974426 batch: 105/224\n",
      "Batch loss: 0.1987658441066742 batch: 106/224\n",
      "Batch loss: 0.18053029477596283 batch: 107/224\n",
      "Batch loss: 0.19593532383441925 batch: 108/224\n",
      "Batch loss: 0.17210520803928375 batch: 109/224\n",
      "Batch loss: 0.16937105357646942 batch: 110/224\n",
      "Batch loss: 0.1804627776145935 batch: 111/224\n",
      "Batch loss: 0.1474277228116989 batch: 112/224\n",
      "Batch loss: 0.24768318235874176 batch: 113/224\n",
      "Batch loss: 0.18547983467578888 batch: 114/224\n",
      "Batch loss: 0.16441002488136292 batch: 115/224\n",
      "Batch loss: 0.1986062377691269 batch: 116/224\n",
      "Batch loss: 0.16975298523902893 batch: 117/224\n",
      "Batch loss: 0.20570065081119537 batch: 118/224\n",
      "Batch loss: 0.19184674322605133 batch: 119/224\n",
      "Batch loss: 0.2088935524225235 batch: 120/224\n",
      "Batch loss: 0.19953130185604095 batch: 121/224\n",
      "Batch loss: 0.18558326363563538 batch: 122/224\n",
      "Batch loss: 0.1644349992275238 batch: 123/224\n",
      "Batch loss: 0.22869238257408142 batch: 124/224\n",
      "Batch loss: 0.17563818395137787 batch: 125/224\n",
      "Batch loss: 0.19195719063282013 batch: 126/224\n",
      "Batch loss: 0.22282777726650238 batch: 127/224\n",
      "Batch loss: 0.17864671349525452 batch: 128/224\n",
      "Batch loss: 0.19392086565494537 batch: 129/224\n",
      "Batch loss: 0.2240877002477646 batch: 130/224\n",
      "Batch loss: 0.15433339774608612 batch: 131/224\n",
      "Batch loss: 0.19230049848556519 batch: 132/224\n",
      "Batch loss: 0.20795847475528717 batch: 133/224\n",
      "Batch loss: 0.19692018628120422 batch: 134/224\n",
      "Batch loss: 0.21016110479831696 batch: 135/224\n",
      "Batch loss: 0.21363596618175507 batch: 136/224\n",
      "Batch loss: 0.1931791752576828 batch: 137/224\n",
      "Batch loss: 0.19791452586650848 batch: 138/224\n",
      "Batch loss: 0.24919423460960388 batch: 139/224\n",
      "Batch loss: 0.23551855981349945 batch: 140/224\n",
      "Batch loss: 0.1483132392168045 batch: 141/224\n",
      "Batch loss: 0.19308236241340637 batch: 142/224\n",
      "Batch loss: 0.1795448213815689 batch: 143/224\n",
      "Batch loss: 0.19854483008384705 batch: 144/224\n",
      "Batch loss: 0.19454967975616455 batch: 145/224\n",
      "Batch loss: 0.22845838963985443 batch: 146/224\n",
      "Batch loss: 0.1863292157649994 batch: 147/224\n",
      "Batch loss: 0.1991923749446869 batch: 148/224\n",
      "Batch loss: 0.22844655811786652 batch: 149/224\n",
      "Batch loss: 0.2269858717918396 batch: 150/224\n",
      "Batch loss: 0.2260133922100067 batch: 151/224\n",
      "Batch loss: 0.20531879365444183 batch: 152/224\n",
      "Batch loss: 0.250160276889801 batch: 153/224\n",
      "Batch loss: 0.2307855784893036 batch: 154/224\n",
      "Batch loss: 0.17204031348228455 batch: 155/224\n",
      "Batch loss: 0.16835492849349976 batch: 156/224\n",
      "Batch loss: 0.2553761303424835 batch: 157/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.2922022044658661 batch: 158/224\n",
      "Batch loss: 0.1811296045780182 batch: 159/224\n",
      "Batch loss: 0.2113366425037384 batch: 160/224\n",
      "Batch loss: 0.1539432257413864 batch: 161/224\n",
      "Batch loss: 0.1926221251487732 batch: 162/224\n",
      "Batch loss: 0.17042948305606842 batch: 163/224\n",
      "Batch loss: 0.1733832061290741 batch: 164/224\n",
      "Batch loss: 0.22258511185646057 batch: 165/224\n",
      "Batch loss: 0.21176233887672424 batch: 166/224\n",
      "Batch loss: 0.19203060865402222 batch: 167/224\n",
      "Batch loss: 0.1758946031332016 batch: 168/224\n",
      "Batch loss: 0.22052127122879028 batch: 169/224\n",
      "Batch loss: 0.19243013858795166 batch: 170/224\n",
      "Batch loss: 0.21345186233520508 batch: 171/224\n",
      "Batch loss: 0.20675618946552277 batch: 172/224\n",
      "Batch loss: 0.21911180019378662 batch: 173/224\n",
      "Batch loss: 0.18525078892707825 batch: 174/224\n",
      "Batch loss: 0.20862054824829102 batch: 175/224\n",
      "Batch loss: 0.17196965217590332 batch: 176/224\n",
      "Batch loss: 0.22453653812408447 batch: 177/224\n",
      "Batch loss: 0.2215556651353836 batch: 178/224\n",
      "Batch loss: 0.2269226610660553 batch: 179/224\n",
      "Batch loss: 0.16730396449565887 batch: 180/224\n",
      "Batch loss: 0.22243420779705048 batch: 181/224\n",
      "Batch loss: 0.2213420867919922 batch: 182/224\n",
      "Batch loss: 0.2204403430223465 batch: 183/224\n",
      "Batch loss: 0.21007400751113892 batch: 184/224\n",
      "Batch loss: 0.23100419342517853 batch: 185/224\n",
      "Batch loss: 0.16291427612304688 batch: 186/224\n",
      "Batch loss: 0.1776496022939682 batch: 187/224\n",
      "Batch loss: 0.16834168136119843 batch: 188/224\n",
      "Batch loss: 0.2176528424024582 batch: 189/224\n",
      "Batch loss: 0.20186865329742432 batch: 190/224\n",
      "Batch loss: 0.23883283138275146 batch: 191/224\n",
      "Batch loss: 0.22462216019630432 batch: 192/224\n",
      "Batch loss: 0.19526945054531097 batch: 193/224\n",
      "Batch loss: 0.20542746782302856 batch: 194/224\n",
      "Batch loss: 0.26709237694740295 batch: 195/224\n",
      "Batch loss: 0.23177829384803772 batch: 196/224\n",
      "Batch loss: 0.21049509942531586 batch: 197/224\n",
      "Batch loss: 0.20160220563411713 batch: 198/224\n",
      "Batch loss: 0.21216730773448944 batch: 199/224\n",
      "Batch loss: 0.22183175384998322 batch: 200/224\n",
      "Batch loss: 0.23398931324481964 batch: 201/224\n",
      "Batch loss: 0.20450906455516815 batch: 202/224\n",
      "Batch loss: 0.19973896443843842 batch: 203/224\n",
      "Batch loss: 0.2093908190727234 batch: 204/224\n",
      "Batch loss: 0.2202100157737732 batch: 205/224\n",
      "Batch loss: 0.18844768404960632 batch: 206/224\n",
      "Batch loss: 0.21413975954055786 batch: 207/224\n",
      "Batch loss: 0.18329410254955292 batch: 208/224\n",
      "Batch loss: 0.2290990650653839 batch: 209/224\n",
      "Batch loss: 0.16223865747451782 batch: 210/224\n",
      "Batch loss: 0.19724078476428986 batch: 211/224\n",
      "Batch loss: 0.21498745679855347 batch: 212/224\n",
      "Batch loss: 0.2360564023256302 batch: 213/224\n",
      "Batch loss: 0.20931360125541687 batch: 214/224\n",
      "Batch loss: 0.21592606604099274 batch: 215/224\n",
      "Batch loss: 0.2151504009962082 batch: 216/224\n",
      "Batch loss: 0.21973921358585358 batch: 217/224\n",
      "Batch loss: 0.19421222805976868 batch: 218/224\n",
      "Batch loss: 0.15076182782649994 batch: 219/224\n",
      "Batch loss: 0.13977111876010895 batch: 220/224\n",
      "Batch loss: 0.21347184479236603 batch: 221/224\n",
      "Batch loss: 0.2167237102985382 batch: 222/224\n",
      "Batch loss: 0.20034259557724 batch: 223/224\n",
      "Batch loss: 0.18024836480617523 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 24/75..  Training Loss: 0.00041..  Test Loss: 0.00068..  Test Accuracy: 0.88721\n",
      "Running epoch 25/75\n",
      "Batch loss: 0.15607662498950958 batch: 1/224\n",
      "Batch loss: 0.20976674556732178 batch: 2/224\n",
      "Batch loss: 0.1701989769935608 batch: 3/224\n",
      "Batch loss: 0.17453515529632568 batch: 4/224\n",
      "Batch loss: 0.18997550010681152 batch: 5/224\n",
      "Batch loss: 0.23003078997135162 batch: 6/224\n",
      "Batch loss: 0.18958784639835358 batch: 7/224\n",
      "Batch loss: 0.19280079007148743 batch: 8/224\n",
      "Batch loss: 0.15076880156993866 batch: 9/224\n",
      "Batch loss: 0.18895868957042694 batch: 10/224\n",
      "Batch loss: 0.2528479993343353 batch: 11/224\n",
      "Batch loss: 0.1817202866077423 batch: 12/224\n",
      "Batch loss: 0.1563907265663147 batch: 13/224\n",
      "Batch loss: 0.17491847276687622 batch: 14/224\n",
      "Batch loss: 0.18189720809459686 batch: 15/224\n",
      "Batch loss: 0.21047860383987427 batch: 16/224\n",
      "Batch loss: 0.188655823469162 batch: 17/224\n",
      "Batch loss: 0.22550706565380096 batch: 18/224\n",
      "Batch loss: 0.14861808717250824 batch: 19/224\n",
      "Batch loss: 0.1973656862974167 batch: 20/224\n",
      "Batch loss: 0.20381928980350494 batch: 21/224\n",
      "Batch loss: 0.18046358227729797 batch: 22/224\n",
      "Batch loss: 0.19809241592884064 batch: 23/224\n",
      "Batch loss: 0.2170955240726471 batch: 24/224\n",
      "Batch loss: 0.16715772449970245 batch: 25/224\n",
      "Batch loss: 0.16224993765354156 batch: 26/224\n",
      "Batch loss: 0.18998104333877563 batch: 27/224\n",
      "Batch loss: 0.16611671447753906 batch: 28/224\n",
      "Batch loss: 0.21193382143974304 batch: 29/224\n",
      "Batch loss: 0.187925323843956 batch: 30/224\n",
      "Batch loss: 0.1622418463230133 batch: 31/224\n",
      "Batch loss: 0.22422382235527039 batch: 32/224\n",
      "Batch loss: 0.1749647706747055 batch: 33/224\n",
      "Batch loss: 0.1910771131515503 batch: 34/224\n",
      "Batch loss: 0.18535153567790985 batch: 35/224\n",
      "Batch loss: 0.1958915740251541 batch: 36/224\n",
      "Batch loss: 0.1476115882396698 batch: 37/224\n",
      "Batch loss: 0.18946702778339386 batch: 38/224\n",
      "Batch loss: 0.2060585767030716 batch: 39/224\n",
      "Batch loss: 0.17236199975013733 batch: 40/224\n",
      "Batch loss: 0.22557784616947174 batch: 41/224\n",
      "Batch loss: 0.18946315348148346 batch: 42/224\n",
      "Batch loss: 0.22394834458827972 batch: 43/224\n",
      "Batch loss: 0.16526712477207184 batch: 44/224\n",
      "Batch loss: 0.16960784792900085 batch: 45/224\n",
      "Batch loss: 0.24965961277484894 batch: 46/224\n",
      "Batch loss: 0.22622050344944 batch: 47/224\n",
      "Batch loss: 0.20301441848278046 batch: 48/224\n",
      "Batch loss: 0.16345925629138947 batch: 49/224\n",
      "Batch loss: 0.18438512086868286 batch: 50/224\n",
      "Batch loss: 0.14775778353214264 batch: 51/224\n",
      "Batch loss: 0.20463980734348297 batch: 52/224\n",
      "Batch loss: 0.21937227249145508 batch: 53/224\n",
      "Batch loss: 0.15974143147468567 batch: 54/224\n",
      "Batch loss: 0.18737922608852386 batch: 55/224\n",
      "Batch loss: 0.20226328074932098 batch: 56/224\n",
      "Batch loss: 0.2280207872390747 batch: 57/224\n",
      "Batch loss: 0.18338443338871002 batch: 58/224\n",
      "Batch loss: 0.17396900057792664 batch: 59/224\n",
      "Batch loss: 0.20249944925308228 batch: 60/224\n",
      "Batch loss: 0.2352454960346222 batch: 61/224\n",
      "Batch loss: 0.17116190493106842 batch: 62/224\n",
      "Batch loss: 0.1888326108455658 batch: 63/224\n",
      "Batch loss: 0.2202589511871338 batch: 64/224\n",
      "Batch loss: 0.2028319090604782 batch: 65/224\n",
      "Batch loss: 0.22208653390407562 batch: 66/224\n",
      "Batch loss: 0.17386959493160248 batch: 67/224\n",
      "Batch loss: 0.1619538515806198 batch: 68/224\n",
      "Batch loss: 0.241819828748703 batch: 69/224\n",
      "Batch loss: 0.21309906244277954 batch: 70/224\n",
      "Batch loss: 0.1852473020553589 batch: 71/224\n",
      "Batch loss: 0.1274675726890564 batch: 72/224\n",
      "Batch loss: 0.22706803679466248 batch: 73/224\n",
      "Batch loss: 0.18402978777885437 batch: 74/224\n",
      "Batch loss: 0.1794363111257553 batch: 75/224\n",
      "Batch loss: 0.2110859751701355 batch: 76/224\n",
      "Batch loss: 0.18796934187412262 batch: 77/224\n",
      "Batch loss: 0.20279641449451447 batch: 78/224\n",
      "Batch loss: 0.2206525355577469 batch: 79/224\n",
      "Batch loss: 0.18819043040275574 batch: 80/224\n",
      "Batch loss: 0.2220456749200821 batch: 81/224\n",
      "Batch loss: 0.24187883734703064 batch: 82/224\n",
      "Batch loss: 0.18214574456214905 batch: 83/224\n",
      "Batch loss: 0.13738366961479187 batch: 84/224\n",
      "Batch loss: 0.20805172622203827 batch: 85/224\n",
      "Batch loss: 0.19319626688957214 batch: 86/224\n",
      "Batch loss: 0.23320746421813965 batch: 87/224\n",
      "Batch loss: 0.23891514539718628 batch: 88/224\n",
      "Batch loss: 0.1881127953529358 batch: 89/224\n",
      "Batch loss: 0.22540636360645294 batch: 90/224\n",
      "Batch loss: 0.213077113032341 batch: 91/224\n",
      "Batch loss: 0.23429124057292938 batch: 92/224\n",
      "Batch loss: 0.16038279235363007 batch: 93/224\n",
      "Batch loss: 0.1804588884115219 batch: 94/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.1929674744606018 batch: 95/224\n",
      "Batch loss: 0.18334709107875824 batch: 96/224\n",
      "Batch loss: 0.19117267429828644 batch: 97/224\n",
      "Batch loss: 0.15210838615894318 batch: 98/224\n",
      "Batch loss: 0.2308354675769806 batch: 99/224\n",
      "Batch loss: 0.20274965465068817 batch: 100/224\n",
      "Batch loss: 0.21225056052207947 batch: 101/224\n",
      "Batch loss: 0.22728659212589264 batch: 102/224\n",
      "Batch loss: 0.19178783893585205 batch: 103/224\n",
      "Batch loss: 0.1578771471977234 batch: 104/224\n",
      "Batch loss: 0.12607106566429138 batch: 105/224\n",
      "Batch loss: 0.20360521972179413 batch: 106/224\n",
      "Batch loss: 0.17316293716430664 batch: 107/224\n",
      "Batch loss: 0.20634014904499054 batch: 108/224\n",
      "Batch loss: 0.19312290847301483 batch: 109/224\n",
      "Batch loss: 0.1698072850704193 batch: 110/224\n",
      "Batch loss: 0.17828652262687683 batch: 111/224\n",
      "Batch loss: 0.17257559299468994 batch: 112/224\n",
      "Batch loss: 0.22538062930107117 batch: 113/224\n",
      "Batch loss: 0.19943198561668396 batch: 114/224\n",
      "Batch loss: 0.19238866865634918 batch: 115/224\n",
      "Batch loss: 0.17604190111160278 batch: 116/224\n",
      "Batch loss: 0.16877534985542297 batch: 117/224\n",
      "Batch loss: 0.21869216859340668 batch: 118/224\n",
      "Batch loss: 0.20185154676437378 batch: 119/224\n",
      "Batch loss: 0.2108016312122345 batch: 120/224\n",
      "Batch loss: 0.13543032109737396 batch: 121/224\n",
      "Batch loss: 0.2235257476568222 batch: 122/224\n",
      "Batch loss: 0.1269257515668869 batch: 123/224\n",
      "Batch loss: 0.21062356233596802 batch: 124/224\n",
      "Batch loss: 0.18125180900096893 batch: 125/224\n",
      "Batch loss: 0.20674481987953186 batch: 126/224\n",
      "Batch loss: 0.19318966567516327 batch: 127/224\n",
      "Batch loss: 0.15920361876487732 batch: 128/224\n",
      "Batch loss: 0.21299995481967926 batch: 129/224\n",
      "Batch loss: 0.17129819095134735 batch: 130/224\n",
      "Batch loss: 0.1379716694355011 batch: 131/224\n",
      "Batch loss: 0.18696603178977966 batch: 132/224\n",
      "Batch loss: 0.19777855277061462 batch: 133/224\n",
      "Batch loss: 0.21226449310779572 batch: 134/224\n",
      "Batch loss: 0.19225135445594788 batch: 135/224\n",
      "Batch loss: 0.16841652989387512 batch: 136/224\n",
      "Batch loss: 0.2045360803604126 batch: 137/224\n",
      "Batch loss: 0.184536412358284 batch: 138/224\n",
      "Batch loss: 0.2387055903673172 batch: 139/224\n",
      "Batch loss: 0.20248474180698395 batch: 140/224\n",
      "Batch loss: 0.14071501791477203 batch: 141/224\n",
      "Batch loss: 0.1692017912864685 batch: 142/224\n",
      "Batch loss: 0.18966403603553772 batch: 143/224\n",
      "Batch loss: 0.22355444729328156 batch: 144/224\n",
      "Batch loss: 0.22113338112831116 batch: 145/224\n",
      "Batch loss: 0.21829067170619965 batch: 146/224\n",
      "Batch loss: 0.18670880794525146 batch: 147/224\n",
      "Batch loss: 0.17969317734241486 batch: 148/224\n",
      "Batch loss: 0.20420080423355103 batch: 149/224\n",
      "Batch loss: 0.23897859454154968 batch: 150/224\n",
      "Batch loss: 0.20680980384349823 batch: 151/224\n",
      "Batch loss: 0.21070484817028046 batch: 152/224\n",
      "Batch loss: 0.22848603129386902 batch: 153/224\n",
      "Batch loss: 0.23215068876743317 batch: 154/224\n",
      "Batch loss: 0.216938778758049 batch: 155/224\n",
      "Batch loss: 0.2145112156867981 batch: 156/224\n",
      "Batch loss: 0.22261150181293488 batch: 157/224\n",
      "Batch loss: 0.24310797452926636 batch: 158/224\n",
      "Batch loss: 0.1690511554479599 batch: 159/224\n",
      "Batch loss: 0.17117266356945038 batch: 160/224\n",
      "Batch loss: 0.15800265967845917 batch: 161/224\n",
      "Batch loss: 0.18403266370296478 batch: 162/224\n",
      "Batch loss: 0.17861096560955048 batch: 163/224\n",
      "Batch loss: 0.20053525269031525 batch: 164/224\n",
      "Batch loss: 0.21469424664974213 batch: 165/224\n",
      "Batch loss: 0.18168675899505615 batch: 166/224\n",
      "Batch loss: 0.15086661279201508 batch: 167/224\n",
      "Batch loss: 0.21183541417121887 batch: 168/224\n",
      "Batch loss: 0.1971864402294159 batch: 169/224\n",
      "Batch loss: 0.17224012315273285 batch: 170/224\n",
      "Batch loss: 0.1633796989917755 batch: 171/224\n",
      "Batch loss: 0.1908538043498993 batch: 172/224\n",
      "Batch loss: 0.22435881197452545 batch: 173/224\n",
      "Batch loss: 0.17680014669895172 batch: 174/224\n",
      "Batch loss: 0.16135624051094055 batch: 175/224\n",
      "Batch loss: 0.20072150230407715 batch: 176/224\n",
      "Batch loss: 0.21280530095100403 batch: 177/224\n",
      "Batch loss: 0.18511351943016052 batch: 178/224\n",
      "Batch loss: 0.23799332976341248 batch: 179/224\n",
      "Batch loss: 0.1516563445329666 batch: 180/224\n",
      "Batch loss: 0.18419039249420166 batch: 181/224\n",
      "Batch loss: 0.2026086151599884 batch: 182/224\n",
      "Batch loss: 0.19293758273124695 batch: 183/224\n",
      "Batch loss: 0.1909773200750351 batch: 184/224\n",
      "Batch loss: 0.22351661324501038 batch: 185/224\n",
      "Batch loss: 0.19006003439426422 batch: 186/224\n",
      "Batch loss: 0.18884772062301636 batch: 187/224\n",
      "Batch loss: 0.1625072956085205 batch: 188/224\n",
      "Batch loss: 0.20423424243927002 batch: 189/224\n",
      "Batch loss: 0.21007494628429413 batch: 190/224\n",
      "Batch loss: 0.18641088902950287 batch: 191/224\n",
      "Batch loss: 0.21578355133533478 batch: 192/224\n",
      "Batch loss: 0.19103437662124634 batch: 193/224\n",
      "Batch loss: 0.1928681582212448 batch: 194/224\n",
      "Batch loss: 0.19431287050247192 batch: 195/224\n",
      "Batch loss: 0.2112388014793396 batch: 196/224\n",
      "Batch loss: 0.21252259612083435 batch: 197/224\n",
      "Batch loss: 0.17701901495456696 batch: 198/224\n",
      "Batch loss: 0.1602667272090912 batch: 199/224\n",
      "Batch loss: 0.1972666084766388 batch: 200/224\n",
      "Batch loss: 0.20873640477657318 batch: 201/224\n",
      "Batch loss: 0.2174544334411621 batch: 202/224\n",
      "Batch loss: 0.17374759912490845 batch: 203/224\n",
      "Batch loss: 0.18115542829036713 batch: 204/224\n",
      "Batch loss: 0.19567739963531494 batch: 205/224\n",
      "Batch loss: 0.20076441764831543 batch: 206/224\n",
      "Batch loss: 0.20322126150131226 batch: 207/224\n",
      "Batch loss: 0.16893237829208374 batch: 208/224\n",
      "Batch loss: 0.2144390493631363 batch: 209/224\n",
      "Batch loss: 0.1697702556848526 batch: 210/224\n",
      "Batch loss: 0.1975552886724472 batch: 211/224\n",
      "Batch loss: 0.1975913643836975 batch: 212/224\n",
      "Batch loss: 0.21362362802028656 batch: 213/224\n",
      "Batch loss: 0.25192034244537354 batch: 214/224\n",
      "Batch loss: 0.2364347130060196 batch: 215/224\n",
      "Batch loss: 0.18007567524909973 batch: 216/224\n",
      "Batch loss: 0.19868741929531097 batch: 217/224\n",
      "Batch loss: 0.21474413573741913 batch: 218/224\n",
      "Batch loss: 0.17719987034797668 batch: 219/224\n",
      "Batch loss: 0.16739025712013245 batch: 220/224\n",
      "Batch loss: 0.22722218930721283 batch: 221/224\n",
      "Batch loss: 0.17452585697174072 batch: 222/224\n",
      "Batch loss: 0.1895890235900879 batch: 223/224\n",
      "Batch loss: 0.16992439329624176 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 25/75..  Training Loss: 0.00039..  Test Loss: 0.00068..  Test Accuracy: 0.88832\n",
      "Running epoch 26/75\n",
      "Batch loss: 0.1433652639389038 batch: 1/224\n",
      "Batch loss: 0.18139083683490753 batch: 2/224\n",
      "Batch loss: 0.19084249436855316 batch: 3/224\n",
      "Batch loss: 0.21517620980739594 batch: 4/224\n",
      "Batch loss: 0.1808365285396576 batch: 5/224\n",
      "Batch loss: 0.21244998276233673 batch: 6/224\n",
      "Batch loss: 0.16273832321166992 batch: 7/224\n",
      "Batch loss: 0.21573509275913239 batch: 8/224\n",
      "Batch loss: 0.15065427124500275 batch: 9/224\n",
      "Batch loss: 0.19657018780708313 batch: 10/224\n",
      "Batch loss: 0.24433183670043945 batch: 11/224\n",
      "Batch loss: 0.16996759176254272 batch: 12/224\n",
      "Batch loss: 0.13437750935554504 batch: 13/224\n",
      "Batch loss: 0.1790788471698761 batch: 14/224\n",
      "Batch loss: 0.1655518114566803 batch: 15/224\n",
      "Batch loss: 0.23942218720912933 batch: 16/224\n",
      "Batch loss: 0.162705659866333 batch: 17/224\n",
      "Batch loss: 0.2622927725315094 batch: 18/224\n",
      "Batch loss: 0.16861757636070251 batch: 19/224\n",
      "Batch loss: 0.1595063954591751 batch: 20/224\n",
      "Batch loss: 0.18693473935127258 batch: 21/224\n",
      "Batch loss: 0.1837792545557022 batch: 22/224\n",
      "Batch loss: 0.20824255049228668 batch: 23/224\n",
      "Batch loss: 0.1939370334148407 batch: 24/224\n",
      "Batch loss: 0.1347317099571228 batch: 25/224\n",
      "Batch loss: 0.15540005266666412 batch: 26/224\n",
      "Batch loss: 0.16005098819732666 batch: 27/224\n",
      "Batch loss: 0.16846737265586853 batch: 28/224\n",
      "Batch loss: 0.20904165506362915 batch: 29/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.2099398523569107 batch: 30/224\n",
      "Batch loss: 0.1847970336675644 batch: 31/224\n",
      "Batch loss: 0.22812877595424652 batch: 32/224\n",
      "Batch loss: 0.16913338005542755 batch: 33/224\n",
      "Batch loss: 0.17469364404678345 batch: 34/224\n",
      "Batch loss: 0.22775571048259735 batch: 35/224\n",
      "Batch loss: 0.19109265506267548 batch: 36/224\n",
      "Batch loss: 0.22622644901275635 batch: 37/224\n",
      "Batch loss: 0.19677402079105377 batch: 38/224\n",
      "Batch loss: 0.19622959196567535 batch: 39/224\n",
      "Batch loss: 0.1841941773891449 batch: 40/224\n",
      "Batch loss: 0.19254323840141296 batch: 41/224\n",
      "Batch loss: 0.18360361456871033 batch: 42/224\n",
      "Batch loss: 0.21362389624118805 batch: 43/224\n",
      "Batch loss: 0.1592637151479721 batch: 44/224\n",
      "Batch loss: 0.1714266687631607 batch: 45/224\n",
      "Batch loss: 0.22329291701316833 batch: 46/224\n",
      "Batch loss: 0.2330915629863739 batch: 47/224\n",
      "Batch loss: 0.18340790271759033 batch: 48/224\n",
      "Batch loss: 0.167772114276886 batch: 49/224\n",
      "Batch loss: 0.16413412988185883 batch: 50/224\n",
      "Batch loss: 0.19119352102279663 batch: 51/224\n",
      "Batch loss: 0.19595564901828766 batch: 52/224\n",
      "Batch loss: 0.2469516396522522 batch: 53/224\n",
      "Batch loss: 0.14890015125274658 batch: 54/224\n",
      "Batch loss: 0.16825848817825317 batch: 55/224\n",
      "Batch loss: 0.18155719339847565 batch: 56/224\n",
      "Batch loss: 0.22282853722572327 batch: 57/224\n",
      "Batch loss: 0.1872776746749878 batch: 58/224\n",
      "Batch loss: 0.15783461928367615 batch: 59/224\n",
      "Batch loss: 0.22605372965335846 batch: 60/224\n",
      "Batch loss: 0.18855679035186768 batch: 61/224\n",
      "Batch loss: 0.15837664902210236 batch: 62/224\n",
      "Batch loss: 0.20893670618534088 batch: 63/224\n",
      "Batch loss: 0.19087548553943634 batch: 64/224\n",
      "Batch loss: 0.1831974983215332 batch: 65/224\n",
      "Batch loss: 0.1960652470588684 batch: 66/224\n",
      "Batch loss: 0.16729167103767395 batch: 67/224\n",
      "Batch loss: 0.21470068395137787 batch: 68/224\n",
      "Batch loss: 0.2352772355079651 batch: 69/224\n",
      "Batch loss: 0.20668186247348785 batch: 70/224\n",
      "Batch loss: 0.15256740152835846 batch: 71/224\n",
      "Batch loss: 0.1414330154657364 batch: 72/224\n",
      "Batch loss: 0.23685148358345032 batch: 73/224\n",
      "Batch loss: 0.21466757357120514 batch: 74/224\n",
      "Batch loss: 0.20022180676460266 batch: 75/224\n",
      "Batch loss: 0.255910724401474 batch: 76/224\n",
      "Batch loss: 0.1766991764307022 batch: 77/224\n",
      "Batch loss: 0.1751023828983307 batch: 78/224\n",
      "Batch loss: 0.217507004737854 batch: 79/224\n",
      "Batch loss: 0.19552066922187805 batch: 80/224\n",
      "Batch loss: 0.2728932499885559 batch: 81/224\n",
      "Batch loss: 0.2108205258846283 batch: 82/224\n",
      "Batch loss: 0.18856032192707062 batch: 83/224\n",
      "Batch loss: 0.1329776644706726 batch: 84/224\n",
      "Batch loss: 0.20231078565120697 batch: 85/224\n",
      "Batch loss: 0.1826801747083664 batch: 86/224\n",
      "Batch loss: 0.21373286843299866 batch: 87/224\n",
      "Batch loss: 0.20757868885993958 batch: 88/224\n",
      "Batch loss: 0.2016422152519226 batch: 89/224\n",
      "Batch loss: 0.19836460053920746 batch: 90/224\n",
      "Batch loss: 0.21211105585098267 batch: 91/224\n",
      "Batch loss: 0.22024650871753693 batch: 92/224\n",
      "Batch loss: 0.19411402940750122 batch: 93/224\n",
      "Batch loss: 0.18851523101329803 batch: 94/224\n",
      "Batch loss: 0.15189635753631592 batch: 95/224\n",
      "Batch loss: 0.19228462874889374 batch: 96/224\n",
      "Batch loss: 0.18872807919979095 batch: 97/224\n",
      "Batch loss: 0.1747116595506668 batch: 98/224\n",
      "Batch loss: 0.1905801147222519 batch: 99/224\n",
      "Batch loss: 0.13631148636341095 batch: 100/224\n",
      "Batch loss: 0.27120697498321533 batch: 101/224\n",
      "Batch loss: 0.18514280021190643 batch: 102/224\n",
      "Batch loss: 0.22593803703784943 batch: 103/224\n",
      "Batch loss: 0.1608026623725891 batch: 104/224\n",
      "Batch loss: 0.18185918033123016 batch: 105/224\n",
      "Batch loss: 0.18870483338832855 batch: 106/224\n",
      "Batch loss: 0.15790162980556488 batch: 107/224\n",
      "Batch loss: 0.17687389254570007 batch: 108/224\n",
      "Batch loss: 0.19659638404846191 batch: 109/224\n",
      "Batch loss: 0.1859767585992813 batch: 110/224\n",
      "Batch loss: 0.18609081208705902 batch: 111/224\n",
      "Batch loss: 0.16613781452178955 batch: 112/224\n",
      "Batch loss: 0.21320508420467377 batch: 113/224\n",
      "Batch loss: 0.19244550168514252 batch: 114/224\n",
      "Batch loss: 0.170781672000885 batch: 115/224\n",
      "Batch loss: 0.16208654642105103 batch: 116/224\n",
      "Batch loss: 0.161411315202713 batch: 117/224\n",
      "Batch loss: 0.1999097466468811 batch: 118/224\n",
      "Batch loss: 0.16924969851970673 batch: 119/224\n",
      "Batch loss: 0.19891035556793213 batch: 120/224\n",
      "Batch loss: 0.12695050239562988 batch: 121/224\n",
      "Batch loss: 0.1993766725063324 batch: 122/224\n",
      "Batch loss: 0.15451404452323914 batch: 123/224\n",
      "Batch loss: 0.1699851155281067 batch: 124/224\n",
      "Batch loss: 0.17443141341209412 batch: 125/224\n",
      "Batch loss: 0.2390737682580948 batch: 126/224\n",
      "Batch loss: 0.2003399133682251 batch: 127/224\n",
      "Batch loss: 0.18756210803985596 batch: 128/224\n",
      "Batch loss: 0.1896226853132248 batch: 129/224\n",
      "Batch loss: 0.2274542897939682 batch: 130/224\n",
      "Batch loss: 0.16887424886226654 batch: 131/224\n",
      "Batch loss: 0.19670160114765167 batch: 132/224\n",
      "Batch loss: 0.15356650948524475 batch: 133/224\n",
      "Batch loss: 0.20348933339118958 batch: 134/224\n",
      "Batch loss: 0.23410190641880035 batch: 135/224\n",
      "Batch loss: 0.21941320598125458 batch: 136/224\n",
      "Batch loss: 0.18101069331169128 batch: 137/224\n",
      "Batch loss: 0.17657971382141113 batch: 138/224\n",
      "Batch loss: 0.17401784658432007 batch: 139/224\n",
      "Batch loss: 0.22721362113952637 batch: 140/224\n",
      "Batch loss: 0.1308433711528778 batch: 141/224\n",
      "Batch loss: 0.16703207790851593 batch: 142/224\n",
      "Batch loss: 0.16957363486289978 batch: 143/224\n",
      "Batch loss: 0.18444980680942535 batch: 144/224\n",
      "Batch loss: 0.13597753643989563 batch: 145/224\n",
      "Batch loss: 0.23133188486099243 batch: 146/224\n",
      "Batch loss: 0.14534370601177216 batch: 147/224\n",
      "Batch loss: 0.1687103807926178 batch: 148/224\n",
      "Batch loss: 0.20362746715545654 batch: 149/224\n",
      "Batch loss: 0.19633547961711884 batch: 150/224\n",
      "Batch loss: 0.17770253121852875 batch: 151/224\n",
      "Batch loss: 0.17554856836795807 batch: 152/224\n",
      "Batch loss: 0.19820161163806915 batch: 153/224\n",
      "Batch loss: 0.21213312447071075 batch: 154/224\n",
      "Batch loss: 0.17584973573684692 batch: 155/224\n",
      "Batch loss: 0.20497214794158936 batch: 156/224\n",
      "Batch loss: 0.2185813933610916 batch: 157/224\n",
      "Batch loss: 0.23504094779491425 batch: 158/224\n",
      "Batch loss: 0.17912249267101288 batch: 159/224\n",
      "Batch loss: 0.20868100225925446 batch: 160/224\n",
      "Batch loss: 0.17606256902217865 batch: 161/224\n",
      "Batch loss: 0.1994766741991043 batch: 162/224\n",
      "Batch loss: 0.13686548173427582 batch: 163/224\n",
      "Batch loss: 0.16732750833034515 batch: 164/224\n",
      "Batch loss: 0.2613886892795563 batch: 165/224\n",
      "Batch loss: 0.1793341338634491 batch: 166/224\n",
      "Batch loss: 0.18661579489707947 batch: 167/224\n",
      "Batch loss: 0.13168002665042877 batch: 168/224\n",
      "Batch loss: 0.1807287335395813 batch: 169/224\n",
      "Batch loss: 0.16802874207496643 batch: 170/224\n",
      "Batch loss: 0.17517167329788208 batch: 171/224\n",
      "Batch loss: 0.1873338222503662 batch: 172/224\n",
      "Batch loss: 0.17678816616535187 batch: 173/224\n",
      "Batch loss: 0.15590734779834747 batch: 174/224\n",
      "Batch loss: 0.19231528043746948 batch: 175/224\n",
      "Batch loss: 0.17521783709526062 batch: 176/224\n",
      "Batch loss: 0.19706663489341736 batch: 177/224\n",
      "Batch loss: 0.1758175790309906 batch: 178/224\n",
      "Batch loss: 0.23653726279735565 batch: 179/224\n",
      "Batch loss: 0.13468177616596222 batch: 180/224\n",
      "Batch loss: 0.21901018917560577 batch: 181/224\n",
      "Batch loss: 0.19405317306518555 batch: 182/224\n",
      "Batch loss: 0.21679045259952545 batch: 183/224\n",
      "Batch loss: 0.17733754217624664 batch: 184/224\n",
      "Batch loss: 0.22486670315265656 batch: 185/224\n",
      "Batch loss: 0.19899757206439972 batch: 186/224\n",
      "Batch loss: 0.16195431351661682 batch: 187/224\n",
      "Batch loss: 0.1513632833957672 batch: 188/224\n",
      "Batch loss: 0.19697456061840057 batch: 189/224\n",
      "Batch loss: 0.1788279116153717 batch: 190/224\n",
      "Batch loss: 0.17232519388198853 batch: 191/224\n",
      "Batch loss: 0.2012224793434143 batch: 192/224\n",
      "Batch loss: 0.21657508611679077 batch: 193/224\n",
      "Batch loss: 0.17418262362480164 batch: 194/224\n",
      "Batch loss: 0.22572298347949982 batch: 195/224\n",
      "Batch loss: 0.1773287057876587 batch: 196/224\n",
      "Batch loss: 0.20436568558216095 batch: 197/224\n",
      "Batch loss: 0.16284748911857605 batch: 198/224\n",
      "Batch loss: 0.18924152851104736 batch: 199/224\n",
      "Batch loss: 0.20915775001049042 batch: 200/224\n",
      "Batch loss: 0.2114609330892563 batch: 201/224\n",
      "Batch loss: 0.20401820540428162 batch: 202/224\n",
      "Batch loss: 0.17843934893608093 batch: 203/224\n",
      "Batch loss: 0.174152210354805 batch: 204/224\n",
      "Batch loss: 0.1791829913854599 batch: 205/224\n",
      "Batch loss: 0.1994246244430542 batch: 206/224\n",
      "Batch loss: 0.2001110166311264 batch: 207/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.16318508982658386 batch: 208/224\n",
      "Batch loss: 0.1764320731163025 batch: 209/224\n",
      "Batch loss: 0.1815124899148941 batch: 210/224\n",
      "Batch loss: 0.19258445501327515 batch: 211/224\n",
      "Batch loss: 0.1870013177394867 batch: 212/224\n",
      "Batch loss: 0.19188474118709564 batch: 213/224\n",
      "Batch loss: 0.19848527014255524 batch: 214/224\n",
      "Batch loss: 0.23564761877059937 batch: 215/224\n",
      "Batch loss: 0.18154333531856537 batch: 216/224\n",
      "Batch loss: 0.24927139282226562 batch: 217/224\n",
      "Batch loss: 0.18178941309452057 batch: 218/224\n",
      "Batch loss: 0.18016858398914337 batch: 219/224\n",
      "Batch loss: 0.1595316231250763 batch: 220/224\n",
      "Batch loss: 0.18752001225948334 batch: 221/224\n",
      "Batch loss: 0.21855297684669495 batch: 222/224\n",
      "Batch loss: 0.16020481288433075 batch: 223/224\n",
      "Batch loss: 0.16659751534461975 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 26/75..  Training Loss: 0.00038..  Test Loss: 0.00071..  Test Accuracy: 0.88693\n",
      "Running epoch 27/75\n",
      "Batch loss: 0.15923601388931274 batch: 1/224\n",
      "Batch loss: 0.17724654078483582 batch: 2/224\n",
      "Batch loss: 0.16895689070224762 batch: 3/224\n",
      "Batch loss: 0.20303118228912354 batch: 4/224\n",
      "Batch loss: 0.15598371624946594 batch: 5/224\n",
      "Batch loss: 0.18056319653987885 batch: 6/224\n",
      "Batch loss: 0.12859302759170532 batch: 7/224\n",
      "Batch loss: 0.1688324362039566 batch: 8/224\n",
      "Batch loss: 0.18040427565574646 batch: 9/224\n",
      "Batch loss: 0.17733773589134216 batch: 10/224\n",
      "Batch loss: 0.2305983304977417 batch: 11/224\n",
      "Batch loss: 0.1712038666009903 batch: 12/224\n",
      "Batch loss: 0.1568250209093094 batch: 13/224\n",
      "Batch loss: 0.2027926743030548 batch: 14/224\n",
      "Batch loss: 0.14658509194850922 batch: 15/224\n",
      "Batch loss: 0.22053410112857819 batch: 16/224\n",
      "Batch loss: 0.15953697264194489 batch: 17/224\n",
      "Batch loss: 0.20181754231452942 batch: 18/224\n",
      "Batch loss: 0.1480151265859604 batch: 19/224\n",
      "Batch loss: 0.15242139995098114 batch: 20/224\n",
      "Batch loss: 0.17383429408073425 batch: 21/224\n",
      "Batch loss: 0.16538605093955994 batch: 22/224\n",
      "Batch loss: 0.1888599544763565 batch: 23/224\n",
      "Batch loss: 0.21503567695617676 batch: 24/224\n",
      "Batch loss: 0.15142425894737244 batch: 25/224\n",
      "Batch loss: 0.14963574707508087 batch: 26/224\n",
      "Batch loss: 0.1694236844778061 batch: 27/224\n",
      "Batch loss: 0.17214125394821167 batch: 28/224\n",
      "Batch loss: 0.20616482198238373 batch: 29/224\n",
      "Batch loss: 0.20406295359134674 batch: 30/224\n",
      "Batch loss: 0.15940554440021515 batch: 31/224\n",
      "Batch loss: 0.19306300580501556 batch: 32/224\n",
      "Batch loss: 0.15615049004554749 batch: 33/224\n",
      "Batch loss: 0.1514091044664383 batch: 34/224\n",
      "Batch loss: 0.185128316283226 batch: 35/224\n",
      "Batch loss: 0.1839129626750946 batch: 36/224\n",
      "Batch loss: 0.1637885570526123 batch: 37/224\n",
      "Batch loss: 0.20150867104530334 batch: 38/224\n",
      "Batch loss: 0.18614555895328522 batch: 39/224\n",
      "Batch loss: 0.1854083389043808 batch: 40/224\n",
      "Batch loss: 0.2230333685874939 batch: 41/224\n",
      "Batch loss: 0.20539572834968567 batch: 42/224\n",
      "Batch loss: 0.17676378786563873 batch: 43/224\n",
      "Batch loss: 0.17150172591209412 batch: 44/224\n",
      "Batch loss: 0.18442392349243164 batch: 45/224\n",
      "Batch loss: 0.1859433799982071 batch: 46/224\n",
      "Batch loss: 0.21310409903526306 batch: 47/224\n",
      "Batch loss: 0.17308224737644196 batch: 48/224\n",
      "Batch loss: 0.14205271005630493 batch: 49/224\n",
      "Batch loss: 0.14055348932743073 batch: 50/224\n",
      "Batch loss: 0.16518700122833252 batch: 51/224\n",
      "Batch loss: 0.15591098368167877 batch: 52/224\n",
      "Batch loss: 0.20478081703186035 batch: 53/224\n",
      "Batch loss: 0.15415717661380768 batch: 54/224\n",
      "Batch loss: 0.1871919184923172 batch: 55/224\n",
      "Batch loss: 0.1493949294090271 batch: 56/224\n",
      "Batch loss: 0.2490282952785492 batch: 57/224\n",
      "Batch loss: 0.18946179747581482 batch: 58/224\n",
      "Batch loss: 0.15813758969306946 batch: 59/224\n",
      "Batch loss: 0.2199103683233261 batch: 60/224\n",
      "Batch loss: 0.17922309041023254 batch: 61/224\n",
      "Batch loss: 0.16677777469158173 batch: 62/224\n",
      "Batch loss: 0.21101760864257812 batch: 63/224\n",
      "Batch loss: 0.21577072143554688 batch: 64/224\n",
      "Batch loss: 0.1818460077047348 batch: 65/224\n",
      "Batch loss: 0.17785371840000153 batch: 66/224\n",
      "Batch loss: 0.17897558212280273 batch: 67/224\n",
      "Batch loss: 0.17905448377132416 batch: 68/224\n",
      "Batch loss: 0.21065491437911987 batch: 69/224\n",
      "Batch loss: 0.20563900470733643 batch: 70/224\n",
      "Batch loss: 0.15276801586151123 batch: 71/224\n",
      "Batch loss: 0.12716571986675262 batch: 72/224\n",
      "Batch loss: 0.2086184322834015 batch: 73/224\n",
      "Batch loss: 0.1475936621427536 batch: 74/224\n",
      "Batch loss: 0.17021089792251587 batch: 75/224\n",
      "Batch loss: 0.22641688585281372 batch: 76/224\n",
      "Batch loss: 0.1885633021593094 batch: 77/224\n",
      "Batch loss: 0.20169343054294586 batch: 78/224\n",
      "Batch loss: 0.16448968648910522 batch: 79/224\n",
      "Batch loss: 0.19547440111637115 batch: 80/224\n",
      "Batch loss: 0.22488854825496674 batch: 81/224\n",
      "Batch loss: 0.20558209717273712 batch: 82/224\n",
      "Batch loss: 0.1864667385816574 batch: 83/224\n",
      "Batch loss: 0.12762877345085144 batch: 84/224\n",
      "Batch loss: 0.20989876985549927 batch: 85/224\n",
      "Batch loss: 0.18836405873298645 batch: 86/224\n",
      "Batch loss: 0.2342458814382553 batch: 87/224\n",
      "Batch loss: 0.2147323042154312 batch: 88/224\n",
      "Batch loss: 0.19580382108688354 batch: 89/224\n",
      "Batch loss: 0.19658145308494568 batch: 90/224\n",
      "Batch loss: 0.17595605552196503 batch: 91/224\n",
      "Batch loss: 0.1971069872379303 batch: 92/224\n",
      "Batch loss: 0.15167227387428284 batch: 93/224\n",
      "Batch loss: 0.17257745563983917 batch: 94/224\n",
      "Batch loss: 0.1706632524728775 batch: 95/224\n",
      "Batch loss: 0.14569303393363953 batch: 96/224\n",
      "Batch loss: 0.16295336186885834 batch: 97/224\n",
      "Batch loss: 0.12635403871536255 batch: 98/224\n",
      "Batch loss: 0.17854991555213928 batch: 99/224\n",
      "Batch loss: 0.15406371653079987 batch: 100/224\n",
      "Batch loss: 0.1866147220134735 batch: 101/224\n",
      "Batch loss: 0.17776192724704742 batch: 102/224\n",
      "Batch loss: 0.1903974860906601 batch: 103/224\n",
      "Batch loss: 0.17144565284252167 batch: 104/224\n",
      "Batch loss: 0.16022716462612152 batch: 105/224\n",
      "Batch loss: 0.1727484166622162 batch: 106/224\n",
      "Batch loss: 0.14956429600715637 batch: 107/224\n",
      "Batch loss: 0.18533703684806824 batch: 108/224\n",
      "Batch loss: 0.1728498935699463 batch: 109/224\n",
      "Batch loss: 0.16687853634357452 batch: 110/224\n",
      "Batch loss: 0.17813776433467865 batch: 111/224\n",
      "Batch loss: 0.16940681636333466 batch: 112/224\n",
      "Batch loss: 0.18640202283859253 batch: 113/224\n",
      "Batch loss: 0.1616426557302475 batch: 114/224\n",
      "Batch loss: 0.15252892673015594 batch: 115/224\n",
      "Batch loss: 0.14022555947303772 batch: 116/224\n",
      "Batch loss: 0.14469167590141296 batch: 117/224\n",
      "Batch loss: 0.1862456202507019 batch: 118/224\n",
      "Batch loss: 0.19225461781024933 batch: 119/224\n",
      "Batch loss: 0.18632380664348602 batch: 120/224\n",
      "Batch loss: 0.15906880795955658 batch: 121/224\n",
      "Batch loss: 0.22031262516975403 batch: 122/224\n",
      "Batch loss: 0.1469053477048874 batch: 123/224\n",
      "Batch loss: 0.191096231341362 batch: 124/224\n",
      "Batch loss: 0.18369194865226746 batch: 125/224\n",
      "Batch loss: 0.19004829227924347 batch: 126/224\n",
      "Batch loss: 0.21585556864738464 batch: 127/224\n",
      "Batch loss: 0.15561984479427338 batch: 128/224\n",
      "Batch loss: 0.1804041564464569 batch: 129/224\n",
      "Batch loss: 0.19752468168735504 batch: 130/224\n",
      "Batch loss: 0.13595062494277954 batch: 131/224\n",
      "Batch loss: 0.1823856681585312 batch: 132/224\n",
      "Batch loss: 0.17426326870918274 batch: 133/224\n",
      "Batch loss: 0.18333370983600616 batch: 134/224\n",
      "Batch loss: 0.19170814752578735 batch: 135/224\n",
      "Batch loss: 0.1815183460712433 batch: 136/224\n",
      "Batch loss: 0.22220511734485626 batch: 137/224\n",
      "Batch loss: 0.1446126103401184 batch: 138/224\n",
      "Batch loss: 0.19407610595226288 batch: 139/224\n",
      "Batch loss: 0.2333778291940689 batch: 140/224\n",
      "Batch loss: 0.13724015653133392 batch: 141/224\n",
      "Batch loss: 0.186578169465065 batch: 142/224\n",
      "Batch loss: 0.17525283992290497 batch: 143/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.21206101775169373 batch: 144/224\n",
      "Batch loss: 0.17320378124713898 batch: 145/224\n",
      "Batch loss: 0.2314164936542511 batch: 146/224\n",
      "Batch loss: 0.15074828267097473 batch: 147/224\n",
      "Batch loss: 0.21200940012931824 batch: 148/224\n",
      "Batch loss: 0.18826636672019958 batch: 149/224\n",
      "Batch loss: 0.19941802322864532 batch: 150/224\n",
      "Batch loss: 0.17241047322750092 batch: 151/224\n",
      "Batch loss: 0.15774472057819366 batch: 152/224\n",
      "Batch loss: 0.2113012969493866 batch: 153/224\n",
      "Batch loss: 0.17870131134986877 batch: 154/224\n",
      "Batch loss: 0.15063564479351044 batch: 155/224\n",
      "Batch loss: 0.1760096549987793 batch: 156/224\n",
      "Batch loss: 0.1721406877040863 batch: 157/224\n",
      "Batch loss: 0.23406437039375305 batch: 158/224\n",
      "Batch loss: 0.13787026703357697 batch: 159/224\n",
      "Batch loss: 0.179452046751976 batch: 160/224\n",
      "Batch loss: 0.1568019837141037 batch: 161/224\n",
      "Batch loss: 0.16140519082546234 batch: 162/224\n",
      "Batch loss: 0.1615877002477646 batch: 163/224\n",
      "Batch loss: 0.20311187207698822 batch: 164/224\n",
      "Batch loss: 0.20336681604385376 batch: 165/224\n",
      "Batch loss: 0.16744393110275269 batch: 166/224\n",
      "Batch loss: 0.1305345743894577 batch: 167/224\n",
      "Batch loss: 0.17365747690200806 batch: 168/224\n",
      "Batch loss: 0.18396519124507904 batch: 169/224\n",
      "Batch loss: 0.15919442474842072 batch: 170/224\n",
      "Batch loss: 0.17114965617656708 batch: 171/224\n",
      "Batch loss: 0.19468608498573303 batch: 172/224\n",
      "Batch loss: 0.18375933170318604 batch: 173/224\n",
      "Batch loss: 0.17151106894016266 batch: 174/224\n",
      "Batch loss: 0.1653505265712738 batch: 175/224\n",
      "Batch loss: 0.17678451538085938 batch: 176/224\n",
      "Batch loss: 0.17327158153057098 batch: 177/224\n",
      "Batch loss: 0.16762933135032654 batch: 178/224\n",
      "Batch loss: 0.19290252029895782 batch: 179/224\n",
      "Batch loss: 0.1280277818441391 batch: 180/224\n",
      "Batch loss: 0.15007284283638 batch: 181/224\n",
      "Batch loss: 0.1664636731147766 batch: 182/224\n",
      "Batch loss: 0.1971798986196518 batch: 183/224\n",
      "Batch loss: 0.16567489504814148 batch: 184/224\n",
      "Batch loss: 0.23523791134357452 batch: 185/224\n",
      "Batch loss: 0.14256887137889862 batch: 186/224\n",
      "Batch loss: 0.16647224128246307 batch: 187/224\n",
      "Batch loss: 0.1627984642982483 batch: 188/224\n",
      "Batch loss: 0.20640182495117188 batch: 189/224\n",
      "Batch loss: 0.16231223940849304 batch: 190/224\n",
      "Batch loss: 0.1843162626028061 batch: 191/224\n",
      "Batch loss: 0.25110766291618347 batch: 192/224\n",
      "Batch loss: 0.2322084903717041 batch: 193/224\n",
      "Batch loss: 0.17129310965538025 batch: 194/224\n",
      "Batch loss: 0.23522920906543732 batch: 195/224\n",
      "Batch loss: 0.2000941038131714 batch: 196/224\n",
      "Batch loss: 0.1928931623697281 batch: 197/224\n",
      "Batch loss: 0.16258500516414642 batch: 198/224\n",
      "Batch loss: 0.1747380793094635 batch: 199/224\n",
      "Batch loss: 0.1937841922044754 batch: 200/224\n",
      "Batch loss: 0.22128386795520782 batch: 201/224\n",
      "Batch loss: 0.21830815076828003 batch: 202/224\n",
      "Batch loss: 0.20773762464523315 batch: 203/224\n",
      "Batch loss: 0.16189509630203247 batch: 204/224\n",
      "Batch loss: 0.18543905019760132 batch: 205/224\n",
      "Batch loss: 0.16609793901443481 batch: 206/224\n",
      "Batch loss: 0.17255334556102753 batch: 207/224\n",
      "Batch loss: 0.18223795294761658 batch: 208/224\n",
      "Batch loss: 0.18268588185310364 batch: 209/224\n",
      "Batch loss: 0.17557646334171295 batch: 210/224\n",
      "Batch loss: 0.14913183450698853 batch: 211/224\n",
      "Batch loss: 0.19117270410060883 batch: 212/224\n",
      "Batch loss: 0.19694797694683075 batch: 213/224\n",
      "Batch loss: 0.2096865326166153 batch: 214/224\n",
      "Batch loss: 0.2151007354259491 batch: 215/224\n",
      "Batch loss: 0.17537815868854523 batch: 216/224\n",
      "Batch loss: 0.21689216792583466 batch: 217/224\n",
      "Batch loss: 0.1715470552444458 batch: 218/224\n",
      "Batch loss: 0.11626783013343811 batch: 219/224\n",
      "Batch loss: 0.16675353050231934 batch: 220/224\n",
      "Batch loss: 0.2256176471710205 batch: 221/224\n",
      "Batch loss: 0.21066142618656158 batch: 222/224\n",
      "Batch loss: 0.16037872433662415 batch: 223/224\n",
      "Batch loss: 0.21094395220279694 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 27/75..  Training Loss: 0.00036..  Test Loss: 0.00070..  Test Accuracy: 0.88875\n",
      "Running epoch 28/75\n",
      "Batch loss: 0.1359349489212036 batch: 1/224\n",
      "Batch loss: 0.1671701967716217 batch: 2/224\n",
      "Batch loss: 0.14737039804458618 batch: 3/224\n",
      "Batch loss: 0.20035983622074127 batch: 4/224\n",
      "Batch loss: 0.1923755407333374 batch: 5/224\n",
      "Batch loss: 0.19103898108005524 batch: 6/224\n",
      "Batch loss: 0.14774751663208008 batch: 7/224\n",
      "Batch loss: 0.17952857911586761 batch: 8/224\n",
      "Batch loss: 0.15872973203659058 batch: 9/224\n",
      "Batch loss: 0.15118518471717834 batch: 10/224\n",
      "Batch loss: 0.20081482827663422 batch: 11/224\n",
      "Batch loss: 0.1425049901008606 batch: 12/224\n",
      "Batch loss: 0.1458294838666916 batch: 13/224\n",
      "Batch loss: 0.17673340439796448 batch: 14/224\n",
      "Batch loss: 0.1806180477142334 batch: 15/224\n",
      "Batch loss: 0.23593753576278687 batch: 16/224\n",
      "Batch loss: 0.14247949421405792 batch: 17/224\n",
      "Batch loss: 0.21120035648345947 batch: 18/224\n",
      "Batch loss: 0.16759705543518066 batch: 19/224\n",
      "Batch loss: 0.17175689339637756 batch: 20/224\n",
      "Batch loss: 0.18679584562778473 batch: 21/224\n",
      "Batch loss: 0.19317926466464996 batch: 22/224\n",
      "Batch loss: 0.1815050095319748 batch: 23/224\n",
      "Batch loss: 0.22185403108596802 batch: 24/224\n",
      "Batch loss: 0.14373791217803955 batch: 25/224\n",
      "Batch loss: 0.15121570229530334 batch: 26/224\n",
      "Batch loss: 0.16879402101039886 batch: 27/224\n",
      "Batch loss: 0.15272611379623413 batch: 28/224\n",
      "Batch loss: 0.21922439336776733 batch: 29/224\n",
      "Batch loss: 0.18104086816310883 batch: 30/224\n",
      "Batch loss: 0.17712567746639252 batch: 31/224\n",
      "Batch loss: 0.2135029286146164 batch: 32/224\n",
      "Batch loss: 0.13504761457443237 batch: 33/224\n",
      "Batch loss: 0.19088329374790192 batch: 34/224\n",
      "Batch loss: 0.21019881963729858 batch: 35/224\n",
      "Batch loss: 0.16953036189079285 batch: 36/224\n",
      "Batch loss: 0.2017204612493515 batch: 37/224\n",
      "Batch loss: 0.16791865229606628 batch: 38/224\n",
      "Batch loss: 0.16999997198581696 batch: 39/224\n",
      "Batch loss: 0.15895946323871613 batch: 40/224\n",
      "Batch loss: 0.19070746004581451 batch: 41/224\n",
      "Batch loss: 0.17489053308963776 batch: 42/224\n",
      "Batch loss: 0.21103884279727936 batch: 43/224\n",
      "Batch loss: 0.18217262625694275 batch: 44/224\n",
      "Batch loss: 0.18526585400104523 batch: 45/224\n",
      "Batch loss: 0.24855762720108032 batch: 46/224\n",
      "Batch loss: 0.19576707482337952 batch: 47/224\n",
      "Batch loss: 0.16476573050022125 batch: 48/224\n",
      "Batch loss: 0.14323531091213226 batch: 49/224\n",
      "Batch loss: 0.1930244415998459 batch: 50/224\n",
      "Batch loss: 0.1454293131828308 batch: 51/224\n",
      "Batch loss: 0.16322427988052368 batch: 52/224\n",
      "Batch loss: 0.21685954928398132 batch: 53/224\n",
      "Batch loss: 0.15845905244350433 batch: 54/224\n",
      "Batch loss: 0.17701171338558197 batch: 55/224\n",
      "Batch loss: 0.1796887367963791 batch: 56/224\n",
      "Batch loss: 0.2177499681711197 batch: 57/224\n",
      "Batch loss: 0.22670099139213562 batch: 58/224\n",
      "Batch loss: 0.15269792079925537 batch: 59/224\n",
      "Batch loss: 0.1801685243844986 batch: 60/224\n",
      "Batch loss: 0.1859000027179718 batch: 61/224\n",
      "Batch loss: 0.14567387104034424 batch: 62/224\n",
      "Batch loss: 0.19562137126922607 batch: 63/224\n",
      "Batch loss: 0.18935278058052063 batch: 64/224\n",
      "Batch loss: 0.1720934510231018 batch: 65/224\n",
      "Batch loss: 0.19247935712337494 batch: 66/224\n",
      "Batch loss: 0.161554753780365 batch: 67/224\n",
      "Batch loss: 0.1998199224472046 batch: 68/224\n",
      "Batch loss: 0.1929205358028412 batch: 69/224\n",
      "Batch loss: 0.17641380429267883 batch: 70/224\n",
      "Batch loss: 0.17621324956417084 batch: 71/224\n",
      "Batch loss: 0.13755357265472412 batch: 72/224\n",
      "Batch loss: 0.20540808141231537 batch: 73/224\n",
      "Batch loss: 0.17269663512706757 batch: 74/224\n",
      "Batch loss: 0.1734212338924408 batch: 75/224\n",
      "Batch loss: 0.20697921514511108 batch: 76/224\n",
      "Batch loss: 0.18315298855304718 batch: 77/224\n",
      "Batch loss: 0.17979083955287933 batch: 78/224\n",
      "Batch loss: 0.1987396478652954 batch: 79/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.1594584882259369 batch: 80/224\n",
      "Batch loss: 0.2437620311975479 batch: 81/224\n",
      "Batch loss: 0.18719981610774994 batch: 82/224\n",
      "Batch loss: 0.17849867045879364 batch: 83/224\n",
      "Batch loss: 0.1361456662416458 batch: 84/224\n",
      "Batch loss: 0.17744888365268707 batch: 85/224\n",
      "Batch loss: 0.1790822297334671 batch: 86/224\n",
      "Batch loss: 0.22856280207633972 batch: 87/224\n",
      "Batch loss: 0.18972282111644745 batch: 88/224\n",
      "Batch loss: 0.18074387311935425 batch: 89/224\n",
      "Batch loss: 0.18205541372299194 batch: 90/224\n",
      "Batch loss: 0.21102508902549744 batch: 91/224\n",
      "Batch loss: 0.20139984786510468 batch: 92/224\n",
      "Batch loss: 0.1269495189189911 batch: 93/224\n",
      "Batch loss: 0.17465443909168243 batch: 94/224\n",
      "Batch loss: 0.1656188666820526 batch: 95/224\n",
      "Batch loss: 0.18989206850528717 batch: 96/224\n",
      "Batch loss: 0.1560162454843521 batch: 97/224\n",
      "Batch loss: 0.1298127919435501 batch: 98/224\n",
      "Batch loss: 0.17073938250541687 batch: 99/224\n",
      "Batch loss: 0.17526040971279144 batch: 100/224\n",
      "Batch loss: 0.20126992464065552 batch: 101/224\n",
      "Batch loss: 0.2136656939983368 batch: 102/224\n",
      "Batch loss: 0.20356938242912292 batch: 103/224\n",
      "Batch loss: 0.17014889419078827 batch: 104/224\n",
      "Batch loss: 0.13501064479351044 batch: 105/224\n",
      "Batch loss: 0.18666589260101318 batch: 106/224\n",
      "Batch loss: 0.15673816204071045 batch: 107/224\n",
      "Batch loss: 0.15473999083042145 batch: 108/224\n",
      "Batch loss: 0.171473428606987 batch: 109/224\n",
      "Batch loss: 0.15843914449214935 batch: 110/224\n",
      "Batch loss: 0.14547859132289886 batch: 111/224\n",
      "Batch loss: 0.13091805577278137 batch: 112/224\n",
      "Batch loss: 0.18402855098247528 batch: 113/224\n",
      "Batch loss: 0.13442254066467285 batch: 114/224\n",
      "Batch loss: 0.16958478093147278 batch: 115/224\n",
      "Batch loss: 0.17941848933696747 batch: 116/224\n",
      "Batch loss: 0.1525513231754303 batch: 117/224\n",
      "Batch loss: 0.18680256605148315 batch: 118/224\n",
      "Batch loss: 0.14469613134860992 batch: 119/224\n",
      "Batch loss: 0.18166618049144745 batch: 120/224\n",
      "Batch loss: 0.1072760671377182 batch: 121/224\n",
      "Batch loss: 0.17645347118377686 batch: 122/224\n",
      "Batch loss: 0.14163309335708618 batch: 123/224\n",
      "Batch loss: 0.14501439034938812 batch: 124/224\n",
      "Batch loss: 0.15245212614536285 batch: 125/224\n",
      "Batch loss: 0.15448950231075287 batch: 126/224\n",
      "Batch loss: 0.17897389829158783 batch: 127/224\n",
      "Batch loss: 0.15899252891540527 batch: 128/224\n",
      "Batch loss: 0.20922449231147766 batch: 129/224\n",
      "Batch loss: 0.1644274890422821 batch: 130/224\n",
      "Batch loss: 0.16067032516002655 batch: 131/224\n",
      "Batch loss: 0.19876991212368011 batch: 132/224\n",
      "Batch loss: 0.1507319062948227 batch: 133/224\n",
      "Batch loss: 0.15704844892024994 batch: 134/224\n",
      "Batch loss: 0.1947481483221054 batch: 135/224\n",
      "Batch loss: 0.18271049857139587 batch: 136/224\n",
      "Batch loss: 0.15589852631092072 batch: 137/224\n",
      "Batch loss: 0.16644707322120667 batch: 138/224\n",
      "Batch loss: 0.21352718770503998 batch: 139/224\n",
      "Batch loss: 0.2173897922039032 batch: 140/224\n",
      "Batch loss: 0.15325303375720978 batch: 141/224\n",
      "Batch loss: 0.14457812905311584 batch: 142/224\n",
      "Batch loss: 0.14160962402820587 batch: 143/224\n",
      "Batch loss: 0.17547112703323364 batch: 144/224\n",
      "Batch loss: 0.1931108981370926 batch: 145/224\n",
      "Batch loss: 0.21487107872962952 batch: 146/224\n",
      "Batch loss: 0.17413468658924103 batch: 147/224\n",
      "Batch loss: 0.16540834307670593 batch: 148/224\n",
      "Batch loss: 0.2107924520969391 batch: 149/224\n",
      "Batch loss: 0.1857828050851822 batch: 150/224\n",
      "Batch loss: 0.187506303191185 batch: 151/224\n",
      "Batch loss: 0.17041192948818207 batch: 152/224\n",
      "Batch loss: 0.2363591492176056 batch: 153/224\n",
      "Batch loss: 0.16485367715358734 batch: 154/224\n",
      "Batch loss: 0.20324194431304932 batch: 155/224\n",
      "Batch loss: 0.18533144891262054 batch: 156/224\n",
      "Batch loss: 0.20479080080986023 batch: 157/224\n",
      "Batch loss: 0.2690444886684418 batch: 158/224\n",
      "Batch loss: 0.16910646855831146 batch: 159/224\n",
      "Batch loss: 0.17526228725910187 batch: 160/224\n",
      "Batch loss: 0.18384209275245667 batch: 161/224\n",
      "Batch loss: 0.1962101310491562 batch: 162/224\n",
      "Batch loss: 0.15350906550884247 batch: 163/224\n",
      "Batch loss: 0.1837615668773651 batch: 164/224\n",
      "Batch loss: 0.19941239058971405 batch: 165/224\n",
      "Batch loss: 0.19711776077747345 batch: 166/224\n",
      "Batch loss: 0.14829006791114807 batch: 167/224\n",
      "Batch loss: 0.14517226815223694 batch: 168/224\n",
      "Batch loss: 0.1744946986436844 batch: 169/224\n",
      "Batch loss: 0.1969749927520752 batch: 170/224\n",
      "Batch loss: 0.1950863003730774 batch: 171/224\n",
      "Batch loss: 0.14412011206150055 batch: 172/224\n",
      "Batch loss: 0.1808594912290573 batch: 173/224\n",
      "Batch loss: 0.1661253720521927 batch: 174/224\n",
      "Batch loss: 0.173550084233284 batch: 175/224\n",
      "Batch loss: 0.16991421580314636 batch: 176/224\n",
      "Batch loss: 0.18284660577774048 batch: 177/224\n",
      "Batch loss: 0.21329769492149353 batch: 178/224\n",
      "Batch loss: 0.16767412424087524 batch: 179/224\n",
      "Batch loss: 0.13679657876491547 batch: 180/224\n",
      "Batch loss: 0.18133938312530518 batch: 181/224\n",
      "Batch loss: 0.2032557725906372 batch: 182/224\n",
      "Batch loss: 0.17774684727191925 batch: 183/224\n",
      "Batch loss: 0.1639631688594818 batch: 184/224\n",
      "Batch loss: 0.19548463821411133 batch: 185/224\n",
      "Batch loss: 0.161371648311615 batch: 186/224\n",
      "Batch loss: 0.17511405050754547 batch: 187/224\n",
      "Batch loss: 0.11843772977590561 batch: 188/224\n",
      "Batch loss: 0.16466477513313293 batch: 189/224\n",
      "Batch loss: 0.1935998797416687 batch: 190/224\n",
      "Batch loss: 0.20282164216041565 batch: 191/224\n",
      "Batch loss: 0.2126682996749878 batch: 192/224\n",
      "Batch loss: 0.17520837485790253 batch: 193/224\n",
      "Batch loss: 0.17031048238277435 batch: 194/224\n",
      "Batch loss: 0.2016277313232422 batch: 195/224\n",
      "Batch loss: 0.18861526250839233 batch: 196/224\n",
      "Batch loss: 0.2021872103214264 batch: 197/224\n",
      "Batch loss: 0.16912105679512024 batch: 198/224\n",
      "Batch loss: 0.16359400749206543 batch: 199/224\n",
      "Batch loss: 0.2061305195093155 batch: 200/224\n",
      "Batch loss: 0.18664343655109406 batch: 201/224\n",
      "Batch loss: 0.16967253386974335 batch: 202/224\n",
      "Batch loss: 0.1507759690284729 batch: 203/224\n",
      "Batch loss: 0.15626579523086548 batch: 204/224\n",
      "Batch loss: 0.1863992065191269 batch: 205/224\n",
      "Batch loss: 0.1807491034269333 batch: 206/224\n",
      "Batch loss: 0.17407549917697906 batch: 207/224\n",
      "Batch loss: 0.15926754474639893 batch: 208/224\n",
      "Batch loss: 0.17537608742713928 batch: 209/224\n",
      "Batch loss: 0.17793026566505432 batch: 210/224\n",
      "Batch loss: 0.18575115501880646 batch: 211/224\n",
      "Batch loss: 0.20389248430728912 batch: 212/224\n",
      "Batch loss: 0.20840120315551758 batch: 213/224\n",
      "Batch loss: 0.19623963534832 batch: 214/224\n",
      "Batch loss: 0.21482440829277039 batch: 215/224\n",
      "Batch loss: 0.18966932594776154 batch: 216/224\n",
      "Batch loss: 0.17283622920513153 batch: 217/224\n",
      "Batch loss: 0.16650499403476715 batch: 218/224\n",
      "Batch loss: 0.16954489052295685 batch: 219/224\n",
      "Batch loss: 0.13597151637077332 batch: 220/224\n",
      "Batch loss: 0.17048890888690948 batch: 221/224\n",
      "Batch loss: 0.15225224196910858 batch: 222/224\n",
      "Batch loss: 0.16318932175636292 batch: 223/224\n",
      "Batch loss: 0.15730123221874237 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 28/75..  Training Loss: 0.00035..  Test Loss: 0.00074..  Test Accuracy: 0.88479\n",
      "Running epoch 29/75\n",
      "Batch loss: 0.12359091639518738 batch: 1/224\n",
      "Batch loss: 0.17688053846359253 batch: 2/224\n",
      "Batch loss: 0.17232322692871094 batch: 3/224\n",
      "Batch loss: 0.18531173467636108 batch: 4/224\n",
      "Batch loss: 0.17759248614311218 batch: 5/224\n",
      "Batch loss: 0.15263906121253967 batch: 6/224\n",
      "Batch loss: 0.14041340351104736 batch: 7/224\n",
      "Batch loss: 0.20320184528827667 batch: 8/224\n",
      "Batch loss: 0.1538168042898178 batch: 9/224\n",
      "Batch loss: 0.18288369476795197 batch: 10/224\n",
      "Batch loss: 0.2017151117324829 batch: 11/224\n",
      "Batch loss: 0.1833116114139557 batch: 12/224\n",
      "Batch loss: 0.10452629625797272 batch: 13/224\n",
      "Batch loss: 0.18042926490306854 batch: 14/224\n",
      "Batch loss: 0.1314106434583664 batch: 15/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.25139322876930237 batch: 16/224\n",
      "Batch loss: 0.15209965407848358 batch: 17/224\n",
      "Batch loss: 0.20970343053340912 batch: 18/224\n",
      "Batch loss: 0.15855945646762848 batch: 19/224\n",
      "Batch loss: 0.19186721742153168 batch: 20/224\n",
      "Batch loss: 0.17341943085193634 batch: 21/224\n",
      "Batch loss: 0.1519123911857605 batch: 22/224\n",
      "Batch loss: 0.18332061171531677 batch: 23/224\n",
      "Batch loss: 0.19799470901489258 batch: 24/224\n",
      "Batch loss: 0.16740477085113525 batch: 25/224\n",
      "Batch loss: 0.13943196833133698 batch: 26/224\n",
      "Batch loss: 0.1790037453174591 batch: 27/224\n",
      "Batch loss: 0.1447426974773407 batch: 28/224\n",
      "Batch loss: 0.2139674872159958 batch: 29/224\n",
      "Batch loss: 0.16702890396118164 batch: 30/224\n",
      "Batch loss: 0.15221603214740753 batch: 31/224\n",
      "Batch loss: 0.1732916235923767 batch: 32/224\n",
      "Batch loss: 0.18276667594909668 batch: 33/224\n",
      "Batch loss: 0.16443045437335968 batch: 34/224\n",
      "Batch loss: 0.17974406480789185 batch: 35/224\n",
      "Batch loss: 0.14799703657627106 batch: 36/224\n",
      "Batch loss: 0.20546941459178925 batch: 37/224\n",
      "Batch loss: 0.18245500326156616 batch: 38/224\n",
      "Batch loss: 0.1641116440296173 batch: 39/224\n",
      "Batch loss: 0.14075861871242523 batch: 40/224\n",
      "Batch loss: 0.2121681123971939 batch: 41/224\n",
      "Batch loss: 0.1523871123790741 batch: 42/224\n",
      "Batch loss: 0.20461130142211914 batch: 43/224\n",
      "Batch loss: 0.16394740343093872 batch: 44/224\n",
      "Batch loss: 0.12642109394073486 batch: 45/224\n",
      "Batch loss: 0.25274458527565 batch: 46/224\n",
      "Batch loss: 0.18759557604789734 batch: 47/224\n",
      "Batch loss: 0.19396960735321045 batch: 48/224\n",
      "Batch loss: 0.13601750135421753 batch: 49/224\n",
      "Batch loss: 0.1367216259241104 batch: 50/224\n",
      "Batch loss: 0.1553962230682373 batch: 51/224\n",
      "Batch loss: 0.19306407868862152 batch: 52/224\n",
      "Batch loss: 0.2221420556306839 batch: 53/224\n",
      "Batch loss: 0.13168013095855713 batch: 54/224\n",
      "Batch loss: 0.14101698994636536 batch: 55/224\n",
      "Batch loss: 0.15556073188781738 batch: 56/224\n",
      "Batch loss: 0.2314140647649765 batch: 57/224\n",
      "Batch loss: 0.14752542972564697 batch: 58/224\n",
      "Batch loss: 0.16554585099220276 batch: 59/224\n",
      "Batch loss: 0.1906101554632187 batch: 60/224\n",
      "Batch loss: 0.21046918630599976 batch: 61/224\n",
      "Batch loss: 0.15770870447158813 batch: 62/224\n",
      "Batch loss: 0.17817015945911407 batch: 63/224\n",
      "Batch loss: 0.18552018702030182 batch: 64/224\n",
      "Batch loss: 0.1613551378250122 batch: 65/224\n",
      "Batch loss: 0.19441471993923187 batch: 66/224\n",
      "Batch loss: 0.16062791645526886 batch: 67/224\n",
      "Batch loss: 0.17427025735378265 batch: 68/224\n",
      "Batch loss: 0.22247646749019623 batch: 69/224\n",
      "Batch loss: 0.1708165407180786 batch: 70/224\n",
      "Batch loss: 0.15228542685508728 batch: 71/224\n",
      "Batch loss: 0.11742857843637466 batch: 72/224\n",
      "Batch loss: 0.18393252789974213 batch: 73/224\n",
      "Batch loss: 0.14987459778785706 batch: 74/224\n",
      "Batch loss: 0.14058777689933777 batch: 75/224\n",
      "Batch loss: 0.21379582583904266 batch: 76/224\n",
      "Batch loss: 0.1911173164844513 batch: 77/224\n",
      "Batch loss: 0.17876671254634857 batch: 78/224\n",
      "Batch loss: 0.16299618780612946 batch: 79/224\n",
      "Batch loss: 0.16363058984279633 batch: 80/224\n",
      "Batch loss: 0.2084883451461792 batch: 81/224\n",
      "Batch loss: 0.1779276728630066 batch: 82/224\n",
      "Batch loss: 0.16052000224590302 batch: 83/224\n",
      "Batch loss: 0.11812114715576172 batch: 84/224\n",
      "Batch loss: 0.18164296448230743 batch: 85/224\n",
      "Batch loss: 0.18314696848392487 batch: 86/224\n",
      "Batch loss: 0.16425028443336487 batch: 87/224\n",
      "Batch loss: 0.17873814702033997 batch: 88/224\n",
      "Batch loss: 0.1745842695236206 batch: 89/224\n",
      "Batch loss: 0.18022817373275757 batch: 90/224\n",
      "Batch loss: 0.14658696949481964 batch: 91/224\n",
      "Batch loss: 0.17265209555625916 batch: 92/224\n",
      "Batch loss: 0.1690790057182312 batch: 93/224\n",
      "Batch loss: 0.1918608397245407 batch: 94/224\n",
      "Batch loss: 0.14549775421619415 batch: 95/224\n",
      "Batch loss: 0.165874645113945 batch: 96/224\n",
      "Batch loss: 0.16208182275295258 batch: 97/224\n",
      "Batch loss: 0.14080503582954407 batch: 98/224\n",
      "Batch loss: 0.20045888423919678 batch: 99/224\n",
      "Batch loss: 0.15936805307865143 batch: 100/224\n",
      "Batch loss: 0.19942159950733185 batch: 101/224\n",
      "Batch loss: 0.17645637691020966 batch: 102/224\n",
      "Batch loss: 0.19166338443756104 batch: 103/224\n",
      "Batch loss: 0.15287041664123535 batch: 104/224\n",
      "Batch loss: 0.12449519336223602 batch: 105/224\n",
      "Batch loss: 0.16410385072231293 batch: 106/224\n",
      "Batch loss: 0.1682397723197937 batch: 107/224\n",
      "Batch loss: 0.16349659860134125 batch: 108/224\n",
      "Batch loss: 0.1406874656677246 batch: 109/224\n",
      "Batch loss: 0.1497715711593628 batch: 110/224\n",
      "Batch loss: 0.14915142953395844 batch: 111/224\n",
      "Batch loss: 0.14226581156253815 batch: 112/224\n",
      "Batch loss: 0.2059742957353592 batch: 113/224\n",
      "Batch loss: 0.148505300283432 batch: 114/224\n",
      "Batch loss: 0.15504783391952515 batch: 115/224\n",
      "Batch loss: 0.17355437576770782 batch: 116/224\n",
      "Batch loss: 0.14053519070148468 batch: 117/224\n",
      "Batch loss: 0.15792837738990784 batch: 118/224\n",
      "Batch loss: 0.13796484470367432 batch: 119/224\n",
      "Batch loss: 0.1614830046892166 batch: 120/224\n",
      "Batch loss: 0.12487304955720901 batch: 121/224\n",
      "Batch loss: 0.19035771489143372 batch: 122/224\n",
      "Batch loss: 0.13358335196971893 batch: 123/224\n",
      "Batch loss: 0.19703979790210724 batch: 124/224\n",
      "Batch loss: 0.17465917766094208 batch: 125/224\n",
      "Batch loss: 0.16300757229328156 batch: 126/224\n",
      "Batch loss: 0.21207432448863983 batch: 127/224\n",
      "Batch loss: 0.12552285194396973 batch: 128/224\n",
      "Batch loss: 0.14027199149131775 batch: 129/224\n",
      "Batch loss: 0.1581122875213623 batch: 130/224\n",
      "Batch loss: 0.1142854318022728 batch: 131/224\n",
      "Batch loss: 0.1689022034406662 batch: 132/224\n",
      "Batch loss: 0.19797946512699127 batch: 133/224\n",
      "Batch loss: 0.15564705431461334 batch: 134/224\n",
      "Batch loss: 0.16125094890594482 batch: 135/224\n",
      "Batch loss: 0.177981436252594 batch: 136/224\n",
      "Batch loss: 0.19542251527309418 batch: 137/224\n",
      "Batch loss: 0.16227763891220093 batch: 138/224\n",
      "Batch loss: 0.2020711600780487 batch: 139/224\n",
      "Batch loss: 0.1801646649837494 batch: 140/224\n",
      "Batch loss: 0.13764259219169617 batch: 141/224\n",
      "Batch loss: 0.17909447848796844 batch: 142/224\n",
      "Batch loss: 0.15945740044116974 batch: 143/224\n",
      "Batch loss: 0.1722494512796402 batch: 144/224\n",
      "Batch loss: 0.1572398990392685 batch: 145/224\n",
      "Batch loss: 0.1960531771183014 batch: 146/224\n",
      "Batch loss: 0.15231452882289886 batch: 147/224\n",
      "Batch loss: 0.15151268243789673 batch: 148/224\n",
      "Batch loss: 0.19467197358608246 batch: 149/224\n",
      "Batch loss: 0.19081677496433258 batch: 150/224\n",
      "Batch loss: 0.1473449170589447 batch: 151/224\n",
      "Batch loss: 0.1644117534160614 batch: 152/224\n",
      "Batch loss: 0.19262436032295227 batch: 153/224\n",
      "Batch loss: 0.15674707293510437 batch: 154/224\n",
      "Batch loss: 0.14136351644992828 batch: 155/224\n",
      "Batch loss: 0.1403282880783081 batch: 156/224\n",
      "Batch loss: 0.20902343094348907 batch: 157/224\n",
      "Batch loss: 0.22241859138011932 batch: 158/224\n",
      "Batch loss: 0.15936625003814697 batch: 159/224\n",
      "Batch loss: 0.1386898010969162 batch: 160/224\n",
      "Batch loss: 0.16538920998573303 batch: 161/224\n",
      "Batch loss: 0.15535040199756622 batch: 162/224\n",
      "Batch loss: 0.1697407066822052 batch: 163/224\n",
      "Batch loss: 0.12887397408485413 batch: 164/224\n",
      "Batch loss: 0.20094741880893707 batch: 165/224\n",
      "Batch loss: 0.1513892412185669 batch: 166/224\n",
      "Batch loss: 0.1404595971107483 batch: 167/224\n",
      "Batch loss: 0.15860091149806976 batch: 168/224\n",
      "Batch loss: 0.16222472488880157 batch: 169/224\n",
      "Batch loss: 0.1577361524105072 batch: 170/224\n",
      "Batch loss: 0.16013936698436737 batch: 171/224\n",
      "Batch loss: 0.15679889917373657 batch: 172/224\n",
      "Batch loss: 0.16639892756938934 batch: 173/224\n",
      "Batch loss: 0.18472036719322205 batch: 174/224\n",
      "Batch loss: 0.16630038619041443 batch: 175/224\n",
      "Batch loss: 0.132422536611557 batch: 176/224\n",
      "Batch loss: 0.1559526026248932 batch: 177/224\n",
      "Batch loss: 0.16352368891239166 batch: 178/224\n",
      "Batch loss: 0.18012170493602753 batch: 179/224\n",
      "Batch loss: 0.13903029263019562 batch: 180/224\n",
      "Batch loss: 0.15510761737823486 batch: 181/224\n",
      "Batch loss: 0.19264815747737885 batch: 182/224\n",
      "Batch loss: 0.21756500005722046 batch: 183/224\n",
      "Batch loss: 0.1810016930103302 batch: 184/224\n",
      "Batch loss: 0.2027113288640976 batch: 185/224\n",
      "Batch loss: 0.14400851726531982 batch: 186/224\n",
      "Batch loss: 0.15280750393867493 batch: 187/224\n",
      "Batch loss: 0.12360134720802307 batch: 188/224\n",
      "Batch loss: 0.15674762427806854 batch: 189/224\n",
      "Batch loss: 0.15633924305438995 batch: 190/224\n",
      "Batch loss: 0.16457287967205048 batch: 191/224\n",
      "Batch loss: 0.20551393926143646 batch: 192/224\n",
      "Batch loss: 0.17280690371990204 batch: 193/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.16312626004219055 batch: 194/224\n",
      "Batch loss: 0.20952247083187103 batch: 195/224\n",
      "Batch loss: 0.23288510739803314 batch: 196/224\n",
      "Batch loss: 0.1985236257314682 batch: 197/224\n",
      "Batch loss: 0.1898522824048996 batch: 198/224\n",
      "Batch loss: 0.14253009855747223 batch: 199/224\n",
      "Batch loss: 0.16628916561603546 batch: 200/224\n",
      "Batch loss: 0.2540165185928345 batch: 201/224\n",
      "Batch loss: 0.17663149535655975 batch: 202/224\n",
      "Batch loss: 0.16104522347450256 batch: 203/224\n",
      "Batch loss: 0.15588265657424927 batch: 204/224\n",
      "Batch loss: 0.17596177756786346 batch: 205/224\n",
      "Batch loss: 0.15565796196460724 batch: 206/224\n",
      "Batch loss: 0.17304229736328125 batch: 207/224\n",
      "Batch loss: 0.149930939078331 batch: 208/224\n",
      "Batch loss: 0.1903931200504303 batch: 209/224\n",
      "Batch loss: 0.16508634388446808 batch: 210/224\n",
      "Batch loss: 0.18458713591098785 batch: 211/224\n",
      "Batch loss: 0.1646416336297989 batch: 212/224\n",
      "Batch loss: 0.2063315212726593 batch: 213/224\n",
      "Batch loss: 0.15925277769565582 batch: 214/224\n",
      "Batch loss: 0.1911090910434723 batch: 215/224\n",
      "Batch loss: 0.22867649793624878 batch: 216/224\n",
      "Batch loss: 0.1986556053161621 batch: 217/224\n",
      "Batch loss: 0.18132446706295013 batch: 218/224\n",
      "Batch loss: 0.13364306092262268 batch: 219/224\n",
      "Batch loss: 0.18357591331005096 batch: 220/224\n",
      "Batch loss: 0.21862007677555084 batch: 221/224\n",
      "Batch loss: 0.18537943065166473 batch: 222/224\n",
      "Batch loss: 0.12091632932424545 batch: 223/224\n",
      "Batch loss: 0.15363626182079315 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 29/75..  Training Loss: 0.00034..  Test Loss: 0.00071..  Test Accuracy: 0.88864\n",
      "Running epoch 30/75\n",
      "Batch loss: 0.13379520177841187 batch: 1/224\n",
      "Batch loss: 0.17119772732257843 batch: 2/224\n",
      "Batch loss: 0.17099560797214508 batch: 3/224\n",
      "Batch loss: 0.17164455354213715 batch: 4/224\n",
      "Batch loss: 0.1549295336008072 batch: 5/224\n",
      "Batch loss: 0.16068635880947113 batch: 6/224\n",
      "Batch loss: 0.12468675523996353 batch: 7/224\n",
      "Batch loss: 0.18035508692264557 batch: 8/224\n",
      "Batch loss: 0.1357605755329132 batch: 9/224\n",
      "Batch loss: 0.1340910792350769 batch: 10/224\n",
      "Batch loss: 0.18010805547237396 batch: 11/224\n",
      "Batch loss: 0.16864575445652008 batch: 12/224\n",
      "Batch loss: 0.13846668601036072 batch: 13/224\n",
      "Batch loss: 0.13875816762447357 batch: 14/224\n",
      "Batch loss: 0.15493163466453552 batch: 15/224\n",
      "Batch loss: 0.2227567732334137 batch: 16/224\n",
      "Batch loss: 0.1439056098461151 batch: 17/224\n",
      "Batch loss: 0.18537241220474243 batch: 18/224\n",
      "Batch loss: 0.1542101949453354 batch: 19/224\n",
      "Batch loss: 0.15434886515140533 batch: 20/224\n",
      "Batch loss: 0.16979636251926422 batch: 21/224\n",
      "Batch loss: 0.16799390316009521 batch: 22/224\n",
      "Batch loss: 0.19358958303928375 batch: 23/224\n",
      "Batch loss: 0.1708638221025467 batch: 24/224\n",
      "Batch loss: 0.14246389269828796 batch: 25/224\n",
      "Batch loss: 0.1278958022594452 batch: 26/224\n",
      "Batch loss: 0.1556980311870575 batch: 27/224\n",
      "Batch loss: 0.16580824553966522 batch: 28/224\n",
      "Batch loss: 0.16924673318862915 batch: 29/224\n",
      "Batch loss: 0.14294195175170898 batch: 30/224\n",
      "Batch loss: 0.16931939125061035 batch: 31/224\n",
      "Batch loss: 0.15672308206558228 batch: 32/224\n",
      "Batch loss: 0.12561044096946716 batch: 33/224\n",
      "Batch loss: 0.16068346798419952 batch: 34/224\n",
      "Batch loss: 0.1551818549633026 batch: 35/224\n",
      "Batch loss: 0.18389548361301422 batch: 36/224\n",
      "Batch loss: 0.17827963829040527 batch: 37/224\n",
      "Batch loss: 0.15268300473690033 batch: 38/224\n",
      "Batch loss: 0.17786282300949097 batch: 39/224\n",
      "Batch loss: 0.15441012382507324 batch: 40/224\n",
      "Batch loss: 0.19173242151737213 batch: 41/224\n",
      "Batch loss: 0.162295401096344 batch: 42/224\n",
      "Batch loss: 0.1804402470588684 batch: 43/224\n",
      "Batch loss: 0.13794457912445068 batch: 44/224\n",
      "Batch loss: 0.1628471314907074 batch: 45/224\n",
      "Batch loss: 0.19490988552570343 batch: 46/224\n",
      "Batch loss: 0.20574790239334106 batch: 47/224\n",
      "Batch loss: 0.1520199477672577 batch: 48/224\n",
      "Batch loss: 0.15873290598392487 batch: 49/224\n",
      "Batch loss: 0.11236488074064255 batch: 50/224\n",
      "Batch loss: 0.1293543577194214 batch: 51/224\n",
      "Batch loss: 0.13893359899520874 batch: 52/224\n",
      "Batch loss: 0.1920718103647232 batch: 53/224\n",
      "Batch loss: 0.14180666208267212 batch: 54/224\n",
      "Batch loss: 0.13324680924415588 batch: 55/224\n",
      "Batch loss: 0.1847524493932724 batch: 56/224\n",
      "Batch loss: 0.16770224273204803 batch: 57/224\n",
      "Batch loss: 0.16719026863574982 batch: 58/224\n",
      "Batch loss: 0.17840082943439484 batch: 59/224\n",
      "Batch loss: 0.19353064894676208 batch: 60/224\n",
      "Batch loss: 0.1698513627052307 batch: 61/224\n",
      "Batch loss: 0.16719000041484833 batch: 62/224\n",
      "Batch loss: 0.15804411470890045 batch: 63/224\n",
      "Batch loss: 0.19696851074695587 batch: 64/224\n",
      "Batch loss: 0.1731727421283722 batch: 65/224\n",
      "Batch loss: 0.15501618385314941 batch: 66/224\n",
      "Batch loss: 0.15662460029125214 batch: 67/224\n",
      "Batch loss: 0.14524784684181213 batch: 68/224\n",
      "Batch loss: 0.179376021027565 batch: 69/224\n",
      "Batch loss: 0.14805284142494202 batch: 70/224\n",
      "Batch loss: 0.14667266607284546 batch: 71/224\n",
      "Batch loss: 0.13571901619434357 batch: 72/224\n",
      "Batch loss: 0.19568297266960144 batch: 73/224\n",
      "Batch loss: 0.14366769790649414 batch: 74/224\n",
      "Batch loss: 0.18005426228046417 batch: 75/224\n",
      "Batch loss: 0.16714175045490265 batch: 76/224\n",
      "Batch loss: 0.16080307960510254 batch: 77/224\n",
      "Batch loss: 0.19825714826583862 batch: 78/224\n",
      "Batch loss: 0.17591902613639832 batch: 79/224\n",
      "Batch loss: 0.17429444193840027 batch: 80/224\n",
      "Batch loss: 0.214657723903656 batch: 81/224\n",
      "Batch loss: 0.19503742456436157 batch: 82/224\n",
      "Batch loss: 0.150657519698143 batch: 83/224\n",
      "Batch loss: 0.12075542658567429 batch: 84/224\n",
      "Batch loss: 0.1336657702922821 batch: 85/224\n",
      "Batch loss: 0.16620026528835297 batch: 86/224\n",
      "Batch loss: 0.1316419541835785 batch: 87/224\n",
      "Batch loss: 0.17561277747154236 batch: 88/224\n",
      "Batch loss: 0.17341342568397522 batch: 89/224\n",
      "Batch loss: 0.18529249727725983 batch: 90/224\n",
      "Batch loss: 0.1747055947780609 batch: 91/224\n",
      "Batch loss: 0.18698503077030182 batch: 92/224\n",
      "Batch loss: 0.14626522362232208 batch: 93/224\n",
      "Batch loss: 0.17244002223014832 batch: 94/224\n",
      "Batch loss: 0.15736731886863708 batch: 95/224\n",
      "Batch loss: 0.13909192383289337 batch: 96/224\n",
      "Batch loss: 0.11575447022914886 batch: 97/224\n",
      "Batch loss: 0.10866421461105347 batch: 98/224\n",
      "Batch loss: 0.15409345924854279 batch: 99/224\n",
      "Batch loss: 0.1633644700050354 batch: 100/224\n",
      "Batch loss: 0.2214568555355072 batch: 101/224\n",
      "Batch loss: 0.15667769312858582 batch: 102/224\n",
      "Batch loss: 0.19197672605514526 batch: 103/224\n",
      "Batch loss: 0.1372627168893814 batch: 104/224\n",
      "Batch loss: 0.14462991058826447 batch: 105/224\n",
      "Batch loss: 0.12936291098594666 batch: 106/224\n",
      "Batch loss: 0.15522019565105438 batch: 107/224\n",
      "Batch loss: 0.1692771166563034 batch: 108/224\n",
      "Batch loss: 0.13739147782325745 batch: 109/224\n",
      "Batch loss: 0.1408350020647049 batch: 110/224\n",
      "Batch loss: 0.14180530607700348 batch: 111/224\n",
      "Batch loss: 0.12609998881816864 batch: 112/224\n",
      "Batch loss: 0.1832638531923294 batch: 113/224\n",
      "Batch loss: 0.1287626326084137 batch: 114/224\n",
      "Batch loss: 0.17272038757801056 batch: 115/224\n",
      "Batch loss: 0.17164988815784454 batch: 116/224\n",
      "Batch loss: 0.1530301421880722 batch: 117/224\n",
      "Batch loss: 0.17899754643440247 batch: 118/224\n",
      "Batch loss: 0.14798836410045624 batch: 119/224\n",
      "Batch loss: 0.13294926285743713 batch: 120/224\n",
      "Batch loss: 0.15720853209495544 batch: 121/224\n",
      "Batch loss: 0.16216562688350677 batch: 122/224\n",
      "Batch loss: 0.1602959930896759 batch: 123/224\n",
      "Batch loss: 0.1590198129415512 batch: 124/224\n",
      "Batch loss: 0.1707720160484314 batch: 125/224\n",
      "Batch loss: 0.15926580131053925 batch: 126/224\n",
      "Batch loss: 0.19440309703350067 batch: 127/224\n",
      "Batch loss: 0.16188691556453705 batch: 128/224\n",
      "Batch loss: 0.14219748973846436 batch: 129/224\n",
      "Batch loss: 0.1880715936422348 batch: 130/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.1531805843114853 batch: 131/224\n",
      "Batch loss: 0.1446942389011383 batch: 132/224\n",
      "Batch loss: 0.14576558768749237 batch: 133/224\n",
      "Batch loss: 0.15873385965824127 batch: 134/224\n",
      "Batch loss: 0.20177900791168213 batch: 135/224\n",
      "Batch loss: 0.1933574378490448 batch: 136/224\n",
      "Batch loss: 0.1456129550933838 batch: 137/224\n",
      "Batch loss: 0.1407821923494339 batch: 138/224\n",
      "Batch loss: 0.19086386263370514 batch: 139/224\n",
      "Batch loss: 0.19512981176376343 batch: 140/224\n",
      "Batch loss: 0.09068122506141663 batch: 141/224\n",
      "Batch loss: 0.1940430998802185 batch: 142/224\n",
      "Batch loss: 0.16292494535446167 batch: 143/224\n",
      "Batch loss: 0.17184406518936157 batch: 144/224\n",
      "Batch loss: 0.15180741250514984 batch: 145/224\n",
      "Batch loss: 0.19187220931053162 batch: 146/224\n",
      "Batch loss: 0.15258647501468658 batch: 147/224\n",
      "Batch loss: 0.15043295919895172 batch: 148/224\n",
      "Batch loss: 0.18101897835731506 batch: 149/224\n",
      "Batch loss: 0.18189485371112823 batch: 150/224\n",
      "Batch loss: 0.1658160388469696 batch: 151/224\n",
      "Batch loss: 0.13878785073757172 batch: 152/224\n",
      "Batch loss: 0.1772821545600891 batch: 153/224\n",
      "Batch loss: 0.19041705131530762 batch: 154/224\n",
      "Batch loss: 0.14835317432880402 batch: 155/224\n",
      "Batch loss: 0.14768117666244507 batch: 156/224\n",
      "Batch loss: 0.16424570977687836 batch: 157/224\n",
      "Batch loss: 0.22170932590961456 batch: 158/224\n",
      "Batch loss: 0.16051781177520752 batch: 159/224\n",
      "Batch loss: 0.1519593894481659 batch: 160/224\n",
      "Batch loss: 0.14873509109020233 batch: 161/224\n",
      "Batch loss: 0.13261492550373077 batch: 162/224\n",
      "Batch loss: 0.1455310881137848 batch: 163/224\n",
      "Batch loss: 0.18951867520809174 batch: 164/224\n",
      "Batch loss: 0.18646730482578278 batch: 165/224\n",
      "Batch loss: 0.16156844794750214 batch: 166/224\n",
      "Batch loss: 0.11382732540369034 batch: 167/224\n",
      "Batch loss: 0.13296374678611755 batch: 168/224\n",
      "Batch loss: 0.14890579879283905 batch: 169/224\n",
      "Batch loss: 0.13429637253284454 batch: 170/224\n",
      "Batch loss: 0.1516183465719223 batch: 171/224\n",
      "Batch loss: 0.1514253169298172 batch: 172/224\n",
      "Batch loss: 0.1553240865468979 batch: 173/224\n",
      "Batch loss: 0.15006712079048157 batch: 174/224\n",
      "Batch loss: 0.16881343722343445 batch: 175/224\n",
      "Batch loss: 0.18176399171352386 batch: 176/224\n",
      "Batch loss: 0.2123887985944748 batch: 177/224\n",
      "Batch loss: 0.1630561202764511 batch: 178/224\n",
      "Batch loss: 0.19750826060771942 batch: 179/224\n",
      "Batch loss: 0.1262863427400589 batch: 180/224\n",
      "Batch loss: 0.1640213131904602 batch: 181/224\n",
      "Batch loss: 0.17075394093990326 batch: 182/224\n",
      "Batch loss: 0.16698117554187775 batch: 183/224\n",
      "Batch loss: 0.1452256590127945 batch: 184/224\n",
      "Batch loss: 0.1900988072156906 batch: 185/224\n",
      "Batch loss: 0.12675534188747406 batch: 186/224\n",
      "Batch loss: 0.14599287509918213 batch: 187/224\n",
      "Batch loss: 0.13287262618541718 batch: 188/224\n",
      "Batch loss: 0.15498259663581848 batch: 189/224\n",
      "Batch loss: 0.15416793525218964 batch: 190/224\n",
      "Batch loss: 0.15569594502449036 batch: 191/224\n",
      "Batch loss: 0.18059879541397095 batch: 192/224\n",
      "Batch loss: 0.17551067471504211 batch: 193/224\n",
      "Batch loss: 0.14264914393424988 batch: 194/224\n",
      "Batch loss: 0.20946727693080902 batch: 195/224\n",
      "Batch loss: 0.19830818474292755 batch: 196/224\n",
      "Batch loss: 0.19302663207054138 batch: 197/224\n",
      "Batch loss: 0.12424476444721222 batch: 198/224\n",
      "Batch loss: 0.11062256991863251 batch: 199/224\n",
      "Batch loss: 0.19794631004333496 batch: 200/224\n",
      "Batch loss: 0.16867688298225403 batch: 201/224\n",
      "Batch loss: 0.18055588006973267 batch: 202/224\n",
      "Batch loss: 0.15456397831439972 batch: 203/224\n",
      "Batch loss: 0.15740661323070526 batch: 204/224\n",
      "Batch loss: 0.17761877179145813 batch: 205/224\n",
      "Batch loss: 0.14318957924842834 batch: 206/224\n",
      "Batch loss: 0.1398484706878662 batch: 207/224\n",
      "Batch loss: 0.1807759553194046 batch: 208/224\n",
      "Batch loss: 0.18020346760749817 batch: 209/224\n",
      "Batch loss: 0.14050784707069397 batch: 210/224\n",
      "Batch loss: 0.11940977722406387 batch: 211/224\n",
      "Batch loss: 0.18026995658874512 batch: 212/224\n",
      "Batch loss: 0.169704869389534 batch: 213/224\n",
      "Batch loss: 0.143098846077919 batch: 214/224\n",
      "Batch loss: 0.17228995263576508 batch: 215/224\n",
      "Batch loss: 0.17787675559520721 batch: 216/224\n",
      "Batch loss: 0.18471166491508484 batch: 217/224\n",
      "Batch loss: 0.1502285748720169 batch: 218/224\n",
      "Batch loss: 0.13470996916294098 batch: 219/224\n",
      "Batch loss: 0.13971735537052155 batch: 220/224\n",
      "Batch loss: 0.1825944036245346 batch: 221/224\n",
      "Batch loss: 0.20676110684871674 batch: 222/224\n",
      "Batch loss: 0.12719547748565674 batch: 223/224\n",
      "Batch loss: 0.14957255125045776 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 30/75..  Training Loss: 0.00032..  Test Loss: 0.00076..  Test Accuracy: 0.88782\n",
      "Running epoch 31/75\n",
      "Batch loss: 0.128257155418396 batch: 1/224\n",
      "Batch loss: 0.14695219695568085 batch: 2/224\n",
      "Batch loss: 0.15771910548210144 batch: 3/224\n",
      "Batch loss: 0.18597400188446045 batch: 4/224\n",
      "Batch loss: 0.142501100897789 batch: 5/224\n",
      "Batch loss: 0.16456742584705353 batch: 6/224\n",
      "Batch loss: 0.1366325467824936 batch: 7/224\n",
      "Batch loss: 0.16442911326885223 batch: 8/224\n",
      "Batch loss: 0.13382430374622345 batch: 9/224\n",
      "Batch loss: 0.14724613726139069 batch: 10/224\n",
      "Batch loss: 0.1925298571586609 batch: 11/224\n",
      "Batch loss: 0.14705581963062286 batch: 12/224\n",
      "Batch loss: 0.14047275483608246 batch: 13/224\n",
      "Batch loss: 0.15359807014465332 batch: 14/224\n",
      "Batch loss: 0.14832326769828796 batch: 15/224\n",
      "Batch loss: 0.18764862418174744 batch: 16/224\n",
      "Batch loss: 0.15898680686950684 batch: 17/224\n",
      "Batch loss: 0.1743566244840622 batch: 18/224\n",
      "Batch loss: 0.15530914068222046 batch: 19/224\n",
      "Batch loss: 0.17535217106342316 batch: 20/224\n",
      "Batch loss: 0.12285325676202774 batch: 21/224\n",
      "Batch loss: 0.12648119032382965 batch: 22/224\n",
      "Batch loss: 0.14446014165878296 batch: 23/224\n",
      "Batch loss: 0.19993968307971954 batch: 24/224\n",
      "Batch loss: 0.13503943383693695 batch: 25/224\n",
      "Batch loss: 0.14863373339176178 batch: 26/224\n",
      "Batch loss: 0.19188565015792847 batch: 27/224\n",
      "Batch loss: 0.13681092858314514 batch: 28/224\n",
      "Batch loss: 0.2039245218038559 batch: 29/224\n",
      "Batch loss: 0.16025106608867645 batch: 30/224\n",
      "Batch loss: 0.14589479565620422 batch: 31/224\n",
      "Batch loss: 0.1808895468711853 batch: 32/224\n",
      "Batch loss: 0.11037707328796387 batch: 33/224\n",
      "Batch loss: 0.16353382170200348 batch: 34/224\n",
      "Batch loss: 0.17660818994045258 batch: 35/224\n",
      "Batch loss: 0.1617869734764099 batch: 36/224\n",
      "Batch loss: 0.15884602069854736 batch: 37/224\n",
      "Batch loss: 0.1597476452589035 batch: 38/224\n",
      "Batch loss: 0.16780629754066467 batch: 39/224\n",
      "Batch loss: 0.12992334365844727 batch: 40/224\n",
      "Batch loss: 0.14557254314422607 batch: 41/224\n",
      "Batch loss: 0.16693061590194702 batch: 42/224\n",
      "Batch loss: 0.19231507182121277 batch: 43/224\n",
      "Batch loss: 0.1347285807132721 batch: 44/224\n",
      "Batch loss: 0.12004349380731583 batch: 45/224\n",
      "Batch loss: 0.19105207920074463 batch: 46/224\n",
      "Batch loss: 0.15686342120170593 batch: 47/224\n",
      "Batch loss: 0.13490425050258636 batch: 48/224\n",
      "Batch loss: 0.13517022132873535 batch: 49/224\n",
      "Batch loss: 0.1461910903453827 batch: 50/224\n",
      "Batch loss: 0.1419692188501358 batch: 51/224\n",
      "Batch loss: 0.16421088576316833 batch: 52/224\n",
      "Batch loss: 0.19647498428821564 batch: 53/224\n",
      "Batch loss: 0.14036999642848969 batch: 54/224\n",
      "Batch loss: 0.12138631194829941 batch: 55/224\n",
      "Batch loss: 0.1387685388326645 batch: 56/224\n",
      "Batch loss: 0.20840555429458618 batch: 57/224\n",
      "Batch loss: 0.17756399512290955 batch: 58/224\n",
      "Batch loss: 0.15001243352890015 batch: 59/224\n",
      "Batch loss: 0.18020278215408325 batch: 60/224\n",
      "Batch loss: 0.16020949184894562 batch: 61/224\n",
      "Batch loss: 0.13856154680252075 batch: 62/224\n",
      "Batch loss: 0.15014812350273132 batch: 63/224\n",
      "Batch loss: 0.15572968125343323 batch: 64/224\n",
      "Batch loss: 0.17191804945468903 batch: 65/224\n",
      "Batch loss: 0.17860367894172668 batch: 66/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.1371823251247406 batch: 67/224\n",
      "Batch loss: 0.1650208830833435 batch: 68/224\n",
      "Batch loss: 0.18061448633670807 batch: 69/224\n",
      "Batch loss: 0.13793838024139404 batch: 70/224\n",
      "Batch loss: 0.1539684534072876 batch: 71/224\n",
      "Batch loss: 0.1103355884552002 batch: 72/224\n",
      "Batch loss: 0.18615801632404327 batch: 73/224\n",
      "Batch loss: 0.14504052698612213 batch: 74/224\n",
      "Batch loss: 0.16405589878559113 batch: 75/224\n",
      "Batch loss: 0.185674250125885 batch: 76/224\n",
      "Batch loss: 0.15734335780143738 batch: 77/224\n",
      "Batch loss: 0.1721065789461136 batch: 78/224\n",
      "Batch loss: 0.16823403537273407 batch: 79/224\n",
      "Batch loss: 0.1857365369796753 batch: 80/224\n",
      "Batch loss: 0.2137480080127716 batch: 81/224\n",
      "Batch loss: 0.17140501737594604 batch: 82/224\n",
      "Batch loss: 0.1611669659614563 batch: 83/224\n",
      "Batch loss: 0.11646126210689545 batch: 84/224\n",
      "Batch loss: 0.13411997258663177 batch: 85/224\n",
      "Batch loss: 0.1617879718542099 batch: 86/224\n",
      "Batch loss: 0.1672874093055725 batch: 87/224\n",
      "Batch loss: 0.21049541234970093 batch: 88/224\n",
      "Batch loss: 0.15484103560447693 batch: 89/224\n",
      "Batch loss: 0.2074342519044876 batch: 90/224\n",
      "Batch loss: 0.14130747318267822 batch: 91/224\n",
      "Batch loss: 0.15484601259231567 batch: 92/224\n",
      "Batch loss: 0.13413886725902557 batch: 93/224\n",
      "Batch loss: 0.13945534825325012 batch: 94/224\n",
      "Batch loss: 0.11047879606485367 batch: 95/224\n",
      "Batch loss: 0.12945860624313354 batch: 96/224\n",
      "Batch loss: 0.15022781491279602 batch: 97/224\n",
      "Batch loss: 0.14339493215084076 batch: 98/224\n",
      "Batch loss: 0.19032913446426392 batch: 99/224\n",
      "Batch loss: 0.1468760073184967 batch: 100/224\n",
      "Batch loss: 0.14860843122005463 batch: 101/224\n",
      "Batch loss: 0.1656751036643982 batch: 102/224\n",
      "Batch loss: 0.18717624247074127 batch: 103/224\n",
      "Batch loss: 0.14353038370609283 batch: 104/224\n",
      "Batch loss: 0.127541184425354 batch: 105/224\n",
      "Batch loss: 0.12263786047697067 batch: 106/224\n",
      "Batch loss: 0.1336684226989746 batch: 107/224\n",
      "Batch loss: 0.1715897172689438 batch: 108/224\n",
      "Batch loss: 0.1175989881157875 batch: 109/224\n",
      "Batch loss: 0.1697753667831421 batch: 110/224\n",
      "Batch loss: 0.11281801015138626 batch: 111/224\n",
      "Batch loss: 0.1368110030889511 batch: 112/224\n",
      "Batch loss: 0.16212023794651031 batch: 113/224\n",
      "Batch loss: 0.14510323107242584 batch: 114/224\n",
      "Batch loss: 0.15828241407871246 batch: 115/224\n",
      "Batch loss: 0.16166572272777557 batch: 116/224\n",
      "Batch loss: 0.12660208344459534 batch: 117/224\n",
      "Batch loss: 0.16597189009189606 batch: 118/224\n",
      "Batch loss: 0.1224592849612236 batch: 119/224\n",
      "Batch loss: 0.14289072155952454 batch: 120/224\n",
      "Batch loss: 0.12211623787879944 batch: 121/224\n",
      "Batch loss: 0.12046942114830017 batch: 122/224\n",
      "Batch loss: 0.15607091784477234 batch: 123/224\n",
      "Batch loss: 0.16590261459350586 batch: 124/224\n",
      "Batch loss: 0.1582256257534027 batch: 125/224\n",
      "Batch loss: 0.17049500346183777 batch: 126/224\n",
      "Batch loss: 0.18210308253765106 batch: 127/224\n",
      "Batch loss: 0.16670441627502441 batch: 128/224\n",
      "Batch loss: 0.1572420746088028 batch: 129/224\n",
      "Batch loss: 0.1542617529630661 batch: 130/224\n",
      "Batch loss: 0.09802930057048798 batch: 131/224\n",
      "Batch loss: 0.17407718300819397 batch: 132/224\n",
      "Batch loss: 0.1824822574853897 batch: 133/224\n",
      "Batch loss: 0.1355980783700943 batch: 134/224\n",
      "Batch loss: 0.18235653638839722 batch: 135/224\n",
      "Batch loss: 0.15538658201694489 batch: 136/224\n",
      "Batch loss: 0.16291800141334534 batch: 137/224\n",
      "Batch loss: 0.12964381277561188 batch: 138/224\n",
      "Batch loss: 0.14679886400699615 batch: 139/224\n",
      "Batch loss: 0.1957489252090454 batch: 140/224\n",
      "Batch loss: 0.1385713815689087 batch: 141/224\n",
      "Batch loss: 0.12240351736545563 batch: 142/224\n",
      "Batch loss: 0.126008540391922 batch: 143/224\n",
      "Batch loss: 0.15891017019748688 batch: 144/224\n",
      "Batch loss: 0.1598769575357437 batch: 145/224\n",
      "Batch loss: 0.19538018107414246 batch: 146/224\n",
      "Batch loss: 0.1433989703655243 batch: 147/224\n",
      "Batch loss: 0.1848263144493103 batch: 148/224\n",
      "Batch loss: 0.17232251167297363 batch: 149/224\n",
      "Batch loss: 0.23175592720508575 batch: 150/224\n",
      "Batch loss: 0.1608004868030548 batch: 151/224\n",
      "Batch loss: 0.1680831015110016 batch: 152/224\n",
      "Batch loss: 0.18991385400295258 batch: 153/224\n",
      "Batch loss: 0.20239126682281494 batch: 154/224\n",
      "Batch loss: 0.12099379301071167 batch: 155/224\n",
      "Batch loss: 0.16165821254253387 batch: 156/224\n",
      "Batch loss: 0.19902944564819336 batch: 157/224\n",
      "Batch loss: 0.23293447494506836 batch: 158/224\n",
      "Batch loss: 0.1520601361989975 batch: 159/224\n",
      "Batch loss: 0.1485361009836197 batch: 160/224\n",
      "Batch loss: 0.12895865738391876 batch: 161/224\n",
      "Batch loss: 0.1441720724105835 batch: 162/224\n",
      "Batch loss: 0.11385805159807205 batch: 163/224\n",
      "Batch loss: 0.11863633245229721 batch: 164/224\n",
      "Batch loss: 0.1856977492570877 batch: 165/224\n",
      "Batch loss: 0.1407334804534912 batch: 166/224\n",
      "Batch loss: 0.10477080941200256 batch: 167/224\n",
      "Batch loss: 0.13513389229774475 batch: 168/224\n",
      "Batch loss: 0.16208574175834656 batch: 169/224\n",
      "Batch loss: 0.13411907851696014 batch: 170/224\n",
      "Batch loss: 0.13965755701065063 batch: 171/224\n",
      "Batch loss: 0.1344255805015564 batch: 172/224\n",
      "Batch loss: 0.14046849310398102 batch: 173/224\n",
      "Batch loss: 0.12460886687040329 batch: 174/224\n",
      "Batch loss: 0.1386641412973404 batch: 175/224\n",
      "Batch loss: 0.1157572865486145 batch: 176/224\n",
      "Batch loss: 0.17740094661712646 batch: 177/224\n",
      "Batch loss: 0.15599097311496735 batch: 178/224\n",
      "Batch loss: 0.18216769397258759 batch: 179/224\n",
      "Batch loss: 0.10784316807985306 batch: 180/224\n",
      "Batch loss: 0.12588220834732056 batch: 181/224\n",
      "Batch loss: 0.18608082830905914 batch: 182/224\n",
      "Batch loss: 0.17231839895248413 batch: 183/224\n",
      "Batch loss: 0.15538276731967926 batch: 184/224\n",
      "Batch loss: 0.15993840992450714 batch: 185/224\n",
      "Batch loss: 0.16042590141296387 batch: 186/224\n",
      "Batch loss: 0.15853889286518097 batch: 187/224\n",
      "Batch loss: 0.15428036451339722 batch: 188/224\n",
      "Batch loss: 0.17659024894237518 batch: 189/224\n",
      "Batch loss: 0.16702571511268616 batch: 190/224\n",
      "Batch loss: 0.1676672101020813 batch: 191/224\n",
      "Batch loss: 0.17497248947620392 batch: 192/224\n",
      "Batch loss: 0.1584973931312561 batch: 193/224\n",
      "Batch loss: 0.17012561857700348 batch: 194/224\n",
      "Batch loss: 0.19666290283203125 batch: 195/224\n",
      "Batch loss: 0.20226062834262848 batch: 196/224\n",
      "Batch loss: 0.13785289227962494 batch: 197/224\n",
      "Batch loss: 0.15193109214305878 batch: 198/224\n",
      "Batch loss: 0.12658007442951202 batch: 199/224\n",
      "Batch loss: 0.1780363768339157 batch: 200/224\n",
      "Batch loss: 0.1662072241306305 batch: 201/224\n",
      "Batch loss: 0.14482225477695465 batch: 202/224\n",
      "Batch loss: 0.12678425014019012 batch: 203/224\n",
      "Batch loss: 0.14876317977905273 batch: 204/224\n",
      "Batch loss: 0.1966792792081833 batch: 205/224\n",
      "Batch loss: 0.18061567842960358 batch: 206/224\n",
      "Batch loss: 0.15018445253372192 batch: 207/224\n",
      "Batch loss: 0.1338486224412918 batch: 208/224\n",
      "Batch loss: 0.14513906836509705 batch: 209/224\n",
      "Batch loss: 0.14279064536094666 batch: 210/224\n",
      "Batch loss: 0.16362504661083221 batch: 211/224\n",
      "Batch loss: 0.1480993628501892 batch: 212/224\n",
      "Batch loss: 0.18052777647972107 batch: 213/224\n",
      "Batch loss: 0.16786551475524902 batch: 214/224\n",
      "Batch loss: 0.18581868708133698 batch: 215/224\n",
      "Batch loss: 0.1347084939479828 batch: 216/224\n",
      "Batch loss: 0.18075063824653625 batch: 217/224\n",
      "Batch loss: 0.16899365186691284 batch: 218/224\n",
      "Batch loss: 0.1197039783000946 batch: 219/224\n",
      "Batch loss: 0.11148448288440704 batch: 220/224\n",
      "Batch loss: 0.1817469298839569 batch: 221/224\n",
      "Batch loss: 0.1716124415397644 batch: 222/224\n",
      "Batch loss: 0.12201958894729614 batch: 223/224\n",
      "Batch loss: 0.11681029945611954 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 31/75..  Training Loss: 0.00031..  Test Loss: 0.00075..  Test Accuracy: 0.88925\n",
      "Running epoch 32/75\n",
      "Batch loss: 0.14854644238948822 batch: 1/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.15050740540027618 batch: 2/224\n",
      "Batch loss: 0.12094371765851974 batch: 3/224\n",
      "Batch loss: 0.14591337740421295 batch: 4/224\n",
      "Batch loss: 0.1862615942955017 batch: 5/224\n",
      "Batch loss: 0.11993449181318283 batch: 6/224\n",
      "Batch loss: 0.13283707201480865 batch: 7/224\n",
      "Batch loss: 0.1537650227546692 batch: 8/224\n",
      "Batch loss: 0.15785536170005798 batch: 9/224\n",
      "Batch loss: 0.15283651649951935 batch: 10/224\n",
      "Batch loss: 0.16568239033222198 batch: 11/224\n",
      "Batch loss: 0.15151487290859222 batch: 12/224\n",
      "Batch loss: 0.11374643445014954 batch: 13/224\n",
      "Batch loss: 0.10928449034690857 batch: 14/224\n",
      "Batch loss: 0.1455731838941574 batch: 15/224\n",
      "Batch loss: 0.1812455654144287 batch: 16/224\n",
      "Batch loss: 0.13487285375595093 batch: 17/224\n",
      "Batch loss: 0.18737220764160156 batch: 18/224\n",
      "Batch loss: 0.11322852224111557 batch: 19/224\n",
      "Batch loss: 0.11699800938367844 batch: 20/224\n",
      "Batch loss: 0.1878051906824112 batch: 21/224\n",
      "Batch loss: 0.14402249455451965 batch: 22/224\n",
      "Batch loss: 0.14709797501564026 batch: 23/224\n",
      "Batch loss: 0.17669452726840973 batch: 24/224\n",
      "Batch loss: 0.10170304775238037 batch: 25/224\n",
      "Batch loss: 0.13303737342357635 batch: 26/224\n",
      "Batch loss: 0.16361425817012787 batch: 27/224\n",
      "Batch loss: 0.1305263191461563 batch: 28/224\n",
      "Batch loss: 0.17091141641139984 batch: 29/224\n",
      "Batch loss: 0.14643847942352295 batch: 30/224\n",
      "Batch loss: 0.13297486305236816 batch: 31/224\n",
      "Batch loss: 0.18620848655700684 batch: 32/224\n",
      "Batch loss: 0.1389935165643692 batch: 33/224\n",
      "Batch loss: 0.17989593744277954 batch: 34/224\n",
      "Batch loss: 0.1986851841211319 batch: 35/224\n",
      "Batch loss: 0.16683587431907654 batch: 36/224\n",
      "Batch loss: 0.1334245204925537 batch: 37/224\n",
      "Batch loss: 0.19068433344364166 batch: 38/224\n",
      "Batch loss: 0.1520037204027176 batch: 39/224\n",
      "Batch loss: 0.14772570133209229 batch: 40/224\n",
      "Batch loss: 0.19782821834087372 batch: 41/224\n",
      "Batch loss: 0.17263701558113098 batch: 42/224\n",
      "Batch loss: 0.14973299205303192 batch: 43/224\n",
      "Batch loss: 0.12021481245756149 batch: 44/224\n",
      "Batch loss: 0.10199755430221558 batch: 45/224\n",
      "Batch loss: 0.19176487624645233 batch: 46/224\n",
      "Batch loss: 0.1620427370071411 batch: 47/224\n",
      "Batch loss: 0.15669788420200348 batch: 48/224\n",
      "Batch loss: 0.10805816203355789 batch: 49/224\n",
      "Batch loss: 0.11918376386165619 batch: 50/224\n",
      "Batch loss: 0.14537091553211212 batch: 51/224\n",
      "Batch loss: 0.1280260980129242 batch: 52/224\n",
      "Batch loss: 0.19293387234210968 batch: 53/224\n",
      "Batch loss: 0.11624782532453537 batch: 54/224\n",
      "Batch loss: 0.1335023194551468 batch: 55/224\n",
      "Batch loss: 0.12680673599243164 batch: 56/224\n",
      "Batch loss: 0.18951362371444702 batch: 57/224\n",
      "Batch loss: 0.14719216525554657 batch: 58/224\n",
      "Batch loss: 0.1384909451007843 batch: 59/224\n",
      "Batch loss: 0.16856949031352997 batch: 60/224\n",
      "Batch loss: 0.14650195837020874 batch: 61/224\n",
      "Batch loss: 0.12118936330080032 batch: 62/224\n",
      "Batch loss: 0.16579081118106842 batch: 63/224\n",
      "Batch loss: 0.1897541880607605 batch: 64/224\n",
      "Batch loss: 0.1959858387708664 batch: 65/224\n",
      "Batch loss: 0.19297125935554504 batch: 66/224\n",
      "Batch loss: 0.13704711198806763 batch: 67/224\n",
      "Batch loss: 0.19180892407894135 batch: 68/224\n",
      "Batch loss: 0.17357753217220306 batch: 69/224\n",
      "Batch loss: 0.1360298991203308 batch: 70/224\n",
      "Batch loss: 0.15109778940677643 batch: 71/224\n",
      "Batch loss: 0.12928642332553864 batch: 72/224\n",
      "Batch loss: 0.19060470163822174 batch: 73/224\n",
      "Batch loss: 0.17634275555610657 batch: 74/224\n",
      "Batch loss: 0.15550543367862701 batch: 75/224\n",
      "Batch loss: 0.18651443719863892 batch: 76/224\n",
      "Batch loss: 0.14583751559257507 batch: 77/224\n",
      "Batch loss: 0.14956481754779816 batch: 78/224\n",
      "Batch loss: 0.18304473161697388 batch: 79/224\n",
      "Batch loss: 0.153263658285141 batch: 80/224\n",
      "Batch loss: 0.20244790613651276 batch: 81/224\n",
      "Batch loss: 0.1785905659198761 batch: 82/224\n",
      "Batch loss: 0.16340957581996918 batch: 83/224\n",
      "Batch loss: 0.12832064926624298 batch: 84/224\n",
      "Batch loss: 0.13504265248775482 batch: 85/224\n",
      "Batch loss: 0.13753050565719604 batch: 86/224\n",
      "Batch loss: 0.15093465149402618 batch: 87/224\n",
      "Batch loss: 0.1683039367198944 batch: 88/224\n",
      "Batch loss: 0.1308092325925827 batch: 89/224\n",
      "Batch loss: 0.19119150936603546 batch: 90/224\n",
      "Batch loss: 0.14078570902347565 batch: 91/224\n",
      "Batch loss: 0.16476912796497345 batch: 92/224\n",
      "Batch loss: 0.1627497524023056 batch: 93/224\n",
      "Batch loss: 0.15421485900878906 batch: 94/224\n",
      "Batch loss: 0.12088728696107864 batch: 95/224\n",
      "Batch loss: 0.16027700901031494 batch: 96/224\n",
      "Batch loss: 0.12388472259044647 batch: 97/224\n",
      "Batch loss: 0.09472786635160446 batch: 98/224\n",
      "Batch loss: 0.160366028547287 batch: 99/224\n",
      "Batch loss: 0.15189704298973083 batch: 100/224\n",
      "Batch loss: 0.1614605337381363 batch: 101/224\n",
      "Batch loss: 0.13474328815937042 batch: 102/224\n",
      "Batch loss: 0.16474057734012604 batch: 103/224\n",
      "Batch loss: 0.15273107588291168 batch: 104/224\n",
      "Batch loss: 0.12831661105155945 batch: 105/224\n",
      "Batch loss: 0.1441066563129425 batch: 106/224\n",
      "Batch loss: 0.1193646788597107 batch: 107/224\n",
      "Batch loss: 0.12496350705623627 batch: 108/224\n",
      "Batch loss: 0.12743696570396423 batch: 109/224\n",
      "Batch loss: 0.1634833663702011 batch: 110/224\n",
      "Batch loss: 0.14870639145374298 batch: 111/224\n",
      "Batch loss: 0.1333007961511612 batch: 112/224\n",
      "Batch loss: 0.15656663477420807 batch: 113/224\n",
      "Batch loss: 0.1251549869775772 batch: 114/224\n",
      "Batch loss: 0.1416919231414795 batch: 115/224\n",
      "Batch loss: 0.12798240780830383 batch: 116/224\n",
      "Batch loss: 0.11449984461069107 batch: 117/224\n",
      "Batch loss: 0.180254727602005 batch: 118/224\n",
      "Batch loss: 0.1351798176765442 batch: 119/224\n",
      "Batch loss: 0.19780205190181732 batch: 120/224\n",
      "Batch loss: 0.13786041736602783 batch: 121/224\n",
      "Batch loss: 0.1389368623495102 batch: 122/224\n",
      "Batch loss: 0.12112920731306076 batch: 123/224\n",
      "Batch loss: 0.1435052901506424 batch: 124/224\n",
      "Batch loss: 0.12480069696903229 batch: 125/224\n",
      "Batch loss: 0.1774349808692932 batch: 126/224\n",
      "Batch loss: 0.15967245399951935 batch: 127/224\n",
      "Batch loss: 0.10872672498226166 batch: 128/224\n",
      "Batch loss: 0.1462821513414383 batch: 129/224\n",
      "Batch loss: 0.13808050751686096 batch: 130/224\n",
      "Batch loss: 0.11057809740304947 batch: 131/224\n",
      "Batch loss: 0.17819756269454956 batch: 132/224\n",
      "Batch loss: 0.1555921584367752 batch: 133/224\n",
      "Batch loss: 0.16286802291870117 batch: 134/224\n",
      "Batch loss: 0.15028071403503418 batch: 135/224\n",
      "Batch loss: 0.1714135855436325 batch: 136/224\n",
      "Batch loss: 0.16903725266456604 batch: 137/224\n",
      "Batch loss: 0.16658419370651245 batch: 138/224\n",
      "Batch loss: 0.1669030487537384 batch: 139/224\n",
      "Batch loss: 0.17233607172966003 batch: 140/224\n",
      "Batch loss: 0.10588567703962326 batch: 141/224\n",
      "Batch loss: 0.1462487429380417 batch: 142/224\n",
      "Batch loss: 0.15163971483707428 batch: 143/224\n",
      "Batch loss: 0.1565258502960205 batch: 144/224\n",
      "Batch loss: 0.1767803281545639 batch: 145/224\n",
      "Batch loss: 0.18097850680351257 batch: 146/224\n",
      "Batch loss: 0.14699284732341766 batch: 147/224\n",
      "Batch loss: 0.14007090032100677 batch: 148/224\n",
      "Batch loss: 0.1596881002187729 batch: 149/224\n",
      "Batch loss: 0.19120731949806213 batch: 150/224\n",
      "Batch loss: 0.15497466921806335 batch: 151/224\n",
      "Batch loss: 0.14652006328105927 batch: 152/224\n",
      "Batch loss: 0.1918521523475647 batch: 153/224\n",
      "Batch loss: 0.19149290025234222 batch: 154/224\n",
      "Batch loss: 0.1341938078403473 batch: 155/224\n",
      "Batch loss: 0.1179361566901207 batch: 156/224\n",
      "Batch loss: 0.15270625054836273 batch: 157/224\n",
      "Batch loss: 0.2179565578699112 batch: 158/224\n",
      "Batch loss: 0.16060075163841248 batch: 159/224\n",
      "Batch loss: 0.13300491869449615 batch: 160/224\n",
      "Batch loss: 0.12010378390550613 batch: 161/224\n",
      "Batch loss: 0.15246862173080444 batch: 162/224\n",
      "Batch loss: 0.12827366590499878 batch: 163/224\n",
      "Batch loss: 0.1513075977563858 batch: 164/224\n",
      "Batch loss: 0.17462684214115143 batch: 165/224\n",
      "Batch loss: 0.15421205759048462 batch: 166/224\n",
      "Batch loss: 0.1228364035487175 batch: 167/224\n",
      "Batch loss: 0.1327945739030838 batch: 168/224\n",
      "Batch loss: 0.13631226122379303 batch: 169/224\n",
      "Batch loss: 0.13986679911613464 batch: 170/224\n",
      "Batch loss: 0.13822872936725616 batch: 171/224\n",
      "Batch loss: 0.1398230940103531 batch: 172/224\n",
      "Batch loss: 0.1714981347322464 batch: 173/224\n",
      "Batch loss: 0.14165647327899933 batch: 174/224\n",
      "Batch loss: 0.15586169064044952 batch: 175/224\n",
      "Batch loss: 0.12755733728408813 batch: 176/224\n",
      "Batch loss: 0.16566865146160126 batch: 177/224\n",
      "Batch loss: 0.16539470851421356 batch: 178/224\n",
      "Batch loss: 0.1466728001832962 batch: 179/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.09054672718048096 batch: 180/224\n",
      "Batch loss: 0.11906340718269348 batch: 181/224\n",
      "Batch loss: 0.15307320654392242 batch: 182/224\n",
      "Batch loss: 0.19625011086463928 batch: 183/224\n",
      "Batch loss: 0.1655779927968979 batch: 184/224\n",
      "Batch loss: 0.2009725719690323 batch: 185/224\n",
      "Batch loss: 0.12650324404239655 batch: 186/224\n",
      "Batch loss: 0.13620637357234955 batch: 187/224\n",
      "Batch loss: 0.12524595856666565 batch: 188/224\n",
      "Batch loss: 0.17354275286197662 batch: 189/224\n",
      "Batch loss: 0.14572861790657043 batch: 190/224\n",
      "Batch loss: 0.1475854218006134 batch: 191/224\n",
      "Batch loss: 0.13487444818019867 batch: 192/224\n",
      "Batch loss: 0.145605206489563 batch: 193/224\n",
      "Batch loss: 0.1423080712556839 batch: 194/224\n",
      "Batch loss: 0.18653719127178192 batch: 195/224\n",
      "Batch loss: 0.19516263902187347 batch: 196/224\n",
      "Batch loss: 0.15571492910385132 batch: 197/224\n",
      "Batch loss: 0.1271476149559021 batch: 198/224\n",
      "Batch loss: 0.1310807764530182 batch: 199/224\n",
      "Batch loss: 0.1724977195262909 batch: 200/224\n",
      "Batch loss: 0.1787063181400299 batch: 201/224\n",
      "Batch loss: 0.1723177134990692 batch: 202/224\n",
      "Batch loss: 0.1365114450454712 batch: 203/224\n",
      "Batch loss: 0.17003552615642548 batch: 204/224\n",
      "Batch loss: 0.1740293651819229 batch: 205/224\n",
      "Batch loss: 0.1329338401556015 batch: 206/224\n",
      "Batch loss: 0.1488824337720871 batch: 207/224\n",
      "Batch loss: 0.12555906176567078 batch: 208/224\n",
      "Batch loss: 0.14661630988121033 batch: 209/224\n",
      "Batch loss: 0.13235414028167725 batch: 210/224\n",
      "Batch loss: 0.14501510560512543 batch: 211/224\n",
      "Batch loss: 0.16452069580554962 batch: 212/224\n",
      "Batch loss: 0.16678176820278168 batch: 213/224\n",
      "Batch loss: 0.16204406321048737 batch: 214/224\n",
      "Batch loss: 0.17746663093566895 batch: 215/224\n",
      "Batch loss: 0.1281176656484604 batch: 216/224\n",
      "Batch loss: 0.15525448322296143 batch: 217/224\n",
      "Batch loss: 0.17138753831386566 batch: 218/224\n",
      "Batch loss: 0.11524954438209534 batch: 219/224\n",
      "Batch loss: 0.11648275703191757 batch: 220/224\n",
      "Batch loss: 0.16990213096141815 batch: 221/224\n",
      "Batch loss: 0.17673245072364807 batch: 222/224\n",
      "Batch loss: 0.13189586997032166 batch: 223/224\n",
      "Batch loss: 0.1323319524526596 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 32/75..  Training Loss: 0.00030..  Test Loss: 0.00078..  Test Accuracy: 0.88814\n",
      "Running epoch 33/75\n",
      "Batch loss: 0.13290643692016602 batch: 1/224\n",
      "Batch loss: 0.14125481247901917 batch: 2/224\n",
      "Batch loss: 0.1102423444390297 batch: 3/224\n",
      "Batch loss: 0.15860673785209656 batch: 4/224\n",
      "Batch loss: 0.13676504790782928 batch: 5/224\n",
      "Batch loss: 0.13585349917411804 batch: 6/224\n",
      "Batch loss: 0.11707676947116852 batch: 7/224\n",
      "Batch loss: 0.12317362427711487 batch: 8/224\n",
      "Batch loss: 0.15098269283771515 batch: 9/224\n",
      "Batch loss: 0.13663583993911743 batch: 10/224\n",
      "Batch loss: 0.1534813940525055 batch: 11/224\n",
      "Batch loss: 0.15053313970565796 batch: 12/224\n",
      "Batch loss: 0.11669468879699707 batch: 13/224\n",
      "Batch loss: 0.09844645857810974 batch: 14/224\n",
      "Batch loss: 0.12508530914783478 batch: 15/224\n",
      "Batch loss: 0.17413030564785004 batch: 16/224\n",
      "Batch loss: 0.15322189033031464 batch: 17/224\n",
      "Batch loss: 0.1841261386871338 batch: 18/224\n",
      "Batch loss: 0.13595746457576752 batch: 19/224\n",
      "Batch loss: 0.14614379405975342 batch: 20/224\n",
      "Batch loss: 0.1373690664768219 batch: 21/224\n",
      "Batch loss: 0.1215335950255394 batch: 22/224\n",
      "Batch loss: 0.12177151441574097 batch: 23/224\n",
      "Batch loss: 0.1598200649023056 batch: 24/224\n",
      "Batch loss: 0.11312439292669296 batch: 25/224\n",
      "Batch loss: 0.10746714472770691 batch: 26/224\n",
      "Batch loss: 0.16004209220409393 batch: 27/224\n",
      "Batch loss: 0.15082629024982452 batch: 28/224\n",
      "Batch loss: 0.1701577603816986 batch: 29/224\n",
      "Batch loss: 0.14453764259815216 batch: 30/224\n",
      "Batch loss: 0.14042910933494568 batch: 31/224\n",
      "Batch loss: 0.1599685102701187 batch: 32/224\n",
      "Batch loss: 0.1545049250125885 batch: 33/224\n",
      "Batch loss: 0.1461396962404251 batch: 34/224\n",
      "Batch loss: 0.1551886796951294 batch: 35/224\n",
      "Batch loss: 0.13916213810443878 batch: 36/224\n",
      "Batch loss: 0.13053293526172638 batch: 37/224\n",
      "Batch loss: 0.13083365559577942 batch: 38/224\n",
      "Batch loss: 0.1514504998922348 batch: 39/224\n",
      "Batch loss: 0.15600475668907166 batch: 40/224\n",
      "Batch loss: 0.1559012234210968 batch: 41/224\n",
      "Batch loss: 0.15851660072803497 batch: 42/224\n",
      "Batch loss: 0.15386560559272766 batch: 43/224\n",
      "Batch loss: 0.1218942329287529 batch: 44/224\n",
      "Batch loss: 0.14341440796852112 batch: 45/224\n",
      "Batch loss: 0.1766294538974762 batch: 46/224\n",
      "Batch loss: 0.16901636123657227 batch: 47/224\n",
      "Batch loss: 0.13259795308113098 batch: 48/224\n",
      "Batch loss: 0.12263650447130203 batch: 49/224\n",
      "Batch loss: 0.13892890512943268 batch: 50/224\n",
      "Batch loss: 0.14311741292476654 batch: 51/224\n",
      "Batch loss: 0.13456521928310394 batch: 52/224\n",
      "Batch loss: 0.1471582055091858 batch: 53/224\n",
      "Batch loss: 0.12289898842573166 batch: 54/224\n",
      "Batch loss: 0.123726025223732 batch: 55/224\n",
      "Batch loss: 0.1352328360080719 batch: 56/224\n",
      "Batch loss: 0.1983032375574112 batch: 57/224\n",
      "Batch loss: 0.19233930110931396 batch: 58/224\n",
      "Batch loss: 0.1367163062095642 batch: 59/224\n",
      "Batch loss: 0.16986873745918274 batch: 60/224\n",
      "Batch loss: 0.16111449897289276 batch: 61/224\n",
      "Batch loss: 0.11606914550065994 batch: 62/224\n",
      "Batch loss: 0.16371609270572662 batch: 63/224\n",
      "Batch loss: 0.13218005001544952 batch: 64/224\n",
      "Batch loss: 0.1345016211271286 batch: 65/224\n",
      "Batch loss: 0.1434277594089508 batch: 66/224\n",
      "Batch loss: 0.12056868523359299 batch: 67/224\n",
      "Batch loss: 0.13565663993358612 batch: 68/224\n",
      "Batch loss: 0.14623156189918518 batch: 69/224\n",
      "Batch loss: 0.16945068538188934 batch: 70/224\n",
      "Batch loss: 0.1511141061782837 batch: 71/224\n",
      "Batch loss: 0.12481603771448135 batch: 72/224\n",
      "Batch loss: 0.14804863929748535 batch: 73/224\n",
      "Batch loss: 0.1603253036737442 batch: 74/224\n",
      "Batch loss: 0.1776212751865387 batch: 75/224\n",
      "Batch loss: 0.17764030396938324 batch: 76/224\n",
      "Batch loss: 0.12728524208068848 batch: 77/224\n",
      "Batch loss: 0.15472960472106934 batch: 78/224\n",
      "Batch loss: 0.16125550866127014 batch: 79/224\n",
      "Batch loss: 0.1516323834657669 batch: 80/224\n",
      "Batch loss: 0.16356900334358215 batch: 81/224\n",
      "Batch loss: 0.17940261960029602 batch: 82/224\n",
      "Batch loss: 0.14483080804347992 batch: 83/224\n",
      "Batch loss: 0.10729773342609406 batch: 84/224\n",
      "Batch loss: 0.15333209931850433 batch: 85/224\n",
      "Batch loss: 0.1337169110774994 batch: 86/224\n",
      "Batch loss: 0.16455917060375214 batch: 87/224\n",
      "Batch loss: 0.16361720860004425 batch: 88/224\n",
      "Batch loss: 0.13498380780220032 batch: 89/224\n",
      "Batch loss: 0.1589718759059906 batch: 90/224\n",
      "Batch loss: 0.13679108023643494 batch: 91/224\n",
      "Batch loss: 0.14458246529102325 batch: 92/224\n",
      "Batch loss: 0.09207393229007721 batch: 93/224\n",
      "Batch loss: 0.12556470930576324 batch: 94/224\n",
      "Batch loss: 0.14981035888195038 batch: 95/224\n",
      "Batch loss: 0.12738117575645447 batch: 96/224\n",
      "Batch loss: 0.1002429723739624 batch: 97/224\n",
      "Batch loss: 0.11905431002378464 batch: 98/224\n",
      "Batch loss: 0.19184952974319458 batch: 99/224\n",
      "Batch loss: 0.12589780986309052 batch: 100/224\n",
      "Batch loss: 0.16137836873531342 batch: 101/224\n",
      "Batch loss: 0.14808869361877441 batch: 102/224\n",
      "Batch loss: 0.1381050944328308 batch: 103/224\n",
      "Batch loss: 0.15847426652908325 batch: 104/224\n",
      "Batch loss: 0.10675642639398575 batch: 105/224\n",
      "Batch loss: 0.13014300167560577 batch: 106/224\n",
      "Batch loss: 0.10343237966299057 batch: 107/224\n",
      "Batch loss: 0.13254354894161224 batch: 108/224\n",
      "Batch loss: 0.14102737605571747 batch: 109/224\n",
      "Batch loss: 0.13350248336791992 batch: 110/224\n",
      "Batch loss: 0.17955397069454193 batch: 111/224\n",
      "Batch loss: 0.11373355984687805 batch: 112/224\n",
      "Batch loss: 0.1498001664876938 batch: 113/224\n",
      "Batch loss: 0.12002057582139969 batch: 114/224\n",
      "Batch loss: 0.1284182220697403 batch: 115/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.12372280657291412 batch: 116/224\n",
      "Batch loss: 0.09103459119796753 batch: 117/224\n",
      "Batch loss: 0.1356375515460968 batch: 118/224\n",
      "Batch loss: 0.10734191536903381 batch: 119/224\n",
      "Batch loss: 0.17052730917930603 batch: 120/224\n",
      "Batch loss: 0.11886601895093918 batch: 121/224\n",
      "Batch loss: 0.16520032286643982 batch: 122/224\n",
      "Batch loss: 0.10119830816984177 batch: 123/224\n",
      "Batch loss: 0.14313766360282898 batch: 124/224\n",
      "Batch loss: 0.1588761955499649 batch: 125/224\n",
      "Batch loss: 0.12225860357284546 batch: 126/224\n",
      "Batch loss: 0.15250219404697418 batch: 127/224\n",
      "Batch loss: 0.13295555114746094 batch: 128/224\n",
      "Batch loss: 0.13620135188102722 batch: 129/224\n",
      "Batch loss: 0.15869072079658508 batch: 130/224\n",
      "Batch loss: 0.13868920505046844 batch: 131/224\n",
      "Batch loss: 0.15542948246002197 batch: 132/224\n",
      "Batch loss: 0.14502336084842682 batch: 133/224\n",
      "Batch loss: 0.16187390685081482 batch: 134/224\n",
      "Batch loss: 0.1741766333580017 batch: 135/224\n",
      "Batch loss: 0.16987676918506622 batch: 136/224\n",
      "Batch loss: 0.1552167683839798 batch: 137/224\n",
      "Batch loss: 0.11357813328504562 batch: 138/224\n",
      "Batch loss: 0.16791744530200958 batch: 139/224\n",
      "Batch loss: 0.17171445488929749 batch: 140/224\n",
      "Batch loss: 0.13578474521636963 batch: 141/224\n",
      "Batch loss: 0.15467406809329987 batch: 142/224\n",
      "Batch loss: 0.13983038067817688 batch: 143/224\n",
      "Batch loss: 0.14425785839557648 batch: 144/224\n",
      "Batch loss: 0.15813176333904266 batch: 145/224\n",
      "Batch loss: 0.19705957174301147 batch: 146/224\n",
      "Batch loss: 0.10397343337535858 batch: 147/224\n",
      "Batch loss: 0.1457301676273346 batch: 148/224\n",
      "Batch loss: 0.17088980972766876 batch: 149/224\n",
      "Batch loss: 0.2061799168586731 batch: 150/224\n",
      "Batch loss: 0.10976612567901611 batch: 151/224\n",
      "Batch loss: 0.18048720061779022 batch: 152/224\n",
      "Batch loss: 0.17304280400276184 batch: 153/224\n",
      "Batch loss: 0.14493167400360107 batch: 154/224\n",
      "Batch loss: 0.11275512725114822 batch: 155/224\n",
      "Batch loss: 0.1274086982011795 batch: 156/224\n",
      "Batch loss: 0.14470800757408142 batch: 157/224\n",
      "Batch loss: 0.1778416931629181 batch: 158/224\n",
      "Batch loss: 0.10511571913957596 batch: 159/224\n",
      "Batch loss: 0.12859240174293518 batch: 160/224\n",
      "Batch loss: 0.11238305270671844 batch: 161/224\n",
      "Batch loss: 0.1343989074230194 batch: 162/224\n",
      "Batch loss: 0.10819002985954285 batch: 163/224\n",
      "Batch loss: 0.10215617716312408 batch: 164/224\n",
      "Batch loss: 0.1744975447654724 batch: 165/224\n",
      "Batch loss: 0.15248796343803406 batch: 166/224\n",
      "Batch loss: 0.13136109709739685 batch: 167/224\n",
      "Batch loss: 0.12885750830173492 batch: 168/224\n",
      "Batch loss: 0.13686928153038025 batch: 169/224\n",
      "Batch loss: 0.10955660790205002 batch: 170/224\n",
      "Batch loss: 0.13156454265117645 batch: 171/224\n",
      "Batch loss: 0.13826534152030945 batch: 172/224\n",
      "Batch loss: 0.1493106484413147 batch: 173/224\n",
      "Batch loss: 0.1307746171951294 batch: 174/224\n",
      "Batch loss: 0.13725712895393372 batch: 175/224\n",
      "Batch loss: 0.14374351501464844 batch: 176/224\n",
      "Batch loss: 0.1697978973388672 batch: 177/224\n",
      "Batch loss: 0.12655122578144073 batch: 178/224\n",
      "Batch loss: 0.14019005000591278 batch: 179/224\n",
      "Batch loss: 0.10269150137901306 batch: 180/224\n",
      "Batch loss: 0.14969554543495178 batch: 181/224\n",
      "Batch loss: 0.16929113864898682 batch: 182/224\n",
      "Batch loss: 0.16013209521770477 batch: 183/224\n",
      "Batch loss: 0.16207508742809296 batch: 184/224\n",
      "Batch loss: 0.140608012676239 batch: 185/224\n",
      "Batch loss: 0.12210862338542938 batch: 186/224\n",
      "Batch loss: 0.13907268643379211 batch: 187/224\n",
      "Batch loss: 0.10701410472393036 batch: 188/224\n",
      "Batch loss: 0.12692789733409882 batch: 189/224\n",
      "Batch loss: 0.18238678574562073 batch: 190/224\n",
      "Batch loss: 0.1097312793135643 batch: 191/224\n",
      "Batch loss: 0.13802944123744965 batch: 192/224\n",
      "Batch loss: 0.17611926794052124 batch: 193/224\n",
      "Batch loss: 0.11210788786411285 batch: 194/224\n",
      "Batch loss: 0.17264865338802338 batch: 195/224\n",
      "Batch loss: 0.1799822896718979 batch: 196/224\n",
      "Batch loss: 0.13260740041732788 batch: 197/224\n",
      "Batch loss: 0.11844077706336975 batch: 198/224\n",
      "Batch loss: 0.13367298245429993 batch: 199/224\n",
      "Batch loss: 0.17589420080184937 batch: 200/224\n",
      "Batch loss: 0.16015924513339996 batch: 201/224\n",
      "Batch loss: 0.1935904324054718 batch: 202/224\n",
      "Batch loss: 0.1447959840297699 batch: 203/224\n",
      "Batch loss: 0.1536499559879303 batch: 204/224\n",
      "Batch loss: 0.16614378988742828 batch: 205/224\n",
      "Batch loss: 0.16138075292110443 batch: 206/224\n",
      "Batch loss: 0.11915173381567001 batch: 207/224\n",
      "Batch loss: 0.12684020400047302 batch: 208/224\n",
      "Batch loss: 0.1609242856502533 batch: 209/224\n",
      "Batch loss: 0.14115071296691895 batch: 210/224\n",
      "Batch loss: 0.14490067958831787 batch: 211/224\n",
      "Batch loss: 0.14478448033332825 batch: 212/224\n",
      "Batch loss: 0.15522950887680054 batch: 213/224\n",
      "Batch loss: 0.15410862863063812 batch: 214/224\n",
      "Batch loss: 0.14312773942947388 batch: 215/224\n",
      "Batch loss: 0.14056630432605743 batch: 216/224\n",
      "Batch loss: 0.19590246677398682 batch: 217/224\n",
      "Batch loss: 0.14604946970939636 batch: 218/224\n",
      "Batch loss: 0.12508416175842285 batch: 219/224\n",
      "Batch loss: 0.1202736496925354 batch: 220/224\n",
      "Batch loss: 0.1839248687028885 batch: 221/224\n",
      "Batch loss: 0.13563723862171173 batch: 222/224\n",
      "Batch loss: 0.1358126997947693 batch: 223/224\n",
      "Batch loss: 0.12208022177219391 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 33/75..  Training Loss: 0.00029..  Test Loss: 0.00076..  Test Accuracy: 0.88857\n",
      "Running epoch 34/75\n",
      "Batch loss: 0.09737677127122879 batch: 1/224\n",
      "Batch loss: 0.1493287831544876 batch: 2/224\n",
      "Batch loss: 0.15800149738788605 batch: 3/224\n",
      "Batch loss: 0.13660678267478943 batch: 4/224\n",
      "Batch loss: 0.1703716218471527 batch: 5/224\n",
      "Batch loss: 0.13932721316814423 batch: 6/224\n",
      "Batch loss: 0.12076518684625626 batch: 7/224\n",
      "Batch loss: 0.1014455184340477 batch: 8/224\n",
      "Batch loss: 0.13501760363578796 batch: 9/224\n",
      "Batch loss: 0.12223338335752487 batch: 10/224\n",
      "Batch loss: 0.14993736147880554 batch: 11/224\n",
      "Batch loss: 0.1509389728307724 batch: 12/224\n",
      "Batch loss: 0.10870493948459625 batch: 13/224\n",
      "Batch loss: 0.10789167881011963 batch: 14/224\n",
      "Batch loss: 0.11834502220153809 batch: 15/224\n",
      "Batch loss: 0.1518562287092209 batch: 16/224\n",
      "Batch loss: 0.13962873816490173 batch: 17/224\n",
      "Batch loss: 0.15266676247119904 batch: 18/224\n",
      "Batch loss: 0.11900106817483902 batch: 19/224\n",
      "Batch loss: 0.10339406132698059 batch: 20/224\n",
      "Batch loss: 0.1484764814376831 batch: 21/224\n",
      "Batch loss: 0.12014012783765793 batch: 22/224\n",
      "Batch loss: 0.1459331065416336 batch: 23/224\n",
      "Batch loss: 0.16529908776283264 batch: 24/224\n",
      "Batch loss: 0.11742096394300461 batch: 25/224\n",
      "Batch loss: 0.14078211784362793 batch: 26/224\n",
      "Batch loss: 0.12858857214450836 batch: 27/224\n",
      "Batch loss: 0.13627126812934875 batch: 28/224\n",
      "Batch loss: 0.17065909504890442 batch: 29/224\n",
      "Batch loss: 0.12793979048728943 batch: 30/224\n",
      "Batch loss: 0.1210082396864891 batch: 31/224\n",
      "Batch loss: 0.15903165936470032 batch: 32/224\n",
      "Batch loss: 0.12750281393527985 batch: 33/224\n",
      "Batch loss: 0.13694414496421814 batch: 34/224\n",
      "Batch loss: 0.1496085524559021 batch: 35/224\n",
      "Batch loss: 0.14974859356880188 batch: 36/224\n",
      "Batch loss: 0.14798901975154877 batch: 37/224\n",
      "Batch loss: 0.16048593819141388 batch: 38/224\n",
      "Batch loss: 0.11930350214242935 batch: 39/224\n",
      "Batch loss: 0.12505653500556946 batch: 40/224\n",
      "Batch loss: 0.16479012370109558 batch: 41/224\n",
      "Batch loss: 0.14915195107460022 batch: 42/224\n",
      "Batch loss: 0.16110357642173767 batch: 43/224\n",
      "Batch loss: 0.10914938151836395 batch: 44/224\n",
      "Batch loss: 0.10226307809352875 batch: 45/224\n",
      "Batch loss: 0.18829751014709473 batch: 46/224\n",
      "Batch loss: 0.18005909025669098 batch: 47/224\n",
      "Batch loss: 0.13862907886505127 batch: 48/224\n",
      "Batch loss: 0.08729800581932068 batch: 49/224\n",
      "Batch loss: 0.11199456453323364 batch: 50/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.11456698179244995 batch: 51/224\n",
      "Batch loss: 0.11749675124883652 batch: 52/224\n",
      "Batch loss: 0.1420706808567047 batch: 53/224\n",
      "Batch loss: 0.12783773243427277 batch: 54/224\n",
      "Batch loss: 0.11291822791099548 batch: 55/224\n",
      "Batch loss: 0.12214569747447968 batch: 56/224\n",
      "Batch loss: 0.18733417987823486 batch: 57/224\n",
      "Batch loss: 0.15604116022586823 batch: 58/224\n",
      "Batch loss: 0.13106763362884521 batch: 59/224\n",
      "Batch loss: 0.1729431450366974 batch: 60/224\n",
      "Batch loss: 0.16498886048793793 batch: 61/224\n",
      "Batch loss: 0.11174965649843216 batch: 62/224\n",
      "Batch loss: 0.18296141922473907 batch: 63/224\n",
      "Batch loss: 0.17190493643283844 batch: 64/224\n",
      "Batch loss: 0.14440448582172394 batch: 65/224\n",
      "Batch loss: 0.15900366008281708 batch: 66/224\n",
      "Batch loss: 0.14287354052066803 batch: 67/224\n",
      "Batch loss: 0.17822110652923584 batch: 68/224\n",
      "Batch loss: 0.155531108379364 batch: 69/224\n",
      "Batch loss: 0.13922935724258423 batch: 70/224\n",
      "Batch loss: 0.1539025604724884 batch: 71/224\n",
      "Batch loss: 0.10459940880537033 batch: 72/224\n",
      "Batch loss: 0.14604750275611877 batch: 73/224\n",
      "Batch loss: 0.12539242208003998 batch: 74/224\n",
      "Batch loss: 0.16845665872097015 batch: 75/224\n",
      "Batch loss: 0.17082445323467255 batch: 76/224\n",
      "Batch loss: 0.1387558877468109 batch: 77/224\n",
      "Batch loss: 0.19117411971092224 batch: 78/224\n",
      "Batch loss: 0.12480654567480087 batch: 79/224\n",
      "Batch loss: 0.14050395786762238 batch: 80/224\n",
      "Batch loss: 0.1890811026096344 batch: 81/224\n",
      "Batch loss: 0.1487407386302948 batch: 82/224\n",
      "Batch loss: 0.12065570056438446 batch: 83/224\n",
      "Batch loss: 0.09371301531791687 batch: 84/224\n",
      "Batch loss: 0.11159106343984604 batch: 85/224\n",
      "Batch loss: 0.12760096788406372 batch: 86/224\n",
      "Batch loss: 0.1585095375776291 batch: 87/224\n",
      "Batch loss: 0.15355932712554932 batch: 88/224\n",
      "Batch loss: 0.12778840959072113 batch: 89/224\n",
      "Batch loss: 0.16720622777938843 batch: 90/224\n",
      "Batch loss: 0.13805226981639862 batch: 91/224\n",
      "Batch loss: 0.14804652333259583 batch: 92/224\n",
      "Batch loss: 0.13595987856388092 batch: 93/224\n",
      "Batch loss: 0.1399819552898407 batch: 94/224\n",
      "Batch loss: 0.09930442273616791 batch: 95/224\n",
      "Batch loss: 0.10568653792142868 batch: 96/224\n",
      "Batch loss: 0.09349473565816879 batch: 97/224\n",
      "Batch loss: 0.13852037489414215 batch: 98/224\n",
      "Batch loss: 0.14240965247154236 batch: 99/224\n",
      "Batch loss: 0.1588992178440094 batch: 100/224\n",
      "Batch loss: 0.14013901352882385 batch: 101/224\n",
      "Batch loss: 0.14390812814235687 batch: 102/224\n",
      "Batch loss: 0.16167010366916656 batch: 103/224\n",
      "Batch loss: 0.1501033902168274 batch: 104/224\n",
      "Batch loss: 0.11436370760202408 batch: 105/224\n",
      "Batch loss: 0.1484031230211258 batch: 106/224\n",
      "Batch loss: 0.13129645586013794 batch: 107/224\n",
      "Batch loss: 0.13928024470806122 batch: 108/224\n",
      "Batch loss: 0.12400446832180023 batch: 109/224\n",
      "Batch loss: 0.12208736687898636 batch: 110/224\n",
      "Batch loss: 0.1454494297504425 batch: 111/224\n",
      "Batch loss: 0.11704841256141663 batch: 112/224\n",
      "Batch loss: 0.14191167056560516 batch: 113/224\n",
      "Batch loss: 0.11421516537666321 batch: 114/224\n",
      "Batch loss: 0.14635136723518372 batch: 115/224\n",
      "Batch loss: 0.14056071639060974 batch: 116/224\n",
      "Batch loss: 0.10580325871706009 batch: 117/224\n",
      "Batch loss: 0.12745137512683868 batch: 118/224\n",
      "Batch loss: 0.11122730374336243 batch: 119/224\n",
      "Batch loss: 0.11647628992795944 batch: 120/224\n",
      "Batch loss: 0.13060031831264496 batch: 121/224\n",
      "Batch loss: 0.1481078565120697 batch: 122/224\n",
      "Batch loss: 0.11362537741661072 batch: 123/224\n",
      "Batch loss: 0.12176978588104248 batch: 124/224\n",
      "Batch loss: 0.10914645344018936 batch: 125/224\n",
      "Batch loss: 0.10875217616558075 batch: 126/224\n",
      "Batch loss: 0.16082069277763367 batch: 127/224\n",
      "Batch loss: 0.12389860302209854 batch: 128/224\n",
      "Batch loss: 0.13466478884220123 batch: 129/224\n",
      "Batch loss: 0.1524389535188675 batch: 130/224\n",
      "Batch loss: 0.10077761113643646 batch: 131/224\n",
      "Batch loss: 0.12457118928432465 batch: 132/224\n",
      "Batch loss: 0.14605417847633362 batch: 133/224\n",
      "Batch loss: 0.15804465115070343 batch: 134/224\n",
      "Batch loss: 0.16362206637859344 batch: 135/224\n",
      "Batch loss: 0.14566870033740997 batch: 136/224\n",
      "Batch loss: 0.11698346585035324 batch: 137/224\n",
      "Batch loss: 0.11562597751617432 batch: 138/224\n",
      "Batch loss: 0.15239691734313965 batch: 139/224\n",
      "Batch loss: 0.1770007163286209 batch: 140/224\n",
      "Batch loss: 0.10310882329940796 batch: 141/224\n",
      "Batch loss: 0.10425473004579544 batch: 142/224\n",
      "Batch loss: 0.12172951549291611 batch: 143/224\n",
      "Batch loss: 0.13227014243602753 batch: 144/224\n",
      "Batch loss: 0.14719657599925995 batch: 145/224\n",
      "Batch loss: 0.16901323199272156 batch: 146/224\n",
      "Batch loss: 0.12730102241039276 batch: 147/224\n",
      "Batch loss: 0.11572135984897614 batch: 148/224\n",
      "Batch loss: 0.1590336263179779 batch: 149/224\n",
      "Batch loss: 0.1499207764863968 batch: 150/224\n",
      "Batch loss: 0.12575741112232208 batch: 151/224\n",
      "Batch loss: 0.1357806771993637 batch: 152/224\n",
      "Batch loss: 0.15186291933059692 batch: 153/224\n",
      "Batch loss: 0.1274760663509369 batch: 154/224\n",
      "Batch loss: 0.11514594405889511 batch: 155/224\n",
      "Batch loss: 0.095917709171772 batch: 156/224\n",
      "Batch loss: 0.15438848733901978 batch: 157/224\n",
      "Batch loss: 0.20844867825508118 batch: 158/224\n",
      "Batch loss: 0.1293175220489502 batch: 159/224\n",
      "Batch loss: 0.11007227003574371 batch: 160/224\n",
      "Batch loss: 0.11059357225894928 batch: 161/224\n",
      "Batch loss: 0.12481854856014252 batch: 162/224\n",
      "Batch loss: 0.14715120196342468 batch: 163/224\n",
      "Batch loss: 0.11738196760416031 batch: 164/224\n",
      "Batch loss: 0.1790236532688141 batch: 165/224\n",
      "Batch loss: 0.16408909857273102 batch: 166/224\n",
      "Batch loss: 0.1322317272424698 batch: 167/224\n",
      "Batch loss: 0.10790833085775375 batch: 168/224\n",
      "Batch loss: 0.1320342719554901 batch: 169/224\n",
      "Batch loss: 0.10858592391014099 batch: 170/224\n",
      "Batch loss: 0.12038503587245941 batch: 171/224\n",
      "Batch loss: 0.13230091333389282 batch: 172/224\n",
      "Batch loss: 0.14416737854480743 batch: 173/224\n",
      "Batch loss: 0.12263881415128708 batch: 174/224\n",
      "Batch loss: 0.15201492607593536 batch: 175/224\n",
      "Batch loss: 0.1278795748949051 batch: 176/224\n",
      "Batch loss: 0.1684863567352295 batch: 177/224\n",
      "Batch loss: 0.13896550238132477 batch: 178/224\n",
      "Batch loss: 0.13393941521644592 batch: 179/224\n",
      "Batch loss: 0.11682786047458649 batch: 180/224\n",
      "Batch loss: 0.1291562020778656 batch: 181/224\n",
      "Batch loss: 0.142908975481987 batch: 182/224\n",
      "Batch loss: 0.1425011157989502 batch: 183/224\n",
      "Batch loss: 0.14237813651561737 batch: 184/224\n",
      "Batch loss: 0.14278064668178558 batch: 185/224\n",
      "Batch loss: 0.08867098391056061 batch: 186/224\n",
      "Batch loss: 0.15463772416114807 batch: 187/224\n",
      "Batch loss: 0.11387757211923599 batch: 188/224\n",
      "Batch loss: 0.14982514083385468 batch: 189/224\n",
      "Batch loss: 0.130672425031662 batch: 190/224\n",
      "Batch loss: 0.1250903308391571 batch: 191/224\n",
      "Batch loss: 0.1650516837835312 batch: 192/224\n",
      "Batch loss: 0.14780503511428833 batch: 193/224\n",
      "Batch loss: 0.17379121482372284 batch: 194/224\n",
      "Batch loss: 0.1539914608001709 batch: 195/224\n",
      "Batch loss: 0.15934671461582184 batch: 196/224\n",
      "Batch loss: 0.15207575261592865 batch: 197/224\n",
      "Batch loss: 0.14321419596672058 batch: 198/224\n",
      "Batch loss: 0.1193993091583252 batch: 199/224\n",
      "Batch loss: 0.17545370757579803 batch: 200/224\n",
      "Batch loss: 0.20590713620185852 batch: 201/224\n",
      "Batch loss: 0.14776106178760529 batch: 202/224\n",
      "Batch loss: 0.13556703925132751 batch: 203/224\n",
      "Batch loss: 0.12192408740520477 batch: 204/224\n",
      "Batch loss: 0.1638086438179016 batch: 205/224\n",
      "Batch loss: 0.13596265017986298 batch: 206/224\n",
      "Batch loss: 0.12233909964561462 batch: 207/224\n",
      "Batch loss: 0.14792722463607788 batch: 208/224\n",
      "Batch loss: 0.13057127594947815 batch: 209/224\n",
      "Batch loss: 0.1383972018957138 batch: 210/224\n",
      "Batch loss: 0.1274484097957611 batch: 211/224\n",
      "Batch loss: 0.171192929148674 batch: 212/224\n",
      "Batch loss: 0.18425267934799194 batch: 213/224\n",
      "Batch loss: 0.13913360238075256 batch: 214/224\n",
      "Batch loss: 0.17750400304794312 batch: 215/224\n",
      "Batch loss: 0.15569357573986053 batch: 216/224\n",
      "Batch loss: 0.18057698011398315 batch: 217/224\n",
      "Batch loss: 0.13004495203495026 batch: 218/224\n",
      "Batch loss: 0.10449029505252838 batch: 219/224\n",
      "Batch loss: 0.1398085504770279 batch: 220/224\n",
      "Batch loss: 0.18008172512054443 batch: 221/224\n",
      "Batch loss: 0.14800307154655457 batch: 222/224\n",
      "Batch loss: 0.11712204664945602 batch: 223/224\n",
      "Batch loss: 0.13518227636814117 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 34/75..  Training Loss: 0.00028..  Test Loss: 0.00080..  Test Accuracy: 0.88746\n",
      "Running epoch 35/75\n",
      "Batch loss: 0.13539384305477142 batch: 1/224\n",
      "Batch loss: 0.13400720059871674 batch: 2/224\n",
      "Batch loss: 0.1316969394683838 batch: 3/224\n",
      "Batch loss: 0.1716681718826294 batch: 4/224\n",
      "Batch loss: 0.13338685035705566 batch: 5/224\n",
      "Batch loss: 0.14908158779144287 batch: 6/224\n",
      "Batch loss: 0.12901076674461365 batch: 7/224\n",
      "Batch loss: 0.14528262615203857 batch: 8/224\n",
      "Batch loss: 0.09761976450681686 batch: 9/224\n",
      "Batch loss: 0.15661264955997467 batch: 10/224\n",
      "Batch loss: 0.14090681076049805 batch: 11/224\n",
      "Batch loss: 0.14649716019630432 batch: 12/224\n",
      "Batch loss: 0.10148932039737701 batch: 13/224\n",
      "Batch loss: 0.14421288669109344 batch: 14/224\n",
      "Batch loss: 0.13851822912693024 batch: 15/224\n",
      "Batch loss: 0.18866132199764252 batch: 16/224\n",
      "Batch loss: 0.1426558494567871 batch: 17/224\n",
      "Batch loss: 0.17689964175224304 batch: 18/224\n",
      "Batch loss: 0.1346592754125595 batch: 19/224\n",
      "Batch loss: 0.1616893857717514 batch: 20/224\n",
      "Batch loss: 0.1603323072195053 batch: 21/224\n",
      "Batch loss: 0.12055014073848724 batch: 22/224\n",
      "Batch loss: 0.1448572427034378 batch: 23/224\n",
      "Batch loss: 0.16164587438106537 batch: 24/224\n",
      "Batch loss: 0.09220990538597107 batch: 25/224\n",
      "Batch loss: 0.1120493933558464 batch: 26/224\n",
      "Batch loss: 0.14822956919670105 batch: 27/224\n",
      "Batch loss: 0.14352035522460938 batch: 28/224\n",
      "Batch loss: 0.1574452817440033 batch: 29/224\n",
      "Batch loss: 0.19394631683826447 batch: 30/224\n",
      "Batch loss: 0.11776521801948547 batch: 31/224\n",
      "Batch loss: 0.11457008123397827 batch: 32/224\n",
      "Batch loss: 0.10755188018083572 batch: 33/224\n",
      "Batch loss: 0.15095320343971252 batch: 34/224\n",
      "Batch loss: 0.1525741070508957 batch: 35/224\n",
      "Batch loss: 0.14808779954910278 batch: 36/224\n",
      "Batch loss: 0.1279384344816208 batch: 37/224\n",
      "Batch loss: 0.1442902535200119 batch: 38/224\n",
      "Batch loss: 0.16485658288002014 batch: 39/224\n",
      "Batch loss: 0.135030597448349 batch: 40/224\n",
      "Batch loss: 0.13574883341789246 batch: 41/224\n",
      "Batch loss: 0.1029675081372261 batch: 42/224\n",
      "Batch loss: 0.13579410314559937 batch: 43/224\n",
      "Batch loss: 0.14365652203559875 batch: 44/224\n",
      "Batch loss: 0.09156034141778946 batch: 45/224\n",
      "Batch loss: 0.1647040694952011 batch: 46/224\n",
      "Batch loss: 0.12924164533615112 batch: 47/224\n",
      "Batch loss: 0.11692418158054352 batch: 48/224\n",
      "Batch loss: 0.12927204370498657 batch: 49/224\n",
      "Batch loss: 0.10827596485614777 batch: 50/224\n",
      "Batch loss: 0.12498270720243454 batch: 51/224\n",
      "Batch loss: 0.16344641149044037 batch: 52/224\n",
      "Batch loss: 0.19900619983673096 batch: 53/224\n",
      "Batch loss: 0.0935957208275795 batch: 54/224\n",
      "Batch loss: 0.1280137151479721 batch: 55/224\n",
      "Batch loss: 0.11462870240211487 batch: 56/224\n",
      "Batch loss: 0.169878289103508 batch: 57/224\n",
      "Batch loss: 0.11327487975358963 batch: 58/224\n",
      "Batch loss: 0.11970901489257812 batch: 59/224\n",
      "Batch loss: 0.15261073410511017 batch: 60/224\n",
      "Batch loss: 0.15069466829299927 batch: 61/224\n",
      "Batch loss: 0.11500950157642365 batch: 62/224\n",
      "Batch loss: 0.15308557450771332 batch: 63/224\n",
      "Batch loss: 0.17327144742012024 batch: 64/224\n",
      "Batch loss: 0.10692927241325378 batch: 65/224\n",
      "Batch loss: 0.1909266710281372 batch: 66/224\n",
      "Batch loss: 0.11508211493492126 batch: 67/224\n",
      "Batch loss: 0.1059911921620369 batch: 68/224\n",
      "Batch loss: 0.1582799255847931 batch: 69/224\n",
      "Batch loss: 0.1570194512605667 batch: 70/224\n",
      "Batch loss: 0.13268855214118958 batch: 71/224\n",
      "Batch loss: 0.12797865271568298 batch: 72/224\n",
      "Batch loss: 0.15562039613723755 batch: 73/224\n",
      "Batch loss: 0.14256849884986877 batch: 74/224\n",
      "Batch loss: 0.1473466455936432 batch: 75/224\n",
      "Batch loss: 0.17781738936901093 batch: 76/224\n",
      "Batch loss: 0.13242557644844055 batch: 77/224\n",
      "Batch loss: 0.11195176839828491 batch: 78/224\n",
      "Batch loss: 0.15428084135055542 batch: 79/224\n",
      "Batch loss: 0.12451213598251343 batch: 80/224\n",
      "Batch loss: 0.20930391550064087 batch: 81/224\n",
      "Batch loss: 0.18233303725719452 batch: 82/224\n",
      "Batch loss: 0.1278999000787735 batch: 83/224\n",
      "Batch loss: 0.08065327256917953 batch: 84/224\n",
      "Batch loss: 0.12746042013168335 batch: 85/224\n",
      "Batch loss: 0.127085879445076 batch: 86/224\n",
      "Batch loss: 0.16552194952964783 batch: 87/224\n",
      "Batch loss: 0.13052594661712646 batch: 88/224\n",
      "Batch loss: 0.12906862795352936 batch: 89/224\n",
      "Batch loss: 0.15314970910549164 batch: 90/224\n",
      "Batch loss: 0.11494480073451996 batch: 91/224\n",
      "Batch loss: 0.15786391496658325 batch: 92/224\n",
      "Batch loss: 0.11651167273521423 batch: 93/224\n",
      "Batch loss: 0.14234571158885956 batch: 94/224\n",
      "Batch loss: 0.13959847390651703 batch: 95/224\n",
      "Batch loss: 0.10401444137096405 batch: 96/224\n",
      "Batch loss: 0.15798573195934296 batch: 97/224\n",
      "Batch loss: 0.11385374516248703 batch: 98/224\n",
      "Batch loss: 0.1449689418077469 batch: 99/224\n",
      "Batch loss: 0.1101030707359314 batch: 100/224\n",
      "Batch loss: 0.19936852157115936 batch: 101/224\n",
      "Batch loss: 0.12944728136062622 batch: 102/224\n",
      "Batch loss: 0.16078661382198334 batch: 103/224\n",
      "Batch loss: 0.11322295665740967 batch: 104/224\n",
      "Batch loss: 0.11697251349687576 batch: 105/224\n",
      "Batch loss: 0.11941483616828918 batch: 106/224\n",
      "Batch loss: 0.12915942072868347 batch: 107/224\n",
      "Batch loss: 0.11480671912431717 batch: 108/224\n",
      "Batch loss: 0.13670100271701813 batch: 109/224\n",
      "Batch loss: 0.15386614203453064 batch: 110/224\n",
      "Batch loss: 0.15217837691307068 batch: 111/224\n",
      "Batch loss: 0.11308083683252335 batch: 112/224\n",
      "Batch loss: 0.11896054446697235 batch: 113/224\n",
      "Batch loss: 0.1118171438574791 batch: 114/224\n",
      "Batch loss: 0.11734199523925781 batch: 115/224\n",
      "Batch loss: 0.12511314451694489 batch: 116/224\n",
      "Batch loss: 0.10105948895215988 batch: 117/224\n",
      "Batch loss: 0.16155582666397095 batch: 118/224\n",
      "Batch loss: 0.09101345390081406 batch: 119/224\n",
      "Batch loss: 0.16344353556632996 batch: 120/224\n",
      "Batch loss: 0.12098261713981628 batch: 121/224\n",
      "Batch loss: 0.13139325380325317 batch: 122/224\n",
      "Batch loss: 0.0808398649096489 batch: 123/224\n",
      "Batch loss: 0.1245841309428215 batch: 124/224\n",
      "Batch loss: 0.14862586557865143 batch: 125/224\n",
      "Batch loss: 0.15985558927059174 batch: 126/224\n",
      "Batch loss: 0.14179490506649017 batch: 127/224\n",
      "Batch loss: 0.1278134286403656 batch: 128/224\n",
      "Batch loss: 0.10133504122495651 batch: 129/224\n",
      "Batch loss: 0.1281389594078064 batch: 130/224\n",
      "Batch loss: 0.1138448417186737 batch: 131/224\n",
      "Batch loss: 0.09451174736022949 batch: 132/224\n",
      "Batch loss: 0.12902818620204926 batch: 133/224\n",
      "Batch loss: 0.13657057285308838 batch: 134/224\n",
      "Batch loss: 0.14571413397789001 batch: 135/224\n",
      "Batch loss: 0.14278946816921234 batch: 136/224\n",
      "Batch loss: 0.16080139577388763 batch: 137/224\n",
      "Batch loss: 0.16736550629138947 batch: 138/224\n",
      "Batch loss: 0.1799609661102295 batch: 139/224\n",
      "Batch loss: 0.15789376199245453 batch: 140/224\n",
      "Batch loss: 0.10087192058563232 batch: 141/224\n",
      "Batch loss: 0.1284046471118927 batch: 142/224\n",
      "Batch loss: 0.13632124662399292 batch: 143/224\n",
      "Batch loss: 0.14208480715751648 batch: 144/224\n",
      "Batch loss: 0.14916391670703888 batch: 145/224\n",
      "Batch loss: 0.17345893383026123 batch: 146/224\n",
      "Batch loss: 0.13727061450481415 batch: 147/224\n",
      "Batch loss: 0.142064169049263 batch: 148/224\n",
      "Batch loss: 0.1381441056728363 batch: 149/224\n",
      "Batch loss: 0.15912078320980072 batch: 150/224\n",
      "Batch loss: 0.12294632196426392 batch: 151/224\n",
      "Batch loss: 0.152117520570755 batch: 152/224\n",
      "Batch loss: 0.13989388942718506 batch: 153/224\n",
      "Batch loss: 0.1445894092321396 batch: 154/224\n",
      "Batch loss: 0.14464972913265228 batch: 155/224\n",
      "Batch loss: 0.11172547936439514 batch: 156/224\n",
      "Batch loss: 0.17669600248336792 batch: 157/224\n",
      "Batch loss: 0.22318346798419952 batch: 158/224\n",
      "Batch loss: 0.1387348473072052 batch: 159/224\n",
      "Batch loss: 0.12666858732700348 batch: 160/224\n",
      "Batch loss: 0.12699736654758453 batch: 161/224\n",
      "Batch loss: 0.09507174789905548 batch: 162/224\n",
      "Batch loss: 0.10375986248254776 batch: 163/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.1137392446398735 batch: 164/224\n",
      "Batch loss: 0.1868981271982193 batch: 165/224\n",
      "Batch loss: 0.14604948461055756 batch: 166/224\n",
      "Batch loss: 0.1280554085969925 batch: 167/224\n",
      "Batch loss: 0.10464879125356674 batch: 168/224\n",
      "Batch loss: 0.1406726986169815 batch: 169/224\n",
      "Batch loss: 0.11601699143648148 batch: 170/224\n",
      "Batch loss: 0.12028715759515762 batch: 171/224\n",
      "Batch loss: 0.12012277543544769 batch: 172/224\n",
      "Batch loss: 0.12851589918136597 batch: 173/224\n",
      "Batch loss: 0.14440639317035675 batch: 174/224\n",
      "Batch loss: 0.1295122355222702 batch: 175/224\n",
      "Batch loss: 0.09428860247135162 batch: 176/224\n",
      "Batch loss: 0.14566364884376526 batch: 177/224\n",
      "Batch loss: 0.14310090243816376 batch: 178/224\n",
      "Batch loss: 0.15806743502616882 batch: 179/224\n",
      "Batch loss: 0.10518493503332138 batch: 180/224\n",
      "Batch loss: 0.1291751116514206 batch: 181/224\n",
      "Batch loss: 0.13563238084316254 batch: 182/224\n",
      "Batch loss: 0.16985604166984558 batch: 183/224\n",
      "Batch loss: 0.12567800283432007 batch: 184/224\n",
      "Batch loss: 0.1284130960702896 batch: 185/224\n",
      "Batch loss: 0.109297975897789 batch: 186/224\n",
      "Batch loss: 0.1198505386710167 batch: 187/224\n",
      "Batch loss: 0.09057560563087463 batch: 188/224\n",
      "Batch loss: 0.12365896254777908 batch: 189/224\n",
      "Batch loss: 0.09696797281503677 batch: 190/224\n",
      "Batch loss: 0.11838707327842712 batch: 191/224\n",
      "Batch loss: 0.15067794919013977 batch: 192/224\n",
      "Batch loss: 0.1432529240846634 batch: 193/224\n",
      "Batch loss: 0.1281048059463501 batch: 194/224\n",
      "Batch loss: 0.160986989736557 batch: 195/224\n",
      "Batch loss: 0.1465141773223877 batch: 196/224\n",
      "Batch loss: 0.1785464882850647 batch: 197/224\n",
      "Batch loss: 0.10732210427522659 batch: 198/224\n",
      "Batch loss: 0.12859196960926056 batch: 199/224\n",
      "Batch loss: 0.1401168256998062 batch: 200/224\n",
      "Batch loss: 0.1410776972770691 batch: 201/224\n",
      "Batch loss: 0.11910782009363174 batch: 202/224\n",
      "Batch loss: 0.11059332638978958 batch: 203/224\n",
      "Batch loss: 0.1379203498363495 batch: 204/224\n",
      "Batch loss: 0.14305061101913452 batch: 205/224\n",
      "Batch loss: 0.16185754537582397 batch: 206/224\n",
      "Batch loss: 0.11243794113397598 batch: 207/224\n",
      "Batch loss: 0.13920380175113678 batch: 208/224\n",
      "Batch loss: 0.14941617846488953 batch: 209/224\n",
      "Batch loss: 0.17767034471035004 batch: 210/224\n",
      "Batch loss: 0.14754405617713928 batch: 211/224\n",
      "Batch loss: 0.1373157948255539 batch: 212/224\n",
      "Batch loss: 0.15941816568374634 batch: 213/224\n",
      "Batch loss: 0.15744955837726593 batch: 214/224\n",
      "Batch loss: 0.1321195811033249 batch: 215/224\n",
      "Batch loss: 0.1327793300151825 batch: 216/224\n",
      "Batch loss: 0.13918960094451904 batch: 217/224\n",
      "Batch loss: 0.13525976240634918 batch: 218/224\n",
      "Batch loss: 0.10828258097171783 batch: 219/224\n",
      "Batch loss: 0.14489102363586426 batch: 220/224\n",
      "Batch loss: 0.11713341623544693 batch: 221/224\n",
      "Batch loss: 0.15120072662830353 batch: 222/224\n",
      "Batch loss: 0.11266869306564331 batch: 223/224\n",
      "Batch loss: 0.11373184621334076 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 35/75..  Training Loss: 0.00027..  Test Loss: 0.00083..  Test Accuracy: 0.88939\n",
      "Running epoch 36/75\n",
      "Batch loss: 0.13438791036605835 batch: 1/224\n",
      "Batch loss: 0.1620364636182785 batch: 2/224\n",
      "Batch loss: 0.13577388226985931 batch: 3/224\n",
      "Batch loss: 0.15988698601722717 batch: 4/224\n",
      "Batch loss: 0.10651436448097229 batch: 5/224\n",
      "Batch loss: 0.13381846249103546 batch: 6/224\n",
      "Batch loss: 0.12130425125360489 batch: 7/224\n",
      "Batch loss: 0.14032453298568726 batch: 8/224\n",
      "Batch loss: 0.09983394294977188 batch: 9/224\n",
      "Batch loss: 0.12649346888065338 batch: 10/224\n",
      "Batch loss: 0.14639201760292053 batch: 11/224\n",
      "Batch loss: 0.13422441482543945 batch: 12/224\n",
      "Batch loss: 0.1327665150165558 batch: 13/224\n",
      "Batch loss: 0.11680855602025986 batch: 14/224\n",
      "Batch loss: 0.16083160042762756 batch: 15/224\n",
      "Batch loss: 0.15317998826503754 batch: 16/224\n",
      "Batch loss: 0.1390056163072586 batch: 17/224\n",
      "Batch loss: 0.1606227457523346 batch: 18/224\n",
      "Batch loss: 0.12903231382369995 batch: 19/224\n",
      "Batch loss: 0.14763757586479187 batch: 20/224\n",
      "Batch loss: 0.14785663783550262 batch: 21/224\n",
      "Batch loss: 0.11927352845668793 batch: 22/224\n",
      "Batch loss: 0.1510811299085617 batch: 23/224\n",
      "Batch loss: 0.14070448279380798 batch: 24/224\n",
      "Batch loss: 0.12856963276863098 batch: 25/224\n",
      "Batch loss: 0.09291819483041763 batch: 26/224\n",
      "Batch loss: 0.14662234485149384 batch: 27/224\n",
      "Batch loss: 0.13012327253818512 batch: 28/224\n",
      "Batch loss: 0.15578387677669525 batch: 29/224\n",
      "Batch loss: 0.12232471257448196 batch: 30/224\n",
      "Batch loss: 0.11133990436792374 batch: 31/224\n",
      "Batch loss: 0.13134253025054932 batch: 32/224\n",
      "Batch loss: 0.10889211297035217 batch: 33/224\n",
      "Batch loss: 0.13832250237464905 batch: 34/224\n",
      "Batch loss: 0.1299155056476593 batch: 35/224\n",
      "Batch loss: 0.12634891271591187 batch: 36/224\n",
      "Batch loss: 0.11537274718284607 batch: 37/224\n",
      "Batch loss: 0.12634824216365814 batch: 38/224\n",
      "Batch loss: 0.1458951234817505 batch: 39/224\n",
      "Batch loss: 0.08426669239997864 batch: 40/224\n",
      "Batch loss: 0.13342435657978058 batch: 41/224\n",
      "Batch loss: 0.13283614814281464 batch: 42/224\n",
      "Batch loss: 0.14574195444583893 batch: 43/224\n",
      "Batch loss: 0.13286657631397247 batch: 44/224\n",
      "Batch loss: 0.11744623631238937 batch: 45/224\n",
      "Batch loss: 0.1739986687898636 batch: 46/224\n",
      "Batch loss: 0.13870732486248016 batch: 47/224\n",
      "Batch loss: 0.1043408140540123 batch: 48/224\n",
      "Batch loss: 0.09572403877973557 batch: 49/224\n",
      "Batch loss: 0.1327732354402542 batch: 50/224\n",
      "Batch loss: 0.13755080103874207 batch: 51/224\n",
      "Batch loss: 0.11656984686851501 batch: 52/224\n",
      "Batch loss: 0.15113361179828644 batch: 53/224\n",
      "Batch loss: 0.09648440778255463 batch: 54/224\n",
      "Batch loss: 0.11851996928453445 batch: 55/224\n",
      "Batch loss: 0.1658591479063034 batch: 56/224\n",
      "Batch loss: 0.14512097835540771 batch: 57/224\n",
      "Batch loss: 0.11937019973993301 batch: 58/224\n",
      "Batch loss: 0.11699352413415909 batch: 59/224\n",
      "Batch loss: 0.18590515851974487 batch: 60/224\n",
      "Batch loss: 0.11876727640628815 batch: 61/224\n",
      "Batch loss: 0.10981226712465286 batch: 62/224\n",
      "Batch loss: 0.12196647375822067 batch: 63/224\n",
      "Batch loss: 0.13720440864562988 batch: 64/224\n",
      "Batch loss: 0.156241774559021 batch: 65/224\n",
      "Batch loss: 0.1463068127632141 batch: 66/224\n",
      "Batch loss: 0.11462941020727158 batch: 67/224\n",
      "Batch loss: 0.13322986662387848 batch: 68/224\n",
      "Batch loss: 0.1658533364534378 batch: 69/224\n",
      "Batch loss: 0.14762912690639496 batch: 70/224\n",
      "Batch loss: 0.11072559654712677 batch: 71/224\n",
      "Batch loss: 0.08597371727228165 batch: 72/224\n",
      "Batch loss: 0.13990983366966248 batch: 73/224\n",
      "Batch loss: 0.14255069196224213 batch: 74/224\n",
      "Batch loss: 0.14458665251731873 batch: 75/224\n",
      "Batch loss: 0.17369291186332703 batch: 76/224\n",
      "Batch loss: 0.16610278189182281 batch: 77/224\n",
      "Batch loss: 0.13086700439453125 batch: 78/224\n",
      "Batch loss: 0.11790444701910019 batch: 79/224\n",
      "Batch loss: 0.15737737715244293 batch: 80/224\n",
      "Batch loss: 0.1519041657447815 batch: 81/224\n",
      "Batch loss: 0.13995404541492462 batch: 82/224\n",
      "Batch loss: 0.11830025166273117 batch: 83/224\n",
      "Batch loss: 0.09349063038825989 batch: 84/224\n",
      "Batch loss: 0.10534916073083878 batch: 85/224\n",
      "Batch loss: 0.1405339241027832 batch: 86/224\n",
      "Batch loss: 0.11816056817770004 batch: 87/224\n",
      "Batch loss: 0.16574926674365997 batch: 88/224\n",
      "Batch loss: 0.14852359890937805 batch: 89/224\n",
      "Batch loss: 0.1866077035665512 batch: 90/224\n",
      "Batch loss: 0.12496402859687805 batch: 91/224\n",
      "Batch loss: 0.1703541874885559 batch: 92/224\n",
      "Batch loss: 0.09871566295623779 batch: 93/224\n",
      "Batch loss: 0.12899410724639893 batch: 94/224\n",
      "Batch loss: 0.1078852117061615 batch: 95/224\n",
      "Batch loss: 0.1343020647764206 batch: 96/224\n",
      "Batch loss: 0.13881145417690277 batch: 97/224\n",
      "Batch loss: 0.08716604858636856 batch: 98/224\n",
      "Batch loss: 0.12344357371330261 batch: 99/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.10472680628299713 batch: 100/224\n",
      "Batch loss: 0.1198970153927803 batch: 101/224\n",
      "Batch loss: 0.13888013362884521 batch: 102/224\n",
      "Batch loss: 0.13787835836410522 batch: 103/224\n",
      "Batch loss: 0.10027755796909332 batch: 104/224\n",
      "Batch loss: 0.09615383297204971 batch: 105/224\n",
      "Batch loss: 0.11286591738462448 batch: 106/224\n",
      "Batch loss: 0.1449946016073227 batch: 107/224\n",
      "Batch loss: 0.12828151881694794 batch: 108/224\n",
      "Batch loss: 0.11888585984706879 batch: 109/224\n",
      "Batch loss: 0.12241518497467041 batch: 110/224\n",
      "Batch loss: 0.13978028297424316 batch: 111/224\n",
      "Batch loss: 0.11389729380607605 batch: 112/224\n",
      "Batch loss: 0.17144066095352173 batch: 113/224\n",
      "Batch loss: 0.09934620559215546 batch: 114/224\n",
      "Batch loss: 0.13241183757781982 batch: 115/224\n",
      "Batch loss: 0.12988707423210144 batch: 116/224\n",
      "Batch loss: 0.0809035450220108 batch: 117/224\n",
      "Batch loss: 0.10666732490062714 batch: 118/224\n",
      "Batch loss: 0.1213703379034996 batch: 119/224\n",
      "Batch loss: 0.1280120313167572 batch: 120/224\n",
      "Batch loss: 0.12339364737272263 batch: 121/224\n",
      "Batch loss: 0.10367823392152786 batch: 122/224\n",
      "Batch loss: 0.10459580272436142 batch: 123/224\n",
      "Batch loss: 0.12427699565887451 batch: 124/224\n",
      "Batch loss: 0.13300573825836182 batch: 125/224\n",
      "Batch loss: 0.15075094997882843 batch: 126/224\n",
      "Batch loss: 0.14324086904525757 batch: 127/224\n",
      "Batch loss: 0.12219087779521942 batch: 128/224\n",
      "Batch loss: 0.11979725956916809 batch: 129/224\n",
      "Batch loss: 0.15497496724128723 batch: 130/224\n",
      "Batch loss: 0.10753703862428665 batch: 131/224\n",
      "Batch loss: 0.12104270607233047 batch: 132/224\n",
      "Batch loss: 0.16366040706634521 batch: 133/224\n",
      "Batch loss: 0.12658728659152985 batch: 134/224\n",
      "Batch loss: 0.14787904918193817 batch: 135/224\n",
      "Batch loss: 0.12445607036352158 batch: 136/224\n",
      "Batch loss: 0.10068587958812714 batch: 137/224\n",
      "Batch loss: 0.13138991594314575 batch: 138/224\n",
      "Batch loss: 0.1496017724275589 batch: 139/224\n",
      "Batch loss: 0.17906031012535095 batch: 140/224\n",
      "Batch loss: 0.10850914567708969 batch: 141/224\n",
      "Batch loss: 0.10403788834810257 batch: 142/224\n",
      "Batch loss: 0.12606459856033325 batch: 143/224\n",
      "Batch loss: 0.15880034863948822 batch: 144/224\n",
      "Batch loss: 0.10507260262966156 batch: 145/224\n",
      "Batch loss: 0.18444697558879852 batch: 146/224\n",
      "Batch loss: 0.11426445096731186 batch: 147/224\n",
      "Batch loss: 0.12849146127700806 batch: 148/224\n",
      "Batch loss: 0.15052226185798645 batch: 149/224\n",
      "Batch loss: 0.16010954976081848 batch: 150/224\n",
      "Batch loss: 0.13711710274219513 batch: 151/224\n",
      "Batch loss: 0.12549810111522675 batch: 152/224\n",
      "Batch loss: 0.16799457371234894 batch: 153/224\n",
      "Batch loss: 0.12840870022773743 batch: 154/224\n",
      "Batch loss: 0.13871781527996063 batch: 155/224\n",
      "Batch loss: 0.1085679680109024 batch: 156/224\n",
      "Batch loss: 0.12663331627845764 batch: 157/224\n",
      "Batch loss: 0.19132564961910248 batch: 158/224\n",
      "Batch loss: 0.11261522769927979 batch: 159/224\n",
      "Batch loss: 0.14158175885677338 batch: 160/224\n",
      "Batch loss: 0.11456875503063202 batch: 161/224\n",
      "Batch loss: 0.16070884466171265 batch: 162/224\n",
      "Batch loss: 0.12100467830896378 batch: 163/224\n",
      "Batch loss: 0.12302814424037933 batch: 164/224\n",
      "Batch loss: 0.15558110177516937 batch: 165/224\n",
      "Batch loss: 0.10335782170295715 batch: 166/224\n",
      "Batch loss: 0.1330207884311676 batch: 167/224\n",
      "Batch loss: 0.09799860417842865 batch: 168/224\n",
      "Batch loss: 0.1326647847890854 batch: 169/224\n",
      "Batch loss: 0.13745099306106567 batch: 170/224\n",
      "Batch loss: 0.14194661378860474 batch: 171/224\n",
      "Batch loss: 0.11263668537139893 batch: 172/224\n",
      "Batch loss: 0.15952709317207336 batch: 173/224\n",
      "Batch loss: 0.1354818195104599 batch: 174/224\n",
      "Batch loss: 0.12142390757799149 batch: 175/224\n",
      "Batch loss: 0.11522386968135834 batch: 176/224\n",
      "Batch loss: 0.15877008438110352 batch: 177/224\n",
      "Batch loss: 0.13786126673221588 batch: 178/224\n",
      "Batch loss: 0.1642986685037613 batch: 179/224\n",
      "Batch loss: 0.1067117378115654 batch: 180/224\n",
      "Batch loss: 0.15955182909965515 batch: 181/224\n",
      "Batch loss: 0.15463918447494507 batch: 182/224\n",
      "Batch loss: 0.16557027399539948 batch: 183/224\n",
      "Batch loss: 0.13038679957389832 batch: 184/224\n",
      "Batch loss: 0.13403205573558807 batch: 185/224\n",
      "Batch loss: 0.1120976135134697 batch: 186/224\n",
      "Batch loss: 0.12487604469060898 batch: 187/224\n",
      "Batch loss: 0.09950269013643265 batch: 188/224\n",
      "Batch loss: 0.13015300035476685 batch: 189/224\n",
      "Batch loss: 0.1333332508802414 batch: 190/224\n",
      "Batch loss: 0.09291812777519226 batch: 191/224\n",
      "Batch loss: 0.17097997665405273 batch: 192/224\n",
      "Batch loss: 0.13840293884277344 batch: 193/224\n",
      "Batch loss: 0.11607584357261658 batch: 194/224\n",
      "Batch loss: 0.14481210708618164 batch: 195/224\n",
      "Batch loss: 0.13941709697246552 batch: 196/224\n",
      "Batch loss: 0.1479543298482895 batch: 197/224\n",
      "Batch loss: 0.10483478009700775 batch: 198/224\n",
      "Batch loss: 0.0886797234416008 batch: 199/224\n",
      "Batch loss: 0.15868476033210754 batch: 200/224\n",
      "Batch loss: 0.1368902325630188 batch: 201/224\n",
      "Batch loss: 0.1572827398777008 batch: 202/224\n",
      "Batch loss: 0.11867538094520569 batch: 203/224\n",
      "Batch loss: 0.1539958417415619 batch: 204/224\n",
      "Batch loss: 0.12347009032964706 batch: 205/224\n",
      "Batch loss: 0.12101808190345764 batch: 206/224\n",
      "Batch loss: 0.11211278289556503 batch: 207/224\n",
      "Batch loss: 0.11869753897190094 batch: 208/224\n",
      "Batch loss: 0.13342629373073578 batch: 209/224\n",
      "Batch loss: 0.12802454829216003 batch: 210/224\n",
      "Batch loss: 0.12354355305433273 batch: 211/224\n",
      "Batch loss: 0.13765956461429596 batch: 212/224\n",
      "Batch loss: 0.16004499793052673 batch: 213/224\n",
      "Batch loss: 0.16275082528591156 batch: 214/224\n",
      "Batch loss: 0.1641397327184677 batch: 215/224\n",
      "Batch loss: 0.13716451823711395 batch: 216/224\n",
      "Batch loss: 0.11907850950956345 batch: 217/224\n",
      "Batch loss: 0.1158500611782074 batch: 218/224\n",
      "Batch loss: 0.07308956235647202 batch: 219/224\n",
      "Batch loss: 0.11849264800548553 batch: 220/224\n",
      "Batch loss: 0.14614509046077728 batch: 221/224\n",
      "Batch loss: 0.13577158749103546 batch: 222/224\n",
      "Batch loss: 0.11474423110485077 batch: 223/224\n",
      "Batch loss: 0.1554131954908371 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 36/75..  Training Loss: 0.00026..  Test Loss: 0.00085..  Test Accuracy: 0.88864\n",
      "Running epoch 37/75\n",
      "Batch loss: 0.12652575969696045 batch: 1/224\n",
      "Batch loss: 0.12512117624282837 batch: 2/224\n",
      "Batch loss: 0.10025117546319962 batch: 3/224\n",
      "Batch loss: 0.1640487164258957 batch: 4/224\n",
      "Batch loss: 0.15744814276695251 batch: 5/224\n",
      "Batch loss: 0.148224875330925 batch: 6/224\n",
      "Batch loss: 0.1017875224351883 batch: 7/224\n",
      "Batch loss: 0.15973827242851257 batch: 8/224\n",
      "Batch loss: 0.13867340981960297 batch: 9/224\n",
      "Batch loss: 0.14505289494991302 batch: 10/224\n",
      "Batch loss: 0.14622339606285095 batch: 11/224\n",
      "Batch loss: 0.12009010463953018 batch: 12/224\n",
      "Batch loss: 0.10913151502609253 batch: 13/224\n",
      "Batch loss: 0.10238509625196457 batch: 14/224\n",
      "Batch loss: 0.1174534261226654 batch: 15/224\n",
      "Batch loss: 0.14127832651138306 batch: 16/224\n",
      "Batch loss: 0.10348450392484665 batch: 17/224\n",
      "Batch loss: 0.1416185349225998 batch: 18/224\n",
      "Batch loss: 0.1044817864894867 batch: 19/224\n",
      "Batch loss: 0.1303691864013672 batch: 20/224\n",
      "Batch loss: 0.1413859724998474 batch: 21/224\n",
      "Batch loss: 0.11796444654464722 batch: 22/224\n",
      "Batch loss: 0.132258340716362 batch: 23/224\n",
      "Batch loss: 0.17076952755451202 batch: 24/224\n",
      "Batch loss: 0.11324837803840637 batch: 25/224\n",
      "Batch loss: 0.1235191822052002 batch: 26/224\n",
      "Batch loss: 0.13063845038414001 batch: 27/224\n",
      "Batch loss: 0.12889578938484192 batch: 28/224\n",
      "Batch loss: 0.1465381383895874 batch: 29/224\n",
      "Batch loss: 0.13968420028686523 batch: 30/224\n",
      "Batch loss: 0.09214849770069122 batch: 31/224\n",
      "Batch loss: 0.1451166421175003 batch: 32/224\n",
      "Batch loss: 0.11666269600391388 batch: 33/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.14553681015968323 batch: 34/224\n",
      "Batch loss: 0.14940205216407776 batch: 35/224\n",
      "Batch loss: 0.138914555311203 batch: 36/224\n",
      "Batch loss: 0.14625298976898193 batch: 37/224\n",
      "Batch loss: 0.11531408876180649 batch: 38/224\n",
      "Batch loss: 0.1194581389427185 batch: 39/224\n",
      "Batch loss: 0.10128781944513321 batch: 40/224\n",
      "Batch loss: 0.15531697869300842 batch: 41/224\n",
      "Batch loss: 0.13009683787822723 batch: 42/224\n",
      "Batch loss: 0.12458881735801697 batch: 43/224\n",
      "Batch loss: 0.10093613713979721 batch: 44/224\n",
      "Batch loss: 0.09004521369934082 batch: 45/224\n",
      "Batch loss: 0.16056647896766663 batch: 46/224\n",
      "Batch loss: 0.11216630041599274 batch: 47/224\n",
      "Batch loss: 0.11925411969423294 batch: 48/224\n",
      "Batch loss: 0.09762417525053024 batch: 49/224\n",
      "Batch loss: 0.08732189983129501 batch: 50/224\n",
      "Batch loss: 0.12946200370788574 batch: 51/224\n",
      "Batch loss: 0.11818354576826096 batch: 52/224\n",
      "Batch loss: 0.1350538730621338 batch: 53/224\n",
      "Batch loss: 0.12932461500167847 batch: 54/224\n",
      "Batch loss: 0.13543884456157684 batch: 55/224\n",
      "Batch loss: 0.11482877284288406 batch: 56/224\n",
      "Batch loss: 0.14339646697044373 batch: 57/224\n",
      "Batch loss: 0.11079643666744232 batch: 58/224\n",
      "Batch loss: 0.1511644572019577 batch: 59/224\n",
      "Batch loss: 0.1888233721256256 batch: 60/224\n",
      "Batch loss: 0.14239801466464996 batch: 61/224\n",
      "Batch loss: 0.07941975444555283 batch: 62/224\n",
      "Batch loss: 0.12902426719665527 batch: 63/224\n",
      "Batch loss: 0.15790024399757385 batch: 64/224\n",
      "Batch loss: 0.1278785765171051 batch: 65/224\n",
      "Batch loss: 0.15641078352928162 batch: 66/224\n",
      "Batch loss: 0.12175253033638 batch: 67/224\n",
      "Batch loss: 0.13980188965797424 batch: 68/224\n",
      "Batch loss: 0.10489330440759659 batch: 69/224\n",
      "Batch loss: 0.12431503087282181 batch: 70/224\n",
      "Batch loss: 0.1337527632713318 batch: 71/224\n",
      "Batch loss: 0.09089663624763489 batch: 72/224\n",
      "Batch loss: 0.1610444188117981 batch: 73/224\n",
      "Batch loss: 0.1002286821603775 batch: 74/224\n",
      "Batch loss: 0.09919803589582443 batch: 75/224\n",
      "Batch loss: 0.163568913936615 batch: 76/224\n",
      "Batch loss: 0.1469767540693283 batch: 77/224\n",
      "Batch loss: 0.10126331448554993 batch: 78/224\n",
      "Batch loss: 0.15802529454231262 batch: 79/224\n",
      "Batch loss: 0.12920543551445007 batch: 80/224\n",
      "Batch loss: 0.16328750550746918 batch: 81/224\n",
      "Batch loss: 0.1811324506998062 batch: 82/224\n",
      "Batch loss: 0.1242620050907135 batch: 83/224\n",
      "Batch loss: 0.10236158967018127 batch: 84/224\n",
      "Batch loss: 0.1438681036233902 batch: 85/224\n",
      "Batch loss: 0.1133301705121994 batch: 86/224\n",
      "Batch loss: 0.13445322215557098 batch: 87/224\n",
      "Batch loss: 0.13056349754333496 batch: 88/224\n",
      "Batch loss: 0.12539948523044586 batch: 89/224\n",
      "Batch loss: 0.13804951310157776 batch: 90/224\n",
      "Batch loss: 0.10242676734924316 batch: 91/224\n",
      "Batch loss: 0.11135940998792648 batch: 92/224\n",
      "Batch loss: 0.09002719074487686 batch: 93/224\n",
      "Batch loss: 0.14932402968406677 batch: 94/224\n",
      "Batch loss: 0.1316794753074646 batch: 95/224\n",
      "Batch loss: 0.11965523660182953 batch: 96/224\n",
      "Batch loss: 0.09752026200294495 batch: 97/224\n",
      "Batch loss: 0.12860223650932312 batch: 98/224\n",
      "Batch loss: 0.12668928503990173 batch: 99/224\n",
      "Batch loss: 0.11432711035013199 batch: 100/224\n",
      "Batch loss: 0.15452250838279724 batch: 101/224\n",
      "Batch loss: 0.11856792122125626 batch: 102/224\n",
      "Batch loss: 0.11536738276481628 batch: 103/224\n",
      "Batch loss: 0.10058512538671494 batch: 104/224\n",
      "Batch loss: 0.11746743321418762 batch: 105/224\n",
      "Batch loss: 0.12453844398260117 batch: 106/224\n",
      "Batch loss: 0.13923364877700806 batch: 107/224\n",
      "Batch loss: 0.12378624826669693 batch: 108/224\n",
      "Batch loss: 0.10669068247079849 batch: 109/224\n",
      "Batch loss: 0.10353420674800873 batch: 110/224\n",
      "Batch loss: 0.11048394441604614 batch: 111/224\n",
      "Batch loss: 0.13062375783920288 batch: 112/224\n",
      "Batch loss: 0.11186288297176361 batch: 113/224\n",
      "Batch loss: 0.12394387274980545 batch: 114/224\n",
      "Batch loss: 0.1396973729133606 batch: 115/224\n",
      "Batch loss: 0.10572734475135803 batch: 116/224\n",
      "Batch loss: 0.08148427307605743 batch: 117/224\n",
      "Batch loss: 0.12741898000240326 batch: 118/224\n",
      "Batch loss: 0.11356137692928314 batch: 119/224\n",
      "Batch loss: 0.12249037623405457 batch: 120/224\n",
      "Batch loss: 0.10515092313289642 batch: 121/224\n",
      "Batch loss: 0.11311845481395721 batch: 122/224\n",
      "Batch loss: 0.11842457950115204 batch: 123/224\n",
      "Batch loss: 0.10560045391321182 batch: 124/224\n",
      "Batch loss: 0.1331963986158371 batch: 125/224\n",
      "Batch loss: 0.1305047571659088 batch: 126/224\n",
      "Batch loss: 0.1508869081735611 batch: 127/224\n",
      "Batch loss: 0.0873933881521225 batch: 128/224\n",
      "Batch loss: 0.10198118537664413 batch: 129/224\n",
      "Batch loss: 0.11244114488363266 batch: 130/224\n",
      "Batch loss: 0.09249762445688248 batch: 131/224\n",
      "Batch loss: 0.11978834867477417 batch: 132/224\n",
      "Batch loss: 0.13493211567401886 batch: 133/224\n",
      "Batch loss: 0.09458231180906296 batch: 134/224\n",
      "Batch loss: 0.10083617269992828 batch: 135/224\n",
      "Batch loss: 0.11859259009361267 batch: 136/224\n",
      "Batch loss: 0.10783544182777405 batch: 137/224\n",
      "Batch loss: 0.114841990172863 batch: 138/224\n",
      "Batch loss: 0.140333354473114 batch: 139/224\n",
      "Batch loss: 0.17092294991016388 batch: 140/224\n",
      "Batch loss: 0.10607393831014633 batch: 141/224\n",
      "Batch loss: 0.09110873192548752 batch: 142/224\n",
      "Batch loss: 0.0865333303809166 batch: 143/224\n",
      "Batch loss: 0.10583014041185379 batch: 144/224\n",
      "Batch loss: 0.1432088315486908 batch: 145/224\n",
      "Batch loss: 0.11744671314954758 batch: 146/224\n",
      "Batch loss: 0.11333485692739487 batch: 147/224\n",
      "Batch loss: 0.14680244028568268 batch: 148/224\n",
      "Batch loss: 0.12731091678142548 batch: 149/224\n",
      "Batch loss: 0.13787640631198883 batch: 150/224\n",
      "Batch loss: 0.09996839612722397 batch: 151/224\n",
      "Batch loss: 0.1178862452507019 batch: 152/224\n",
      "Batch loss: 0.17637097835540771 batch: 153/224\n",
      "Batch loss: 0.14565911889076233 batch: 154/224\n",
      "Batch loss: 0.10401570051908493 batch: 155/224\n",
      "Batch loss: 0.11601787060499191 batch: 156/224\n",
      "Batch loss: 0.15274761617183685 batch: 157/224\n",
      "Batch loss: 0.20196884870529175 batch: 158/224\n",
      "Batch loss: 0.11676285415887833 batch: 159/224\n",
      "Batch loss: 0.10694730281829834 batch: 160/224\n",
      "Batch loss: 0.09158419072628021 batch: 161/224\n",
      "Batch loss: 0.10871157795190811 batch: 162/224\n",
      "Batch loss: 0.11852714419364929 batch: 163/224\n",
      "Batch loss: 0.1324501484632492 batch: 164/224\n",
      "Batch loss: 0.16630737483501434 batch: 165/224\n",
      "Batch loss: 0.13309399783611298 batch: 166/224\n",
      "Batch loss: 0.12292533367872238 batch: 167/224\n",
      "Batch loss: 0.09821122884750366 batch: 168/224\n",
      "Batch loss: 0.12529705464839935 batch: 169/224\n",
      "Batch loss: 0.14405088126659393 batch: 170/224\n",
      "Batch loss: 0.1265936642885208 batch: 171/224\n",
      "Batch loss: 0.0844540148973465 batch: 172/224\n",
      "Batch loss: 0.13049551844596863 batch: 173/224\n",
      "Batch loss: 0.1420236974954605 batch: 174/224\n",
      "Batch loss: 0.12137769907712936 batch: 175/224\n",
      "Batch loss: 0.1360601782798767 batch: 176/224\n",
      "Batch loss: 0.14514534175395966 batch: 177/224\n",
      "Batch loss: 0.08936339616775513 batch: 178/224\n",
      "Batch loss: 0.13270196318626404 batch: 179/224\n",
      "Batch loss: 0.09145274013280869 batch: 180/224\n",
      "Batch loss: 0.12461619079113007 batch: 181/224\n",
      "Batch loss: 0.1220565214753151 batch: 182/224\n",
      "Batch loss: 0.14580774307250977 batch: 183/224\n",
      "Batch loss: 0.11165519803762436 batch: 184/224\n",
      "Batch loss: 0.16557273268699646 batch: 185/224\n",
      "Batch loss: 0.09682773798704147 batch: 186/224\n",
      "Batch loss: 0.13145971298217773 batch: 187/224\n",
      "Batch loss: 0.13669422268867493 batch: 188/224\n",
      "Batch loss: 0.13552285730838776 batch: 189/224\n",
      "Batch loss: 0.11922992020845413 batch: 190/224\n",
      "Batch loss: 0.12055512517690659 batch: 191/224\n",
      "Batch loss: 0.15915487706661224 batch: 192/224\n",
      "Batch loss: 0.12727059423923492 batch: 193/224\n",
      "Batch loss: 0.14452941715717316 batch: 194/224\n",
      "Batch loss: 0.14409343898296356 batch: 195/224\n",
      "Batch loss: 0.14943505823612213 batch: 196/224\n",
      "Batch loss: 0.13889312744140625 batch: 197/224\n",
      "Batch loss: 0.11339566111564636 batch: 198/224\n",
      "Batch loss: 0.14350883662700653 batch: 199/224\n",
      "Batch loss: 0.10730234533548355 batch: 200/224\n",
      "Batch loss: 0.1025317832827568 batch: 201/224\n",
      "Batch loss: 0.11522013694047928 batch: 202/224\n",
      "Batch loss: 0.10866176337003708 batch: 203/224\n",
      "Batch loss: 0.12969180941581726 batch: 204/224\n",
      "Batch loss: 0.14895713329315186 batch: 205/224\n",
      "Batch loss: 0.13367705047130585 batch: 206/224\n",
      "Batch loss: 0.11403731256723404 batch: 207/224\n",
      "Batch loss: 0.11843642592430115 batch: 208/224\n",
      "Batch loss: 0.11609211564064026 batch: 209/224\n",
      "Batch loss: 0.12175974249839783 batch: 210/224\n",
      "Batch loss: 0.11200957000255585 batch: 211/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.12709420919418335 batch: 212/224\n",
      "Batch loss: 0.1357962191104889 batch: 213/224\n",
      "Batch loss: 0.15192703902721405 batch: 214/224\n",
      "Batch loss: 0.10273458063602448 batch: 215/224\n",
      "Batch loss: 0.14170414209365845 batch: 216/224\n",
      "Batch loss: 0.14890453219413757 batch: 217/224\n",
      "Batch loss: 0.12320363521575928 batch: 218/224\n",
      "Batch loss: 0.09617754071950912 batch: 219/224\n",
      "Batch loss: 0.09053193032741547 batch: 220/224\n",
      "Batch loss: 0.1469123661518097 batch: 221/224\n",
      "Batch loss: 0.13669821619987488 batch: 222/224\n",
      "Batch loss: 0.08538024127483368 batch: 223/224\n",
      "Batch loss: 0.10731952637434006 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 37/75..  Training Loss: 0.00025..  Test Loss: 0.00084..  Test Accuracy: 0.88889\n",
      "Running epoch 38/75\n",
      "Batch loss: 0.09576749056577682 batch: 1/224\n",
      "Batch loss: 0.14846006035804749 batch: 2/224\n",
      "Batch loss: 0.12330513447523117 batch: 3/224\n",
      "Batch loss: 0.15308372676372528 batch: 4/224\n",
      "Batch loss: 0.11641868948936462 batch: 5/224\n",
      "Batch loss: 0.13018375635147095 batch: 6/224\n",
      "Batch loss: 0.10547937452793121 batch: 7/224\n",
      "Batch loss: 0.10230284184217453 batch: 8/224\n",
      "Batch loss: 0.09856284409761429 batch: 9/224\n",
      "Batch loss: 0.11457773298025131 batch: 10/224\n",
      "Batch loss: 0.17729227244853973 batch: 11/224\n",
      "Batch loss: 0.12179500609636307 batch: 12/224\n",
      "Batch loss: 0.08540815860033035 batch: 13/224\n",
      "Batch loss: 0.10742657631635666 batch: 14/224\n",
      "Batch loss: 0.10794577747583389 batch: 15/224\n",
      "Batch loss: 0.1511523276567459 batch: 16/224\n",
      "Batch loss: 0.10181187838315964 batch: 17/224\n",
      "Batch loss: 0.14942467212677002 batch: 18/224\n",
      "Batch loss: 0.1314801573753357 batch: 19/224\n",
      "Batch loss: 0.13345153629779816 batch: 20/224\n",
      "Batch loss: 0.14048157632350922 batch: 21/224\n",
      "Batch loss: 0.12461333721876144 batch: 22/224\n",
      "Batch loss: 0.13368946313858032 batch: 23/224\n",
      "Batch loss: 0.11950206756591797 batch: 24/224\n",
      "Batch loss: 0.123968705534935 batch: 25/224\n",
      "Batch loss: 0.09443937987089157 batch: 26/224\n",
      "Batch loss: 0.11874137818813324 batch: 27/224\n",
      "Batch loss: 0.14618830382823944 batch: 28/224\n",
      "Batch loss: 0.14050397276878357 batch: 29/224\n",
      "Batch loss: 0.13340386748313904 batch: 30/224\n",
      "Batch loss: 0.09760086238384247 batch: 31/224\n",
      "Batch loss: 0.11505462974309921 batch: 32/224\n",
      "Batch loss: 0.09227003157138824 batch: 33/224\n",
      "Batch loss: 0.1412353813648224 batch: 34/224\n",
      "Batch loss: 0.11651510745286942 batch: 35/224\n",
      "Batch loss: 0.11732931435108185 batch: 36/224\n",
      "Batch loss: 0.13823890686035156 batch: 37/224\n",
      "Batch loss: 0.11688912659883499 batch: 38/224\n",
      "Batch loss: 0.11133722960948944 batch: 39/224\n",
      "Batch loss: 0.0939510241150856 batch: 40/224\n",
      "Batch loss: 0.1303868144750595 batch: 41/224\n",
      "Batch loss: 0.10886021703481674 batch: 42/224\n",
      "Batch loss: 0.12379477173089981 batch: 43/224\n",
      "Batch loss: 0.10534372180700302 batch: 44/224\n",
      "Batch loss: 0.08907430619001389 batch: 45/224\n",
      "Batch loss: 0.17397107183933258 batch: 46/224\n",
      "Batch loss: 0.1141144409775734 batch: 47/224\n",
      "Batch loss: 0.1023324579000473 batch: 48/224\n",
      "Batch loss: 0.13930566608905792 batch: 49/224\n",
      "Batch loss: 0.0854010283946991 batch: 50/224\n",
      "Batch loss: 0.15440990030765533 batch: 51/224\n",
      "Batch loss: 0.14551395177841187 batch: 52/224\n",
      "Batch loss: 0.14142222702503204 batch: 53/224\n",
      "Batch loss: 0.09618936479091644 batch: 54/224\n",
      "Batch loss: 0.11850948631763458 batch: 55/224\n",
      "Batch loss: 0.12447647005319595 batch: 56/224\n",
      "Batch loss: 0.19059114158153534 batch: 57/224\n",
      "Batch loss: 0.13301412761211395 batch: 58/224\n",
      "Batch loss: 0.11217718571424484 batch: 59/224\n",
      "Batch loss: 0.16036954522132874 batch: 60/224\n",
      "Batch loss: 0.13432399928569794 batch: 61/224\n",
      "Batch loss: 0.10889395326375961 batch: 62/224\n",
      "Batch loss: 0.1552887111902237 batch: 63/224\n",
      "Batch loss: 0.1356695145368576 batch: 64/224\n",
      "Batch loss: 0.13179190456867218 batch: 65/224\n",
      "Batch loss: 0.13422229886054993 batch: 66/224\n",
      "Batch loss: 0.14227372407913208 batch: 67/224\n",
      "Batch loss: 0.11180157214403152 batch: 68/224\n",
      "Batch loss: 0.1573651283979416 batch: 69/224\n",
      "Batch loss: 0.13512572646141052 batch: 70/224\n",
      "Batch loss: 0.12655429542064667 batch: 71/224\n",
      "Batch loss: 0.10877329856157303 batch: 72/224\n",
      "Batch loss: 0.1143384650349617 batch: 73/224\n",
      "Batch loss: 0.125682070851326 batch: 74/224\n",
      "Batch loss: 0.15382173657417297 batch: 75/224\n",
      "Batch loss: 0.1599324643611908 batch: 76/224\n",
      "Batch loss: 0.13898031413555145 batch: 77/224\n",
      "Batch loss: 0.13678143918514252 batch: 78/224\n",
      "Batch loss: 0.14514487981796265 batch: 79/224\n",
      "Batch loss: 0.10989055782556534 batch: 80/224\n",
      "Batch loss: 0.14242048561573029 batch: 81/224\n",
      "Batch loss: 0.12684446573257446 batch: 82/224\n",
      "Batch loss: 0.1519063264131546 batch: 83/224\n",
      "Batch loss: 0.09569960832595825 batch: 84/224\n",
      "Batch loss: 0.14879806339740753 batch: 85/224\n",
      "Batch loss: 0.1100226417183876 batch: 86/224\n",
      "Batch loss: 0.1113482192158699 batch: 87/224\n",
      "Batch loss: 0.128170445561409 batch: 88/224\n",
      "Batch loss: 0.14223791658878326 batch: 89/224\n",
      "Batch loss: 0.14373931288719177 batch: 90/224\n",
      "Batch loss: 0.1120409145951271 batch: 91/224\n",
      "Batch loss: 0.13397902250289917 batch: 92/224\n",
      "Batch loss: 0.07923966646194458 batch: 93/224\n",
      "Batch loss: 0.1371280699968338 batch: 94/224\n",
      "Batch loss: 0.09023978561162949 batch: 95/224\n",
      "Batch loss: 0.1288522630929947 batch: 96/224\n",
      "Batch loss: 0.12400653958320618 batch: 97/224\n",
      "Batch loss: 0.11276281625032425 batch: 98/224\n",
      "Batch loss: 0.13481880724430084 batch: 99/224\n",
      "Batch loss: 0.08378220349550247 batch: 100/224\n",
      "Batch loss: 0.1279217153787613 batch: 101/224\n",
      "Batch loss: 0.1332348734140396 batch: 102/224\n",
      "Batch loss: 0.10609504580497742 batch: 103/224\n",
      "Batch loss: 0.08435811847448349 batch: 104/224\n",
      "Batch loss: 0.10034439712762833 batch: 105/224\n",
      "Batch loss: 0.10919850319623947 batch: 106/224\n",
      "Batch loss: 0.10491956025362015 batch: 107/224\n",
      "Batch loss: 0.12773923575878143 batch: 108/224\n",
      "Batch loss: 0.11309213191270828 batch: 109/224\n",
      "Batch loss: 0.13139113783836365 batch: 110/224\n",
      "Batch loss: 0.14659282565116882 batch: 111/224\n",
      "Batch loss: 0.14902444183826447 batch: 112/224\n",
      "Batch loss: 0.10294480621814728 batch: 113/224\n",
      "Batch loss: 0.1042279601097107 batch: 114/224\n",
      "Batch loss: 0.09997347742319107 batch: 115/224\n",
      "Batch loss: 0.10049116611480713 batch: 116/224\n",
      "Batch loss: 0.09017301350831985 batch: 117/224\n",
      "Batch loss: 0.11326423287391663 batch: 118/224\n",
      "Batch loss: 0.09917297959327698 batch: 119/224\n",
      "Batch loss: 0.12863725423812866 batch: 120/224\n",
      "Batch loss: 0.09669946879148483 batch: 121/224\n",
      "Batch loss: 0.10851491242647171 batch: 122/224\n",
      "Batch loss: 0.09408062696456909 batch: 123/224\n",
      "Batch loss: 0.0899425745010376 batch: 124/224\n",
      "Batch loss: 0.10488446801900864 batch: 125/224\n",
      "Batch loss: 0.13216808438301086 batch: 126/224\n",
      "Batch loss: 0.13444019854068756 batch: 127/224\n",
      "Batch loss: 0.11038154363632202 batch: 128/224\n",
      "Batch loss: 0.1057889387011528 batch: 129/224\n",
      "Batch loss: 0.13351157307624817 batch: 130/224\n",
      "Batch loss: 0.11709251999855042 batch: 131/224\n",
      "Batch loss: 0.10646958649158478 batch: 132/224\n",
      "Batch loss: 0.12974052131175995 batch: 133/224\n",
      "Batch loss: 0.10873845219612122 batch: 134/224\n",
      "Batch loss: 0.17176823318004608 batch: 135/224\n",
      "Batch loss: 0.12823893129825592 batch: 136/224\n",
      "Batch loss: 0.1197541132569313 batch: 137/224\n",
      "Batch loss: 0.1349986046552658 batch: 138/224\n",
      "Batch loss: 0.1110696867108345 batch: 139/224\n",
      "Batch loss: 0.12129697948694229 batch: 140/224\n",
      "Batch loss: 0.08367689698934555 batch: 141/224\n",
      "Batch loss: 0.1246982142329216 batch: 142/224\n",
      "Batch loss: 0.11670105159282684 batch: 143/224\n",
      "Batch loss: 0.11479555815458298 batch: 144/224\n",
      "Batch loss: 0.12410234659910202 batch: 145/224\n",
      "Batch loss: 0.1631108969449997 batch: 146/224\n",
      "Batch loss: 0.11738508939743042 batch: 147/224\n",
      "Batch loss: 0.1411757618188858 batch: 148/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.12235090136528015 batch: 149/224\n",
      "Batch loss: 0.1725686937570572 batch: 150/224\n",
      "Batch loss: 0.10310236364603043 batch: 151/224\n",
      "Batch loss: 0.12669937312602997 batch: 152/224\n",
      "Batch loss: 0.15532998740673065 batch: 153/224\n",
      "Batch loss: 0.12129046767950058 batch: 154/224\n",
      "Batch loss: 0.10854041576385498 batch: 155/224\n",
      "Batch loss: 0.11726824194192886 batch: 156/224\n",
      "Batch loss: 0.15561017394065857 batch: 157/224\n",
      "Batch loss: 0.18140318989753723 batch: 158/224\n",
      "Batch loss: 0.13057537376880646 batch: 159/224\n",
      "Batch loss: 0.10612713545560837 batch: 160/224\n",
      "Batch loss: 0.10767663270235062 batch: 161/224\n",
      "Batch loss: 0.11388509720563889 batch: 162/224\n",
      "Batch loss: 0.11755477637052536 batch: 163/224\n",
      "Batch loss: 0.11268823593854904 batch: 164/224\n",
      "Batch loss: 0.15825559198856354 batch: 165/224\n",
      "Batch loss: 0.10830333828926086 batch: 166/224\n",
      "Batch loss: 0.09335701167583466 batch: 167/224\n",
      "Batch loss: 0.13657237589359283 batch: 168/224\n",
      "Batch loss: 0.11973156034946442 batch: 169/224\n",
      "Batch loss: 0.08631956577301025 batch: 170/224\n",
      "Batch loss: 0.104121133685112 batch: 171/224\n",
      "Batch loss: 0.14547574520111084 batch: 172/224\n",
      "Batch loss: 0.11593213677406311 batch: 173/224\n",
      "Batch loss: 0.12263661623001099 batch: 174/224\n",
      "Batch loss: 0.1199588030576706 batch: 175/224\n",
      "Batch loss: 0.09911828488111496 batch: 176/224\n",
      "Batch loss: 0.13145609200000763 batch: 177/224\n",
      "Batch loss: 0.08206477016210556 batch: 178/224\n",
      "Batch loss: 0.1062365248799324 batch: 179/224\n",
      "Batch loss: 0.07599515467882156 batch: 180/224\n",
      "Batch loss: 0.13345101475715637 batch: 181/224\n",
      "Batch loss: 0.11296281218528748 batch: 182/224\n",
      "Batch loss: 0.14236138761043549 batch: 183/224\n",
      "Batch loss: 0.1158943623304367 batch: 184/224\n",
      "Batch loss: 0.11311455070972443 batch: 185/224\n",
      "Batch loss: 0.11627882719039917 batch: 186/224\n",
      "Batch loss: 0.12549465894699097 batch: 187/224\n",
      "Batch loss: 0.10527900606393814 batch: 188/224\n",
      "Batch loss: 0.12190333753824234 batch: 189/224\n",
      "Batch loss: 0.1228099837899208 batch: 190/224\n",
      "Batch loss: 0.1591084599494934 batch: 191/224\n",
      "Batch loss: 0.10238735377788544 batch: 192/224\n",
      "Batch loss: 0.1410018354654312 batch: 193/224\n",
      "Batch loss: 0.11100120097398758 batch: 194/224\n",
      "Batch loss: 0.14963413774967194 batch: 195/224\n",
      "Batch loss: 0.16405366361141205 batch: 196/224\n",
      "Batch loss: 0.1332327127456665 batch: 197/224\n",
      "Batch loss: 0.1258489489555359 batch: 198/224\n",
      "Batch loss: 0.1195400133728981 batch: 199/224\n",
      "Batch loss: 0.12491860240697861 batch: 200/224\n",
      "Batch loss: 0.14786311984062195 batch: 201/224\n",
      "Batch loss: 0.09992342442274094 batch: 202/224\n",
      "Batch loss: 0.11425045877695084 batch: 203/224\n",
      "Batch loss: 0.1345038264989853 batch: 204/224\n",
      "Batch loss: 0.11278125643730164 batch: 205/224\n",
      "Batch loss: 0.10910893231630325 batch: 206/224\n",
      "Batch loss: 0.11204203963279724 batch: 207/224\n",
      "Batch loss: 0.08166923373937607 batch: 208/224\n",
      "Batch loss: 0.11475471407175064 batch: 209/224\n",
      "Batch loss: 0.11967432498931885 batch: 210/224\n",
      "Batch loss: 0.08268886804580688 batch: 211/224\n",
      "Batch loss: 0.09066864103078842 batch: 212/224\n",
      "Batch loss: 0.1410907357931137 batch: 213/224\n",
      "Batch loss: 0.11164598912000656 batch: 214/224\n",
      "Batch loss: 0.12347757071256638 batch: 215/224\n",
      "Batch loss: 0.1133892834186554 batch: 216/224\n",
      "Batch loss: 0.1368444412946701 batch: 217/224\n",
      "Batch loss: 0.12122444808483124 batch: 218/224\n",
      "Batch loss: 0.07419908046722412 batch: 219/224\n",
      "Batch loss: 0.12578517198562622 batch: 220/224\n",
      "Batch loss: 0.13230878114700317 batch: 221/224\n",
      "Batch loss: 0.11934306472539902 batch: 222/224\n",
      "Batch loss: 0.13444891571998596 batch: 223/224\n",
      "Batch loss: 0.12393281608819962 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 38/75..  Training Loss: 0.00024..  Test Loss: 0.00083..  Test Accuracy: 0.89086\n",
      "Running epoch 39/75\n",
      "Batch loss: 0.08821573853492737 batch: 1/224\n",
      "Batch loss: 0.11841532588005066 batch: 2/224\n",
      "Batch loss: 0.09456872195005417 batch: 3/224\n",
      "Batch loss: 0.13546431064605713 batch: 4/224\n",
      "Batch loss: 0.08327527344226837 batch: 5/224\n",
      "Batch loss: 0.1133982241153717 batch: 6/224\n",
      "Batch loss: 0.11318542063236237 batch: 7/224\n",
      "Batch loss: 0.10730494558811188 batch: 8/224\n",
      "Batch loss: 0.10728015750646591 batch: 9/224\n",
      "Batch loss: 0.13130423426628113 batch: 10/224\n",
      "Batch loss: 0.11426351964473724 batch: 11/224\n",
      "Batch loss: 0.10249069333076477 batch: 12/224\n",
      "Batch loss: 0.09171957522630692 batch: 13/224\n",
      "Batch loss: 0.1306118667125702 batch: 14/224\n",
      "Batch loss: 0.10020382702350616 batch: 15/224\n",
      "Batch loss: 0.14943453669548035 batch: 16/224\n",
      "Batch loss: 0.09622903168201447 batch: 17/224\n",
      "Batch loss: 0.1509706676006317 batch: 18/224\n",
      "Batch loss: 0.08916173130273819 batch: 19/224\n",
      "Batch loss: 0.0889538899064064 batch: 20/224\n",
      "Batch loss: 0.13506345450878143 batch: 21/224\n",
      "Batch loss: 0.1261747181415558 batch: 22/224\n",
      "Batch loss: 0.13322407007217407 batch: 23/224\n",
      "Batch loss: 0.14747793972492218 batch: 24/224\n",
      "Batch loss: 0.08782313019037247 batch: 25/224\n",
      "Batch loss: 0.09430374950170517 batch: 26/224\n",
      "Batch loss: 0.1216522827744484 batch: 27/224\n",
      "Batch loss: 0.105549156665802 batch: 28/224\n",
      "Batch loss: 0.15364336967468262 batch: 29/224\n",
      "Batch loss: 0.1099076122045517 batch: 30/224\n",
      "Batch loss: 0.09754250198602676 batch: 31/224\n",
      "Batch loss: 0.12727275490760803 batch: 32/224\n",
      "Batch loss: 0.1132025271654129 batch: 33/224\n",
      "Batch loss: 0.14768895506858826 batch: 34/224\n",
      "Batch loss: 0.12267398089170456 batch: 35/224\n",
      "Batch loss: 0.12344389408826828 batch: 36/224\n",
      "Batch loss: 0.11790255457162857 batch: 37/224\n",
      "Batch loss: 0.1263672560453415 batch: 38/224\n",
      "Batch loss: 0.12937207520008087 batch: 39/224\n",
      "Batch loss: 0.1029464453458786 batch: 40/224\n",
      "Batch loss: 0.1264573633670807 batch: 41/224\n",
      "Batch loss: 0.12084965407848358 batch: 42/224\n",
      "Batch loss: 0.13636499643325806 batch: 43/224\n",
      "Batch loss: 0.09342557936906815 batch: 44/224\n",
      "Batch loss: 0.07329900562763214 batch: 45/224\n",
      "Batch loss: 0.13940170407295227 batch: 46/224\n",
      "Batch loss: 0.11838459968566895 batch: 47/224\n",
      "Batch loss: 0.10284601151943207 batch: 48/224\n",
      "Batch loss: 0.11337480694055557 batch: 49/224\n",
      "Batch loss: 0.09756439179182053 batch: 50/224\n",
      "Batch loss: 0.10453902184963226 batch: 51/224\n",
      "Batch loss: 0.1064806655049324 batch: 52/224\n",
      "Batch loss: 0.165982186794281 batch: 53/224\n",
      "Batch loss: 0.08874421566724777 batch: 54/224\n",
      "Batch loss: 0.1005779281258583 batch: 55/224\n",
      "Batch loss: 0.12845554947853088 batch: 56/224\n",
      "Batch loss: 0.1504843533039093 batch: 57/224\n",
      "Batch loss: 0.14990423619747162 batch: 58/224\n",
      "Batch loss: 0.09144864976406097 batch: 59/224\n",
      "Batch loss: 0.12233885377645493 batch: 60/224\n",
      "Batch loss: 0.12172847986221313 batch: 61/224\n",
      "Batch loss: 0.08202025294303894 batch: 62/224\n",
      "Batch loss: 0.11546856164932251 batch: 63/224\n",
      "Batch loss: 0.14326706528663635 batch: 64/224\n",
      "Batch loss: 0.10371989011764526 batch: 65/224\n",
      "Batch loss: 0.12566539645195007 batch: 66/224\n",
      "Batch loss: 0.08206798136234283 batch: 67/224\n",
      "Batch loss: 0.10715201497077942 batch: 68/224\n",
      "Batch loss: 0.11978071928024292 batch: 69/224\n",
      "Batch loss: 0.11688394844532013 batch: 70/224\n",
      "Batch loss: 0.10919322073459625 batch: 71/224\n",
      "Batch loss: 0.08104513585567474 batch: 72/224\n",
      "Batch loss: 0.16203084588050842 batch: 73/224\n",
      "Batch loss: 0.09450358897447586 batch: 74/224\n",
      "Batch loss: 0.1155087798833847 batch: 75/224\n",
      "Batch loss: 0.16624900698661804 batch: 76/224\n",
      "Batch loss: 0.12796032428741455 batch: 77/224\n",
      "Batch loss: 0.151911199092865 batch: 78/224\n",
      "Batch loss: 0.09898886829614639 batch: 79/224\n",
      "Batch loss: 0.11559313535690308 batch: 80/224\n",
      "Batch loss: 0.1596592515707016 batch: 81/224\n",
      "Batch loss: 0.12319061905145645 batch: 82/224\n",
      "Batch loss: 0.1458931565284729 batch: 83/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.10550379008054733 batch: 84/224\n",
      "Batch loss: 0.10539641231298447 batch: 85/224\n",
      "Batch loss: 0.12510071694850922 batch: 86/224\n",
      "Batch loss: 0.10897576808929443 batch: 87/224\n",
      "Batch loss: 0.12538668513298035 batch: 88/224\n",
      "Batch loss: 0.1360364556312561 batch: 89/224\n",
      "Batch loss: 0.13896594941616058 batch: 90/224\n",
      "Batch loss: 0.13221174478530884 batch: 91/224\n",
      "Batch loss: 0.120509572327137 batch: 92/224\n",
      "Batch loss: 0.08564937114715576 batch: 93/224\n",
      "Batch loss: 0.11417342722415924 batch: 94/224\n",
      "Batch loss: 0.09921896457672119 batch: 95/224\n",
      "Batch loss: 0.11258082836866379 batch: 96/224\n",
      "Batch loss: 0.11088152974843979 batch: 97/224\n",
      "Batch loss: 0.11252019554376602 batch: 98/224\n",
      "Batch loss: 0.12095146626234055 batch: 99/224\n",
      "Batch loss: 0.11393748223781586 batch: 100/224\n",
      "Batch loss: 0.12532572448253632 batch: 101/224\n",
      "Batch loss: 0.11894244700670242 batch: 102/224\n",
      "Batch loss: 0.11657362431287766 batch: 103/224\n",
      "Batch loss: 0.12645864486694336 batch: 104/224\n",
      "Batch loss: 0.10230565816164017 batch: 105/224\n",
      "Batch loss: 0.10985364764928818 batch: 106/224\n",
      "Batch loss: 0.0987425148487091 batch: 107/224\n",
      "Batch loss: 0.12889645993709564 batch: 108/224\n",
      "Batch loss: 0.12718576192855835 batch: 109/224\n",
      "Batch loss: 0.10660069435834885 batch: 110/224\n",
      "Batch loss: 0.08424589782953262 batch: 111/224\n",
      "Batch loss: 0.08312194049358368 batch: 112/224\n",
      "Batch loss: 0.16877257823944092 batch: 113/224\n",
      "Batch loss: 0.134397953748703 batch: 114/224\n",
      "Batch loss: 0.1063133254647255 batch: 115/224\n",
      "Batch loss: 0.11530791968107224 batch: 116/224\n",
      "Batch loss: 0.10084186494350433 batch: 117/224\n",
      "Batch loss: 0.1267194151878357 batch: 118/224\n",
      "Batch loss: 0.1078891009092331 batch: 119/224\n",
      "Batch loss: 0.11312927305698395 batch: 120/224\n",
      "Batch loss: 0.11270961165428162 batch: 121/224\n",
      "Batch loss: 0.11065687239170074 batch: 122/224\n",
      "Batch loss: 0.1017012670636177 batch: 123/224\n",
      "Batch loss: 0.09177209436893463 batch: 124/224\n",
      "Batch loss: 0.10012941807508469 batch: 125/224\n",
      "Batch loss: 0.10198000818490982 batch: 126/224\n",
      "Batch loss: 0.15802565217018127 batch: 127/224\n",
      "Batch loss: 0.11329762637615204 batch: 128/224\n",
      "Batch loss: 0.13955530524253845 batch: 129/224\n",
      "Batch loss: 0.09320617467164993 batch: 130/224\n",
      "Batch loss: 0.1320728361606598 batch: 131/224\n",
      "Batch loss: 0.11863818019628525 batch: 132/224\n",
      "Batch loss: 0.1404699832201004 batch: 133/224\n",
      "Batch loss: 0.11380640417337418 batch: 134/224\n",
      "Batch loss: 0.12291431427001953 batch: 135/224\n",
      "Batch loss: 0.10939102619886398 batch: 136/224\n",
      "Batch loss: 0.10108489543199539 batch: 137/224\n",
      "Batch loss: 0.0924120768904686 batch: 138/224\n",
      "Batch loss: 0.18093755841255188 batch: 139/224\n",
      "Batch loss: 0.18859849870204926 batch: 140/224\n",
      "Batch loss: 0.11961884796619415 batch: 141/224\n",
      "Batch loss: 0.11250606179237366 batch: 142/224\n",
      "Batch loss: 0.10985666513442993 batch: 143/224\n",
      "Batch loss: 0.09732064604759216 batch: 144/224\n",
      "Batch loss: 0.11971104890108109 batch: 145/224\n",
      "Batch loss: 0.16146861016750336 batch: 146/224\n",
      "Batch loss: 0.09637891501188278 batch: 147/224\n",
      "Batch loss: 0.07909617573022842 batch: 148/224\n",
      "Batch loss: 0.15562137961387634 batch: 149/224\n",
      "Batch loss: 0.13976840674877167 batch: 150/224\n",
      "Batch loss: 0.0982440710067749 batch: 151/224\n",
      "Batch loss: 0.09159441292285919 batch: 152/224\n",
      "Batch loss: 0.1345779299736023 batch: 153/224\n",
      "Batch loss: 0.1134985163807869 batch: 154/224\n",
      "Batch loss: 0.08687830716371536 batch: 155/224\n",
      "Batch loss: 0.12551167607307434 batch: 156/224\n",
      "Batch loss: 0.12216311693191528 batch: 157/224\n",
      "Batch loss: 0.18492324650287628 batch: 158/224\n",
      "Batch loss: 0.08354915678501129 batch: 159/224\n",
      "Batch loss: 0.12166644632816315 batch: 160/224\n",
      "Batch loss: 0.10549449175596237 batch: 161/224\n",
      "Batch loss: 0.11329632252454758 batch: 162/224\n",
      "Batch loss: 0.08017102628946304 batch: 163/224\n",
      "Batch loss: 0.10303390771150589 batch: 164/224\n",
      "Batch loss: 0.1495932787656784 batch: 165/224\n",
      "Batch loss: 0.11739137023687363 batch: 166/224\n",
      "Batch loss: 0.13016344606876373 batch: 167/224\n",
      "Batch loss: 0.08765440434217453 batch: 168/224\n",
      "Batch loss: 0.09887360781431198 batch: 169/224\n",
      "Batch loss: 0.10972058027982712 batch: 170/224\n",
      "Batch loss: 0.10439111292362213 batch: 171/224\n",
      "Batch loss: 0.1094067245721817 batch: 172/224\n",
      "Batch loss: 0.1077323630452156 batch: 173/224\n",
      "Batch loss: 0.10657346248626709 batch: 174/224\n",
      "Batch loss: 0.10065776109695435 batch: 175/224\n",
      "Batch loss: 0.10260486602783203 batch: 176/224\n",
      "Batch loss: 0.15328586101531982 batch: 177/224\n",
      "Batch loss: 0.1102856695652008 batch: 178/224\n",
      "Batch loss: 0.13181813061237335 batch: 179/224\n",
      "Batch loss: 0.0999564602971077 batch: 180/224\n",
      "Batch loss: 0.12067607045173645 batch: 181/224\n",
      "Batch loss: 0.12271089851856232 batch: 182/224\n",
      "Batch loss: 0.13493303954601288 batch: 183/224\n",
      "Batch loss: 0.11838357150554657 batch: 184/224\n",
      "Batch loss: 0.16330964863300323 batch: 185/224\n",
      "Batch loss: 0.09578040987253189 batch: 186/224\n",
      "Batch loss: 0.11670097708702087 batch: 187/224\n",
      "Batch loss: 0.1269908994436264 batch: 188/224\n",
      "Batch loss: 0.11349160224199295 batch: 189/224\n",
      "Batch loss: 0.13780561089515686 batch: 190/224\n",
      "Batch loss: 0.09884080290794373 batch: 191/224\n",
      "Batch loss: 0.1307251751422882 batch: 192/224\n",
      "Batch loss: 0.11401797831058502 batch: 193/224\n",
      "Batch loss: 0.11901041865348816 batch: 194/224\n",
      "Batch loss: 0.09583845734596252 batch: 195/224\n",
      "Batch loss: 0.11749935150146484 batch: 196/224\n",
      "Batch loss: 0.11781016737222672 batch: 197/224\n",
      "Batch loss: 0.12224481999874115 batch: 198/224\n",
      "Batch loss: 0.11214075982570648 batch: 199/224\n",
      "Batch loss: 0.14808817207813263 batch: 200/224\n",
      "Batch loss: 0.16435717046260834 batch: 201/224\n",
      "Batch loss: 0.11337673664093018 batch: 202/224\n",
      "Batch loss: 0.0912039503455162 batch: 203/224\n",
      "Batch loss: 0.15876173973083496 batch: 204/224\n",
      "Batch loss: 0.12466602027416229 batch: 205/224\n",
      "Batch loss: 0.13006867468357086 batch: 206/224\n",
      "Batch loss: 0.11616495996713638 batch: 207/224\n",
      "Batch loss: 0.09740382432937622 batch: 208/224\n",
      "Batch loss: 0.10840572416782379 batch: 209/224\n",
      "Batch loss: 0.1378210037946701 batch: 210/224\n",
      "Batch loss: 0.12857232987880707 batch: 211/224\n",
      "Batch loss: 0.11200473457574844 batch: 212/224\n",
      "Batch loss: 0.13452382385730743 batch: 213/224\n",
      "Batch loss: 0.11574498564004898 batch: 214/224\n",
      "Batch loss: 0.12270563840866089 batch: 215/224\n",
      "Batch loss: 0.09308188408613205 batch: 216/224\n",
      "Batch loss: 0.11603030562400818 batch: 217/224\n",
      "Batch loss: 0.12440314143896103 batch: 218/224\n",
      "Batch loss: 0.11419545859098434 batch: 219/224\n",
      "Batch loss: 0.12485307455062866 batch: 220/224\n",
      "Batch loss: 0.11189370602369308 batch: 221/224\n",
      "Batch loss: 0.14250241219997406 batch: 222/224\n",
      "Batch loss: 0.11448995023965836 batch: 223/224\n",
      "Batch loss: 0.11604003608226776 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 39/75..  Training Loss: 0.00024..  Test Loss: 0.00084..  Test Accuracy: 0.89125\n",
      "Running epoch 40/75\n",
      "Batch loss: 0.08797430992126465 batch: 1/224\n",
      "Batch loss: 0.10484467446804047 batch: 2/224\n",
      "Batch loss: 0.0935722067952156 batch: 3/224\n",
      "Batch loss: 0.15636734664440155 batch: 4/224\n",
      "Batch loss: 0.13554444909095764 batch: 5/224\n",
      "Batch loss: 0.10929670929908752 batch: 6/224\n",
      "Batch loss: 0.10999441891908646 batch: 7/224\n",
      "Batch loss: 0.1374744027853012 batch: 8/224\n",
      "Batch loss: 0.1114872619509697 batch: 9/224\n",
      "Batch loss: 0.10802584141492844 batch: 10/224\n",
      "Batch loss: 0.16643866896629333 batch: 11/224\n",
      "Batch loss: 0.0901007130742073 batch: 12/224\n",
      "Batch loss: 0.09856881946325302 batch: 13/224\n",
      "Batch loss: 0.0982813760638237 batch: 14/224\n",
      "Batch loss: 0.1079113632440567 batch: 15/224\n",
      "Batch loss: 0.17154133319854736 batch: 16/224\n",
      "Batch loss: 0.12030596286058426 batch: 17/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.1271444857120514 batch: 18/224\n",
      "Batch loss: 0.11183638870716095 batch: 19/224\n",
      "Batch loss: 0.1028796061873436 batch: 20/224\n",
      "Batch loss: 0.11653079092502594 batch: 21/224\n",
      "Batch loss: 0.09379395842552185 batch: 22/224\n",
      "Batch loss: 0.09851177781820297 batch: 23/224\n",
      "Batch loss: 0.12809109687805176 batch: 24/224\n",
      "Batch loss: 0.10724236816167831 batch: 25/224\n",
      "Batch loss: 0.08052193373441696 batch: 26/224\n",
      "Batch loss: 0.1274353563785553 batch: 27/224\n",
      "Batch loss: 0.1248222067952156 batch: 28/224\n",
      "Batch loss: 0.1295238584280014 batch: 29/224\n",
      "Batch loss: 0.10647096484899521 batch: 30/224\n",
      "Batch loss: 0.12405623495578766 batch: 31/224\n",
      "Batch loss: 0.12385251373052597 batch: 32/224\n",
      "Batch loss: 0.09005772322416306 batch: 33/224\n",
      "Batch loss: 0.100051149725914 batch: 34/224\n",
      "Batch loss: 0.12642912566661835 batch: 35/224\n",
      "Batch loss: 0.13188789784908295 batch: 36/224\n",
      "Batch loss: 0.1006883755326271 batch: 37/224\n",
      "Batch loss: 0.12160728126764297 batch: 38/224\n",
      "Batch loss: 0.1253005415201187 batch: 39/224\n",
      "Batch loss: 0.11695310473442078 batch: 40/224\n",
      "Batch loss: 0.10566498339176178 batch: 41/224\n",
      "Batch loss: 0.12168898433446884 batch: 42/224\n",
      "Batch loss: 0.12670472264289856 batch: 43/224\n",
      "Batch loss: 0.09400518983602524 batch: 44/224\n",
      "Batch loss: 0.10558640211820602 batch: 45/224\n",
      "Batch loss: 0.12695403397083282 batch: 46/224\n",
      "Batch loss: 0.13905808329582214 batch: 47/224\n",
      "Batch loss: 0.09978558868169785 batch: 48/224\n",
      "Batch loss: 0.06953687220811844 batch: 49/224\n",
      "Batch loss: 0.07610709220170975 batch: 50/224\n",
      "Batch loss: 0.09530486166477203 batch: 51/224\n",
      "Batch loss: 0.09390117973089218 batch: 52/224\n",
      "Batch loss: 0.1629146933555603 batch: 53/224\n",
      "Batch loss: 0.08044029772281647 batch: 54/224\n",
      "Batch loss: 0.07227686792612076 batch: 55/224\n",
      "Batch loss: 0.13032376766204834 batch: 56/224\n",
      "Batch loss: 0.14371657371520996 batch: 57/224\n",
      "Batch loss: 0.09369596838951111 batch: 58/224\n",
      "Batch loss: 0.10912870615720749 batch: 59/224\n",
      "Batch loss: 0.14320608973503113 batch: 60/224\n",
      "Batch loss: 0.13308143615722656 batch: 61/224\n",
      "Batch loss: 0.09520410746335983 batch: 62/224\n",
      "Batch loss: 0.16599833965301514 batch: 63/224\n",
      "Batch loss: 0.14481958746910095 batch: 64/224\n",
      "Batch loss: 0.1257304698228836 batch: 65/224\n",
      "Batch loss: 0.15032871067523956 batch: 66/224\n",
      "Batch loss: 0.08135577291250229 batch: 67/224\n",
      "Batch loss: 0.12292739003896713 batch: 68/224\n",
      "Batch loss: 0.1404588371515274 batch: 69/224\n",
      "Batch loss: 0.09524953365325928 batch: 70/224\n",
      "Batch loss: 0.12871360778808594 batch: 71/224\n",
      "Batch loss: 0.09406483173370361 batch: 72/224\n",
      "Batch loss: 0.15518324077129364 batch: 73/224\n",
      "Batch loss: 0.10985969752073288 batch: 74/224\n",
      "Batch loss: 0.1348501741886139 batch: 75/224\n",
      "Batch loss: 0.12089868634939194 batch: 76/224\n",
      "Batch loss: 0.12384092062711716 batch: 77/224\n",
      "Batch loss: 0.12164147943258286 batch: 78/224\n",
      "Batch loss: 0.1462976634502411 batch: 79/224\n",
      "Batch loss: 0.09782096743583679 batch: 80/224\n",
      "Batch loss: 0.14490678906440735 batch: 81/224\n",
      "Batch loss: 0.16020676493644714 batch: 82/224\n",
      "Batch loss: 0.09996704012155533 batch: 83/224\n",
      "Batch loss: 0.08636338263750076 batch: 84/224\n",
      "Batch loss: 0.11416812241077423 batch: 85/224\n",
      "Batch loss: 0.11743979901075363 batch: 86/224\n",
      "Batch loss: 0.13083037734031677 batch: 87/224\n",
      "Batch loss: 0.11080261319875717 batch: 88/224\n",
      "Batch loss: 0.11461751163005829 batch: 89/224\n",
      "Batch loss: 0.14233410358428955 batch: 90/224\n",
      "Batch loss: 0.09377111494541168 batch: 91/224\n",
      "Batch loss: 0.10178565233945847 batch: 92/224\n",
      "Batch loss: 0.11965682357549667 batch: 93/224\n",
      "Batch loss: 0.12383121997117996 batch: 94/224\n",
      "Batch loss: 0.09494801610708237 batch: 95/224\n",
      "Batch loss: 0.08846275508403778 batch: 96/224\n",
      "Batch loss: 0.11748867481946945 batch: 97/224\n",
      "Batch loss: 0.10725664347410202 batch: 98/224\n",
      "Batch loss: 0.12450146675109863 batch: 99/224\n",
      "Batch loss: 0.09993261098861694 batch: 100/224\n",
      "Batch loss: 0.11972400546073914 batch: 101/224\n",
      "Batch loss: 0.13512052595615387 batch: 102/224\n",
      "Batch loss: 0.12503495812416077 batch: 103/224\n",
      "Batch loss: 0.10951642692089081 batch: 104/224\n",
      "Batch loss: 0.09540093690156937 batch: 105/224\n",
      "Batch loss: 0.14058247208595276 batch: 106/224\n",
      "Batch loss: 0.1028769239783287 batch: 107/224\n",
      "Batch loss: 0.0882275328040123 batch: 108/224\n",
      "Batch loss: 0.08524107187986374 batch: 109/224\n",
      "Batch loss: 0.11125369369983673 batch: 110/224\n",
      "Batch loss: 0.11622685194015503 batch: 111/224\n",
      "Batch loss: 0.10088548809289932 batch: 112/224\n",
      "Batch loss: 0.1032586544752121 batch: 113/224\n",
      "Batch loss: 0.12890219688415527 batch: 114/224\n",
      "Batch loss: 0.07526493817567825 batch: 115/224\n",
      "Batch loss: 0.10773441940546036 batch: 116/224\n",
      "Batch loss: 0.08265861868858337 batch: 117/224\n",
      "Batch loss: 0.14018793404102325 batch: 118/224\n",
      "Batch loss: 0.09642019122838974 batch: 119/224\n",
      "Batch loss: 0.11394833773374557 batch: 120/224\n",
      "Batch loss: 0.11380776762962341 batch: 121/224\n",
      "Batch loss: 0.10006023198366165 batch: 122/224\n",
      "Batch loss: 0.10773872584104538 batch: 123/224\n",
      "Batch loss: 0.11621420085430145 batch: 124/224\n",
      "Batch loss: 0.09717977046966553 batch: 125/224\n",
      "Batch loss: 0.1356353908777237 batch: 126/224\n",
      "Batch loss: 0.13942191004753113 batch: 127/224\n",
      "Batch loss: 0.1092091053724289 batch: 128/224\n",
      "Batch loss: 0.06768599897623062 batch: 129/224\n",
      "Batch loss: 0.1450854390859604 batch: 130/224\n",
      "Batch loss: 0.09895756840705872 batch: 131/224\n",
      "Batch loss: 0.08982624858617783 batch: 132/224\n",
      "Batch loss: 0.11718373745679855 batch: 133/224\n",
      "Batch loss: 0.09882903099060059 batch: 134/224\n",
      "Batch loss: 0.10076718777418137 batch: 135/224\n",
      "Batch loss: 0.10472948104143143 batch: 136/224\n",
      "Batch loss: 0.11350792646408081 batch: 137/224\n",
      "Batch loss: 0.09747784584760666 batch: 138/224\n",
      "Batch loss: 0.11069079488515854 batch: 139/224\n",
      "Batch loss: 0.18910208344459534 batch: 140/224\n",
      "Batch loss: 0.09176194667816162 batch: 141/224\n",
      "Batch loss: 0.1113487258553505 batch: 142/224\n",
      "Batch loss: 0.08447697013616562 batch: 143/224\n",
      "Batch loss: 0.1434641033411026 batch: 144/224\n",
      "Batch loss: 0.09682662785053253 batch: 145/224\n",
      "Batch loss: 0.14924658834934235 batch: 146/224\n",
      "Batch loss: 0.0928298830986023 batch: 147/224\n",
      "Batch loss: 0.08307215571403503 batch: 148/224\n",
      "Batch loss: 0.12393178790807724 batch: 149/224\n",
      "Batch loss: 0.14704294502735138 batch: 150/224\n",
      "Batch loss: 0.10320950299501419 batch: 151/224\n",
      "Batch loss: 0.11495159566402435 batch: 152/224\n",
      "Batch loss: 0.1323125809431076 batch: 153/224\n",
      "Batch loss: 0.09747812896966934 batch: 154/224\n",
      "Batch loss: 0.08690145611763 batch: 155/224\n",
      "Batch loss: 0.08299815654754639 batch: 156/224\n",
      "Batch loss: 0.11670542508363724 batch: 157/224\n",
      "Batch loss: 0.1693570911884308 batch: 158/224\n",
      "Batch loss: 0.09594756364822388 batch: 159/224\n",
      "Batch loss: 0.10250265151262283 batch: 160/224\n",
      "Batch loss: 0.09816710650920868 batch: 161/224\n",
      "Batch loss: 0.09709477424621582 batch: 162/224\n",
      "Batch loss: 0.10780573636293411 batch: 163/224\n",
      "Batch loss: 0.0800987184047699 batch: 164/224\n",
      "Batch loss: 0.13573381304740906 batch: 165/224\n",
      "Batch loss: 0.13654889166355133 batch: 166/224\n",
      "Batch loss: 0.09744742512702942 batch: 167/224\n",
      "Batch loss: 0.08230338990688324 batch: 168/224\n",
      "Batch loss: 0.10777412354946136 batch: 169/224\n",
      "Batch loss: 0.10534875094890594 batch: 170/224\n",
      "Batch loss: 0.12364599108695984 batch: 171/224\n",
      "Batch loss: 0.12589053809642792 batch: 172/224\n",
      "Batch loss: 0.13704411685466766 batch: 173/224\n",
      "Batch loss: 0.07771658897399902 batch: 174/224\n",
      "Batch loss: 0.11221196502447128 batch: 175/224\n",
      "Batch loss: 0.08513536304235458 batch: 176/224\n",
      "Batch loss: 0.09834878891706467 batch: 177/224\n",
      "Batch loss: 0.08878439664840698 batch: 178/224\n",
      "Batch loss: 0.14686736464500427 batch: 179/224\n",
      "Batch loss: 0.07513468712568283 batch: 180/224\n",
      "Batch loss: 0.08657445758581161 batch: 181/224\n",
      "Batch loss: 0.15121136605739594 batch: 182/224\n",
      "Batch loss: 0.1376950442790985 batch: 183/224\n",
      "Batch loss: 0.10548630356788635 batch: 184/224\n",
      "Batch loss: 0.13080871105194092 batch: 185/224\n",
      "Batch loss: 0.09323178976774216 batch: 186/224\n",
      "Batch loss: 0.12795913219451904 batch: 187/224\n",
      "Batch loss: 0.09527437388896942 batch: 188/224\n",
      "Batch loss: 0.11703640967607498 batch: 189/224\n",
      "Batch loss: 0.0892978310585022 batch: 190/224\n",
      "Batch loss: 0.11920029670000076 batch: 191/224\n",
      "Batch loss: 0.12396540492773056 batch: 192/224\n",
      "Batch loss: 0.12118922173976898 batch: 193/224\n",
      "Batch loss: 0.11716941744089127 batch: 194/224\n",
      "Batch loss: 0.14015990495681763 batch: 195/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.14104478061199188 batch: 196/224\n",
      "Batch loss: 0.09027601778507233 batch: 197/224\n",
      "Batch loss: 0.1009431779384613 batch: 198/224\n",
      "Batch loss: 0.09966368228197098 batch: 199/224\n",
      "Batch loss: 0.16401764750480652 batch: 200/224\n",
      "Batch loss: 0.11136998236179352 batch: 201/224\n",
      "Batch loss: 0.10710883140563965 batch: 202/224\n",
      "Batch loss: 0.09819961339235306 batch: 203/224\n",
      "Batch loss: 0.09671604633331299 batch: 204/224\n",
      "Batch loss: 0.13034741580486298 batch: 205/224\n",
      "Batch loss: 0.10270338505506516 batch: 206/224\n",
      "Batch loss: 0.11987695842981339 batch: 207/224\n",
      "Batch loss: 0.08160381019115448 batch: 208/224\n",
      "Batch loss: 0.09270615130662918 batch: 209/224\n",
      "Batch loss: 0.11277898401021957 batch: 210/224\n",
      "Batch loss: 0.09665938466787338 batch: 211/224\n",
      "Batch loss: 0.10972043126821518 batch: 212/224\n",
      "Batch loss: 0.10931506752967834 batch: 213/224\n",
      "Batch loss: 0.11498478800058365 batch: 214/224\n",
      "Batch loss: 0.11684124171733856 batch: 215/224\n",
      "Batch loss: 0.10252071917057037 batch: 216/224\n",
      "Batch loss: 0.09730713814496994 batch: 217/224\n",
      "Batch loss: 0.12740685045719147 batch: 218/224\n",
      "Batch loss: 0.05049685016274452 batch: 219/224\n",
      "Batch loss: 0.10520424693822861 batch: 220/224\n",
      "Batch loss: 0.12544022500514984 batch: 221/224\n",
      "Batch loss: 0.1259959489107132 batch: 222/224\n",
      "Batch loss: 0.10630512237548828 batch: 223/224\n",
      "Batch loss: 0.08808068931102753 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 40/75..  Training Loss: 0.00023..  Test Loss: 0.00088..  Test Accuracy: 0.89046\n",
      "Running epoch 41/75\n",
      "Batch loss: 0.08821962028741837 batch: 1/224\n",
      "Batch loss: 0.10959205776453018 batch: 2/224\n",
      "Batch loss: 0.10627651959657669 batch: 3/224\n",
      "Batch loss: 0.12802045047283173 batch: 4/224\n",
      "Batch loss: 0.13827939331531525 batch: 5/224\n",
      "Batch loss: 0.10157708823680878 batch: 6/224\n",
      "Batch loss: 0.08430786430835724 batch: 7/224\n",
      "Batch loss: 0.12178654223680496 batch: 8/224\n",
      "Batch loss: 0.0924162045121193 batch: 9/224\n",
      "Batch loss: 0.1176450178027153 batch: 10/224\n",
      "Batch loss: 0.131244495511055 batch: 11/224\n",
      "Batch loss: 0.09483853727579117 batch: 12/224\n",
      "Batch loss: 0.0752909928560257 batch: 13/224\n",
      "Batch loss: 0.09528325498104095 batch: 14/224\n",
      "Batch loss: 0.10096277296543121 batch: 15/224\n",
      "Batch loss: 0.0972394347190857 batch: 16/224\n",
      "Batch loss: 0.10170124471187592 batch: 17/224\n",
      "Batch loss: 0.12287499010562897 batch: 18/224\n",
      "Batch loss: 0.10404585301876068 batch: 19/224\n",
      "Batch loss: 0.10935791581869125 batch: 20/224\n",
      "Batch loss: 0.12517030537128448 batch: 21/224\n",
      "Batch loss: 0.09682036191225052 batch: 22/224\n",
      "Batch loss: 0.11382417380809784 batch: 23/224\n",
      "Batch loss: 0.13004769384860992 batch: 24/224\n",
      "Batch loss: 0.07564889639616013 batch: 25/224\n",
      "Batch loss: 0.08877662569284439 batch: 26/224\n",
      "Batch loss: 0.10371150076389313 batch: 27/224\n",
      "Batch loss: 0.08458024263381958 batch: 28/224\n",
      "Batch loss: 0.12594638764858246 batch: 29/224\n",
      "Batch loss: 0.11094114929437637 batch: 30/224\n",
      "Batch loss: 0.11325144022703171 batch: 31/224\n",
      "Batch loss: 0.09779402613639832 batch: 32/224\n",
      "Batch loss: 0.07102067768573761 batch: 33/224\n",
      "Batch loss: 0.12109405547380447 batch: 34/224\n",
      "Batch loss: 0.10499197244644165 batch: 35/224\n",
      "Batch loss: 0.10858214646577835 batch: 36/224\n",
      "Batch loss: 0.15455646812915802 batch: 37/224\n",
      "Batch loss: 0.10899139195680618 batch: 38/224\n",
      "Batch loss: 0.1355484277009964 batch: 39/224\n",
      "Batch loss: 0.08838953077793121 batch: 40/224\n",
      "Batch loss: 0.12115783989429474 batch: 41/224\n",
      "Batch loss: 0.14930708706378937 batch: 42/224\n",
      "Batch loss: 0.11289113759994507 batch: 43/224\n",
      "Batch loss: 0.11171869933605194 batch: 44/224\n",
      "Batch loss: 0.08837167918682098 batch: 45/224\n",
      "Batch loss: 0.14437805116176605 batch: 46/224\n",
      "Batch loss: 0.08608726412057877 batch: 47/224\n",
      "Batch loss: 0.1218031793832779 batch: 48/224\n",
      "Batch loss: 0.07713686674833298 batch: 49/224\n",
      "Batch loss: 0.1178605929017067 batch: 50/224\n",
      "Batch loss: 0.11138932406902313 batch: 51/224\n",
      "Batch loss: 0.07769884914159775 batch: 52/224\n",
      "Batch loss: 0.1076165959239006 batch: 53/224\n",
      "Batch loss: 0.0728113055229187 batch: 54/224\n",
      "Batch loss: 0.09117306023836136 batch: 55/224\n",
      "Batch loss: 0.07922349870204926 batch: 56/224\n",
      "Batch loss: 0.16512100398540497 batch: 57/224\n",
      "Batch loss: 0.10238955914974213 batch: 58/224\n",
      "Batch loss: 0.10319574177265167 batch: 59/224\n",
      "Batch loss: 0.15352696180343628 batch: 60/224\n",
      "Batch loss: 0.14365629851818085 batch: 61/224\n",
      "Batch loss: 0.10291478037834167 batch: 62/224\n",
      "Batch loss: 0.10036668181419373 batch: 63/224\n",
      "Batch loss: 0.15804021060466766 batch: 64/224\n",
      "Batch loss: 0.10193375498056412 batch: 65/224\n",
      "Batch loss: 0.14493350684642792 batch: 66/224\n",
      "Batch loss: 0.08671029657125473 batch: 67/224\n",
      "Batch loss: 0.09403949230909348 batch: 68/224\n",
      "Batch loss: 0.13705210387706757 batch: 69/224\n",
      "Batch loss: 0.10022492706775665 batch: 70/224\n",
      "Batch loss: 0.1115352064371109 batch: 71/224\n",
      "Batch loss: 0.08448461443185806 batch: 72/224\n",
      "Batch loss: 0.11787344515323639 batch: 73/224\n",
      "Batch loss: 0.11279083788394928 batch: 74/224\n",
      "Batch loss: 0.15305835008621216 batch: 75/224\n",
      "Batch loss: 0.11965058743953705 batch: 76/224\n",
      "Batch loss: 0.07555867731571198 batch: 77/224\n",
      "Batch loss: 0.11924449354410172 batch: 78/224\n",
      "Batch loss: 0.10102955996990204 batch: 79/224\n",
      "Batch loss: 0.09097621589899063 batch: 80/224\n",
      "Batch loss: 0.11907007545232773 batch: 81/224\n",
      "Batch loss: 0.13343805074691772 batch: 82/224\n",
      "Batch loss: 0.10089405626058578 batch: 83/224\n",
      "Batch loss: 0.08164052665233612 batch: 84/224\n",
      "Batch loss: 0.10483483970165253 batch: 85/224\n",
      "Batch loss: 0.1081133708357811 batch: 86/224\n",
      "Batch loss: 0.11257414519786835 batch: 87/224\n",
      "Batch loss: 0.1161770448088646 batch: 88/224\n",
      "Batch loss: 0.10810374468564987 batch: 89/224\n",
      "Batch loss: 0.15344391763210297 batch: 90/224\n",
      "Batch loss: 0.0760241374373436 batch: 91/224\n",
      "Batch loss: 0.11733226478099823 batch: 92/224\n",
      "Batch loss: 0.08693583309650421 batch: 93/224\n",
      "Batch loss: 0.08290297538042068 batch: 94/224\n",
      "Batch loss: 0.10348071902990341 batch: 95/224\n",
      "Batch loss: 0.12528499960899353 batch: 96/224\n",
      "Batch loss: 0.0913621261715889 batch: 97/224\n",
      "Batch loss: 0.0832970142364502 batch: 98/224\n",
      "Batch loss: 0.11410452425479889 batch: 99/224\n",
      "Batch loss: 0.10299979150295258 batch: 100/224\n",
      "Batch loss: 0.11738469451665878 batch: 101/224\n",
      "Batch loss: 0.11895502358675003 batch: 102/224\n",
      "Batch loss: 0.1282116025686264 batch: 103/224\n",
      "Batch loss: 0.09772905707359314 batch: 104/224\n",
      "Batch loss: 0.0719466432929039 batch: 105/224\n",
      "Batch loss: 0.1091136783361435 batch: 106/224\n",
      "Batch loss: 0.08478330075740814 batch: 107/224\n",
      "Batch loss: 0.09127898514270782 batch: 108/224\n",
      "Batch loss: 0.12487170100212097 batch: 109/224\n",
      "Batch loss: 0.10019323974847794 batch: 110/224\n",
      "Batch loss: 0.12191374599933624 batch: 111/224\n",
      "Batch loss: 0.09722989052534103 batch: 112/224\n",
      "Batch loss: 0.10752212256193161 batch: 113/224\n",
      "Batch loss: 0.07811292260885239 batch: 114/224\n",
      "Batch loss: 0.08464337885379791 batch: 115/224\n",
      "Batch loss: 0.10242436826229095 batch: 116/224\n",
      "Batch loss: 0.07148151844739914 batch: 117/224\n",
      "Batch loss: 0.092836894094944 batch: 118/224\n",
      "Batch loss: 0.12363342195749283 batch: 119/224\n",
      "Batch loss: 0.12299131602048874 batch: 120/224\n",
      "Batch loss: 0.06695162504911423 batch: 121/224\n",
      "Batch loss: 0.09903468936681747 batch: 122/224\n",
      "Batch loss: 0.0913575068116188 batch: 123/224\n",
      "Batch loss: 0.09994654357433319 batch: 124/224\n",
      "Batch loss: 0.09192585200071335 batch: 125/224\n",
      "Batch loss: 0.1059715747833252 batch: 126/224\n",
      "Batch loss: 0.127073734998703 batch: 127/224\n",
      "Batch loss: 0.08358075469732285 batch: 128/224\n",
      "Batch loss: 0.09466549754142761 batch: 129/224\n",
      "Batch loss: 0.11749761551618576 batch: 130/224\n",
      "Batch loss: 0.09058327972888947 batch: 131/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.1049208790063858 batch: 132/224\n",
      "Batch loss: 0.1366218775510788 batch: 133/224\n",
      "Batch loss: 0.10140910744667053 batch: 134/224\n",
      "Batch loss: 0.1307634562253952 batch: 135/224\n",
      "Batch loss: 0.12522977590560913 batch: 136/224\n",
      "Batch loss: 0.1005115658044815 batch: 137/224\n",
      "Batch loss: 0.1184401586651802 batch: 138/224\n",
      "Batch loss: 0.09194374084472656 batch: 139/224\n",
      "Batch loss: 0.13042467832565308 batch: 140/224\n",
      "Batch loss: 0.06896525621414185 batch: 141/224\n",
      "Batch loss: 0.10489173978567123 batch: 142/224\n",
      "Batch loss: 0.0917496383190155 batch: 143/224\n",
      "Batch loss: 0.09081743657588959 batch: 144/224\n",
      "Batch loss: 0.14159706234931946 batch: 145/224\n",
      "Batch loss: 0.162499338388443 batch: 146/224\n",
      "Batch loss: 0.08925843983888626 batch: 147/224\n",
      "Batch loss: 0.09061842411756516 batch: 148/224\n",
      "Batch loss: 0.11610334366559982 batch: 149/224\n",
      "Batch loss: 0.12193123996257782 batch: 150/224\n",
      "Batch loss: 0.10115858167409897 batch: 151/224\n",
      "Batch loss: 0.09857594221830368 batch: 152/224\n",
      "Batch loss: 0.12449135631322861 batch: 153/224\n",
      "Batch loss: 0.10003993660211563 batch: 154/224\n",
      "Batch loss: 0.11717518419027328 batch: 155/224\n",
      "Batch loss: 0.09962280094623566 batch: 156/224\n",
      "Batch loss: 0.10925000160932541 batch: 157/224\n",
      "Batch loss: 0.15955786406993866 batch: 158/224\n",
      "Batch loss: 0.10323182493448257 batch: 159/224\n",
      "Batch loss: 0.0884014219045639 batch: 160/224\n",
      "Batch loss: 0.09058526903390884 batch: 161/224\n",
      "Batch loss: 0.07150716334581375 batch: 162/224\n",
      "Batch loss: 0.08372842520475388 batch: 163/224\n",
      "Batch loss: 0.11950022727251053 batch: 164/224\n",
      "Batch loss: 0.12845444679260254 batch: 165/224\n",
      "Batch loss: 0.1257089525461197 batch: 166/224\n",
      "Batch loss: 0.08490918576717377 batch: 167/224\n",
      "Batch loss: 0.08939027041196823 batch: 168/224\n",
      "Batch loss: 0.08315706998109818 batch: 169/224\n",
      "Batch loss: 0.07757620513439178 batch: 170/224\n",
      "Batch loss: 0.08818818628787994 batch: 171/224\n",
      "Batch loss: 0.12582208216190338 batch: 172/224\n",
      "Batch loss: 0.169960156083107 batch: 173/224\n",
      "Batch loss: 0.08709650486707687 batch: 174/224\n",
      "Batch loss: 0.13514180481433868 batch: 175/224\n",
      "Batch loss: 0.11688731610774994 batch: 176/224\n",
      "Batch loss: 0.11776198446750641 batch: 177/224\n",
      "Batch loss: 0.11920946836471558 batch: 178/224\n",
      "Batch loss: 0.12560418248176575 batch: 179/224\n",
      "Batch loss: 0.05650513619184494 batch: 180/224\n",
      "Batch loss: 0.10741981863975525 batch: 181/224\n",
      "Batch loss: 0.12792959809303284 batch: 182/224\n",
      "Batch loss: 0.1291571706533432 batch: 183/224\n",
      "Batch loss: 0.13285933434963226 batch: 184/224\n",
      "Batch loss: 0.0930275246500969 batch: 185/224\n",
      "Batch loss: 0.0920991599559784 batch: 186/224\n",
      "Batch loss: 0.12961693108081818 batch: 187/224\n",
      "Batch loss: 0.0966554656624794 batch: 188/224\n",
      "Batch loss: 0.09495502710342407 batch: 189/224\n",
      "Batch loss: 0.09354942291975021 batch: 190/224\n",
      "Batch loss: 0.07342583686113358 batch: 191/224\n",
      "Batch loss: 0.1072612851858139 batch: 192/224\n",
      "Batch loss: 0.13661722838878632 batch: 193/224\n",
      "Batch loss: 0.12996737658977509 batch: 194/224\n",
      "Batch loss: 0.11821341514587402 batch: 195/224\n",
      "Batch loss: 0.1282593160867691 batch: 196/224\n",
      "Batch loss: 0.1147545650601387 batch: 197/224\n",
      "Batch loss: 0.09815961867570877 batch: 198/224\n",
      "Batch loss: 0.06288106739521027 batch: 199/224\n",
      "Batch loss: 0.1105276495218277 batch: 200/224\n",
      "Batch loss: 0.10601546615362167 batch: 201/224\n",
      "Batch loss: 0.11229471117258072 batch: 202/224\n",
      "Batch loss: 0.1344100683927536 batch: 203/224\n",
      "Batch loss: 0.10785284638404846 batch: 204/224\n",
      "Batch loss: 0.09329817444086075 batch: 205/224\n",
      "Batch loss: 0.14621596038341522 batch: 206/224\n",
      "Batch loss: 0.14130178093910217 batch: 207/224\n",
      "Batch loss: 0.09098488837480545 batch: 208/224\n",
      "Batch loss: 0.11750391870737076 batch: 209/224\n",
      "Batch loss: 0.11178450286388397 batch: 210/224\n",
      "Batch loss: 0.13042987883090973 batch: 211/224\n",
      "Batch loss: 0.09028065204620361 batch: 212/224\n",
      "Batch loss: 0.10412155091762543 batch: 213/224\n",
      "Batch loss: 0.12380551546812057 batch: 214/224\n",
      "Batch loss: 0.12109648436307907 batch: 215/224\n",
      "Batch loss: 0.10456723719835281 batch: 216/224\n",
      "Batch loss: 0.12143801897764206 batch: 217/224\n",
      "Batch loss: 0.13575643301010132 batch: 218/224\n",
      "Batch loss: 0.08922814577817917 batch: 219/224\n",
      "Batch loss: 0.1016617938876152 batch: 220/224\n",
      "Batch loss: 0.13135558366775513 batch: 221/224\n",
      "Batch loss: 0.12141525745391846 batch: 222/224\n",
      "Batch loss: 0.09201349318027496 batch: 223/224\n",
      "Batch loss: 0.0991646945476532 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 41/75..  Training Loss: 0.00022..  Test Loss: 0.00088..  Test Accuracy: 0.88954\n",
      "Running epoch 42/75\n",
      "Batch loss: 0.09691447764635086 batch: 1/224\n",
      "Batch loss: 0.12160194665193558 batch: 2/224\n",
      "Batch loss: 0.12832677364349365 batch: 3/224\n",
      "Batch loss: 0.12190327793359756 batch: 4/224\n",
      "Batch loss: 0.11897241324186325 batch: 5/224\n",
      "Batch loss: 0.11837396025657654 batch: 6/224\n",
      "Batch loss: 0.1000908836722374 batch: 7/224\n",
      "Batch loss: 0.08488184213638306 batch: 8/224\n",
      "Batch loss: 0.0938902199268341 batch: 9/224\n",
      "Batch loss: 0.10049927979707718 batch: 10/224\n",
      "Batch loss: 0.13070543110370636 batch: 11/224\n",
      "Batch loss: 0.09298510104417801 batch: 12/224\n",
      "Batch loss: 0.08026455342769623 batch: 13/224\n",
      "Batch loss: 0.08208933472633362 batch: 14/224\n",
      "Batch loss: 0.09537797421216965 batch: 15/224\n",
      "Batch loss: 0.10509961098432541 batch: 16/224\n",
      "Batch loss: 0.10580579936504364 batch: 17/224\n",
      "Batch loss: 0.11072542518377304 batch: 18/224\n",
      "Batch loss: 0.09119229018688202 batch: 19/224\n",
      "Batch loss: 0.09826504439115524 batch: 20/224\n",
      "Batch loss: 0.09836283326148987 batch: 21/224\n",
      "Batch loss: 0.10337867587804794 batch: 22/224\n",
      "Batch loss: 0.12600882351398468 batch: 23/224\n",
      "Batch loss: 0.12162298709154129 batch: 24/224\n",
      "Batch loss: 0.09192118793725967 batch: 25/224\n",
      "Batch loss: 0.07606595754623413 batch: 26/224\n",
      "Batch loss: 0.12124364823102951 batch: 27/224\n",
      "Batch loss: 0.10886187851428986 batch: 28/224\n",
      "Batch loss: 0.1461462527513504 batch: 29/224\n",
      "Batch loss: 0.09067265689373016 batch: 30/224\n",
      "Batch loss: 0.10183852910995483 batch: 31/224\n",
      "Batch loss: 0.09384561330080032 batch: 32/224\n",
      "Batch loss: 0.11001241952180862 batch: 33/224\n",
      "Batch loss: 0.13763190805912018 batch: 34/224\n",
      "Batch loss: 0.13023988902568817 batch: 35/224\n",
      "Batch loss: 0.12660828232765198 batch: 36/224\n",
      "Batch loss: 0.09956183284521103 batch: 37/224\n",
      "Batch loss: 0.11697978526353836 batch: 38/224\n",
      "Batch loss: 0.10218138247728348 batch: 39/224\n",
      "Batch loss: 0.10462415218353271 batch: 40/224\n",
      "Batch loss: 0.09901121258735657 batch: 41/224\n",
      "Batch loss: 0.07990812510251999 batch: 42/224\n",
      "Batch loss: 0.09907862544059753 batch: 43/224\n",
      "Batch loss: 0.11511003226041794 batch: 44/224\n",
      "Batch loss: 0.12132578343153 batch: 45/224\n",
      "Batch loss: 0.14909997582435608 batch: 46/224\n",
      "Batch loss: 0.0991622656583786 batch: 47/224\n",
      "Batch loss: 0.10340255498886108 batch: 48/224\n",
      "Batch loss: 0.06960111856460571 batch: 49/224\n",
      "Batch loss: 0.09910932183265686 batch: 50/224\n",
      "Batch loss: 0.10530712455511093 batch: 51/224\n",
      "Batch loss: 0.07305514067411423 batch: 52/224\n",
      "Batch loss: 0.12215382605791092 batch: 53/224\n",
      "Batch loss: 0.09204200655221939 batch: 54/224\n",
      "Batch loss: 0.13561540842056274 batch: 55/224\n",
      "Batch loss: 0.08474352955818176 batch: 56/224\n",
      "Batch loss: 0.11429855972528458 batch: 57/224\n",
      "Batch loss: 0.11926516890525818 batch: 58/224\n",
      "Batch loss: 0.07680986821651459 batch: 59/224\n",
      "Batch loss: 0.11534857004880905 batch: 60/224\n",
      "Batch loss: 0.12662836909294128 batch: 61/224\n",
      "Batch loss: 0.11785823106765747 batch: 62/224\n",
      "Batch loss: 0.13021636009216309 batch: 63/224\n",
      "Batch loss: 0.12121465057134628 batch: 64/224\n",
      "Batch loss: 0.10847630351781845 batch: 65/224\n",
      "Batch loss: 0.10423191636800766 batch: 66/224\n",
      "Batch loss: 0.10325612127780914 batch: 67/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.12258549779653549 batch: 68/224\n",
      "Batch loss: 0.11485261470079422 batch: 69/224\n",
      "Batch loss: 0.14236287772655487 batch: 70/224\n",
      "Batch loss: 0.13090267777442932 batch: 71/224\n",
      "Batch loss: 0.06595923751592636 batch: 72/224\n",
      "Batch loss: 0.1330888718366623 batch: 73/224\n",
      "Batch loss: 0.11702906340360641 batch: 74/224\n",
      "Batch loss: 0.1369597315788269 batch: 75/224\n",
      "Batch loss: 0.1608673334121704 batch: 76/224\n",
      "Batch loss: 0.11420756578445435 batch: 77/224\n",
      "Batch loss: 0.10211233049631119 batch: 78/224\n",
      "Batch loss: 0.14220821857452393 batch: 79/224\n",
      "Batch loss: 0.10328352451324463 batch: 80/224\n",
      "Batch loss: 0.13262443244457245 batch: 81/224\n",
      "Batch loss: 0.116762176156044 batch: 82/224\n",
      "Batch loss: 0.1086118072271347 batch: 83/224\n",
      "Batch loss: 0.08539418876171112 batch: 84/224\n",
      "Batch loss: 0.14707064628601074 batch: 85/224\n",
      "Batch loss: 0.12465611100196838 batch: 86/224\n",
      "Batch loss: 0.09926601499319077 batch: 87/224\n",
      "Batch loss: 0.0954020693898201 batch: 88/224\n",
      "Batch loss: 0.09418456256389618 batch: 89/224\n",
      "Batch loss: 0.11743609607219696 batch: 90/224\n",
      "Batch loss: 0.08512921631336212 batch: 91/224\n",
      "Batch loss: 0.09810435026884079 batch: 92/224\n",
      "Batch loss: 0.07869164645671844 batch: 93/224\n",
      "Batch loss: 0.13340899348258972 batch: 94/224\n",
      "Batch loss: 0.10175817459821701 batch: 95/224\n",
      "Batch loss: 0.08752976357936859 batch: 96/224\n",
      "Batch loss: 0.0896361693739891 batch: 97/224\n",
      "Batch loss: 0.07267555594444275 batch: 98/224\n",
      "Batch loss: 0.14070934057235718 batch: 99/224\n",
      "Batch loss: 0.10554156452417374 batch: 100/224\n",
      "Batch loss: 0.09659817069768906 batch: 101/224\n",
      "Batch loss: 0.11288898438215256 batch: 102/224\n",
      "Batch loss: 0.10703542828559875 batch: 103/224\n",
      "Batch loss: 0.09359315037727356 batch: 104/224\n",
      "Batch loss: 0.09296223521232605 batch: 105/224\n",
      "Batch loss: 0.0867423489689827 batch: 106/224\n",
      "Batch loss: 0.11678694188594818 batch: 107/224\n",
      "Batch loss: 0.10806383192539215 batch: 108/224\n",
      "Batch loss: 0.09879916161298752 batch: 109/224\n",
      "Batch loss: 0.11756228655576706 batch: 110/224\n",
      "Batch loss: 0.11476098001003265 batch: 111/224\n",
      "Batch loss: 0.0879942998290062 batch: 112/224\n",
      "Batch loss: 0.10222279280424118 batch: 113/224\n",
      "Batch loss: 0.08637996762990952 batch: 114/224\n",
      "Batch loss: 0.10715879499912262 batch: 115/224\n",
      "Batch loss: 0.09554637223482132 batch: 116/224\n",
      "Batch loss: 0.07298161089420319 batch: 117/224\n",
      "Batch loss: 0.09662078320980072 batch: 118/224\n",
      "Batch loss: 0.06687024980783463 batch: 119/224\n",
      "Batch loss: 0.17620949447155 batch: 120/224\n",
      "Batch loss: 0.08297790586948395 batch: 121/224\n",
      "Batch loss: 0.10390605032444 batch: 122/224\n",
      "Batch loss: 0.07280348241329193 batch: 123/224\n",
      "Batch loss: 0.09093467891216278 batch: 124/224\n",
      "Batch loss: 0.09822280704975128 batch: 125/224\n",
      "Batch loss: 0.11032920330762863 batch: 126/224\n",
      "Batch loss: 0.12295781075954437 batch: 127/224\n",
      "Batch loss: 0.08335017412900925 batch: 128/224\n",
      "Batch loss: 0.09610989689826965 batch: 129/224\n",
      "Batch loss: 0.10997526347637177 batch: 130/224\n",
      "Batch loss: 0.10401187092065811 batch: 131/224\n",
      "Batch loss: 0.1310918629169464 batch: 132/224\n",
      "Batch loss: 0.12931779026985168 batch: 133/224\n",
      "Batch loss: 0.0909443274140358 batch: 134/224\n",
      "Batch loss: 0.09189127385616302 batch: 135/224\n",
      "Batch loss: 0.13660022616386414 batch: 136/224\n",
      "Batch loss: 0.08656974881887436 batch: 137/224\n",
      "Batch loss: 0.09123549610376358 batch: 138/224\n",
      "Batch loss: 0.16409678757190704 batch: 139/224\n",
      "Batch loss: 0.1697230339050293 batch: 140/224\n",
      "Batch loss: 0.062486615031957626 batch: 141/224\n",
      "Batch loss: 0.10883767902851105 batch: 142/224\n",
      "Batch loss: 0.09316281974315643 batch: 143/224\n",
      "Batch loss: 0.08411535620689392 batch: 144/224\n",
      "Batch loss: 0.10692716389894485 batch: 145/224\n",
      "Batch loss: 0.10800717025995255 batch: 146/224\n",
      "Batch loss: 0.09149525314569473 batch: 147/224\n",
      "Batch loss: 0.07831965386867523 batch: 148/224\n",
      "Batch loss: 0.13374000787734985 batch: 149/224\n",
      "Batch loss: 0.11832515895366669 batch: 150/224\n",
      "Batch loss: 0.1015634760260582 batch: 151/224\n",
      "Batch loss: 0.10730423033237457 batch: 152/224\n",
      "Batch loss: 0.11816681921482086 batch: 153/224\n",
      "Batch loss: 0.13061171770095825 batch: 154/224\n",
      "Batch loss: 0.10556122660636902 batch: 155/224\n",
      "Batch loss: 0.10206902027130127 batch: 156/224\n",
      "Batch loss: 0.08912072330713272 batch: 157/224\n",
      "Batch loss: 0.13369710743427277 batch: 158/224\n",
      "Batch loss: 0.08512431383132935 batch: 159/224\n",
      "Batch loss: 0.111936055123806 batch: 160/224\n",
      "Batch loss: 0.08960782736539841 batch: 161/224\n",
      "Batch loss: 0.0756397545337677 batch: 162/224\n",
      "Batch loss: 0.09764803200960159 batch: 163/224\n",
      "Batch loss: 0.11123847216367722 batch: 164/224\n",
      "Batch loss: 0.1340421587228775 batch: 165/224\n",
      "Batch loss: 0.10418370366096497 batch: 166/224\n",
      "Batch loss: 0.08205737173557281 batch: 167/224\n",
      "Batch loss: 0.11318802833557129 batch: 168/224\n",
      "Batch loss: 0.10003193467855453 batch: 169/224\n",
      "Batch loss: 0.08419303596019745 batch: 170/224\n",
      "Batch loss: 0.07894612103700638 batch: 171/224\n",
      "Batch loss: 0.09333255141973495 batch: 172/224\n",
      "Batch loss: 0.1120334267616272 batch: 173/224\n",
      "Batch loss: 0.09236083924770355 batch: 174/224\n",
      "Batch loss: 0.12380874156951904 batch: 175/224\n",
      "Batch loss: 0.08569121360778809 batch: 176/224\n",
      "Batch loss: 0.13067248463630676 batch: 177/224\n",
      "Batch loss: 0.0858352929353714 batch: 178/224\n",
      "Batch loss: 0.11480123549699783 batch: 179/224\n",
      "Batch loss: 0.08490882068872452 batch: 180/224\n",
      "Batch loss: 0.09730871766805649 batch: 181/224\n",
      "Batch loss: 0.1278783231973648 batch: 182/224\n",
      "Batch loss: 0.12267133593559265 batch: 183/224\n",
      "Batch loss: 0.09604150056838989 batch: 184/224\n",
      "Batch loss: 0.11122452467679977 batch: 185/224\n",
      "Batch loss: 0.08443085849285126 batch: 186/224\n",
      "Batch loss: 0.08355022966861725 batch: 187/224\n",
      "Batch loss: 0.10314078629016876 batch: 188/224\n",
      "Batch loss: 0.14243686199188232 batch: 189/224\n",
      "Batch loss: 0.0742202177643776 batch: 190/224\n",
      "Batch loss: 0.1026008129119873 batch: 191/224\n",
      "Batch loss: 0.13622578978538513 batch: 192/224\n",
      "Batch loss: 0.14253206551074982 batch: 193/224\n",
      "Batch loss: 0.09958373755216599 batch: 194/224\n",
      "Batch loss: 0.11979477852582932 batch: 195/224\n",
      "Batch loss: 0.08702189475297928 batch: 196/224\n",
      "Batch loss: 0.0897444412112236 batch: 197/224\n",
      "Batch loss: 0.08754147589206696 batch: 198/224\n",
      "Batch loss: 0.0801975429058075 batch: 199/224\n",
      "Batch loss: 0.1171688660979271 batch: 200/224\n",
      "Batch loss: 0.10516853630542755 batch: 201/224\n",
      "Batch loss: 0.12918977439403534 batch: 202/224\n",
      "Batch loss: 0.08649507164955139 batch: 203/224\n",
      "Batch loss: 0.08532752841711044 batch: 204/224\n",
      "Batch loss: 0.14885841310024261 batch: 205/224\n",
      "Batch loss: 0.11840757727622986 batch: 206/224\n",
      "Batch loss: 0.07286382466554642 batch: 207/224\n",
      "Batch loss: 0.11202787607908249 batch: 208/224\n",
      "Batch loss: 0.10041096806526184 batch: 209/224\n",
      "Batch loss: 0.10223080962896347 batch: 210/224\n",
      "Batch loss: 0.12143062055110931 batch: 211/224\n",
      "Batch loss: 0.12434665113687515 batch: 212/224\n",
      "Batch loss: 0.10914742946624756 batch: 213/224\n",
      "Batch loss: 0.11369479447603226 batch: 214/224\n",
      "Batch loss: 0.10896614193916321 batch: 215/224\n",
      "Batch loss: 0.12252815067768097 batch: 216/224\n",
      "Batch loss: 0.10655763745307922 batch: 217/224\n",
      "Batch loss: 0.09204335510730743 batch: 218/224\n",
      "Batch loss: 0.10071854293346405 batch: 219/224\n",
      "Batch loss: 0.13020174205303192 batch: 220/224\n",
      "Batch loss: 0.1167483851313591 batch: 221/224\n",
      "Batch loss: 0.11719538271427155 batch: 222/224\n",
      "Batch loss: 0.08264975249767303 batch: 223/224\n",
      "Batch loss: 0.09360793232917786 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 42/75..  Training Loss: 0.00021..  Test Loss: 0.00087..  Test Accuracy: 0.89125\n",
      "Running epoch 43/75\n",
      "Batch loss: 0.09954418241977692 batch: 1/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.13415873050689697 batch: 2/224\n",
      "Batch loss: 0.08254092931747437 batch: 3/224\n",
      "Batch loss: 0.12270710617303848 batch: 4/224\n",
      "Batch loss: 0.11325494199991226 batch: 5/224\n",
      "Batch loss: 0.10158766061067581 batch: 6/224\n",
      "Batch loss: 0.12680447101593018 batch: 7/224\n",
      "Batch loss: 0.1173328086733818 batch: 8/224\n",
      "Batch loss: 0.08968044817447662 batch: 9/224\n",
      "Batch loss: 0.11935541778802872 batch: 10/224\n",
      "Batch loss: 0.11687960475683212 batch: 11/224\n",
      "Batch loss: 0.10531274974346161 batch: 12/224\n",
      "Batch loss: 0.08436180651187897 batch: 13/224\n",
      "Batch loss: 0.09790963679552078 batch: 14/224\n",
      "Batch loss: 0.08422563970088959 batch: 15/224\n",
      "Batch loss: 0.12871725857257843 batch: 16/224\n",
      "Batch loss: 0.1001637876033783 batch: 17/224\n",
      "Batch loss: 0.14351649582386017 batch: 18/224\n",
      "Batch loss: 0.09510763734579086 batch: 19/224\n",
      "Batch loss: 0.07138091325759888 batch: 20/224\n",
      "Batch loss: 0.09040495753288269 batch: 21/224\n",
      "Batch loss: 0.0855121985077858 batch: 22/224\n",
      "Batch loss: 0.11074499785900116 batch: 23/224\n",
      "Batch loss: 0.11590788513422012 batch: 24/224\n",
      "Batch loss: 0.08502690494060516 batch: 25/224\n",
      "Batch loss: 0.11538488417863846 batch: 26/224\n",
      "Batch loss: 0.08701037615537643 batch: 27/224\n",
      "Batch loss: 0.12322486937046051 batch: 28/224\n",
      "Batch loss: 0.09497744590044022 batch: 29/224\n",
      "Batch loss: 0.10628596693277359 batch: 30/224\n",
      "Batch loss: 0.13017092645168304 batch: 31/224\n",
      "Batch loss: 0.11089368164539337 batch: 32/224\n",
      "Batch loss: 0.08364886790513992 batch: 33/224\n",
      "Batch loss: 0.09640590101480484 batch: 34/224\n",
      "Batch loss: 0.10369477421045303 batch: 35/224\n",
      "Batch loss: 0.10038483887910843 batch: 36/224\n",
      "Batch loss: 0.08925119042396545 batch: 37/224\n",
      "Batch loss: 0.1345635950565338 batch: 38/224\n",
      "Batch loss: 0.13768187165260315 batch: 39/224\n",
      "Batch loss: 0.09443922340869904 batch: 40/224\n",
      "Batch loss: 0.11246172338724136 batch: 41/224\n",
      "Batch loss: 0.07298284769058228 batch: 42/224\n",
      "Batch loss: 0.10602112114429474 batch: 43/224\n",
      "Batch loss: 0.06956975907087326 batch: 44/224\n",
      "Batch loss: 0.09514977782964706 batch: 45/224\n",
      "Batch loss: 0.11390913277864456 batch: 46/224\n",
      "Batch loss: 0.10838252305984497 batch: 47/224\n",
      "Batch loss: 0.10684007406234741 batch: 48/224\n",
      "Batch loss: 0.11226624995470047 batch: 49/224\n",
      "Batch loss: 0.07705852389335632 batch: 50/224\n",
      "Batch loss: 0.11185720562934875 batch: 51/224\n",
      "Batch loss: 0.07668890058994293 batch: 52/224\n",
      "Batch loss: 0.08667803555727005 batch: 53/224\n",
      "Batch loss: 0.09331058710813522 batch: 54/224\n",
      "Batch loss: 0.0744251161813736 batch: 55/224\n",
      "Batch loss: 0.1064743846654892 batch: 56/224\n",
      "Batch loss: 0.14744055271148682 batch: 57/224\n",
      "Batch loss: 0.13511788845062256 batch: 58/224\n",
      "Batch loss: 0.09359759837388992 batch: 59/224\n",
      "Batch loss: 0.14481432735919952 batch: 60/224\n",
      "Batch loss: 0.08079468458890915 batch: 61/224\n",
      "Batch loss: 0.07422738522291183 batch: 62/224\n",
      "Batch loss: 0.12308962643146515 batch: 63/224\n",
      "Batch loss: 0.12420628219842911 batch: 64/224\n",
      "Batch loss: 0.10169407725334167 batch: 65/224\n",
      "Batch loss: 0.1209719181060791 batch: 66/224\n",
      "Batch loss: 0.11711878329515457 batch: 67/224\n",
      "Batch loss: 0.08899330347776413 batch: 68/224\n",
      "Batch loss: 0.09654261916875839 batch: 69/224\n",
      "Batch loss: 0.11661989986896515 batch: 70/224\n",
      "Batch loss: 0.0979197546839714 batch: 71/224\n",
      "Batch loss: 0.0646839588880539 batch: 72/224\n",
      "Batch loss: 0.1282350867986679 batch: 73/224\n",
      "Batch loss: 0.08457683026790619 batch: 74/224\n",
      "Batch loss: 0.09546586871147156 batch: 75/224\n",
      "Batch loss: 0.09457762539386749 batch: 76/224\n",
      "Batch loss: 0.08935897052288055 batch: 77/224\n",
      "Batch loss: 0.11551370471715927 batch: 78/224\n",
      "Batch loss: 0.12248040735721588 batch: 79/224\n",
      "Batch loss: 0.12078633159399033 batch: 80/224\n",
      "Batch loss: 0.12346295267343521 batch: 81/224\n",
      "Batch loss: 0.11599041521549225 batch: 82/224\n",
      "Batch loss: 0.09387818723917007 batch: 83/224\n",
      "Batch loss: 0.06860429793596268 batch: 84/224\n",
      "Batch loss: 0.13007643818855286 batch: 85/224\n",
      "Batch loss: 0.09746002405881882 batch: 86/224\n",
      "Batch loss: 0.08812687546014786 batch: 87/224\n",
      "Batch loss: 0.09796346724033356 batch: 88/224\n",
      "Batch loss: 0.0780518501996994 batch: 89/224\n",
      "Batch loss: 0.09720775485038757 batch: 90/224\n",
      "Batch loss: 0.10715401917695999 batch: 91/224\n",
      "Batch loss: 0.09220468997955322 batch: 92/224\n",
      "Batch loss: 0.08053404837846756 batch: 93/224\n",
      "Batch loss: 0.12306879460811615 batch: 94/224\n",
      "Batch loss: 0.0666968896985054 batch: 95/224\n",
      "Batch loss: 0.10132656991481781 batch: 96/224\n",
      "Batch loss: 0.0795523151755333 batch: 97/224\n",
      "Batch loss: 0.1027216836810112 batch: 98/224\n",
      "Batch loss: 0.0857769325375557 batch: 99/224\n",
      "Batch loss: 0.07858304679393768 batch: 100/224\n",
      "Batch loss: 0.13060519099235535 batch: 101/224\n",
      "Batch loss: 0.08452088385820389 batch: 102/224\n",
      "Batch loss: 0.10145571827888489 batch: 103/224\n",
      "Batch loss: 0.08216813206672668 batch: 104/224\n",
      "Batch loss: 0.09232474118471146 batch: 105/224\n",
      "Batch loss: 0.09737646579742432 batch: 106/224\n",
      "Batch loss: 0.08250006288290024 batch: 107/224\n",
      "Batch loss: 0.09871403872966766 batch: 108/224\n",
      "Batch loss: 0.08672871440649033 batch: 109/224\n",
      "Batch loss: 0.10092793405056 batch: 110/224\n",
      "Batch loss: 0.07464460283517838 batch: 111/224\n",
      "Batch loss: 0.08884148299694061 batch: 112/224\n",
      "Batch loss: 0.09855286777019501 batch: 113/224\n",
      "Batch loss: 0.07334238290786743 batch: 114/224\n",
      "Batch loss: 0.06415626406669617 batch: 115/224\n",
      "Batch loss: 0.09192649275064468 batch: 116/224\n",
      "Batch loss: 0.048460785299539566 batch: 117/224\n",
      "Batch loss: 0.06960967183113098 batch: 118/224\n",
      "Batch loss: 0.07508282363414764 batch: 119/224\n",
      "Batch loss: 0.0903485044836998 batch: 120/224\n",
      "Batch loss: 0.07488283514976501 batch: 121/224\n",
      "Batch loss: 0.10120446979999542 batch: 122/224\n",
      "Batch loss: 0.08161171525716782 batch: 123/224\n",
      "Batch loss: 0.10170453786849976 batch: 124/224\n",
      "Batch loss: 0.14202405512332916 batch: 125/224\n",
      "Batch loss: 0.11149279773235321 batch: 126/224\n",
      "Batch loss: 0.12745678424835205 batch: 127/224\n",
      "Batch loss: 0.07436120510101318 batch: 128/224\n",
      "Batch loss: 0.12002067267894745 batch: 129/224\n",
      "Batch loss: 0.10584887117147446 batch: 130/224\n",
      "Batch loss: 0.09750977903604507 batch: 131/224\n",
      "Batch loss: 0.08240924775600433 batch: 132/224\n",
      "Batch loss: 0.11745049804449081 batch: 133/224\n",
      "Batch loss: 0.12243713438510895 batch: 134/224\n",
      "Batch loss: 0.10131479054689407 batch: 135/224\n",
      "Batch loss: 0.11317743360996246 batch: 136/224\n",
      "Batch loss: 0.09872887283563614 batch: 137/224\n",
      "Batch loss: 0.09579925239086151 batch: 138/224\n",
      "Batch loss: 0.11508335173130035 batch: 139/224\n",
      "Batch loss: 0.09155380725860596 batch: 140/224\n",
      "Batch loss: 0.10357256233692169 batch: 141/224\n",
      "Batch loss: 0.06876561045646667 batch: 142/224\n",
      "Batch loss: 0.08265148103237152 batch: 143/224\n",
      "Batch loss: 0.08991312980651855 batch: 144/224\n",
      "Batch loss: 0.10554435849189758 batch: 145/224\n",
      "Batch loss: 0.11741357296705246 batch: 146/224\n",
      "Batch loss: 0.07719414681196213 batch: 147/224\n",
      "Batch loss: 0.09840205311775208 batch: 148/224\n",
      "Batch loss: 0.09929212927818298 batch: 149/224\n",
      "Batch loss: 0.13384273648262024 batch: 150/224\n",
      "Batch loss: 0.12067820876836777 batch: 151/224\n",
      "Batch loss: 0.08193773776292801 batch: 152/224\n",
      "Batch loss: 0.14370715618133545 batch: 153/224\n",
      "Batch loss: 0.11899057775735855 batch: 154/224\n",
      "Batch loss: 0.10082712024450302 batch: 155/224\n",
      "Batch loss: 0.07987020164728165 batch: 156/224\n",
      "Batch loss: 0.09928611665964127 batch: 157/224\n",
      "Batch loss: 0.1546865850687027 batch: 158/224\n",
      "Batch loss: 0.09346851706504822 batch: 159/224\n",
      "Batch loss: 0.09365076571702957 batch: 160/224\n",
      "Batch loss: 0.09903525561094284 batch: 161/224\n",
      "Batch loss: 0.10943576693534851 batch: 162/224\n",
      "Batch loss: 0.08745858818292618 batch: 163/224\n",
      "Batch loss: 0.07507064938545227 batch: 164/224\n",
      "Batch loss: 0.20794372260570526 batch: 165/224\n",
      "Batch loss: 0.101341113448143 batch: 166/224\n",
      "Batch loss: 0.06434477120637894 batch: 167/224\n",
      "Batch loss: 0.07589700818061829 batch: 168/224\n",
      "Batch loss: 0.09564121067523956 batch: 169/224\n",
      "Batch loss: 0.10656250268220901 batch: 170/224\n",
      "Batch loss: 0.051760490983724594 batch: 171/224\n",
      "Batch loss: 0.07726284116506577 batch: 172/224\n",
      "Batch loss: 0.10768037289381027 batch: 173/224\n",
      "Batch loss: 0.06982108950614929 batch: 174/224\n",
      "Batch loss: 0.10745523124933243 batch: 175/224\n",
      "Batch loss: 0.11509630084037781 batch: 176/224\n",
      "Batch loss: 0.10958131402730942 batch: 177/224\n",
      "Batch loss: 0.06854909658432007 batch: 178/224\n",
      "Batch loss: 0.10297860205173492 batch: 179/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.07208722829818726 batch: 180/224\n",
      "Batch loss: 0.08784352242946625 batch: 181/224\n",
      "Batch loss: 0.12370288372039795 batch: 182/224\n",
      "Batch loss: 0.1277783364057541 batch: 183/224\n",
      "Batch loss: 0.08585173636674881 batch: 184/224\n",
      "Batch loss: 0.11338753998279572 batch: 185/224\n",
      "Batch loss: 0.10031991451978683 batch: 186/224\n",
      "Batch loss: 0.11720728129148483 batch: 187/224\n",
      "Batch loss: 0.07225075364112854 batch: 188/224\n",
      "Batch loss: 0.11012253165245056 batch: 189/224\n",
      "Batch loss: 0.12565618753433228 batch: 190/224\n",
      "Batch loss: 0.13329549133777618 batch: 191/224\n",
      "Batch loss: 0.10342344641685486 batch: 192/224\n",
      "Batch loss: 0.10350056737661362 batch: 193/224\n",
      "Batch loss: 0.0905584767460823 batch: 194/224\n",
      "Batch loss: 0.14139890670776367 batch: 195/224\n",
      "Batch loss: 0.12973734736442566 batch: 196/224\n",
      "Batch loss: 0.11031482368707657 batch: 197/224\n",
      "Batch loss: 0.0868963897228241 batch: 198/224\n",
      "Batch loss: 0.06475414335727692 batch: 199/224\n",
      "Batch loss: 0.09961516410112381 batch: 200/224\n",
      "Batch loss: 0.104377381503582 batch: 201/224\n",
      "Batch loss: 0.10348104685544968 batch: 202/224\n",
      "Batch loss: 0.08917830884456635 batch: 203/224\n",
      "Batch loss: 0.10622304677963257 batch: 204/224\n",
      "Batch loss: 0.11107796430587769 batch: 205/224\n",
      "Batch loss: 0.10505583882331848 batch: 206/224\n",
      "Batch loss: 0.1302870362997055 batch: 207/224\n",
      "Batch loss: 0.07137105613946915 batch: 208/224\n",
      "Batch loss: 0.08582127094268799 batch: 209/224\n",
      "Batch loss: 0.11657598614692688 batch: 210/224\n",
      "Batch loss: 0.12356022745370865 batch: 211/224\n",
      "Batch loss: 0.08954189717769623 batch: 212/224\n",
      "Batch loss: 0.1504247784614563 batch: 213/224\n",
      "Batch loss: 0.12021949887275696 batch: 214/224\n",
      "Batch loss: 0.1072700247168541 batch: 215/224\n",
      "Batch loss: 0.09967917203903198 batch: 216/224\n",
      "Batch loss: 0.10932984203100204 batch: 217/224\n",
      "Batch loss: 0.08306051045656204 batch: 218/224\n",
      "Batch loss: 0.06852351874113083 batch: 219/224\n",
      "Batch loss: 0.09723931550979614 batch: 220/224\n",
      "Batch loss: 0.16112548112869263 batch: 221/224\n",
      "Batch loss: 0.12444206327199936 batch: 222/224\n",
      "Batch loss: 0.07520858198404312 batch: 223/224\n",
      "Batch loss: 0.09602713584899902 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 43/75..  Training Loss: 0.00020..  Test Loss: 0.00090..  Test Accuracy: 0.89111\n",
      "Running epoch 44/75\n",
      "Batch loss: 0.08706557750701904 batch: 1/224\n",
      "Batch loss: 0.09704490005970001 batch: 2/224\n",
      "Batch loss: 0.10528992116451263 batch: 3/224\n",
      "Batch loss: 0.10549566894769669 batch: 4/224\n",
      "Batch loss: 0.09478849917650223 batch: 5/224\n",
      "Batch loss: 0.08082995563745499 batch: 6/224\n",
      "Batch loss: 0.06478136032819748 batch: 7/224\n",
      "Batch loss: 0.09220022708177567 batch: 8/224\n",
      "Batch loss: 0.13809503614902496 batch: 9/224\n",
      "Batch loss: 0.12642011046409607 batch: 10/224\n",
      "Batch loss: 0.11085797846317291 batch: 11/224\n",
      "Batch loss: 0.11598547548055649 batch: 12/224\n",
      "Batch loss: 0.07017023116350174 batch: 13/224\n",
      "Batch loss: 0.07841785252094269 batch: 14/224\n",
      "Batch loss: 0.10418707877397537 batch: 15/224\n",
      "Batch loss: 0.12356870621442795 batch: 16/224\n",
      "Batch loss: 0.09234137088060379 batch: 17/224\n",
      "Batch loss: 0.1047229915857315 batch: 18/224\n",
      "Batch loss: 0.07292237132787704 batch: 19/224\n",
      "Batch loss: 0.07218608260154724 batch: 20/224\n",
      "Batch loss: 0.09131938219070435 batch: 21/224\n",
      "Batch loss: 0.09826546907424927 batch: 22/224\n",
      "Batch loss: 0.0967998206615448 batch: 23/224\n",
      "Batch loss: 0.13158376514911652 batch: 24/224\n",
      "Batch loss: 0.0759277492761612 batch: 25/224\n",
      "Batch loss: 0.07440411299467087 batch: 26/224\n",
      "Batch loss: 0.08281327784061432 batch: 27/224\n",
      "Batch loss: 0.09064468741416931 batch: 28/224\n",
      "Batch loss: 0.08942157030105591 batch: 29/224\n",
      "Batch loss: 0.07543924450874329 batch: 30/224\n",
      "Batch loss: 0.06813149899244308 batch: 31/224\n",
      "Batch loss: 0.15217478573322296 batch: 32/224\n",
      "Batch loss: 0.08072847872972488 batch: 33/224\n",
      "Batch loss: 0.09478958696126938 batch: 34/224\n",
      "Batch loss: 0.11584408581256866 batch: 35/224\n",
      "Batch loss: 0.0921308696269989 batch: 36/224\n",
      "Batch loss: 0.11277171969413757 batch: 37/224\n",
      "Batch loss: 0.0815017893910408 batch: 38/224\n",
      "Batch loss: 0.09654081612825394 batch: 39/224\n",
      "Batch loss: 0.11175061017274857 batch: 40/224\n",
      "Batch loss: 0.1388377696275711 batch: 41/224\n",
      "Batch loss: 0.09430496394634247 batch: 42/224\n",
      "Batch loss: 0.13115337491035461 batch: 43/224\n",
      "Batch loss: 0.05992724001407623 batch: 44/224\n",
      "Batch loss: 0.09481928497552872 batch: 45/224\n",
      "Batch loss: 0.10497230291366577 batch: 46/224\n",
      "Batch loss: 0.10082055628299713 batch: 47/224\n",
      "Batch loss: 0.0790594294667244 batch: 48/224\n",
      "Batch loss: 0.07787546515464783 batch: 49/224\n",
      "Batch loss: 0.08004909753799438 batch: 50/224\n",
      "Batch loss: 0.10632771253585815 batch: 51/224\n",
      "Batch loss: 0.06733857095241547 batch: 52/224\n",
      "Batch loss: 0.10735470801591873 batch: 53/224\n",
      "Batch loss: 0.11509077250957489 batch: 54/224\n",
      "Batch loss: 0.06717304140329361 batch: 55/224\n",
      "Batch loss: 0.069082111120224 batch: 56/224\n",
      "Batch loss: 0.115765780210495 batch: 57/224\n",
      "Batch loss: 0.0780172124505043 batch: 58/224\n",
      "Batch loss: 0.08687125891447067 batch: 59/224\n",
      "Batch loss: 0.13738781213760376 batch: 60/224\n",
      "Batch loss: 0.09893064200878143 batch: 61/224\n",
      "Batch loss: 0.08583042025566101 batch: 62/224\n",
      "Batch loss: 0.12815217673778534 batch: 63/224\n",
      "Batch loss: 0.10631083697080612 batch: 64/224\n",
      "Batch loss: 0.1136123389005661 batch: 65/224\n",
      "Batch loss: 0.10373355448246002 batch: 66/224\n",
      "Batch loss: 0.09511832147836685 batch: 67/224\n",
      "Batch loss: 0.09108485281467438 batch: 68/224\n",
      "Batch loss: 0.11820966005325317 batch: 69/224\n",
      "Batch loss: 0.08330148458480835 batch: 70/224\n",
      "Batch loss: 0.09018789976835251 batch: 71/224\n",
      "Batch loss: 0.06592998653650284 batch: 72/224\n",
      "Batch loss: 0.10209435224533081 batch: 73/224\n",
      "Batch loss: 0.08801641315221786 batch: 74/224\n",
      "Batch loss: 0.09393452852964401 batch: 75/224\n",
      "Batch loss: 0.13250228762626648 batch: 76/224\n",
      "Batch loss: 0.09238709509372711 batch: 77/224\n",
      "Batch loss: 0.08732067048549652 batch: 78/224\n",
      "Batch loss: 0.10886122286319733 batch: 79/224\n",
      "Batch loss: 0.11542387306690216 batch: 80/224\n",
      "Batch loss: 0.12357787042856216 batch: 81/224\n",
      "Batch loss: 0.1234821155667305 batch: 82/224\n",
      "Batch loss: 0.09998998790979385 batch: 83/224\n",
      "Batch loss: 0.05859487131237984 batch: 84/224\n",
      "Batch loss: 0.11072224378585815 batch: 85/224\n",
      "Batch loss: 0.08951379358768463 batch: 86/224\n",
      "Batch loss: 0.08662191778421402 batch: 87/224\n",
      "Batch loss: 0.10348065942525864 batch: 88/224\n",
      "Batch loss: 0.10597292333841324 batch: 89/224\n",
      "Batch loss: 0.08858168125152588 batch: 90/224\n",
      "Batch loss: 0.0983402281999588 batch: 91/224\n",
      "Batch loss: 0.0788789913058281 batch: 92/224\n",
      "Batch loss: 0.08277666568756104 batch: 93/224\n",
      "Batch loss: 0.10830787569284439 batch: 94/224\n",
      "Batch loss: 0.08298143744468689 batch: 95/224\n",
      "Batch loss: 0.09773433208465576 batch: 96/224\n",
      "Batch loss: 0.09010663628578186 batch: 97/224\n",
      "Batch loss: 0.07877005636692047 batch: 98/224\n",
      "Batch loss: 0.13353241980075836 batch: 99/224\n",
      "Batch loss: 0.08213473111391068 batch: 100/224\n",
      "Batch loss: 0.1327086091041565 batch: 101/224\n",
      "Batch loss: 0.07553025335073471 batch: 102/224\n",
      "Batch loss: 0.1287143975496292 batch: 103/224\n",
      "Batch loss: 0.08528583496809006 batch: 104/224\n",
      "Batch loss: 0.0664082020521164 batch: 105/224\n",
      "Batch loss: 0.13695424795150757 batch: 106/224\n",
      "Batch loss: 0.12022010236978531 batch: 107/224\n",
      "Batch loss: 0.07971196621656418 batch: 108/224\n",
      "Batch loss: 0.06900450587272644 batch: 109/224\n",
      "Batch loss: 0.06357710808515549 batch: 110/224\n",
      "Batch loss: 0.10503221303224564 batch: 111/224\n",
      "Batch loss: 0.0858432948589325 batch: 112/224\n",
      "Batch loss: 0.08485794812440872 batch: 113/224\n",
      "Batch loss: 0.08739355951547623 batch: 114/224\n",
      "Batch loss: 0.09200429171323776 batch: 115/224\n",
      "Batch loss: 0.09981655329465866 batch: 116/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.07552159577608109 batch: 117/224\n",
      "Batch loss: 0.11155270040035248 batch: 118/224\n",
      "Batch loss: 0.0663779079914093 batch: 119/224\n",
      "Batch loss: 0.09972138702869415 batch: 120/224\n",
      "Batch loss: 0.09346794337034225 batch: 121/224\n",
      "Batch loss: 0.07226327806711197 batch: 122/224\n",
      "Batch loss: 0.09249609708786011 batch: 123/224\n",
      "Batch loss: 0.07421005517244339 batch: 124/224\n",
      "Batch loss: 0.09722580760717392 batch: 125/224\n",
      "Batch loss: 0.14224858582019806 batch: 126/224\n",
      "Batch loss: 0.1258392184972763 batch: 127/224\n",
      "Batch loss: 0.1112365797162056 batch: 128/224\n",
      "Batch loss: 0.08841042220592499 batch: 129/224\n",
      "Batch loss: 0.09206195175647736 batch: 130/224\n",
      "Batch loss: 0.07709084451198578 batch: 131/224\n",
      "Batch loss: 0.12532883882522583 batch: 132/224\n",
      "Batch loss: 0.12364574521780014 batch: 133/224\n",
      "Batch loss: 0.1077561005949974 batch: 134/224\n",
      "Batch loss: 0.12132123857736588 batch: 135/224\n",
      "Batch loss: 0.1001230999827385 batch: 136/224\n",
      "Batch loss: 0.07836222648620605 batch: 137/224\n",
      "Batch loss: 0.11891036480665207 batch: 138/224\n",
      "Batch loss: 0.14068597555160522 batch: 139/224\n",
      "Batch loss: 0.11024469137191772 batch: 140/224\n",
      "Batch loss: 0.07560554146766663 batch: 141/224\n",
      "Batch loss: 0.08152734488248825 batch: 142/224\n",
      "Batch loss: 0.0738055557012558 batch: 143/224\n",
      "Batch loss: 0.10435876995325089 batch: 144/224\n",
      "Batch loss: 0.11164676398038864 batch: 145/224\n",
      "Batch loss: 0.151614248752594 batch: 146/224\n",
      "Batch loss: 0.11047299951314926 batch: 147/224\n",
      "Batch loss: 0.08642866462469101 batch: 148/224\n",
      "Batch loss: 0.13587015867233276 batch: 149/224\n",
      "Batch loss: 0.11390254646539688 batch: 150/224\n",
      "Batch loss: 0.10140334069728851 batch: 151/224\n",
      "Batch loss: 0.0799291804432869 batch: 152/224\n",
      "Batch loss: 0.1072225347161293 batch: 153/224\n",
      "Batch loss: 0.10187830775976181 batch: 154/224\n",
      "Batch loss: 0.08778209239244461 batch: 155/224\n",
      "Batch loss: 0.08343327790498734 batch: 156/224\n",
      "Batch loss: 0.0853104293346405 batch: 157/224\n",
      "Batch loss: 0.13332749903202057 batch: 158/224\n",
      "Batch loss: 0.10077083110809326 batch: 159/224\n",
      "Batch loss: 0.07622793316841125 batch: 160/224\n",
      "Batch loss: 0.07540673017501831 batch: 161/224\n",
      "Batch loss: 0.08470018953084946 batch: 162/224\n",
      "Batch loss: 0.08042316883802414 batch: 163/224\n",
      "Batch loss: 0.08805528283119202 batch: 164/224\n",
      "Batch loss: 0.09846863150596619 batch: 165/224\n",
      "Batch loss: 0.11824841052293777 batch: 166/224\n",
      "Batch loss: 0.10268396884202957 batch: 167/224\n",
      "Batch loss: 0.09786014258861542 batch: 168/224\n",
      "Batch loss: 0.08560492843389511 batch: 169/224\n",
      "Batch loss: 0.12049689143896103 batch: 170/224\n",
      "Batch loss: 0.06848347932100296 batch: 171/224\n",
      "Batch loss: 0.10566546022891998 batch: 172/224\n",
      "Batch loss: 0.09452130645513535 batch: 173/224\n",
      "Batch loss: 0.07992096990346909 batch: 174/224\n",
      "Batch loss: 0.08222699910402298 batch: 175/224\n",
      "Batch loss: 0.09112991392612457 batch: 176/224\n",
      "Batch loss: 0.07534217089414597 batch: 177/224\n",
      "Batch loss: 0.06513594090938568 batch: 178/224\n",
      "Batch loss: 0.10301084816455841 batch: 179/224\n",
      "Batch loss: 0.07731117308139801 batch: 180/224\n",
      "Batch loss: 0.0843890905380249 batch: 181/224\n",
      "Batch loss: 0.10335999727249146 batch: 182/224\n",
      "Batch loss: 0.12826628983020782 batch: 183/224\n",
      "Batch loss: 0.09617388993501663 batch: 184/224\n",
      "Batch loss: 0.10033541917800903 batch: 185/224\n",
      "Batch loss: 0.08574236184358597 batch: 186/224\n",
      "Batch loss: 0.09788275510072708 batch: 187/224\n",
      "Batch loss: 0.0829511508345604 batch: 188/224\n",
      "Batch loss: 0.10366491228342056 batch: 189/224\n",
      "Batch loss: 0.12218312919139862 batch: 190/224\n",
      "Batch loss: 0.08456821739673615 batch: 191/224\n",
      "Batch loss: 0.09234907478094101 batch: 192/224\n",
      "Batch loss: 0.11639970541000366 batch: 193/224\n",
      "Batch loss: 0.10828877240419388 batch: 194/224\n",
      "Batch loss: 0.1039741188287735 batch: 195/224\n",
      "Batch loss: 0.15116705000400543 batch: 196/224\n",
      "Batch loss: 0.10535231977701187 batch: 197/224\n",
      "Batch loss: 0.079070083796978 batch: 198/224\n",
      "Batch loss: 0.07436878234148026 batch: 199/224\n",
      "Batch loss: 0.10363590717315674 batch: 200/224\n",
      "Batch loss: 0.1399962455034256 batch: 201/224\n",
      "Batch loss: 0.07530826330184937 batch: 202/224\n",
      "Batch loss: 0.10199086368083954 batch: 203/224\n",
      "Batch loss: 0.10604631155729294 batch: 204/224\n",
      "Batch loss: 0.12830230593681335 batch: 205/224\n",
      "Batch loss: 0.16089360415935516 batch: 206/224\n",
      "Batch loss: 0.0891876146197319 batch: 207/224\n",
      "Batch loss: 0.09065034240484238 batch: 208/224\n",
      "Batch loss: 0.09687437862157822 batch: 209/224\n",
      "Batch loss: 0.08732316642999649 batch: 210/224\n",
      "Batch loss: 0.08721649646759033 batch: 211/224\n",
      "Batch loss: 0.13797304034233093 batch: 212/224\n",
      "Batch loss: 0.14004507660865784 batch: 213/224\n",
      "Batch loss: 0.0963585153222084 batch: 214/224\n",
      "Batch loss: 0.1407163441181183 batch: 215/224\n",
      "Batch loss: 0.12148138135671616 batch: 216/224\n",
      "Batch loss: 0.10196064412593842 batch: 217/224\n",
      "Batch loss: 0.08762280642986298 batch: 218/224\n",
      "Batch loss: 0.09186532348394394 batch: 219/224\n",
      "Batch loss: 0.08258530497550964 batch: 220/224\n",
      "Batch loss: 0.1109352856874466 batch: 221/224\n",
      "Batch loss: 0.1370958685874939 batch: 222/224\n",
      "Batch loss: 0.0670308992266655 batch: 223/224\n",
      "Batch loss: 0.05778231844305992 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 44/75..  Training Loss: 0.00020..  Test Loss: 0.00089..  Test Accuracy: 0.89029\n",
      "Running epoch 45/75\n",
      "Batch loss: 0.07650677859783173 batch: 1/224\n",
      "Batch loss: 0.11896117031574249 batch: 2/224\n",
      "Batch loss: 0.10783739387989044 batch: 3/224\n",
      "Batch loss: 0.09022513031959534 batch: 4/224\n",
      "Batch loss: 0.10005064308643341 batch: 5/224\n",
      "Batch loss: 0.10403425246477127 batch: 6/224\n",
      "Batch loss: 0.0894201472401619 batch: 7/224\n",
      "Batch loss: 0.11713145673274994 batch: 8/224\n",
      "Batch loss: 0.07981940358877182 batch: 9/224\n",
      "Batch loss: 0.08728638291358948 batch: 10/224\n",
      "Batch loss: 0.13260844349861145 batch: 11/224\n",
      "Batch loss: 0.11798478662967682 batch: 12/224\n",
      "Batch loss: 0.0713038519024849 batch: 13/224\n",
      "Batch loss: 0.06457162648439407 batch: 14/224\n",
      "Batch loss: 0.06884004175662994 batch: 15/224\n",
      "Batch loss: 0.09499787539243698 batch: 16/224\n",
      "Batch loss: 0.09143990278244019 batch: 17/224\n",
      "Batch loss: 0.1089363694190979 batch: 18/224\n",
      "Batch loss: 0.07382858544588089 batch: 19/224\n",
      "Batch loss: 0.09247200191020966 batch: 20/224\n",
      "Batch loss: 0.12152288109064102 batch: 21/224\n",
      "Batch loss: 0.09570625424385071 batch: 22/224\n",
      "Batch loss: 0.0698082447052002 batch: 23/224\n",
      "Batch loss: 0.13554319739341736 batch: 24/224\n",
      "Batch loss: 0.06450330466032028 batch: 25/224\n",
      "Batch loss: 0.08818144351243973 batch: 26/224\n",
      "Batch loss: 0.10517602413892746 batch: 27/224\n",
      "Batch loss: 0.08755666762590408 batch: 28/224\n",
      "Batch loss: 0.09470375627279282 batch: 29/224\n",
      "Batch loss: 0.06623032689094543 batch: 30/224\n",
      "Batch loss: 0.07840239256620407 batch: 31/224\n",
      "Batch loss: 0.09616982936859131 batch: 32/224\n",
      "Batch loss: 0.07111858576536179 batch: 33/224\n",
      "Batch loss: 0.10917849838733673 batch: 34/224\n",
      "Batch loss: 0.10625573247671127 batch: 35/224\n",
      "Batch loss: 0.15233036875724792 batch: 36/224\n",
      "Batch loss: 0.11113594472408295 batch: 37/224\n",
      "Batch loss: 0.0959044098854065 batch: 38/224\n",
      "Batch loss: 0.12327886372804642 batch: 39/224\n",
      "Batch loss: 0.08134554326534271 batch: 40/224\n",
      "Batch loss: 0.1326015144586563 batch: 41/224\n",
      "Batch loss: 0.08206257969141006 batch: 42/224\n",
      "Batch loss: 0.1165066733956337 batch: 43/224\n",
      "Batch loss: 0.07904624193906784 batch: 44/224\n",
      "Batch loss: 0.07480354607105255 batch: 45/224\n",
      "Batch loss: 0.12473274767398834 batch: 46/224\n",
      "Batch loss: 0.09992609173059464 batch: 47/224\n",
      "Batch loss: 0.0792340561747551 batch: 48/224\n",
      "Batch loss: 0.07149260491132736 batch: 49/224\n",
      "Batch loss: 0.09488850086927414 batch: 50/224\n",
      "Batch loss: 0.09217708557844162 batch: 51/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.08253902196884155 batch: 52/224\n",
      "Batch loss: 0.08176805078983307 batch: 53/224\n",
      "Batch loss: 0.0840226337313652 batch: 54/224\n",
      "Batch loss: 0.07141008973121643 batch: 55/224\n",
      "Batch loss: 0.11023999750614166 batch: 56/224\n",
      "Batch loss: 0.11323939263820648 batch: 57/224\n",
      "Batch loss: 0.08570097386837006 batch: 58/224\n",
      "Batch loss: 0.10703662037849426 batch: 59/224\n",
      "Batch loss: 0.1089097335934639 batch: 60/224\n",
      "Batch loss: 0.07514610886573792 batch: 61/224\n",
      "Batch loss: 0.08609487861394882 batch: 62/224\n",
      "Batch loss: 0.06914566457271576 batch: 63/224\n",
      "Batch loss: 0.12282120436429977 batch: 64/224\n",
      "Batch loss: 0.10588932782411575 batch: 65/224\n",
      "Batch loss: 0.09619376063346863 batch: 66/224\n",
      "Batch loss: 0.07683540880680084 batch: 67/224\n",
      "Batch loss: 0.11123242229223251 batch: 68/224\n",
      "Batch loss: 0.12092823535203934 batch: 69/224\n",
      "Batch loss: 0.11392799019813538 batch: 70/224\n",
      "Batch loss: 0.08314408361911774 batch: 71/224\n",
      "Batch loss: 0.09269837290048599 batch: 72/224\n",
      "Batch loss: 0.08037475496530533 batch: 73/224\n",
      "Batch loss: 0.0788760557770729 batch: 74/224\n",
      "Batch loss: 0.08847259730100632 batch: 75/224\n",
      "Batch loss: 0.07832950353622437 batch: 76/224\n",
      "Batch loss: 0.12191447615623474 batch: 77/224\n",
      "Batch loss: 0.09362959116697311 batch: 78/224\n",
      "Batch loss: 0.10534113645553589 batch: 79/224\n",
      "Batch loss: 0.07909180223941803 batch: 80/224\n",
      "Batch loss: 0.0895044282078743 batch: 81/224\n",
      "Batch loss: 0.10417105257511139 batch: 82/224\n",
      "Batch loss: 0.07777247577905655 batch: 83/224\n",
      "Batch loss: 0.06975889950990677 batch: 84/224\n",
      "Batch loss: 0.11329786479473114 batch: 85/224\n",
      "Batch loss: 0.12355636805295944 batch: 86/224\n",
      "Batch loss: 0.08518003672361374 batch: 87/224\n",
      "Batch loss: 0.11636659502983093 batch: 88/224\n",
      "Batch loss: 0.09049493074417114 batch: 89/224\n",
      "Batch loss: 0.06934656202793121 batch: 90/224\n",
      "Batch loss: 0.09209666401147842 batch: 91/224\n",
      "Batch loss: 0.08764250576496124 batch: 92/224\n",
      "Batch loss: 0.07911024987697601 batch: 93/224\n",
      "Batch loss: 0.08754891157150269 batch: 94/224\n",
      "Batch loss: 0.08251255005598068 batch: 95/224\n",
      "Batch loss: 0.10467224568128586 batch: 96/224\n",
      "Batch loss: 0.06112850084900856 batch: 97/224\n",
      "Batch loss: 0.08340083062648773 batch: 98/224\n",
      "Batch loss: 0.10569903254508972 batch: 99/224\n",
      "Batch loss: 0.08382142335176468 batch: 100/224\n",
      "Batch loss: 0.1248854324221611 batch: 101/224\n",
      "Batch loss: 0.09795919805765152 batch: 102/224\n",
      "Batch loss: 0.09961418062448502 batch: 103/224\n",
      "Batch loss: 0.09399083256721497 batch: 104/224\n",
      "Batch loss: 0.07957840710878372 batch: 105/224\n",
      "Batch loss: 0.09897181391716003 batch: 106/224\n",
      "Batch loss: 0.06689012050628662 batch: 107/224\n",
      "Batch loss: 0.06926999241113663 batch: 108/224\n",
      "Batch loss: 0.07253611087799072 batch: 109/224\n",
      "Batch loss: 0.08922432363033295 batch: 110/224\n",
      "Batch loss: 0.09012626111507416 batch: 111/224\n",
      "Batch loss: 0.08131399005651474 batch: 112/224\n",
      "Batch loss: 0.08004278689622879 batch: 113/224\n",
      "Batch loss: 0.07391785085201263 batch: 114/224\n",
      "Batch loss: 0.09944314509630203 batch: 115/224\n",
      "Batch loss: 0.10556207597255707 batch: 116/224\n",
      "Batch loss: 0.07053884118795395 batch: 117/224\n",
      "Batch loss: 0.0784224271774292 batch: 118/224\n",
      "Batch loss: 0.051221251487731934 batch: 119/224\n",
      "Batch loss: 0.0727955549955368 batch: 120/224\n",
      "Batch loss: 0.09770769625902176 batch: 121/224\n",
      "Batch loss: 0.10495985299348831 batch: 122/224\n",
      "Batch loss: 0.09738991409540176 batch: 123/224\n",
      "Batch loss: 0.09319610893726349 batch: 124/224\n",
      "Batch loss: 0.11408738046884537 batch: 125/224\n",
      "Batch loss: 0.13712438941001892 batch: 126/224\n",
      "Batch loss: 0.1179381012916565 batch: 127/224\n",
      "Batch loss: 0.08179255574941635 batch: 128/224\n",
      "Batch loss: 0.08735237270593643 batch: 129/224\n",
      "Batch loss: 0.11039619892835617 batch: 130/224\n",
      "Batch loss: 0.09111426770687103 batch: 131/224\n",
      "Batch loss: 0.10938981175422668 batch: 132/224\n",
      "Batch loss: 0.115454763174057 batch: 133/224\n",
      "Batch loss: 0.09731229394674301 batch: 134/224\n",
      "Batch loss: 0.09062772989273071 batch: 135/224\n",
      "Batch loss: 0.08639372140169144 batch: 136/224\n",
      "Batch loss: 0.09388580918312073 batch: 137/224\n",
      "Batch loss: 0.12013586610555649 batch: 138/224\n",
      "Batch loss: 0.11719189584255219 batch: 139/224\n",
      "Batch loss: 0.09738057106733322 batch: 140/224\n",
      "Batch loss: 0.07073421031236649 batch: 141/224\n",
      "Batch loss: 0.07016310095787048 batch: 142/224\n",
      "Batch loss: 0.08175673335790634 batch: 143/224\n",
      "Batch loss: 0.07504278421401978 batch: 144/224\n",
      "Batch loss: 0.09773194789886475 batch: 145/224\n",
      "Batch loss: 0.12421898543834686 batch: 146/224\n",
      "Batch loss: 0.08297472447156906 batch: 147/224\n",
      "Batch loss: 0.0874473974108696 batch: 148/224\n",
      "Batch loss: 0.11333377659320831 batch: 149/224\n",
      "Batch loss: 0.13820406794548035 batch: 150/224\n",
      "Batch loss: 0.10912018269300461 batch: 151/224\n",
      "Batch loss: 0.1071157455444336 batch: 152/224\n",
      "Batch loss: 0.11308583617210388 batch: 153/224\n",
      "Batch loss: 0.11554333567619324 batch: 154/224\n",
      "Batch loss: 0.10752368718385696 batch: 155/224\n",
      "Batch loss: 0.07862352579832077 batch: 156/224\n",
      "Batch loss: 0.09140922129154205 batch: 157/224\n",
      "Batch loss: 0.11306170374155045 batch: 158/224\n",
      "Batch loss: 0.09749621897935867 batch: 159/224\n",
      "Batch loss: 0.10092952102422714 batch: 160/224\n",
      "Batch loss: 0.06795904785394669 batch: 161/224\n",
      "Batch loss: 0.09241223335266113 batch: 162/224\n",
      "Batch loss: 0.06306421756744385 batch: 163/224\n",
      "Batch loss: 0.07886351644992828 batch: 164/224\n",
      "Batch loss: 0.12186907231807709 batch: 165/224\n",
      "Batch loss: 0.06796456128358841 batch: 166/224\n",
      "Batch loss: 0.10335106402635574 batch: 167/224\n",
      "Batch loss: 0.07242634892463684 batch: 168/224\n",
      "Batch loss: 0.10626104474067688 batch: 169/224\n",
      "Batch loss: 0.07131477445363998 batch: 170/224\n",
      "Batch loss: 0.08002401888370514 batch: 171/224\n",
      "Batch loss: 0.08291593194007874 batch: 172/224\n",
      "Batch loss: 0.09604017436504364 batch: 173/224\n",
      "Batch loss: 0.08827295154333115 batch: 174/224\n",
      "Batch loss: 0.09886989742517471 batch: 175/224\n",
      "Batch loss: 0.0967676192522049 batch: 176/224\n",
      "Batch loss: 0.1061176285147667 batch: 177/224\n",
      "Batch loss: 0.10462119430303574 batch: 178/224\n",
      "Batch loss: 0.10506381839513779 batch: 179/224\n",
      "Batch loss: 0.053747035562992096 batch: 180/224\n",
      "Batch loss: 0.07281946390867233 batch: 181/224\n",
      "Batch loss: 0.12924650311470032 batch: 182/224\n",
      "Batch loss: 0.08663701266050339 batch: 183/224\n",
      "Batch loss: 0.0906338095664978 batch: 184/224\n",
      "Batch loss: 0.10084125399589539 batch: 185/224\n",
      "Batch loss: 0.09960310906171799 batch: 186/224\n",
      "Batch loss: 0.09049184620380402 batch: 187/224\n",
      "Batch loss: 0.0944373831152916 batch: 188/224\n",
      "Batch loss: 0.11098966002464294 batch: 189/224\n",
      "Batch loss: 0.09136360138654709 batch: 190/224\n",
      "Batch loss: 0.10737896710634232 batch: 191/224\n",
      "Batch loss: 0.0939350575208664 batch: 192/224\n",
      "Batch loss: 0.10841096192598343 batch: 193/224\n",
      "Batch loss: 0.1270827203989029 batch: 194/224\n",
      "Batch loss: 0.09994307160377502 batch: 195/224\n",
      "Batch loss: 0.13068841397762299 batch: 196/224\n",
      "Batch loss: 0.12646985054016113 batch: 197/224\n",
      "Batch loss: 0.12207026779651642 batch: 198/224\n",
      "Batch loss: 0.09572324901819229 batch: 199/224\n",
      "Batch loss: 0.09490888565778732 batch: 200/224\n",
      "Batch loss: 0.08588426560163498 batch: 201/224\n",
      "Batch loss: 0.10251551866531372 batch: 202/224\n",
      "Batch loss: 0.0825905129313469 batch: 203/224\n",
      "Batch loss: 0.11640734225511551 batch: 204/224\n",
      "Batch loss: 0.10188461095094681 batch: 205/224\n",
      "Batch loss: 0.09159083664417267 batch: 206/224\n",
      "Batch loss: 0.10637347400188446 batch: 207/224\n",
      "Batch loss: 0.06741061806678772 batch: 208/224\n",
      "Batch loss: 0.11010013520717621 batch: 209/224\n",
      "Batch loss: 0.07978176325559616 batch: 210/224\n",
      "Batch loss: 0.084103524684906 batch: 211/224\n",
      "Batch loss: 0.12189462035894394 batch: 212/224\n",
      "Batch loss: 0.13900886476039886 batch: 213/224\n",
      "Batch loss: 0.10192220658063889 batch: 214/224\n",
      "Batch loss: 0.09006686508655548 batch: 215/224\n",
      "Batch loss: 0.10280060768127441 batch: 216/224\n",
      "Batch loss: 0.09601785242557526 batch: 217/224\n",
      "Batch loss: 0.1084645539522171 batch: 218/224\n",
      "Batch loss: 0.05945723503828049 batch: 219/224\n",
      "Batch loss: 0.0710059404373169 batch: 220/224\n",
      "Batch loss: 0.10077337175607681 batch: 221/224\n",
      "Batch loss: 0.12746113538742065 batch: 222/224\n",
      "Batch loss: 0.07384594529867172 batch: 223/224\n",
      "Batch loss: 0.08547236025333405 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 45/75..  Training Loss: 0.00019..  Test Loss: 0.00089..  Test Accuracy: 0.89232\n",
      "Running epoch 46/75\n",
      "Batch loss: 0.08050329238176346 batch: 1/224\n",
      "Batch loss: 0.12154238671064377 batch: 2/224\n",
      "Batch loss: 0.056423120200634 batch: 3/224\n",
      "Batch loss: 0.13298200070858002 batch: 4/224\n",
      "Batch loss: 0.08266638219356537 batch: 5/224\n",
      "Batch loss: 0.07362768799066544 batch: 6/224\n",
      "Batch loss: 0.07955268770456314 batch: 7/224\n",
      "Batch loss: 0.09750199317932129 batch: 8/224\n",
      "Batch loss: 0.08035837113857269 batch: 9/224\n",
      "Batch loss: 0.05999735742807388 batch: 10/224\n",
      "Batch loss: 0.10707513988018036 batch: 11/224\n",
      "Batch loss: 0.07867640256881714 batch: 12/224\n",
      "Batch loss: 0.06160123273730278 batch: 13/224\n",
      "Batch loss: 0.08644765615463257 batch: 14/224\n",
      "Batch loss: 0.0831887349486351 batch: 15/224\n",
      "Batch loss: 0.10887274146080017 batch: 16/224\n",
      "Batch loss: 0.07040121406316757 batch: 17/224\n",
      "Batch loss: 0.09474463015794754 batch: 18/224\n",
      "Batch loss: 0.09414581209421158 batch: 19/224\n",
      "Batch loss: 0.0991586372256279 batch: 20/224\n",
      "Batch loss: 0.08991163969039917 batch: 21/224\n",
      "Batch loss: 0.07613717019557953 batch: 22/224\n",
      "Batch loss: 0.0908791571855545 batch: 23/224\n",
      "Batch loss: 0.14674457907676697 batch: 24/224\n",
      "Batch loss: 0.07918305695056915 batch: 25/224\n",
      "Batch loss: 0.08393954485654831 batch: 26/224\n",
      "Batch loss: 0.08449045568704605 batch: 27/224\n",
      "Batch loss: 0.08155499398708344 batch: 28/224\n",
      "Batch loss: 0.09623762220144272 batch: 29/224\n",
      "Batch loss: 0.10542887449264526 batch: 30/224\n",
      "Batch loss: 0.07573051750659943 batch: 31/224\n",
      "Batch loss: 0.08583170920610428 batch: 32/224\n",
      "Batch loss: 0.06631043553352356 batch: 33/224\n",
      "Batch loss: 0.10494262725114822 batch: 34/224\n",
      "Batch loss: 0.08820244669914246 batch: 35/224\n",
      "Batch loss: 0.09866814315319061 batch: 36/224\n",
      "Batch loss: 0.10177572071552277 batch: 37/224\n",
      "Batch loss: 0.09928910434246063 batch: 38/224\n",
      "Batch loss: 0.10578031092882156 batch: 39/224\n",
      "Batch loss: 0.06742957979440689 batch: 40/224\n",
      "Batch loss: 0.1348382830619812 batch: 41/224\n",
      "Batch loss: 0.07565819472074509 batch: 42/224\n",
      "Batch loss: 0.11105121672153473 batch: 43/224\n",
      "Batch loss: 0.07513300329446793 batch: 44/224\n",
      "Batch loss: 0.05222117900848389 batch: 45/224\n",
      "Batch loss: 0.08866547048091888 batch: 46/224\n",
      "Batch loss: 0.12245023995637894 batch: 47/224\n",
      "Batch loss: 0.06510323286056519 batch: 48/224\n",
      "Batch loss: 0.08806297183036804 batch: 49/224\n",
      "Batch loss: 0.07056112587451935 batch: 50/224\n",
      "Batch loss: 0.07872932404279709 batch: 51/224\n",
      "Batch loss: 0.09200756996870041 batch: 52/224\n",
      "Batch loss: 0.1192518100142479 batch: 53/224\n",
      "Batch loss: 0.06967934221029282 batch: 54/224\n",
      "Batch loss: 0.10471561551094055 batch: 55/224\n",
      "Batch loss: 0.059547051787376404 batch: 56/224\n",
      "Batch loss: 0.10747367888689041 batch: 57/224\n",
      "Batch loss: 0.09999635070562363 batch: 58/224\n",
      "Batch loss: 0.08534356206655502 batch: 59/224\n",
      "Batch loss: 0.12031331658363342 batch: 60/224\n",
      "Batch loss: 0.11202864348888397 batch: 61/224\n",
      "Batch loss: 0.1144169345498085 batch: 62/224\n",
      "Batch loss: 0.10695923864841461 batch: 63/224\n",
      "Batch loss: 0.10515668988227844 batch: 64/224\n",
      "Batch loss: 0.07053162902593613 batch: 65/224\n",
      "Batch loss: 0.12762269377708435 batch: 66/224\n",
      "Batch loss: 0.09770341962575912 batch: 67/224\n",
      "Batch loss: 0.10103370249271393 batch: 68/224\n",
      "Batch loss: 0.09730927646160126 batch: 69/224\n",
      "Batch loss: 0.09098973125219345 batch: 70/224\n",
      "Batch loss: 0.10079774260520935 batch: 71/224\n",
      "Batch loss: 0.07463300228118896 batch: 72/224\n",
      "Batch loss: 0.10317662358283997 batch: 73/224\n",
      "Batch loss: 0.0832647755742073 batch: 74/224\n",
      "Batch loss: 0.09580383449792862 batch: 75/224\n",
      "Batch loss: 0.09015460312366486 batch: 76/224\n",
      "Batch loss: 0.1114262193441391 batch: 77/224\n",
      "Batch loss: 0.1158190444111824 batch: 78/224\n",
      "Batch loss: 0.12622076272964478 batch: 79/224\n",
      "Batch loss: 0.11223060637712479 batch: 80/224\n",
      "Batch loss: 0.11872100830078125 batch: 81/224\n",
      "Batch loss: 0.10594077408313751 batch: 82/224\n",
      "Batch loss: 0.10558338463306427 batch: 83/224\n",
      "Batch loss: 0.0745607390999794 batch: 84/224\n",
      "Batch loss: 0.09484439343214035 batch: 85/224\n",
      "Batch loss: 0.11181559413671494 batch: 86/224\n",
      "Batch loss: 0.11513472348451614 batch: 87/224\n",
      "Batch loss: 0.11405214667320251 batch: 88/224\n",
      "Batch loss: 0.09794855862855911 batch: 89/224\n",
      "Batch loss: 0.10884127020835876 batch: 90/224\n",
      "Batch loss: 0.07257334887981415 batch: 91/224\n",
      "Batch loss: 0.0823267251253128 batch: 92/224\n",
      "Batch loss: 0.06601739674806595 batch: 93/224\n",
      "Batch loss: 0.08550545573234558 batch: 94/224\n",
      "Batch loss: 0.06361151486635208 batch: 95/224\n",
      "Batch loss: 0.10212360322475433 batch: 96/224\n",
      "Batch loss: 0.07880493253469467 batch: 97/224\n",
      "Batch loss: 0.052700433880090714 batch: 98/224\n",
      "Batch loss: 0.08422360569238663 batch: 99/224\n",
      "Batch loss: 0.08027292042970657 batch: 100/224\n",
      "Batch loss: 0.10725995153188705 batch: 101/224\n",
      "Batch loss: 0.11889947205781937 batch: 102/224\n",
      "Batch loss: 0.11095403879880905 batch: 103/224\n",
      "Batch loss: 0.0814836397767067 batch: 104/224\n",
      "Batch loss: 0.07509772479534149 batch: 105/224\n",
      "Batch loss: 0.07204841822385788 batch: 106/224\n",
      "Batch loss: 0.09131072461605072 batch: 107/224\n",
      "Batch loss: 0.08910517394542694 batch: 108/224\n",
      "Batch loss: 0.08329392224550247 batch: 109/224\n",
      "Batch loss: 0.08868978917598724 batch: 110/224\n",
      "Batch loss: 0.11509104073047638 batch: 111/224\n",
      "Batch loss: 0.07190176844596863 batch: 112/224\n",
      "Batch loss: 0.11267252266407013 batch: 113/224\n",
      "Batch loss: 0.07985066622495651 batch: 114/224\n",
      "Batch loss: 0.08189823478460312 batch: 115/224\n",
      "Batch loss: 0.06886474043130875 batch: 116/224\n",
      "Batch loss: 0.07159636914730072 batch: 117/224\n",
      "Batch loss: 0.0787336677312851 batch: 118/224\n",
      "Batch loss: 0.06829115003347397 batch: 119/224\n",
      "Batch loss: 0.09227079153060913 batch: 120/224\n",
      "Batch loss: 0.06849069893360138 batch: 121/224\n",
      "Batch loss: 0.10212200880050659 batch: 122/224\n",
      "Batch loss: 0.08044031262397766 batch: 123/224\n",
      "Batch loss: 0.07155252248048782 batch: 124/224\n",
      "Batch loss: 0.11358407139778137 batch: 125/224\n",
      "Batch loss: 0.08293148130178452 batch: 126/224\n",
      "Batch loss: 0.1109120175242424 batch: 127/224\n",
      "Batch loss: 0.08524720370769501 batch: 128/224\n",
      "Batch loss: 0.09246668964624405 batch: 129/224\n",
      "Batch loss: 0.09991423785686493 batch: 130/224\n",
      "Batch loss: 0.05470791459083557 batch: 131/224\n",
      "Batch loss: 0.11955053359270096 batch: 132/224\n",
      "Batch loss: 0.10580433160066605 batch: 133/224\n",
      "Batch loss: 0.14207349717617035 batch: 134/224\n",
      "Batch loss: 0.12202130258083344 batch: 135/224\n",
      "Batch loss: 0.10887981951236725 batch: 136/224\n",
      "Batch loss: 0.06970465928316116 batch: 137/224\n",
      "Batch loss: 0.10482115298509598 batch: 138/224\n",
      "Batch loss: 0.08403275161981583 batch: 139/224\n",
      "Batch loss: 0.15030845999717712 batch: 140/224\n",
      "Batch loss: 0.09682947397232056 batch: 141/224\n",
      "Batch loss: 0.08175964653491974 batch: 142/224\n",
      "Batch loss: 0.10496668517589569 batch: 143/224\n",
      "Batch loss: 0.07454860955476761 batch: 144/224\n",
      "Batch loss: 0.09971966594457626 batch: 145/224\n",
      "Batch loss: 0.11692079901695251 batch: 146/224\n",
      "Batch loss: 0.10668333619832993 batch: 147/224\n",
      "Batch loss: 0.07778071612119675 batch: 148/224\n",
      "Batch loss: 0.11581336706876755 batch: 149/224\n",
      "Batch loss: 0.10629305243492126 batch: 150/224\n",
      "Batch loss: 0.08437662571668625 batch: 151/224\n",
      "Batch loss: 0.104537233710289 batch: 152/224\n",
      "Batch loss: 0.11639118194580078 batch: 153/224\n",
      "Batch loss: 0.1393766850233078 batch: 154/224\n",
      "Batch loss: 0.09154322743415833 batch: 155/224\n",
      "Batch loss: 0.056291088461875916 batch: 156/224\n",
      "Batch loss: 0.10170173645019531 batch: 157/224\n",
      "Batch loss: 0.14820335805416107 batch: 158/224\n",
      "Batch loss: 0.06931953877210617 batch: 159/224\n",
      "Batch loss: 0.07645377516746521 batch: 160/224\n",
      "Batch loss: 0.06844963133335114 batch: 161/224\n",
      "Batch loss: 0.08350948989391327 batch: 162/224\n",
      "Batch loss: 0.06797190755605698 batch: 163/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.10763084888458252 batch: 164/224\n",
      "Batch loss: 0.13370533287525177 batch: 165/224\n",
      "Batch loss: 0.07091107964515686 batch: 166/224\n",
      "Batch loss: 0.08658768981695175 batch: 167/224\n",
      "Batch loss: 0.061907678842544556 batch: 168/224\n",
      "Batch loss: 0.08282769471406937 batch: 169/224\n",
      "Batch loss: 0.0632287785410881 batch: 170/224\n",
      "Batch loss: 0.08865125477313995 batch: 171/224\n",
      "Batch loss: 0.09171351790428162 batch: 172/224\n",
      "Batch loss: 0.1275395005941391 batch: 173/224\n",
      "Batch loss: 0.1058376282453537 batch: 174/224\n",
      "Batch loss: 0.08811524510383606 batch: 175/224\n",
      "Batch loss: 0.09950264543294907 batch: 176/224\n",
      "Batch loss: 0.11543788760900497 batch: 177/224\n",
      "Batch loss: 0.09119673818349838 batch: 178/224\n",
      "Batch loss: 0.09911557286977768 batch: 179/224\n",
      "Batch loss: 0.06793927401304245 batch: 180/224\n",
      "Batch loss: 0.11511284112930298 batch: 181/224\n",
      "Batch loss: 0.0911567360162735 batch: 182/224\n",
      "Batch loss: 0.07817769050598145 batch: 183/224\n",
      "Batch loss: 0.1006721556186676 batch: 184/224\n",
      "Batch loss: 0.10472653806209564 batch: 185/224\n",
      "Batch loss: 0.05512462183833122 batch: 186/224\n",
      "Batch loss: 0.08169429749250412 batch: 187/224\n",
      "Batch loss: 0.07354956865310669 batch: 188/224\n",
      "Batch loss: 0.09363657981157303 batch: 189/224\n",
      "Batch loss: 0.09848402440547943 batch: 190/224\n",
      "Batch loss: 0.0958525612950325 batch: 191/224\n",
      "Batch loss: 0.09837544709444046 batch: 192/224\n",
      "Batch loss: 0.0907076895236969 batch: 193/224\n",
      "Batch loss: 0.07260479778051376 batch: 194/224\n",
      "Batch loss: 0.07501450181007385 batch: 195/224\n",
      "Batch loss: 0.10149411857128143 batch: 196/224\n",
      "Batch loss: 0.10140097141265869 batch: 197/224\n",
      "Batch loss: 0.10655762255191803 batch: 198/224\n",
      "Batch loss: 0.08287239819765091 batch: 199/224\n",
      "Batch loss: 0.07913581281900406 batch: 200/224\n",
      "Batch loss: 0.09546416252851486 batch: 201/224\n",
      "Batch loss: 0.10197518020868301 batch: 202/224\n",
      "Batch loss: 0.09021708369255066 batch: 203/224\n",
      "Batch loss: 0.08555980771780014 batch: 204/224\n",
      "Batch loss: 0.07357706129550934 batch: 205/224\n",
      "Batch loss: 0.07956156879663467 batch: 206/224\n",
      "Batch loss: 0.09904100745916367 batch: 207/224\n",
      "Batch loss: 0.07400760054588318 batch: 208/224\n",
      "Batch loss: 0.11422217637300491 batch: 209/224\n",
      "Batch loss: 0.10942734032869339 batch: 210/224\n",
      "Batch loss: 0.09424541890621185 batch: 211/224\n",
      "Batch loss: 0.10698530822992325 batch: 212/224\n",
      "Batch loss: 0.10805343836545944 batch: 213/224\n",
      "Batch loss: 0.08938821405172348 batch: 214/224\n",
      "Batch loss: 0.10919557511806488 batch: 215/224\n",
      "Batch loss: 0.09975593537092209 batch: 216/224\n",
      "Batch loss: 0.09093192964792252 batch: 217/224\n",
      "Batch loss: 0.11191555857658386 batch: 218/224\n",
      "Batch loss: 0.057647641748189926 batch: 219/224\n",
      "Batch loss: 0.07566682994365692 batch: 220/224\n",
      "Batch loss: 0.10163183510303497 batch: 221/224\n",
      "Batch loss: 0.12638862431049347 batch: 222/224\n",
      "Batch loss: 0.07010689377784729 batch: 223/224\n",
      "Batch loss: 0.07470186054706573 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 46/75..  Training Loss: 0.00019..  Test Loss: 0.00093..  Test Accuracy: 0.88982\n",
      "Running epoch 47/75\n",
      "Batch loss: 0.06002260744571686 batch: 1/224\n",
      "Batch loss: 0.08232678472995758 batch: 2/224\n",
      "Batch loss: 0.06609155237674713 batch: 3/224\n",
      "Batch loss: 0.11039341241121292 batch: 4/224\n",
      "Batch loss: 0.09734135121107101 batch: 5/224\n",
      "Batch loss: 0.07457355409860611 batch: 6/224\n",
      "Batch loss: 0.08377081155776978 batch: 7/224\n",
      "Batch loss: 0.0835840031504631 batch: 8/224\n",
      "Batch loss: 0.0921638086438179 batch: 9/224\n",
      "Batch loss: 0.08879828453063965 batch: 10/224\n",
      "Batch loss: 0.13824626803398132 batch: 11/224\n",
      "Batch loss: 0.08631119132041931 batch: 12/224\n",
      "Batch loss: 0.1046319380402565 batch: 13/224\n",
      "Batch loss: 0.059647250920534134 batch: 14/224\n",
      "Batch loss: 0.09226738661527634 batch: 15/224\n",
      "Batch loss: 0.1285993903875351 batch: 16/224\n",
      "Batch loss: 0.0767001211643219 batch: 17/224\n",
      "Batch loss: 0.10812053084373474 batch: 18/224\n",
      "Batch loss: 0.06524208188056946 batch: 19/224\n",
      "Batch loss: 0.07677526772022247 batch: 20/224\n",
      "Batch loss: 0.06632868945598602 batch: 21/224\n",
      "Batch loss: 0.09239622205495834 batch: 22/224\n",
      "Batch loss: 0.12207700312137604 batch: 23/224\n",
      "Batch loss: 0.11324385553598404 batch: 24/224\n",
      "Batch loss: 0.07193780690431595 batch: 25/224\n",
      "Batch loss: 0.0718083456158638 batch: 26/224\n",
      "Batch loss: 0.09097456932067871 batch: 27/224\n",
      "Batch loss: 0.09137558937072754 batch: 28/224\n",
      "Batch loss: 0.11701294034719467 batch: 29/224\n",
      "Batch loss: 0.07905728369951248 batch: 30/224\n",
      "Batch loss: 0.08701825141906738 batch: 31/224\n",
      "Batch loss: 0.10501844435930252 batch: 32/224\n",
      "Batch loss: 0.06569862365722656 batch: 33/224\n",
      "Batch loss: 0.0887252539396286 batch: 34/224\n",
      "Batch loss: 0.11296452581882477 batch: 35/224\n",
      "Batch loss: 0.10325183719396591 batch: 36/224\n",
      "Batch loss: 0.09870949387550354 batch: 37/224\n",
      "Batch loss: 0.0957651361823082 batch: 38/224\n",
      "Batch loss: 0.11316047608852386 batch: 39/224\n",
      "Batch loss: 0.06705658882856369 batch: 40/224\n",
      "Batch loss: 0.0889265164732933 batch: 41/224\n",
      "Batch loss: 0.07531373202800751 batch: 42/224\n",
      "Batch loss: 0.11250709742307663 batch: 43/224\n",
      "Batch loss: 0.08751461654901505 batch: 44/224\n",
      "Batch loss: 0.07507787644863129 batch: 45/224\n",
      "Batch loss: 0.0949045792222023 batch: 46/224\n",
      "Batch loss: 0.08344128727912903 batch: 47/224\n",
      "Batch loss: 0.08289419114589691 batch: 48/224\n",
      "Batch loss: 0.04856671765446663 batch: 49/224\n",
      "Batch loss: 0.08242374658584595 batch: 50/224\n",
      "Batch loss: 0.058036673814058304 batch: 51/224\n",
      "Batch loss: 0.10135244578123093 batch: 52/224\n",
      "Batch loss: 0.11414240300655365 batch: 53/224\n",
      "Batch loss: 0.06383763253688812 batch: 54/224\n",
      "Batch loss: 0.09215452522039413 batch: 55/224\n",
      "Batch loss: 0.10431425273418427 batch: 56/224\n",
      "Batch loss: 0.13152258098125458 batch: 57/224\n",
      "Batch loss: 0.09814780950546265 batch: 58/224\n",
      "Batch loss: 0.10038836300373077 batch: 59/224\n",
      "Batch loss: 0.13092604279518127 batch: 60/224\n",
      "Batch loss: 0.107944555580616 batch: 61/224\n",
      "Batch loss: 0.06215345486998558 batch: 62/224\n",
      "Batch loss: 0.08038975298404694 batch: 63/224\n",
      "Batch loss: 0.11969073116779327 batch: 64/224\n",
      "Batch loss: 0.09447385370731354 batch: 65/224\n",
      "Batch loss: 0.11162760108709335 batch: 66/224\n",
      "Batch loss: 0.1044374629855156 batch: 67/224\n",
      "Batch loss: 0.06708306819200516 batch: 68/224\n",
      "Batch loss: 0.07051964849233627 batch: 69/224\n",
      "Batch loss: 0.10260607302188873 batch: 70/224\n",
      "Batch loss: 0.10470446199178696 batch: 71/224\n",
      "Batch loss: 0.0463668517768383 batch: 72/224\n",
      "Batch loss: 0.10405298322439194 batch: 73/224\n",
      "Batch loss: 0.10452747344970703 batch: 74/224\n",
      "Batch loss: 0.11162785440683365 batch: 75/224\n",
      "Batch loss: 0.10473962873220444 batch: 76/224\n",
      "Batch loss: 0.09004884213209152 batch: 77/224\n",
      "Batch loss: 0.09720481187105179 batch: 78/224\n",
      "Batch loss: 0.09618020802736282 batch: 79/224\n",
      "Batch loss: 0.07647430896759033 batch: 80/224\n",
      "Batch loss: 0.11452188342809677 batch: 81/224\n",
      "Batch loss: 0.10390076041221619 batch: 82/224\n",
      "Batch loss: 0.08452662825584412 batch: 83/224\n",
      "Batch loss: 0.06202071160078049 batch: 84/224\n",
      "Batch loss: 0.11114784330129623 batch: 85/224\n",
      "Batch loss: 0.12484096735715866 batch: 86/224\n",
      "Batch loss: 0.0721372663974762 batch: 87/224\n",
      "Batch loss: 0.08536308258771896 batch: 88/224\n",
      "Batch loss: 0.10062407702207565 batch: 89/224\n",
      "Batch loss: 0.11956147849559784 batch: 90/224\n",
      "Batch loss: 0.0821862518787384 batch: 91/224\n",
      "Batch loss: 0.11550556123256683 batch: 92/224\n",
      "Batch loss: 0.05467280000448227 batch: 93/224\n",
      "Batch loss: 0.08770997077226639 batch: 94/224\n",
      "Batch loss: 0.0765916109085083 batch: 95/224\n",
      "Batch loss: 0.08322408050298691 batch: 96/224\n",
      "Batch loss: 0.10043485462665558 batch: 97/224\n",
      "Batch loss: 0.0989159494638443 batch: 98/224\n",
      "Batch loss: 0.10138668119907379 batch: 99/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.06972397863864899 batch: 100/224\n",
      "Batch loss: 0.10473985970020294 batch: 101/224\n",
      "Batch loss: 0.0995415598154068 batch: 102/224\n",
      "Batch loss: 0.07986783236265182 batch: 103/224\n",
      "Batch loss: 0.06032290309667587 batch: 104/224\n",
      "Batch loss: 0.09354604035615921 batch: 105/224\n",
      "Batch loss: 0.07668200880289078 batch: 106/224\n",
      "Batch loss: 0.07890918105840683 batch: 107/224\n",
      "Batch loss: 0.06070195510983467 batch: 108/224\n",
      "Batch loss: 0.09365620464086533 batch: 109/224\n",
      "Batch loss: 0.06851150095462799 batch: 110/224\n",
      "Batch loss: 0.1268327832221985 batch: 111/224\n",
      "Batch loss: 0.08286123722791672 batch: 112/224\n",
      "Batch loss: 0.10452340543270111 batch: 113/224\n",
      "Batch loss: 0.08579320460557938 batch: 114/224\n",
      "Batch loss: 0.0925319716334343 batch: 115/224\n",
      "Batch loss: 0.08158926665782928 batch: 116/224\n",
      "Batch loss: 0.05857497453689575 batch: 117/224\n",
      "Batch loss: 0.10100957006216049 batch: 118/224\n",
      "Batch loss: 0.0735255777835846 batch: 119/224\n",
      "Batch loss: 0.09844337403774261 batch: 120/224\n",
      "Batch loss: 0.08671168982982635 batch: 121/224\n",
      "Batch loss: 0.11618243902921677 batch: 122/224\n",
      "Batch loss: 0.06797407567501068 batch: 123/224\n",
      "Batch loss: 0.09113433957099915 batch: 124/224\n",
      "Batch loss: 0.08400920033454895 batch: 125/224\n",
      "Batch loss: 0.09729322791099548 batch: 126/224\n",
      "Batch loss: 0.1008458361029625 batch: 127/224\n",
      "Batch loss: 0.06800039112567902 batch: 128/224\n",
      "Batch loss: 0.09092641621828079 batch: 129/224\n",
      "Batch loss: 0.12205754220485687 batch: 130/224\n",
      "Batch loss: 0.07837643474340439 batch: 131/224\n",
      "Batch loss: 0.09874863922595978 batch: 132/224\n",
      "Batch loss: 0.0810401514172554 batch: 133/224\n",
      "Batch loss: 0.08849852532148361 batch: 134/224\n",
      "Batch loss: 0.09454750269651413 batch: 135/224\n",
      "Batch loss: 0.08868664503097534 batch: 136/224\n",
      "Batch loss: 0.06346254795789719 batch: 137/224\n",
      "Batch loss: 0.09309354424476624 batch: 138/224\n",
      "Batch loss: 0.10111218690872192 batch: 139/224\n",
      "Batch loss: 0.11320465058088303 batch: 140/224\n",
      "Batch loss: 0.06513657420873642 batch: 141/224\n",
      "Batch loss: 0.11845777928829193 batch: 142/224\n",
      "Batch loss: 0.07473205029964447 batch: 143/224\n",
      "Batch loss: 0.08678679168224335 batch: 144/224\n",
      "Batch loss: 0.09035426378250122 batch: 145/224\n",
      "Batch loss: 0.14231844246387482 batch: 146/224\n",
      "Batch loss: 0.08951832354068756 batch: 147/224\n",
      "Batch loss: 0.08397592604160309 batch: 148/224\n",
      "Batch loss: 0.09755633771419525 batch: 149/224\n",
      "Batch loss: 0.10397022217512131 batch: 150/224\n",
      "Batch loss: 0.05237681046128273 batch: 151/224\n",
      "Batch loss: 0.0943206325173378 batch: 152/224\n",
      "Batch loss: 0.0797831118106842 batch: 153/224\n",
      "Batch loss: 0.09125931560993195 batch: 154/224\n",
      "Batch loss: 0.05803113430738449 batch: 155/224\n",
      "Batch loss: 0.07946708798408508 batch: 156/224\n",
      "Batch loss: 0.1443088948726654 batch: 157/224\n",
      "Batch loss: 0.10973407328128815 batch: 158/224\n",
      "Batch loss: 0.09381064027547836 batch: 159/224\n",
      "Batch loss: 0.0889585092663765 batch: 160/224\n",
      "Batch loss: 0.07509296387434006 batch: 161/224\n",
      "Batch loss: 0.08810056746006012 batch: 162/224\n",
      "Batch loss: 0.0844966322183609 batch: 163/224\n",
      "Batch loss: 0.0828845202922821 batch: 164/224\n",
      "Batch loss: 0.12046050280332565 batch: 165/224\n",
      "Batch loss: 0.09391117095947266 batch: 166/224\n",
      "Batch loss: 0.10162892937660217 batch: 167/224\n",
      "Batch loss: 0.0891847237944603 batch: 168/224\n",
      "Batch loss: 0.1065342128276825 batch: 169/224\n",
      "Batch loss: 0.10429266840219498 batch: 170/224\n",
      "Batch loss: 0.061423495411872864 batch: 171/224\n",
      "Batch loss: 0.07997298985719681 batch: 172/224\n",
      "Batch loss: 0.10552477091550827 batch: 173/224\n",
      "Batch loss: 0.08883994072675705 batch: 174/224\n",
      "Batch loss: 0.13058701157569885 batch: 175/224\n",
      "Batch loss: 0.08509177714586258 batch: 176/224\n",
      "Batch loss: 0.09838878363370895 batch: 177/224\n",
      "Batch loss: 0.08924243599176407 batch: 178/224\n",
      "Batch loss: 0.09532514214515686 batch: 179/224\n",
      "Batch loss: 0.043696243315935135 batch: 180/224\n",
      "Batch loss: 0.11189024150371552 batch: 181/224\n",
      "Batch loss: 0.09167143702507019 batch: 182/224\n",
      "Batch loss: 0.10103010386228561 batch: 183/224\n",
      "Batch loss: 0.10526964068412781 batch: 184/224\n",
      "Batch loss: 0.09443328529596329 batch: 185/224\n",
      "Batch loss: 0.09603666514158249 batch: 186/224\n",
      "Batch loss: 0.09732165932655334 batch: 187/224\n",
      "Batch loss: 0.07598819583654404 batch: 188/224\n",
      "Batch loss: 0.08634283393621445 batch: 189/224\n",
      "Batch loss: 0.08499957621097565 batch: 190/224\n",
      "Batch loss: 0.10857723653316498 batch: 191/224\n",
      "Batch loss: 0.08717197179794312 batch: 192/224\n",
      "Batch loss: 0.0939895436167717 batch: 193/224\n",
      "Batch loss: 0.0892002061009407 batch: 194/224\n",
      "Batch loss: 0.09244107455015182 batch: 195/224\n",
      "Batch loss: 0.09448390454053879 batch: 196/224\n",
      "Batch loss: 0.07100208848714828 batch: 197/224\n",
      "Batch loss: 0.06710781902074814 batch: 198/224\n",
      "Batch loss: 0.08240094780921936 batch: 199/224\n",
      "Batch loss: 0.08291184902191162 batch: 200/224\n",
      "Batch loss: 0.09518969058990479 batch: 201/224\n",
      "Batch loss: 0.12332240492105484 batch: 202/224\n",
      "Batch loss: 0.07771562039852142 batch: 203/224\n",
      "Batch loss: 0.09988529980182648 batch: 204/224\n",
      "Batch loss: 0.09190484136343002 batch: 205/224\n",
      "Batch loss: 0.12902820110321045 batch: 206/224\n",
      "Batch loss: 0.07527931034564972 batch: 207/224\n",
      "Batch loss: 0.06189766526222229 batch: 208/224\n",
      "Batch loss: 0.08217833936214447 batch: 209/224\n",
      "Batch loss: 0.0973682850599289 batch: 210/224\n",
      "Batch loss: 0.07962595671415329 batch: 211/224\n",
      "Batch loss: 0.07534371316432953 batch: 212/224\n",
      "Batch loss: 0.09188957512378693 batch: 213/224\n",
      "Batch loss: 0.11645743995904922 batch: 214/224\n",
      "Batch loss: 0.12148619443178177 batch: 215/224\n",
      "Batch loss: 0.09313615411520004 batch: 216/224\n",
      "Batch loss: 0.0983804240822792 batch: 217/224\n",
      "Batch loss: 0.1226557269692421 batch: 218/224\n",
      "Batch loss: 0.10954849421977997 batch: 219/224\n",
      "Batch loss: 0.08316504955291748 batch: 220/224\n",
      "Batch loss: 0.08099853247404099 batch: 221/224\n",
      "Batch loss: 0.1355160027742386 batch: 222/224\n",
      "Batch loss: 0.04973847419023514 batch: 223/224\n",
      "Batch loss: 0.09471940249204636 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 47/75..  Training Loss: 0.00018..  Test Loss: 0.00094..  Test Accuracy: 0.89046\n",
      "Running epoch 48/75\n",
      "Batch loss: 0.06040352210402489 batch: 1/224\n",
      "Batch loss: 0.08608750998973846 batch: 2/224\n",
      "Batch loss: 0.11240341514348984 batch: 3/224\n",
      "Batch loss: 0.08713751286268234 batch: 4/224\n",
      "Batch loss: 0.10758159309625626 batch: 5/224\n",
      "Batch loss: 0.06599153578281403 batch: 6/224\n",
      "Batch loss: 0.06086811050772667 batch: 7/224\n",
      "Batch loss: 0.1048872172832489 batch: 8/224\n",
      "Batch loss: 0.07990003377199173 batch: 9/224\n",
      "Batch loss: 0.0993601530790329 batch: 10/224\n",
      "Batch loss: 0.07927525043487549 batch: 11/224\n",
      "Batch loss: 0.09674248099327087 batch: 12/224\n",
      "Batch loss: 0.06649070233106613 batch: 13/224\n",
      "Batch loss: 0.0726817175745964 batch: 14/224\n",
      "Batch loss: 0.07821238785982132 batch: 15/224\n",
      "Batch loss: 0.08585427701473236 batch: 16/224\n",
      "Batch loss: 0.0951102003455162 batch: 17/224\n",
      "Batch loss: 0.10180597007274628 batch: 18/224\n",
      "Batch loss: 0.08805087953805923 batch: 19/224\n",
      "Batch loss: 0.0996476262807846 batch: 20/224\n",
      "Batch loss: 0.08109386265277863 batch: 21/224\n",
      "Batch loss: 0.10692517459392548 batch: 22/224\n",
      "Batch loss: 0.10417385399341583 batch: 23/224\n",
      "Batch loss: 0.11021258682012558 batch: 24/224\n",
      "Batch loss: 0.07145021855831146 batch: 25/224\n",
      "Batch loss: 0.07049807906150818 batch: 26/224\n",
      "Batch loss: 0.10715645551681519 batch: 27/224\n",
      "Batch loss: 0.10284894704818726 batch: 28/224\n",
      "Batch loss: 0.08630212396383286 batch: 29/224\n",
      "Batch loss: 0.09797529876232147 batch: 30/224\n",
      "Batch loss: 0.10089598596096039 batch: 31/224\n",
      "Batch loss: 0.07370321452617645 batch: 32/224\n",
      "Batch loss: 0.07706532627344131 batch: 33/224\n",
      "Batch loss: 0.08386266231536865 batch: 34/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.10563134402036667 batch: 35/224\n",
      "Batch loss: 0.09140186011791229 batch: 36/224\n",
      "Batch loss: 0.07989250123500824 batch: 37/224\n",
      "Batch loss: 0.10111099481582642 batch: 38/224\n",
      "Batch loss: 0.1025596484541893 batch: 39/224\n",
      "Batch loss: 0.06519021093845367 batch: 40/224\n",
      "Batch loss: 0.07976745814085007 batch: 41/224\n",
      "Batch loss: 0.067977175116539 batch: 42/224\n",
      "Batch loss: 0.10661415755748749 batch: 43/224\n",
      "Batch loss: 0.06382115930318832 batch: 44/224\n",
      "Batch loss: 0.062140874564647675 batch: 45/224\n",
      "Batch loss: 0.10902224481105804 batch: 46/224\n",
      "Batch loss: 0.09242371469736099 batch: 47/224\n",
      "Batch loss: 0.0614076666533947 batch: 48/224\n",
      "Batch loss: 0.07871727645397186 batch: 49/224\n",
      "Batch loss: 0.07952199876308441 batch: 50/224\n",
      "Batch loss: 0.10318150371313095 batch: 51/224\n",
      "Batch loss: 0.09234798699617386 batch: 52/224\n",
      "Batch loss: 0.10305439680814743 batch: 53/224\n",
      "Batch loss: 0.0984196588397026 batch: 54/224\n",
      "Batch loss: 0.09030824154615402 batch: 55/224\n",
      "Batch loss: 0.11683634668588638 batch: 56/224\n",
      "Batch loss: 0.10116837918758392 batch: 57/224\n",
      "Batch loss: 0.09193160384893417 batch: 58/224\n",
      "Batch loss: 0.0820683017373085 batch: 59/224\n",
      "Batch loss: 0.12491246312856674 batch: 60/224\n",
      "Batch loss: 0.08789938688278198 batch: 61/224\n",
      "Batch loss: 0.05942320078611374 batch: 62/224\n",
      "Batch loss: 0.12069567292928696 batch: 63/224\n",
      "Batch loss: 0.11714959889650345 batch: 64/224\n",
      "Batch loss: 0.07796916365623474 batch: 65/224\n",
      "Batch loss: 0.09915631264448166 batch: 66/224\n",
      "Batch loss: 0.07381019741296768 batch: 67/224\n",
      "Batch loss: 0.08614631742238998 batch: 68/224\n",
      "Batch loss: 0.09732084721326828 batch: 69/224\n",
      "Batch loss: 0.07170341163873672 batch: 70/224\n",
      "Batch loss: 0.07422860711812973 batch: 71/224\n",
      "Batch loss: 0.07662765681743622 batch: 72/224\n",
      "Batch loss: 0.1276005357503891 batch: 73/224\n",
      "Batch loss: 0.07803268730640411 batch: 74/224\n",
      "Batch loss: 0.10062121599912643 batch: 75/224\n",
      "Batch loss: 0.11181281507015228 batch: 76/224\n",
      "Batch loss: 0.10206922888755798 batch: 77/224\n",
      "Batch loss: 0.09714985638856888 batch: 78/224\n",
      "Batch loss: 0.08634313941001892 batch: 79/224\n",
      "Batch loss: 0.06409233808517456 batch: 80/224\n",
      "Batch loss: 0.12323348969221115 batch: 81/224\n",
      "Batch loss: 0.12824249267578125 batch: 82/224\n",
      "Batch loss: 0.10175822675228119 batch: 83/224\n",
      "Batch loss: 0.07531925290822983 batch: 84/224\n",
      "Batch loss: 0.11416000127792358 batch: 85/224\n",
      "Batch loss: 0.0865129753947258 batch: 86/224\n",
      "Batch loss: 0.07862621545791626 batch: 87/224\n",
      "Batch loss: 0.10845337808132172 batch: 88/224\n",
      "Batch loss: 0.07997094839811325 batch: 89/224\n",
      "Batch loss: 0.08208486437797546 batch: 90/224\n",
      "Batch loss: 0.07925243675708771 batch: 91/224\n",
      "Batch loss: 0.09520895034074783 batch: 92/224\n",
      "Batch loss: 0.06841112673282623 batch: 93/224\n",
      "Batch loss: 0.10442822426557541 batch: 94/224\n",
      "Batch loss: 0.06766170263290405 batch: 95/224\n",
      "Batch loss: 0.07949351519346237 batch: 96/224\n",
      "Batch loss: 0.06326369941234589 batch: 97/224\n",
      "Batch loss: 0.10149788856506348 batch: 98/224\n",
      "Batch loss: 0.08058414608240128 batch: 99/224\n",
      "Batch loss: 0.10449143499135971 batch: 100/224\n",
      "Batch loss: 0.0942697748541832 batch: 101/224\n",
      "Batch loss: 0.07227024435997009 batch: 102/224\n",
      "Batch loss: 0.09343718737363815 batch: 103/224\n",
      "Batch loss: 0.0499679259955883 batch: 104/224\n",
      "Batch loss: 0.07256976515054703 batch: 105/224\n",
      "Batch loss: 0.07078248262405396 batch: 106/224\n",
      "Batch loss: 0.07449030131101608 batch: 107/224\n",
      "Batch loss: 0.0853848084807396 batch: 108/224\n",
      "Batch loss: 0.05607573688030243 batch: 109/224\n",
      "Batch loss: 0.03757895529270172 batch: 110/224\n",
      "Batch loss: 0.05223970115184784 batch: 111/224\n",
      "Batch loss: 0.07904468476772308 batch: 112/224\n",
      "Batch loss: 0.09832891076803207 batch: 113/224\n",
      "Batch loss: 0.0554325245320797 batch: 114/224\n",
      "Batch loss: 0.07129406183958054 batch: 115/224\n",
      "Batch loss: 0.07510124891996384 batch: 116/224\n",
      "Batch loss: 0.0663655549287796 batch: 117/224\n",
      "Batch loss: 0.07239622622728348 batch: 118/224\n",
      "Batch loss: 0.07047144323587418 batch: 119/224\n",
      "Batch loss: 0.08305681496858597 batch: 120/224\n",
      "Batch loss: 0.07922042906284332 batch: 121/224\n",
      "Batch loss: 0.0818876400589943 batch: 122/224\n",
      "Batch loss: 0.08964276313781738 batch: 123/224\n",
      "Batch loss: 0.08152737468481064 batch: 124/224\n",
      "Batch loss: 0.07408127933740616 batch: 125/224\n",
      "Batch loss: 0.1011098250746727 batch: 126/224\n",
      "Batch loss: 0.1083221286535263 batch: 127/224\n",
      "Batch loss: 0.08333534002304077 batch: 128/224\n",
      "Batch loss: 0.05751483514904976 batch: 129/224\n",
      "Batch loss: 0.07522859424352646 batch: 130/224\n",
      "Batch loss: 0.07123589515686035 batch: 131/224\n",
      "Batch loss: 0.06889978051185608 batch: 132/224\n",
      "Batch loss: 0.08059969544410706 batch: 133/224\n",
      "Batch loss: 0.09950333833694458 batch: 134/224\n",
      "Batch loss: 0.1106356829404831 batch: 135/224\n",
      "Batch loss: 0.10537786036729813 batch: 136/224\n",
      "Batch loss: 0.10683735460042953 batch: 137/224\n",
      "Batch loss: 0.07987593114376068 batch: 138/224\n",
      "Batch loss: 0.09627941250801086 batch: 139/224\n",
      "Batch loss: 0.12085672467947006 batch: 140/224\n",
      "Batch loss: 0.06381744146347046 batch: 141/224\n",
      "Batch loss: 0.09040967375040054 batch: 142/224\n",
      "Batch loss: 0.07431002706289291 batch: 143/224\n",
      "Batch loss: 0.07755020260810852 batch: 144/224\n",
      "Batch loss: 0.1099509447813034 batch: 145/224\n",
      "Batch loss: 0.09890929609537125 batch: 146/224\n",
      "Batch loss: 0.08128968626260757 batch: 147/224\n",
      "Batch loss: 0.06520192325115204 batch: 148/224\n",
      "Batch loss: 0.08328752964735031 batch: 149/224\n",
      "Batch loss: 0.09635992348194122 batch: 150/224\n",
      "Batch loss: 0.04998308792710304 batch: 151/224\n",
      "Batch loss: 0.10485021770000458 batch: 152/224\n",
      "Batch loss: 0.0790325179696083 batch: 153/224\n",
      "Batch loss: 0.08408785611391068 batch: 154/224\n",
      "Batch loss: 0.10430192202329636 batch: 155/224\n",
      "Batch loss: 0.10091947019100189 batch: 156/224\n",
      "Batch loss: 0.12195911258459091 batch: 157/224\n",
      "Batch loss: 0.14036284387111664 batch: 158/224\n",
      "Batch loss: 0.05672019347548485 batch: 159/224\n",
      "Batch loss: 0.08349739015102386 batch: 160/224\n",
      "Batch loss: 0.06701494008302689 batch: 161/224\n",
      "Batch loss: 0.0584806427359581 batch: 162/224\n",
      "Batch loss: 0.06710851192474365 batch: 163/224\n",
      "Batch loss: 0.10068799555301666 batch: 164/224\n",
      "Batch loss: 0.10313060134649277 batch: 165/224\n",
      "Batch loss: 0.10376563668251038 batch: 166/224\n",
      "Batch loss: 0.06569872051477432 batch: 167/224\n",
      "Batch loss: 0.08206310868263245 batch: 168/224\n",
      "Batch loss: 0.08048905432224274 batch: 169/224\n",
      "Batch loss: 0.07644084095954895 batch: 170/224\n",
      "Batch loss: 0.08247116208076477 batch: 171/224\n",
      "Batch loss: 0.08674449473619461 batch: 172/224\n",
      "Batch loss: 0.09307275712490082 batch: 173/224\n",
      "Batch loss: 0.08050750195980072 batch: 174/224\n",
      "Batch loss: 0.08251801133155823 batch: 175/224\n",
      "Batch loss: 0.09560302644968033 batch: 176/224\n",
      "Batch loss: 0.12433457374572754 batch: 177/224\n",
      "Batch loss: 0.07628180831670761 batch: 178/224\n",
      "Batch loss: 0.10821599513292313 batch: 179/224\n",
      "Batch loss: 0.0540219210088253 batch: 180/224\n",
      "Batch loss: 0.06799369305372238 batch: 181/224\n",
      "Batch loss: 0.07289067655801773 batch: 182/224\n",
      "Batch loss: 0.09962217509746552 batch: 183/224\n",
      "Batch loss: 0.0930505022406578 batch: 184/224\n",
      "Batch loss: 0.09992123395204544 batch: 185/224\n",
      "Batch loss: 0.07255426049232483 batch: 186/224\n",
      "Batch loss: 0.1049085408449173 batch: 187/224\n",
      "Batch loss: 0.08081328123807907 batch: 188/224\n",
      "Batch loss: 0.09441568702459335 batch: 189/224\n",
      "Batch loss: 0.10941115766763687 batch: 190/224\n",
      "Batch loss: 0.07903636991977692 batch: 191/224\n",
      "Batch loss: 0.06653542071580887 batch: 192/224\n",
      "Batch loss: 0.08521690219640732 batch: 193/224\n",
      "Batch loss: 0.0871366485953331 batch: 194/224\n",
      "Batch loss: 0.11454062908887863 batch: 195/224\n",
      "Batch loss: 0.08087368309497833 batch: 196/224\n",
      "Batch loss: 0.08437957614660263 batch: 197/224\n",
      "Batch loss: 0.08546724170446396 batch: 198/224\n",
      "Batch loss: 0.081419937312603 batch: 199/224\n",
      "Batch loss: 0.08273085951805115 batch: 200/224\n",
      "Batch loss: 0.09442009776830673 batch: 201/224\n",
      "Batch loss: 0.0703456848859787 batch: 202/224\n",
      "Batch loss: 0.09393578767776489 batch: 203/224\n",
      "Batch loss: 0.0687594935297966 batch: 204/224\n",
      "Batch loss: 0.0875823050737381 batch: 205/224\n",
      "Batch loss: 0.10876698046922684 batch: 206/224\n",
      "Batch loss: 0.07702558487653732 batch: 207/224\n",
      "Batch loss: 0.06576365232467651 batch: 208/224\n",
      "Batch loss: 0.07702873647212982 batch: 209/224\n",
      "Batch loss: 0.0822589248418808 batch: 210/224\n",
      "Batch loss: 0.0845838263630867 batch: 211/224\n",
      "Batch loss: 0.06623660773038864 batch: 212/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.13109895586967468 batch: 213/224\n",
      "Batch loss: 0.07443029433488846 batch: 214/224\n",
      "Batch loss: 0.09549485146999359 batch: 215/224\n",
      "Batch loss: 0.0787193700671196 batch: 216/224\n",
      "Batch loss: 0.07950536906719208 batch: 217/224\n",
      "Batch loss: 0.09407167136669159 batch: 218/224\n",
      "Batch loss: 0.041221510618925095 batch: 219/224\n",
      "Batch loss: 0.07348380982875824 batch: 220/224\n",
      "Batch loss: 0.13552163541316986 batch: 221/224\n",
      "Batch loss: 0.1376437097787857 batch: 222/224\n",
      "Batch loss: 0.04768070951104164 batch: 223/224\n",
      "Batch loss: 0.06482911109924316 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 48/75..  Training Loss: 0.00017..  Test Loss: 0.00095..  Test Accuracy: 0.89021\n",
      "Running epoch 49/75\n",
      "Batch loss: 0.07073131203651428 batch: 1/224\n",
      "Batch loss: 0.10694079846143723 batch: 2/224\n",
      "Batch loss: 0.08767993748188019 batch: 3/224\n",
      "Batch loss: 0.13369138538837433 batch: 4/224\n",
      "Batch loss: 0.08869358152151108 batch: 5/224\n",
      "Batch loss: 0.06758414953947067 batch: 6/224\n",
      "Batch loss: 0.08685994148254395 batch: 7/224\n",
      "Batch loss: 0.10851416736841202 batch: 8/224\n",
      "Batch loss: 0.05672858655452728 batch: 9/224\n",
      "Batch loss: 0.09840184450149536 batch: 10/224\n",
      "Batch loss: 0.11277075856924057 batch: 11/224\n",
      "Batch loss: 0.112882599234581 batch: 12/224\n",
      "Batch loss: 0.06823112815618515 batch: 13/224\n",
      "Batch loss: 0.0834951102733612 batch: 14/224\n",
      "Batch loss: 0.08824989944696426 batch: 15/224\n",
      "Batch loss: 0.10863432288169861 batch: 16/224\n",
      "Batch loss: 0.0660610944032669 batch: 17/224\n",
      "Batch loss: 0.08076848089694977 batch: 18/224\n",
      "Batch loss: 0.09834090620279312 batch: 19/224\n",
      "Batch loss: 0.07825979590415955 batch: 20/224\n",
      "Batch loss: 0.09925451874732971 batch: 21/224\n",
      "Batch loss: 0.11498580127954483 batch: 22/224\n",
      "Batch loss: 0.09218383580446243 batch: 23/224\n",
      "Batch loss: 0.10537713021039963 batch: 24/224\n",
      "Batch loss: 0.09031157940626144 batch: 25/224\n",
      "Batch loss: 0.06866273283958435 batch: 26/224\n",
      "Batch loss: 0.0915149599313736 batch: 27/224\n",
      "Batch loss: 0.09282129257917404 batch: 28/224\n",
      "Batch loss: 0.11899290233850479 batch: 29/224\n",
      "Batch loss: 0.0963694229722023 batch: 30/224\n",
      "Batch loss: 0.06013431027531624 batch: 31/224\n",
      "Batch loss: 0.08780031651258469 batch: 32/224\n",
      "Batch loss: 0.0656948909163475 batch: 33/224\n",
      "Batch loss: 0.0848412811756134 batch: 34/224\n",
      "Batch loss: 0.10398148000240326 batch: 35/224\n",
      "Batch loss: 0.10332147777080536 batch: 36/224\n",
      "Batch loss: 0.11695937067270279 batch: 37/224\n",
      "Batch loss: 0.11944819986820221 batch: 38/224\n",
      "Batch loss: 0.08849234879016876 batch: 39/224\n",
      "Batch loss: 0.09502501040697098 batch: 40/224\n",
      "Batch loss: 0.10918864607810974 batch: 41/224\n",
      "Batch loss: 0.08622390031814575 batch: 42/224\n",
      "Batch loss: 0.1145220696926117 batch: 43/224\n",
      "Batch loss: 0.06923773139715195 batch: 44/224\n",
      "Batch loss: 0.07601159065961838 batch: 45/224\n",
      "Batch loss: 0.14158731698989868 batch: 46/224\n",
      "Batch loss: 0.0999080240726471 batch: 47/224\n",
      "Batch loss: 0.0504203662276268 batch: 48/224\n",
      "Batch loss: 0.06676600128412247 batch: 49/224\n",
      "Batch loss: 0.06217040866613388 batch: 50/224\n",
      "Batch loss: 0.06516434997320175 batch: 51/224\n",
      "Batch loss: 0.07367468625307083 batch: 52/224\n",
      "Batch loss: 0.09650938957929611 batch: 53/224\n",
      "Batch loss: 0.07587016373872757 batch: 54/224\n",
      "Batch loss: 0.05526179075241089 batch: 55/224\n",
      "Batch loss: 0.0723370760679245 batch: 56/224\n",
      "Batch loss: 0.10792682319879532 batch: 57/224\n",
      "Batch loss: 0.09347138553857803 batch: 58/224\n",
      "Batch loss: 0.07283522933721542 batch: 59/224\n",
      "Batch loss: 0.0942758098244667 batch: 60/224\n",
      "Batch loss: 0.09827269613742828 batch: 61/224\n",
      "Batch loss: 0.050596341490745544 batch: 62/224\n",
      "Batch loss: 0.07529018819332123 batch: 63/224\n",
      "Batch loss: 0.11219766736030579 batch: 64/224\n",
      "Batch loss: 0.09434814006090164 batch: 65/224\n",
      "Batch loss: 0.10162630677223206 batch: 66/224\n",
      "Batch loss: 0.09637383371591568 batch: 67/224\n",
      "Batch loss: 0.06720820814371109 batch: 68/224\n",
      "Batch loss: 0.09065411239862442 batch: 69/224\n",
      "Batch loss: 0.08158396929502487 batch: 70/224\n",
      "Batch loss: 0.091278076171875 batch: 71/224\n",
      "Batch loss: 0.08331705629825592 batch: 72/224\n",
      "Batch loss: 0.10569936037063599 batch: 73/224\n",
      "Batch loss: 0.0630272626876831 batch: 74/224\n",
      "Batch loss: 0.09072572737932205 batch: 75/224\n",
      "Batch loss: 0.09690912067890167 batch: 76/224\n",
      "Batch loss: 0.08699628710746765 batch: 77/224\n",
      "Batch loss: 0.07370875775814056 batch: 78/224\n",
      "Batch loss: 0.0848454013466835 batch: 79/224\n",
      "Batch loss: 0.06544038653373718 batch: 80/224\n",
      "Batch loss: 0.09546712040901184 batch: 81/224\n",
      "Batch loss: 0.1025075614452362 batch: 82/224\n",
      "Batch loss: 0.09636121243238449 batch: 83/224\n",
      "Batch loss: 0.055996090173721313 batch: 84/224\n",
      "Batch loss: 0.0718517079949379 batch: 85/224\n",
      "Batch loss: 0.09166263788938522 batch: 86/224\n",
      "Batch loss: 0.06192013621330261 batch: 87/224\n",
      "Batch loss: 0.0782514289021492 batch: 88/224\n",
      "Batch loss: 0.1273004710674286 batch: 89/224\n",
      "Batch loss: 0.09612888097763062 batch: 90/224\n",
      "Batch loss: 0.06322699040174484 batch: 91/224\n",
      "Batch loss: 0.10181237757205963 batch: 92/224\n",
      "Batch loss: 0.04885273426771164 batch: 93/224\n",
      "Batch loss: 0.11302616447210312 batch: 94/224\n",
      "Batch loss: 0.10830044001340866 batch: 95/224\n",
      "Batch loss: 0.0513060986995697 batch: 96/224\n",
      "Batch loss: 0.09756377339363098 batch: 97/224\n",
      "Batch loss: 0.05268840491771698 batch: 98/224\n",
      "Batch loss: 0.08720247447490692 batch: 99/224\n",
      "Batch loss: 0.07706374675035477 batch: 100/224\n",
      "Batch loss: 0.0992932915687561 batch: 101/224\n",
      "Batch loss: 0.07521768659353256 batch: 102/224\n",
      "Batch loss: 0.09712684899568558 batch: 103/224\n",
      "Batch loss: 0.0489928238093853 batch: 104/224\n",
      "Batch loss: 0.0929708257317543 batch: 105/224\n",
      "Batch loss: 0.069266676902771 batch: 106/224\n",
      "Batch loss: 0.08662191033363342 batch: 107/224\n",
      "Batch loss: 0.07666055858135223 batch: 108/224\n",
      "Batch loss: 0.08068211376667023 batch: 109/224\n",
      "Batch loss: 0.06024700403213501 batch: 110/224\n",
      "Batch loss: 0.0937778651714325 batch: 111/224\n",
      "Batch loss: 0.07714635133743286 batch: 112/224\n",
      "Batch loss: 0.1006845012307167 batch: 113/224\n",
      "Batch loss: 0.08251971751451492 batch: 114/224\n",
      "Batch loss: 0.08744804561138153 batch: 115/224\n",
      "Batch loss: 0.0667816773056984 batch: 116/224\n",
      "Batch loss: 0.06825194507837296 batch: 117/224\n",
      "Batch loss: 0.10170288383960724 batch: 118/224\n",
      "Batch loss: 0.07003449648618698 batch: 119/224\n",
      "Batch loss: 0.10154134035110474 batch: 120/224\n",
      "Batch loss: 0.058875035494565964 batch: 121/224\n",
      "Batch loss: 0.07632210850715637 batch: 122/224\n",
      "Batch loss: 0.06377677619457245 batch: 123/224\n",
      "Batch loss: 0.07579635083675385 batch: 124/224\n",
      "Batch loss: 0.06757962703704834 batch: 125/224\n",
      "Batch loss: 0.1058477833867073 batch: 126/224\n",
      "Batch loss: 0.09261538833379745 batch: 127/224\n",
      "Batch loss: 0.1116931140422821 batch: 128/224\n",
      "Batch loss: 0.06989708542823792 batch: 129/224\n",
      "Batch loss: 0.10444515943527222 batch: 130/224\n",
      "Batch loss: 0.06134748458862305 batch: 131/224\n",
      "Batch loss: 0.09595193713903427 batch: 132/224\n",
      "Batch loss: 0.07657060027122498 batch: 133/224\n",
      "Batch loss: 0.11442913860082626 batch: 134/224\n",
      "Batch loss: 0.09033410996198654 batch: 135/224\n",
      "Batch loss: 0.09915892779827118 batch: 136/224\n",
      "Batch loss: 0.06483235210180283 batch: 137/224\n",
      "Batch loss: 0.10046951472759247 batch: 138/224\n",
      "Batch loss: 0.08491980284452438 batch: 139/224\n",
      "Batch loss: 0.1117476373910904 batch: 140/224\n",
      "Batch loss: 0.08801452815532684 batch: 141/224\n",
      "Batch loss: 0.09145838767290115 batch: 142/224\n",
      "Batch loss: 0.06870255619287491 batch: 143/224\n",
      "Batch loss: 0.08446730673313141 batch: 144/224\n",
      "Batch loss: 0.0816095769405365 batch: 145/224\n",
      "Batch loss: 0.11917898803949356 batch: 146/224\n",
      "Batch loss: 0.06391802430152893 batch: 147/224\n",
      "Batch loss: 0.06526234745979309 batch: 148/224\n",
      "Batch loss: 0.08641021698713303 batch: 149/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.09398799389600754 batch: 150/224\n",
      "Batch loss: 0.08061912655830383 batch: 151/224\n",
      "Batch loss: 0.07196526974439621 batch: 152/224\n",
      "Batch loss: 0.09068810939788818 batch: 153/224\n",
      "Batch loss: 0.07733257114887238 batch: 154/224\n",
      "Batch loss: 0.06887099146842957 batch: 155/224\n",
      "Batch loss: 0.06121453270316124 batch: 156/224\n",
      "Batch loss: 0.08568913489580154 batch: 157/224\n",
      "Batch loss: 0.1582379937171936 batch: 158/224\n",
      "Batch loss: 0.07286132127046585 batch: 159/224\n",
      "Batch loss: 0.061823248863220215 batch: 160/224\n",
      "Batch loss: 0.07125250995159149 batch: 161/224\n",
      "Batch loss: 0.06817428022623062 batch: 162/224\n",
      "Batch loss: 0.049665942788124084 batch: 163/224\n",
      "Batch loss: 0.08472942560911179 batch: 164/224\n",
      "Batch loss: 0.150665745139122 batch: 165/224\n",
      "Batch loss: 0.08024667948484421 batch: 166/224\n",
      "Batch loss: 0.0752563402056694 batch: 167/224\n",
      "Batch loss: 0.08475576341152191 batch: 168/224\n",
      "Batch loss: 0.108481265604496 batch: 169/224\n",
      "Batch loss: 0.08470712602138519 batch: 170/224\n",
      "Batch loss: 0.07309634238481522 batch: 171/224\n",
      "Batch loss: 0.07316577434539795 batch: 172/224\n",
      "Batch loss: 0.07777874171733856 batch: 173/224\n",
      "Batch loss: 0.06703044474124908 batch: 174/224\n",
      "Batch loss: 0.09405162185430527 batch: 175/224\n",
      "Batch loss: 0.10040467232465744 batch: 176/224\n",
      "Batch loss: 0.10024790465831757 batch: 177/224\n",
      "Batch loss: 0.052796971052885056 batch: 178/224\n",
      "Batch loss: 0.08094912022352219 batch: 179/224\n",
      "Batch loss: 0.0711907297372818 batch: 180/224\n",
      "Batch loss: 0.0760345533490181 batch: 181/224\n",
      "Batch loss: 0.0981532484292984 batch: 182/224\n",
      "Batch loss: 0.10912276804447174 batch: 183/224\n",
      "Batch loss: 0.08922898024320602 batch: 184/224\n",
      "Batch loss: 0.1244303286075592 batch: 185/224\n",
      "Batch loss: 0.08344896137714386 batch: 186/224\n",
      "Batch loss: 0.09985068440437317 batch: 187/224\n",
      "Batch loss: 0.08600906282663345 batch: 188/224\n",
      "Batch loss: 0.08007894456386566 batch: 189/224\n",
      "Batch loss: 0.06433805078268051 batch: 190/224\n",
      "Batch loss: 0.0908200591802597 batch: 191/224\n",
      "Batch loss: 0.09136011451482773 batch: 192/224\n",
      "Batch loss: 0.10748457163572311 batch: 193/224\n",
      "Batch loss: 0.07782784104347229 batch: 194/224\n",
      "Batch loss: 0.11492358893156052 batch: 195/224\n",
      "Batch loss: 0.1489979773759842 batch: 196/224\n",
      "Batch loss: 0.08197949826717377 batch: 197/224\n",
      "Batch loss: 0.07100678235292435 batch: 198/224\n",
      "Batch loss: 0.06302759051322937 batch: 199/224\n",
      "Batch loss: 0.13444770872592926 batch: 200/224\n",
      "Batch loss: 0.12435856461524963 batch: 201/224\n",
      "Batch loss: 0.11049798876047134 batch: 202/224\n",
      "Batch loss: 0.053077902644872665 batch: 203/224\n",
      "Batch loss: 0.06900053471326828 batch: 204/224\n",
      "Batch loss: 0.0873606875538826 batch: 205/224\n",
      "Batch loss: 0.11262515187263489 batch: 206/224\n",
      "Batch loss: 0.08761932700872421 batch: 207/224\n",
      "Batch loss: 0.09859214723110199 batch: 208/224\n",
      "Batch loss: 0.0830485001206398 batch: 209/224\n",
      "Batch loss: 0.09037917852401733 batch: 210/224\n",
      "Batch loss: 0.07243821024894714 batch: 211/224\n",
      "Batch loss: 0.0820276066660881 batch: 212/224\n",
      "Batch loss: 0.11765381693840027 batch: 213/224\n",
      "Batch loss: 0.11239935457706451 batch: 214/224\n",
      "Batch loss: 0.0917765349149704 batch: 215/224\n",
      "Batch loss: 0.0889694094657898 batch: 216/224\n",
      "Batch loss: 0.08919910341501236 batch: 217/224\n",
      "Batch loss: 0.09182271361351013 batch: 218/224\n",
      "Batch loss: 0.0805361270904541 batch: 219/224\n",
      "Batch loss: 0.09247387200593948 batch: 220/224\n",
      "Batch loss: 0.11574897915124893 batch: 221/224\n",
      "Batch loss: 0.10770323127508163 batch: 222/224\n",
      "Batch loss: 0.0635889321565628 batch: 223/224\n",
      "Batch loss: 0.06528355926275253 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 49/75..  Training Loss: 0.00017..  Test Loss: 0.00091..  Test Accuracy: 0.89193\n",
      "Running epoch 50/75\n",
      "Batch loss: 0.056659769266843796 batch: 1/224\n",
      "Batch loss: 0.05729169398546219 batch: 2/224\n",
      "Batch loss: 0.05257853493094444 batch: 3/224\n",
      "Batch loss: 0.111180879175663 batch: 4/224\n",
      "Batch loss: 0.07762553542852402 batch: 5/224\n",
      "Batch loss: 0.10458477586507797 batch: 6/224\n",
      "Batch loss: 0.06561964005231857 batch: 7/224\n",
      "Batch loss: 0.08932702243328094 batch: 8/224\n",
      "Batch loss: 0.0854005366563797 batch: 9/224\n",
      "Batch loss: 0.07542742043733597 batch: 10/224\n",
      "Batch loss: 0.08658168464899063 batch: 11/224\n",
      "Batch loss: 0.08884017169475555 batch: 12/224\n",
      "Batch loss: 0.06451227515935898 batch: 13/224\n",
      "Batch loss: 0.07117094844579697 batch: 14/224\n",
      "Batch loss: 0.08192401379346848 batch: 15/224\n",
      "Batch loss: 0.08082898706197739 batch: 16/224\n",
      "Batch loss: 0.083466075360775 batch: 17/224\n",
      "Batch loss: 0.10016003996133804 batch: 18/224\n",
      "Batch loss: 0.05770000442862511 batch: 19/224\n",
      "Batch loss: 0.07673691213130951 batch: 20/224\n",
      "Batch loss: 0.11006078869104385 batch: 21/224\n",
      "Batch loss: 0.0721457302570343 batch: 22/224\n",
      "Batch loss: 0.08552218973636627 batch: 23/224\n",
      "Batch loss: 0.0939934179186821 batch: 24/224\n",
      "Batch loss: 0.09299778938293457 batch: 25/224\n",
      "Batch loss: 0.07816771417856216 batch: 26/224\n",
      "Batch loss: 0.10327857732772827 batch: 27/224\n",
      "Batch loss: 0.09444877505302429 batch: 28/224\n",
      "Batch loss: 0.09663063287734985 batch: 29/224\n",
      "Batch loss: 0.095100536942482 batch: 30/224\n",
      "Batch loss: 0.06580732762813568 batch: 31/224\n",
      "Batch loss: 0.09764471650123596 batch: 32/224\n",
      "Batch loss: 0.08400382101535797 batch: 33/224\n",
      "Batch loss: 0.08965329825878143 batch: 34/224\n",
      "Batch loss: 0.08751167356967926 batch: 35/224\n",
      "Batch loss: 0.06521496176719666 batch: 36/224\n",
      "Batch loss: 0.10809611529111862 batch: 37/224\n",
      "Batch loss: 0.07230385392904282 batch: 38/224\n",
      "Batch loss: 0.07751475274562836 batch: 39/224\n",
      "Batch loss: 0.0701972171664238 batch: 40/224\n",
      "Batch loss: 0.0947178527712822 batch: 41/224\n",
      "Batch loss: 0.07075947523117065 batch: 42/224\n",
      "Batch loss: 0.0857408344745636 batch: 43/224\n",
      "Batch loss: 0.07020964473485947 batch: 44/224\n",
      "Batch loss: 0.07894929498434067 batch: 45/224\n",
      "Batch loss: 0.11529751121997833 batch: 46/224\n",
      "Batch loss: 0.08882507681846619 batch: 47/224\n",
      "Batch loss: 0.05415775999426842 batch: 48/224\n",
      "Batch loss: 0.07342308759689331 batch: 49/224\n",
      "Batch loss: 0.05873731151223183 batch: 50/224\n",
      "Batch loss: 0.05021444335579872 batch: 51/224\n",
      "Batch loss: 0.08493002504110336 batch: 52/224\n",
      "Batch loss: 0.10352347791194916 batch: 53/224\n",
      "Batch loss: 0.09366726875305176 batch: 54/224\n",
      "Batch loss: 0.0796096920967102 batch: 55/224\n",
      "Batch loss: 0.063215471804142 batch: 56/224\n",
      "Batch loss: 0.07719432562589645 batch: 57/224\n",
      "Batch loss: 0.07162299752235413 batch: 58/224\n",
      "Batch loss: 0.07281435281038284 batch: 59/224\n",
      "Batch loss: 0.11209434270858765 batch: 60/224\n",
      "Batch loss: 0.09448524564504623 batch: 61/224\n",
      "Batch loss: 0.07444991171360016 batch: 62/224\n",
      "Batch loss: 0.07769623398780823 batch: 63/224\n",
      "Batch loss: 0.07926663756370544 batch: 64/224\n",
      "Batch loss: 0.0730157420039177 batch: 65/224\n",
      "Batch loss: 0.06975766271352768 batch: 66/224\n",
      "Batch loss: 0.08484579622745514 batch: 67/224\n",
      "Batch loss: 0.06895781308412552 batch: 68/224\n",
      "Batch loss: 0.12685959041118622 batch: 69/224\n",
      "Batch loss: 0.09455906599760056 batch: 70/224\n",
      "Batch loss: 0.062370460480451584 batch: 71/224\n",
      "Batch loss: 0.055030278861522675 batch: 72/224\n",
      "Batch loss: 0.07722289115190506 batch: 73/224\n",
      "Batch loss: 0.1098422035574913 batch: 74/224\n",
      "Batch loss: 0.06756333261728287 batch: 75/224\n",
      "Batch loss: 0.07272641360759735 batch: 76/224\n",
      "Batch loss: 0.11467352509498596 batch: 77/224\n",
      "Batch loss: 0.11759524047374725 batch: 78/224\n",
      "Batch loss: 0.09989555925130844 batch: 79/224\n",
      "Batch loss: 0.057973429560661316 batch: 80/224\n",
      "Batch loss: 0.12322636693716049 batch: 81/224\n",
      "Batch loss: 0.08766528218984604 batch: 82/224\n",
      "Batch loss: 0.08756361901760101 batch: 83/224\n",
      "Batch loss: 0.05362625792622566 batch: 84/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.08363868296146393 batch: 85/224\n",
      "Batch loss: 0.11251258850097656 batch: 86/224\n",
      "Batch loss: 0.11319440603256226 batch: 87/224\n",
      "Batch loss: 0.10707064718008041 batch: 88/224\n",
      "Batch loss: 0.0751495286822319 batch: 89/224\n",
      "Batch loss: 0.10005396604537964 batch: 90/224\n",
      "Batch loss: 0.07527702301740646 batch: 91/224\n",
      "Batch loss: 0.09805487096309662 batch: 92/224\n",
      "Batch loss: 0.06604093313217163 batch: 93/224\n",
      "Batch loss: 0.07188836485147476 batch: 94/224\n",
      "Batch loss: 0.07407671213150024 batch: 95/224\n",
      "Batch loss: 0.09404055774211884 batch: 96/224\n",
      "Batch loss: 0.07218126207590103 batch: 97/224\n",
      "Batch loss: 0.04458402842283249 batch: 98/224\n",
      "Batch loss: 0.09714207053184509 batch: 99/224\n",
      "Batch loss: 0.06280563771724701 batch: 100/224\n",
      "Batch loss: 0.11877080053091049 batch: 101/224\n",
      "Batch loss: 0.10682908445596695 batch: 102/224\n",
      "Batch loss: 0.07554831355810165 batch: 103/224\n",
      "Batch loss: 0.05103714019060135 batch: 104/224\n",
      "Batch loss: 0.04840635880827904 batch: 105/224\n",
      "Batch loss: 0.1328212320804596 batch: 106/224\n",
      "Batch loss: 0.08965469896793365 batch: 107/224\n",
      "Batch loss: 0.09303758293390274 batch: 108/224\n",
      "Batch loss: 0.06996886432170868 batch: 109/224\n",
      "Batch loss: 0.07943057268857956 batch: 110/224\n",
      "Batch loss: 0.08158979564905167 batch: 111/224\n",
      "Batch loss: 0.08691882342100143 batch: 112/224\n",
      "Batch loss: 0.08761222660541534 batch: 113/224\n",
      "Batch loss: 0.08082478493452072 batch: 114/224\n",
      "Batch loss: 0.07168003171682358 batch: 115/224\n",
      "Batch loss: 0.06610958278179169 batch: 116/224\n",
      "Batch loss: 0.06615763902664185 batch: 117/224\n",
      "Batch loss: 0.08641599863767624 batch: 118/224\n",
      "Batch loss: 0.0584745854139328 batch: 119/224\n",
      "Batch loss: 0.07715204358100891 batch: 120/224\n",
      "Batch loss: 0.08449597656726837 batch: 121/224\n",
      "Batch loss: 0.07863928377628326 batch: 122/224\n",
      "Batch loss: 0.0734151229262352 batch: 123/224\n",
      "Batch loss: 0.07298463582992554 batch: 124/224\n",
      "Batch loss: 0.10386344790458679 batch: 125/224\n",
      "Batch loss: 0.09142323583364487 batch: 126/224\n",
      "Batch loss: 0.15538519620895386 batch: 127/224\n",
      "Batch loss: 0.08828441798686981 batch: 128/224\n",
      "Batch loss: 0.09405757486820221 batch: 129/224\n",
      "Batch loss: 0.07344608008861542 batch: 130/224\n",
      "Batch loss: 0.047071538865566254 batch: 131/224\n",
      "Batch loss: 0.07889362424612045 batch: 132/224\n",
      "Batch loss: 0.11201126873493195 batch: 133/224\n",
      "Batch loss: 0.07625515758991241 batch: 134/224\n",
      "Batch loss: 0.10308888554573059 batch: 135/224\n",
      "Batch loss: 0.0962647870182991 batch: 136/224\n",
      "Batch loss: 0.07748009264469147 batch: 137/224\n",
      "Batch loss: 0.09209254384040833 batch: 138/224\n",
      "Batch loss: 0.08398068696260452 batch: 139/224\n",
      "Batch loss: 0.11805691570043564 batch: 140/224\n",
      "Batch loss: 0.05153772234916687 batch: 141/224\n",
      "Batch loss: 0.07137946784496307 batch: 142/224\n",
      "Batch loss: 0.07946047931909561 batch: 143/224\n",
      "Batch loss: 0.05037856847047806 batch: 144/224\n",
      "Batch loss: 0.07121177762746811 batch: 145/224\n",
      "Batch loss: 0.08878383040428162 batch: 146/224\n",
      "Batch loss: 0.09003552794456482 batch: 147/224\n",
      "Batch loss: 0.07498379796743393 batch: 148/224\n",
      "Batch loss: 0.10161591321229935 batch: 149/224\n",
      "Batch loss: 0.10478146374225616 batch: 150/224\n",
      "Batch loss: 0.061046112328767776 batch: 151/224\n",
      "Batch loss: 0.09541010111570358 batch: 152/224\n",
      "Batch loss: 0.09344061464071274 batch: 153/224\n",
      "Batch loss: 0.09314556419849396 batch: 154/224\n",
      "Batch loss: 0.05953871086239815 batch: 155/224\n",
      "Batch loss: 0.08652304857969284 batch: 156/224\n",
      "Batch loss: 0.08169248700141907 batch: 157/224\n",
      "Batch loss: 0.10812535881996155 batch: 158/224\n",
      "Batch loss: 0.058723125606775284 batch: 159/224\n",
      "Batch loss: 0.09433655440807343 batch: 160/224\n",
      "Batch loss: 0.05502377450466156 batch: 161/224\n",
      "Batch loss: 0.05229898542165756 batch: 162/224\n",
      "Batch loss: 0.06842156499624252 batch: 163/224\n",
      "Batch loss: 0.06714698672294617 batch: 164/224\n",
      "Batch loss: 0.10242187976837158 batch: 165/224\n",
      "Batch loss: 0.06446927785873413 batch: 166/224\n",
      "Batch loss: 0.059715382754802704 batch: 167/224\n",
      "Batch loss: 0.0748588889837265 batch: 168/224\n",
      "Batch loss: 0.0882226899266243 batch: 169/224\n",
      "Batch loss: 0.08068247884511948 batch: 170/224\n",
      "Batch loss: 0.052117157727479935 batch: 171/224\n",
      "Batch loss: 0.07107647508382797 batch: 172/224\n",
      "Batch loss: 0.09269065409898758 batch: 173/224\n",
      "Batch loss: 0.0688813254237175 batch: 174/224\n",
      "Batch loss: 0.0581139400601387 batch: 175/224\n",
      "Batch loss: 0.09989932179450989 batch: 176/224\n",
      "Batch loss: 0.07712622731924057 batch: 177/224\n",
      "Batch loss: 0.05456583574414253 batch: 178/224\n",
      "Batch loss: 0.12014634907245636 batch: 179/224\n",
      "Batch loss: 0.0491211898624897 batch: 180/224\n",
      "Batch loss: 0.10013381391763687 batch: 181/224\n",
      "Batch loss: 0.1030486673116684 batch: 182/224\n",
      "Batch loss: 0.0979473739862442 batch: 183/224\n",
      "Batch loss: 0.07767783850431442 batch: 184/224\n",
      "Batch loss: 0.09408392757177353 batch: 185/224\n",
      "Batch loss: 0.0657411515712738 batch: 186/224\n",
      "Batch loss: 0.09332391619682312 batch: 187/224\n",
      "Batch loss: 0.0715426504611969 batch: 188/224\n",
      "Batch loss: 0.08638203889131546 batch: 189/224\n",
      "Batch loss: 0.05563833937048912 batch: 190/224\n",
      "Batch loss: 0.09086054563522339 batch: 191/224\n",
      "Batch loss: 0.07634630799293518 batch: 192/224\n",
      "Batch loss: 0.08491523563861847 batch: 193/224\n",
      "Batch loss: 0.08693629503250122 batch: 194/224\n",
      "Batch loss: 0.09233441203832626 batch: 195/224\n",
      "Batch loss: 0.07069729268550873 batch: 196/224\n",
      "Batch loss: 0.08520346134901047 batch: 197/224\n",
      "Batch loss: 0.05415106564760208 batch: 198/224\n",
      "Batch loss: 0.049500081688165665 batch: 199/224\n",
      "Batch loss: 0.12430109083652496 batch: 200/224\n",
      "Batch loss: 0.10935839265584946 batch: 201/224\n",
      "Batch loss: 0.09157165884971619 batch: 202/224\n",
      "Batch loss: 0.07491888850927353 batch: 203/224\n",
      "Batch loss: 0.07278423011302948 batch: 204/224\n",
      "Batch loss: 0.10520737618207932 batch: 205/224\n",
      "Batch loss: 0.08592867106199265 batch: 206/224\n",
      "Batch loss: 0.11142991483211517 batch: 207/224\n",
      "Batch loss: 0.0699925348162651 batch: 208/224\n",
      "Batch loss: 0.11049854010343552 batch: 209/224\n",
      "Batch loss: 0.1001627966761589 batch: 210/224\n",
      "Batch loss: 0.057620830833911896 batch: 211/224\n",
      "Batch loss: 0.08679267764091492 batch: 212/224\n",
      "Batch loss: 0.11532893776893616 batch: 213/224\n",
      "Batch loss: 0.08804769068956375 batch: 214/224\n",
      "Batch loss: 0.11251276731491089 batch: 215/224\n",
      "Batch loss: 0.07915530353784561 batch: 216/224\n",
      "Batch loss: 0.11149699240922928 batch: 217/224\n",
      "Batch loss: 0.10012312978506088 batch: 218/224\n",
      "Batch loss: 0.07490940392017365 batch: 219/224\n",
      "Batch loss: 0.10706106573343277 batch: 220/224\n",
      "Batch loss: 0.1011652871966362 batch: 221/224\n",
      "Batch loss: 0.12734875082969666 batch: 222/224\n",
      "Batch loss: 0.09284806996583939 batch: 223/224\n",
      "Batch loss: 0.08057704567909241 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 50/75..  Training Loss: 0.00017..  Test Loss: 0.00096..  Test Accuracy: 0.89096\n",
      "Running epoch 51/75\n",
      "Batch loss: 0.07623614370822906 batch: 1/224\n",
      "Batch loss: 0.09697943925857544 batch: 2/224\n",
      "Batch loss: 0.09094122797250748 batch: 3/224\n",
      "Batch loss: 0.10423468798398972 batch: 4/224\n",
      "Batch loss: 0.09429026395082474 batch: 5/224\n",
      "Batch loss: 0.08252272009849548 batch: 6/224\n",
      "Batch loss: 0.07355696707963943 batch: 7/224\n",
      "Batch loss: 0.08148900419473648 batch: 8/224\n",
      "Batch loss: 0.06147698312997818 batch: 9/224\n",
      "Batch loss: 0.07026277482509613 batch: 10/224\n",
      "Batch loss: 0.09855619072914124 batch: 11/224\n",
      "Batch loss: 0.07899712026119232 batch: 12/224\n",
      "Batch loss: 0.04613066464662552 batch: 13/224\n",
      "Batch loss: 0.08347956091165543 batch: 14/224\n",
      "Batch loss: 0.07600487023591995 batch: 15/224\n",
      "Batch loss: 0.09377710521221161 batch: 16/224\n",
      "Batch loss: 0.07625814527273178 batch: 17/224\n",
      "Batch loss: 0.10442610085010529 batch: 18/224\n",
      "Batch loss: 0.05275150015950203 batch: 19/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.08356086909770966 batch: 20/224\n",
      "Batch loss: 0.08803952485322952 batch: 21/224\n",
      "Batch loss: 0.05087319016456604 batch: 22/224\n",
      "Batch loss: 0.08029723167419434 batch: 23/224\n",
      "Batch loss: 0.1078190729022026 batch: 24/224\n",
      "Batch loss: 0.09423400461673737 batch: 25/224\n",
      "Batch loss: 0.06693743169307709 batch: 26/224\n",
      "Batch loss: 0.09107453376054764 batch: 27/224\n",
      "Batch loss: 0.11120915412902832 batch: 28/224\n",
      "Batch loss: 0.0741562694311142 batch: 29/224\n",
      "Batch loss: 0.07392971217632294 batch: 30/224\n",
      "Batch loss: 0.0956813395023346 batch: 31/224\n",
      "Batch loss: 0.10335303843021393 batch: 32/224\n",
      "Batch loss: 0.07388682663440704 batch: 33/224\n",
      "Batch loss: 0.10652396827936172 batch: 34/224\n",
      "Batch loss: 0.09200520813465118 batch: 35/224\n",
      "Batch loss: 0.08695520460605621 batch: 36/224\n",
      "Batch loss: 0.09589249640703201 batch: 37/224\n",
      "Batch loss: 0.08668147772550583 batch: 38/224\n",
      "Batch loss: 0.09342103451490402 batch: 39/224\n",
      "Batch loss: 0.04892577975988388 batch: 40/224\n",
      "Batch loss: 0.08601508289575577 batch: 41/224\n",
      "Batch loss: 0.060972489416599274 batch: 42/224\n",
      "Batch loss: 0.09309250116348267 batch: 43/224\n",
      "Batch loss: 0.04925375431776047 batch: 44/224\n",
      "Batch loss: 0.06497713923454285 batch: 45/224\n",
      "Batch loss: 0.12492572516202927 batch: 46/224\n",
      "Batch loss: 0.07710999250411987 batch: 47/224\n",
      "Batch loss: 0.037091851234436035 batch: 48/224\n",
      "Batch loss: 0.08836047351360321 batch: 49/224\n",
      "Batch loss: 0.08716779202222824 batch: 50/224\n",
      "Batch loss: 0.07452935725450516 batch: 51/224\n",
      "Batch loss: 0.03829031065106392 batch: 52/224\n",
      "Batch loss: 0.08469996601343155 batch: 53/224\n",
      "Batch loss: 0.056679073721170425 batch: 54/224\n",
      "Batch loss: 0.04955355450510979 batch: 55/224\n",
      "Batch loss: 0.08023952692747116 batch: 56/224\n",
      "Batch loss: 0.11142901331186295 batch: 57/224\n",
      "Batch loss: 0.09027314931154251 batch: 58/224\n",
      "Batch loss: 0.07283742725849152 batch: 59/224\n",
      "Batch loss: 0.1152929738163948 batch: 60/224\n",
      "Batch loss: 0.11481939256191254 batch: 61/224\n",
      "Batch loss: 0.05363798141479492 batch: 62/224\n",
      "Batch loss: 0.06829866021871567 batch: 63/224\n",
      "Batch loss: 0.1128072440624237 batch: 64/224\n",
      "Batch loss: 0.08385167270898819 batch: 65/224\n",
      "Batch loss: 0.08337657898664474 batch: 66/224\n",
      "Batch loss: 0.057948846369981766 batch: 67/224\n",
      "Batch loss: 0.06699519604444504 batch: 68/224\n",
      "Batch loss: 0.09472274780273438 batch: 69/224\n",
      "Batch loss: 0.10179652273654938 batch: 70/224\n",
      "Batch loss: 0.09045039862394333 batch: 71/224\n",
      "Batch loss: 0.07828893512487411 batch: 72/224\n",
      "Batch loss: 0.07337026298046112 batch: 73/224\n",
      "Batch loss: 0.09741465002298355 batch: 74/224\n",
      "Batch loss: 0.09561310708522797 batch: 75/224\n",
      "Batch loss: 0.07119680941104889 batch: 76/224\n",
      "Batch loss: 0.09299332648515701 batch: 77/224\n",
      "Batch loss: 0.09744139760732651 batch: 78/224\n",
      "Batch loss: 0.09311755001544952 batch: 79/224\n",
      "Batch loss: 0.08979734778404236 batch: 80/224\n",
      "Batch loss: 0.09877375513315201 batch: 81/224\n",
      "Batch loss: 0.07882970571517944 batch: 82/224\n",
      "Batch loss: 0.09581644833087921 batch: 83/224\n",
      "Batch loss: 0.07932756096124649 batch: 84/224\n",
      "Batch loss: 0.08069666475057602 batch: 85/224\n",
      "Batch loss: 0.07012946903705597 batch: 86/224\n",
      "Batch loss: 0.08307810872793198 batch: 87/224\n",
      "Batch loss: 0.08318822085857391 batch: 88/224\n",
      "Batch loss: 0.07951521873474121 batch: 89/224\n",
      "Batch loss: 0.10557621717453003 batch: 90/224\n",
      "Batch loss: 0.06195138767361641 batch: 91/224\n",
      "Batch loss: 0.09870381653308868 batch: 92/224\n",
      "Batch loss: 0.08280938118696213 batch: 93/224\n",
      "Batch loss: 0.07280830293893814 batch: 94/224\n",
      "Batch loss: 0.04734015092253685 batch: 95/224\n",
      "Batch loss: 0.04522343724966049 batch: 96/224\n",
      "Batch loss: 0.057708047330379486 batch: 97/224\n",
      "Batch loss: 0.06996459513902664 batch: 98/224\n",
      "Batch loss: 0.07692402601242065 batch: 99/224\n",
      "Batch loss: 0.08284121751785278 batch: 100/224\n",
      "Batch loss: 0.11339861154556274 batch: 101/224\n",
      "Batch loss: 0.07537559419870377 batch: 102/224\n",
      "Batch loss: 0.10617556422948837 batch: 103/224\n",
      "Batch loss: 0.0629008486866951 batch: 104/224\n",
      "Batch loss: 0.09687880426645279 batch: 105/224\n",
      "Batch loss: 0.12198304384946823 batch: 106/224\n",
      "Batch loss: 0.06518178433179855 batch: 107/224\n",
      "Batch loss: 0.08658073097467422 batch: 108/224\n",
      "Batch loss: 0.052397772669792175 batch: 109/224\n",
      "Batch loss: 0.08627356588840485 batch: 110/224\n",
      "Batch loss: 0.09338613599538803 batch: 111/224\n",
      "Batch loss: 0.07895345240831375 batch: 112/224\n",
      "Batch loss: 0.06021011620759964 batch: 113/224\n",
      "Batch loss: 0.059113748371601105 batch: 114/224\n",
      "Batch loss: 0.08424972742795944 batch: 115/224\n",
      "Batch loss: 0.12167292833328247 batch: 116/224\n",
      "Batch loss: 0.07943817228078842 batch: 117/224\n",
      "Batch loss: 0.06095171719789505 batch: 118/224\n",
      "Batch loss: 0.07421641796827316 batch: 119/224\n",
      "Batch loss: 0.09186508506536484 batch: 120/224\n",
      "Batch loss: 0.07363958656787872 batch: 121/224\n",
      "Batch loss: 0.05812689661979675 batch: 122/224\n",
      "Batch loss: 0.05547098442912102 batch: 123/224\n",
      "Batch loss: 0.09465621411800385 batch: 124/224\n",
      "Batch loss: 0.07765951752662659 batch: 125/224\n",
      "Batch loss: 0.08678223937749863 batch: 126/224\n",
      "Batch loss: 0.13793066143989563 batch: 127/224\n",
      "Batch loss: 0.06318288296461105 batch: 128/224\n",
      "Batch loss: 0.08737047016620636 batch: 129/224\n",
      "Batch loss: 0.06320993602275848 batch: 130/224\n",
      "Batch loss: 0.06000232696533203 batch: 131/224\n",
      "Batch loss: 0.07244838029146194 batch: 132/224\n",
      "Batch loss: 0.06552115082740784 batch: 133/224\n",
      "Batch loss: 0.07957769930362701 batch: 134/224\n",
      "Batch loss: 0.10556279867887497 batch: 135/224\n",
      "Batch loss: 0.060925114899873734 batch: 136/224\n",
      "Batch loss: 0.07996895909309387 batch: 137/224\n",
      "Batch loss: 0.08040907979011536 batch: 138/224\n",
      "Batch loss: 0.0902465283870697 batch: 139/224\n",
      "Batch loss: 0.09219063073396683 batch: 140/224\n",
      "Batch loss: 0.08646855503320694 batch: 141/224\n",
      "Batch loss: 0.06972089409828186 batch: 142/224\n",
      "Batch loss: 0.0609896220266819 batch: 143/224\n",
      "Batch loss: 0.07736389338970184 batch: 144/224\n",
      "Batch loss: 0.09054067730903625 batch: 145/224\n",
      "Batch loss: 0.08424165099859238 batch: 146/224\n",
      "Batch loss: 0.10493094474077225 batch: 147/224\n",
      "Batch loss: 0.0879005417227745 batch: 148/224\n",
      "Batch loss: 0.09302635490894318 batch: 149/224\n",
      "Batch loss: 0.13255560398101807 batch: 150/224\n",
      "Batch loss: 0.07050605118274689 batch: 151/224\n",
      "Batch loss: 0.09473522007465363 batch: 152/224\n",
      "Batch loss: 0.10267220437526703 batch: 153/224\n",
      "Batch loss: 0.0909111350774765 batch: 154/224\n",
      "Batch loss: 0.06724126636981964 batch: 155/224\n",
      "Batch loss: 0.08883136510848999 batch: 156/224\n",
      "Batch loss: 0.10908623784780502 batch: 157/224\n",
      "Batch loss: 0.08789157122373581 batch: 158/224\n",
      "Batch loss: 0.11298638582229614 batch: 159/224\n",
      "Batch loss: 0.09008768200874329 batch: 160/224\n",
      "Batch loss: 0.08271299302577972 batch: 161/224\n",
      "Batch loss: 0.06089862808585167 batch: 162/224\n",
      "Batch loss: 0.06885138154029846 batch: 163/224\n",
      "Batch loss: 0.07089880853891373 batch: 164/224\n",
      "Batch loss: 0.15993660688400269 batch: 165/224\n",
      "Batch loss: 0.09246345609426498 batch: 166/224\n",
      "Batch loss: 0.05674730986356735 batch: 167/224\n",
      "Batch loss: 0.04337722063064575 batch: 168/224\n",
      "Batch loss: 0.08385719358921051 batch: 169/224\n",
      "Batch loss: 0.0628686398267746 batch: 170/224\n",
      "Batch loss: 0.06907579302787781 batch: 171/224\n",
      "Batch loss: 0.08348949998617172 batch: 172/224\n",
      "Batch loss: 0.10582538694143295 batch: 173/224\n",
      "Batch loss: 0.05283599719405174 batch: 174/224\n",
      "Batch loss: 0.11065507680177689 batch: 175/224\n",
      "Batch loss: 0.07493087649345398 batch: 176/224\n",
      "Batch loss: 0.10106872022151947 batch: 177/224\n",
      "Batch loss: 0.07192772626876831 batch: 178/224\n",
      "Batch loss: 0.08583639562129974 batch: 179/224\n",
      "Batch loss: 0.07957372814416885 batch: 180/224\n",
      "Batch loss: 0.051947906613349915 batch: 181/224\n",
      "Batch loss: 0.10534099489450455 batch: 182/224\n",
      "Batch loss: 0.10377532243728638 batch: 183/224\n",
      "Batch loss: 0.06969588994979858 batch: 184/224\n",
      "Batch loss: 0.07859419286251068 batch: 185/224\n",
      "Batch loss: 0.08773259073495865 batch: 186/224\n",
      "Batch loss: 0.10433702915906906 batch: 187/224\n",
      "Batch loss: 0.05936666578054428 batch: 188/224\n",
      "Batch loss: 0.06204727664589882 batch: 189/224\n",
      "Batch loss: 0.06412850320339203 batch: 190/224\n",
      "Batch loss: 0.06533274799585342 batch: 191/224\n",
      "Batch loss: 0.07154079526662827 batch: 192/224\n",
      "Batch loss: 0.08967391401529312 batch: 193/224\n",
      "Batch loss: 0.08986406028270721 batch: 194/224\n",
      "Batch loss: 0.11736226826906204 batch: 195/224\n",
      "Batch loss: 0.09431007504463196 batch: 196/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.06750527769327164 batch: 197/224\n",
      "Batch loss: 0.05331478640437126 batch: 198/224\n",
      "Batch loss: 0.05655152350664139 batch: 199/224\n",
      "Batch loss: 0.10094031691551208 batch: 200/224\n",
      "Batch loss: 0.10827086120843887 batch: 201/224\n",
      "Batch loss: 0.10278578847646713 batch: 202/224\n",
      "Batch loss: 0.09490853548049927 batch: 203/224\n",
      "Batch loss: 0.09896445274353027 batch: 204/224\n",
      "Batch loss: 0.07553859055042267 batch: 205/224\n",
      "Batch loss: 0.0814729556441307 batch: 206/224\n",
      "Batch loss: 0.07250666618347168 batch: 207/224\n",
      "Batch loss: 0.06841012090444565 batch: 208/224\n",
      "Batch loss: 0.07313376665115356 batch: 209/224\n",
      "Batch loss: 0.0726497545838356 batch: 210/224\n",
      "Batch loss: 0.07350464910268784 batch: 211/224\n",
      "Batch loss: 0.09715846925973892 batch: 212/224\n",
      "Batch loss: 0.10638866573572159 batch: 213/224\n",
      "Batch loss: 0.10268744081258774 batch: 214/224\n",
      "Batch loss: 0.10133810341358185 batch: 215/224\n",
      "Batch loss: 0.07493450492620468 batch: 216/224\n",
      "Batch loss: 0.0796971544623375 batch: 217/224\n",
      "Batch loss: 0.08794750273227692 batch: 218/224\n",
      "Batch loss: 0.07204519957304001 batch: 219/224\n",
      "Batch loss: 0.1046169102191925 batch: 220/224\n",
      "Batch loss: 0.08643794059753418 batch: 221/224\n",
      "Batch loss: 0.08540461212396622 batch: 222/224\n",
      "Batch loss: 0.07235214859247208 batch: 223/224\n",
      "Batch loss: 0.08591017127037048 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 51/75..  Training Loss: 0.00017..  Test Loss: 0.00096..  Test Accuracy: 0.89157\n",
      "Running epoch 52/75\n",
      "Batch loss: 0.060798224061727524 batch: 1/224\n",
      "Batch loss: 0.08149494975805283 batch: 2/224\n",
      "Batch loss: 0.07624763250350952 batch: 3/224\n",
      "Batch loss: 0.10952819883823395 batch: 4/224\n",
      "Batch loss: 0.07757670432329178 batch: 5/224\n",
      "Batch loss: 0.07165259122848511 batch: 6/224\n",
      "Batch loss: 0.07246041297912598 batch: 7/224\n",
      "Batch loss: 0.0660262256860733 batch: 8/224\n",
      "Batch loss: 0.05201035737991333 batch: 9/224\n",
      "Batch loss: 0.0744008794426918 batch: 10/224\n",
      "Batch loss: 0.08185914158821106 batch: 11/224\n",
      "Batch loss: 0.07722894102334976 batch: 12/224\n",
      "Batch loss: 0.07552144676446915 batch: 13/224\n",
      "Batch loss: 0.07812554389238358 batch: 14/224\n",
      "Batch loss: 0.07985109090805054 batch: 15/224\n",
      "Batch loss: 0.10047711431980133 batch: 16/224\n",
      "Batch loss: 0.08571138978004456 batch: 17/224\n",
      "Batch loss: 0.07320670783519745 batch: 18/224\n",
      "Batch loss: 0.08224467933177948 batch: 19/224\n",
      "Batch loss: 0.0930556133389473 batch: 20/224\n",
      "Batch loss: 0.07592006027698517 batch: 21/224\n",
      "Batch loss: 0.10072890669107437 batch: 22/224\n",
      "Batch loss: 0.11174295097589493 batch: 23/224\n",
      "Batch loss: 0.1061612144112587 batch: 24/224\n",
      "Batch loss: 0.06003153696656227 batch: 25/224\n",
      "Batch loss: 0.061269450932741165 batch: 26/224\n",
      "Batch loss: 0.06702370941638947 batch: 27/224\n",
      "Batch loss: 0.07342246919870377 batch: 28/224\n",
      "Batch loss: 0.08697390556335449 batch: 29/224\n",
      "Batch loss: 0.06286624819040298 batch: 30/224\n",
      "Batch loss: 0.06834235042333603 batch: 31/224\n",
      "Batch loss: 0.07451823353767395 batch: 32/224\n",
      "Batch loss: 0.0638296902179718 batch: 33/224\n",
      "Batch loss: 0.07389874011278152 batch: 34/224\n",
      "Batch loss: 0.10914934426546097 batch: 35/224\n",
      "Batch loss: 0.08095085620880127 batch: 36/224\n",
      "Batch loss: 0.10315723717212677 batch: 37/224\n",
      "Batch loss: 0.12272705137729645 batch: 38/224\n",
      "Batch loss: 0.08250600099563599 batch: 39/224\n",
      "Batch loss: 0.06469923257827759 batch: 40/224\n",
      "Batch loss: 0.06916364282369614 batch: 41/224\n",
      "Batch loss: 0.06451110541820526 batch: 42/224\n",
      "Batch loss: 0.12786810100078583 batch: 43/224\n",
      "Batch loss: 0.08839385211467743 batch: 44/224\n",
      "Batch loss: 0.07305313646793365 batch: 45/224\n",
      "Batch loss: 0.07757066190242767 batch: 46/224\n",
      "Batch loss: 0.07978826761245728 batch: 47/224\n",
      "Batch loss: 0.12018682807683945 batch: 48/224\n",
      "Batch loss: 0.0628165602684021 batch: 49/224\n",
      "Batch loss: 0.05318388715386391 batch: 50/224\n",
      "Batch loss: 0.07786709070205688 batch: 51/224\n",
      "Batch loss: 0.06904949992895126 batch: 52/224\n",
      "Batch loss: 0.1167931854724884 batch: 53/224\n",
      "Batch loss: 0.06288160383701324 batch: 54/224\n",
      "Batch loss: 0.05127313360571861 batch: 55/224\n",
      "Batch loss: 0.05857452377676964 batch: 56/224\n",
      "Batch loss: 0.10526860505342484 batch: 57/224\n",
      "Batch loss: 0.09899605065584183 batch: 58/224\n",
      "Batch loss: 0.07067035138607025 batch: 59/224\n",
      "Batch loss: 0.12502343952655792 batch: 60/224\n",
      "Batch loss: 0.07122721523046494 batch: 61/224\n",
      "Batch loss: 0.05513811483979225 batch: 62/224\n",
      "Batch loss: 0.06144191697239876 batch: 63/224\n",
      "Batch loss: 0.09315875917673111 batch: 64/224\n",
      "Batch loss: 0.060960885137319565 batch: 65/224\n",
      "Batch loss: 0.10078370571136475 batch: 66/224\n",
      "Batch loss: 0.0805920958518982 batch: 67/224\n",
      "Batch loss: 0.059316013008356094 batch: 68/224\n",
      "Batch loss: 0.08027134835720062 batch: 69/224\n",
      "Batch loss: 0.09576679766178131 batch: 70/224\n",
      "Batch loss: 0.06587470322847366 batch: 71/224\n",
      "Batch loss: 0.06909236311912537 batch: 72/224\n",
      "Batch loss: 0.10356147587299347 batch: 73/224\n",
      "Batch loss: 0.07760093361139297 batch: 74/224\n",
      "Batch loss: 0.09653254598379135 batch: 75/224\n",
      "Batch loss: 0.1026788055896759 batch: 76/224\n",
      "Batch loss: 0.08937136083841324 batch: 77/224\n",
      "Batch loss: 0.0750725120306015 batch: 78/224\n",
      "Batch loss: 0.08496503531932831 batch: 79/224\n",
      "Batch loss: 0.047701217234134674 batch: 80/224\n",
      "Batch loss: 0.07909200340509415 batch: 81/224\n",
      "Batch loss: 0.09913402795791626 batch: 82/224\n",
      "Batch loss: 0.10237939655780792 batch: 83/224\n",
      "Batch loss: 0.06111380457878113 batch: 84/224\n",
      "Batch loss: 0.08654056489467621 batch: 85/224\n",
      "Batch loss: 0.08352595567703247 batch: 86/224\n",
      "Batch loss: 0.08537126332521439 batch: 87/224\n",
      "Batch loss: 0.0838242694735527 batch: 88/224\n",
      "Batch loss: 0.057183556258678436 batch: 89/224\n",
      "Batch loss: 0.07906794548034668 batch: 90/224\n",
      "Batch loss: 0.04767885431647301 batch: 91/224\n",
      "Batch loss: 0.07979416847229004 batch: 92/224\n",
      "Batch loss: 0.056671399623155594 batch: 93/224\n",
      "Batch loss: 0.05853434279561043 batch: 94/224\n",
      "Batch loss: 0.05518125742673874 batch: 95/224\n",
      "Batch loss: 0.07479628175497055 batch: 96/224\n",
      "Batch loss: 0.06328542530536652 batch: 97/224\n",
      "Batch loss: 0.04855461046099663 batch: 98/224\n",
      "Batch loss: 0.14318586885929108 batch: 99/224\n",
      "Batch loss: 0.0608951598405838 batch: 100/224\n",
      "Batch loss: 0.08661141991615295 batch: 101/224\n",
      "Batch loss: 0.10801766812801361 batch: 102/224\n",
      "Batch loss: 0.08296313881874084 batch: 103/224\n",
      "Batch loss: 0.03744377940893173 batch: 104/224\n",
      "Batch loss: 0.05139579996466637 batch: 105/224\n",
      "Batch loss: 0.08673759549856186 batch: 106/224\n",
      "Batch loss: 0.07005190849304199 batch: 107/224\n",
      "Batch loss: 0.08826971799135208 batch: 108/224\n",
      "Batch loss: 0.06894690543413162 batch: 109/224\n",
      "Batch loss: 0.0941503718495369 batch: 110/224\n",
      "Batch loss: 0.06706426292657852 batch: 111/224\n",
      "Batch loss: 0.0840640664100647 batch: 112/224\n",
      "Batch loss: 0.05552902817726135 batch: 113/224\n",
      "Batch loss: 0.05659330636262894 batch: 114/224\n",
      "Batch loss: 0.09063771367073059 batch: 115/224\n",
      "Batch loss: 0.08254045993089676 batch: 116/224\n",
      "Batch loss: 0.05664214491844177 batch: 117/224\n",
      "Batch loss: 0.06197592243552208 batch: 118/224\n",
      "Batch loss: 0.06971069425344467 batch: 119/224\n",
      "Batch loss: 0.07255706191062927 batch: 120/224\n",
      "Batch loss: 0.09275074303150177 batch: 121/224\n",
      "Batch loss: 0.0953168049454689 batch: 122/224\n",
      "Batch loss: 0.06622567027807236 batch: 123/224\n",
      "Batch loss: 0.06311008334159851 batch: 124/224\n",
      "Batch loss: 0.08746092766523361 batch: 125/224\n",
      "Batch loss: 0.09448093920946121 batch: 126/224\n",
      "Batch loss: 0.10117167234420776 batch: 127/224\n",
      "Batch loss: 0.06299740821123123 batch: 128/224\n",
      "Batch loss: 0.06086841598153114 batch: 129/224\n",
      "Batch loss: 0.0718599408864975 batch: 130/224\n",
      "Batch loss: 0.05993269756436348 batch: 131/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.08865100145339966 batch: 132/224\n",
      "Batch loss: 0.08145681023597717 batch: 133/224\n",
      "Batch loss: 0.07717675715684891 batch: 134/224\n",
      "Batch loss: 0.07586205005645752 batch: 135/224\n",
      "Batch loss: 0.0761617049574852 batch: 136/224\n",
      "Batch loss: 0.064505435526371 batch: 137/224\n",
      "Batch loss: 0.10671166330575943 batch: 138/224\n",
      "Batch loss: 0.0966246947646141 batch: 139/224\n",
      "Batch loss: 0.09076409786939621 batch: 140/224\n",
      "Batch loss: 0.06158909574151039 batch: 141/224\n",
      "Batch loss: 0.07579705864191055 batch: 142/224\n",
      "Batch loss: 0.09891511499881744 batch: 143/224\n",
      "Batch loss: 0.07447794824838638 batch: 144/224\n",
      "Batch loss: 0.0527314729988575 batch: 145/224\n",
      "Batch loss: 0.07337657362222672 batch: 146/224\n",
      "Batch loss: 0.0735069289803505 batch: 147/224\n",
      "Batch loss: 0.07161115109920502 batch: 148/224\n",
      "Batch loss: 0.10055235773324966 batch: 149/224\n",
      "Batch loss: 0.08357945829629898 batch: 150/224\n",
      "Batch loss: 0.06303942203521729 batch: 151/224\n",
      "Batch loss: 0.07933930307626724 batch: 152/224\n",
      "Batch loss: 0.08375459909439087 batch: 153/224\n",
      "Batch loss: 0.06455664336681366 batch: 154/224\n",
      "Batch loss: 0.07163006067276001 batch: 155/224\n",
      "Batch loss: 0.04747168347239494 batch: 156/224\n",
      "Batch loss: 0.09931441396474838 batch: 157/224\n",
      "Batch loss: 0.12247727811336517 batch: 158/224\n",
      "Batch loss: 0.08834268897771835 batch: 159/224\n",
      "Batch loss: 0.05577167123556137 batch: 160/224\n",
      "Batch loss: 0.044207267463207245 batch: 161/224\n",
      "Batch loss: 0.04042369872331619 batch: 162/224\n",
      "Batch loss: 0.08082243800163269 batch: 163/224\n",
      "Batch loss: 0.040707867592573166 batch: 164/224\n",
      "Batch loss: 0.10581114143133163 batch: 165/224\n",
      "Batch loss: 0.07436295598745346 batch: 166/224\n",
      "Batch loss: 0.06077084317803383 batch: 167/224\n",
      "Batch loss: 0.0717732384800911 batch: 168/224\n",
      "Batch loss: 0.10521087795495987 batch: 169/224\n",
      "Batch loss: 0.06751207262277603 batch: 170/224\n",
      "Batch loss: 0.04001926630735397 batch: 171/224\n",
      "Batch loss: 0.08276078850030899 batch: 172/224\n",
      "Batch loss: 0.09000809490680695 batch: 173/224\n",
      "Batch loss: 0.06608135998249054 batch: 174/224\n",
      "Batch loss: 0.05563344061374664 batch: 175/224\n",
      "Batch loss: 0.06443686783313751 batch: 176/224\n",
      "Batch loss: 0.09570587426424026 batch: 177/224\n",
      "Batch loss: 0.048392247408628464 batch: 178/224\n",
      "Batch loss: 0.10150984674692154 batch: 179/224\n",
      "Batch loss: 0.06542322784662247 batch: 180/224\n",
      "Batch loss: 0.0872955173254013 batch: 181/224\n",
      "Batch loss: 0.08506859093904495 batch: 182/224\n",
      "Batch loss: 0.09369880706071854 batch: 183/224\n",
      "Batch loss: 0.06848327815532684 batch: 184/224\n",
      "Batch loss: 0.06776035577058792 batch: 185/224\n",
      "Batch loss: 0.06306400895118713 batch: 186/224\n",
      "Batch loss: 0.0654723197221756 batch: 187/224\n",
      "Batch loss: 0.0652361735701561 batch: 188/224\n",
      "Batch loss: 0.06335902214050293 batch: 189/224\n",
      "Batch loss: 0.07208283245563507 batch: 190/224\n",
      "Batch loss: 0.08519618213176727 batch: 191/224\n",
      "Batch loss: 0.0670623928308487 batch: 192/224\n",
      "Batch loss: 0.1223093643784523 batch: 193/224\n",
      "Batch loss: 0.09035150706768036 batch: 194/224\n",
      "Batch loss: 0.08576298505067825 batch: 195/224\n",
      "Batch loss: 0.09125518053770065 batch: 196/224\n",
      "Batch loss: 0.06618043780326843 batch: 197/224\n",
      "Batch loss: 0.07702184468507767 batch: 198/224\n",
      "Batch loss: 0.08143272995948792 batch: 199/224\n",
      "Batch loss: 0.10017964988946915 batch: 200/224\n",
      "Batch loss: 0.08124959468841553 batch: 201/224\n",
      "Batch loss: 0.10647446662187576 batch: 202/224\n",
      "Batch loss: 0.11865531653165817 batch: 203/224\n",
      "Batch loss: 0.05320071429014206 batch: 204/224\n",
      "Batch loss: 0.09595228731632233 batch: 205/224\n",
      "Batch loss: 0.08315533399581909 batch: 206/224\n",
      "Batch loss: 0.09475617110729218 batch: 207/224\n",
      "Batch loss: 0.08074229955673218 batch: 208/224\n",
      "Batch loss: 0.06454832851886749 batch: 209/224\n",
      "Batch loss: 0.07406933605670929 batch: 210/224\n",
      "Batch loss: 0.07858339697122574 batch: 211/224\n",
      "Batch loss: 0.09624191373586655 batch: 212/224\n",
      "Batch loss: 0.11523415893316269 batch: 213/224\n",
      "Batch loss: 0.0788530707359314 batch: 214/224\n",
      "Batch loss: 0.07662598788738251 batch: 215/224\n",
      "Batch loss: 0.06420633941888809 batch: 216/224\n",
      "Batch loss: 0.09985185414552689 batch: 217/224\n",
      "Batch loss: 0.06981077045202255 batch: 218/224\n",
      "Batch loss: 0.059821829199790955 batch: 219/224\n",
      "Batch loss: 0.10224640369415283 batch: 220/224\n",
      "Batch loss: 0.11801974475383759 batch: 221/224\n",
      "Batch loss: 0.09745984524488449 batch: 222/224\n",
      "Batch loss: 0.0607999823987484 batch: 223/224\n",
      "Batch loss: 0.0733494684100151 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 52/75..  Training Loss: 0.00016..  Test Loss: 0.00094..  Test Accuracy: 0.89111\n",
      "Running epoch 53/75\n",
      "Batch loss: 0.05909579247236252 batch: 1/224\n",
      "Batch loss: 0.07698369026184082 batch: 2/224\n",
      "Batch loss: 0.05993599072098732 batch: 3/224\n",
      "Batch loss: 0.09403704851865768 batch: 4/224\n",
      "Batch loss: 0.09043136239051819 batch: 5/224\n",
      "Batch loss: 0.06527424603700638 batch: 6/224\n",
      "Batch loss: 0.08694402128458023 batch: 7/224\n",
      "Batch loss: 0.08513031154870987 batch: 8/224\n",
      "Batch loss: 0.09015760570764542 batch: 9/224\n",
      "Batch loss: 0.07818765938282013 batch: 10/224\n",
      "Batch loss: 0.11150047928094864 batch: 11/224\n",
      "Batch loss: 0.09409060329198837 batch: 12/224\n",
      "Batch loss: 0.061084624379873276 batch: 13/224\n",
      "Batch loss: 0.06974799931049347 batch: 14/224\n",
      "Batch loss: 0.06816505640745163 batch: 15/224\n",
      "Batch loss: 0.09732092171907425 batch: 16/224\n",
      "Batch loss: 0.06586234271526337 batch: 17/224\n",
      "Batch loss: 0.06957261264324188 batch: 18/224\n",
      "Batch loss: 0.07730415463447571 batch: 19/224\n",
      "Batch loss: 0.07849942892789841 batch: 20/224\n",
      "Batch loss: 0.05437805876135826 batch: 21/224\n",
      "Batch loss: 0.07431042939424515 batch: 22/224\n",
      "Batch loss: 0.0848810002207756 batch: 23/224\n",
      "Batch loss: 0.09398990869522095 batch: 24/224\n",
      "Batch loss: 0.07719997316598892 batch: 25/224\n",
      "Batch loss: 0.06377269327640533 batch: 26/224\n",
      "Batch loss: 0.09171348810195923 batch: 27/224\n",
      "Batch loss: 0.08162089437246323 batch: 28/224\n",
      "Batch loss: 0.1009024903178215 batch: 29/224\n",
      "Batch loss: 0.063783660531044 batch: 30/224\n",
      "Batch loss: 0.055826444178819656 batch: 31/224\n",
      "Batch loss: 0.06779098510742188 batch: 32/224\n",
      "Batch loss: 0.04382792487740517 batch: 33/224\n",
      "Batch loss: 0.07413045316934586 batch: 34/224\n",
      "Batch loss: 0.05821410194039345 batch: 35/224\n",
      "Batch loss: 0.05845106765627861 batch: 36/224\n",
      "Batch loss: 0.07963235676288605 batch: 37/224\n",
      "Batch loss: 0.09304738789796829 batch: 38/224\n",
      "Batch loss: 0.09069286286830902 batch: 39/224\n",
      "Batch loss: 0.07450101524591446 batch: 40/224\n",
      "Batch loss: 0.09165677428245544 batch: 41/224\n",
      "Batch loss: 0.060313355177640915 batch: 42/224\n",
      "Batch loss: 0.07774148881435394 batch: 43/224\n",
      "Batch loss: 0.08031745254993439 batch: 44/224\n",
      "Batch loss: 0.04868793487548828 batch: 45/224\n",
      "Batch loss: 0.09722006320953369 batch: 46/224\n",
      "Batch loss: 0.06456667184829712 batch: 47/224\n",
      "Batch loss: 0.0891958475112915 batch: 48/224\n",
      "Batch loss: 0.05375097319483757 batch: 49/224\n",
      "Batch loss: 0.08917643129825592 batch: 50/224\n",
      "Batch loss: 0.06833821535110474 batch: 51/224\n",
      "Batch loss: 0.06497567892074585 batch: 52/224\n",
      "Batch loss: 0.07085631042718887 batch: 53/224\n",
      "Batch loss: 0.04857328534126282 batch: 54/224\n",
      "Batch loss: 0.053146425634622574 batch: 55/224\n",
      "Batch loss: 0.06922305375337601 batch: 56/224\n",
      "Batch loss: 0.10952749848365784 batch: 57/224\n",
      "Batch loss: 0.08885510265827179 batch: 58/224\n",
      "Batch loss: 0.08974916487932205 batch: 59/224\n",
      "Batch loss: 0.07103879749774933 batch: 60/224\n",
      "Batch loss: 0.06575988233089447 batch: 61/224\n",
      "Batch loss: 0.04949349910020828 batch: 62/224\n",
      "Batch loss: 0.10362007468938828 batch: 63/224\n",
      "Batch loss: 0.07710453867912292 batch: 64/224\n",
      "Batch loss: 0.11899888515472412 batch: 65/224\n",
      "Batch loss: 0.08206391334533691 batch: 66/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.08125801384449005 batch: 67/224\n",
      "Batch loss: 0.06912506371736526 batch: 68/224\n",
      "Batch loss: 0.05152461305260658 batch: 69/224\n",
      "Batch loss: 0.08732622861862183 batch: 70/224\n",
      "Batch loss: 0.07037666440010071 batch: 71/224\n",
      "Batch loss: 0.07653383165597916 batch: 72/224\n",
      "Batch loss: 0.09300112724304199 batch: 73/224\n",
      "Batch loss: 0.06764252483844757 batch: 74/224\n",
      "Batch loss: 0.07369827479124069 batch: 75/224\n",
      "Batch loss: 0.09020726382732391 batch: 76/224\n",
      "Batch loss: 0.07437071949243546 batch: 77/224\n",
      "Batch loss: 0.07449381053447723 batch: 78/224\n",
      "Batch loss: 0.07897508889436722 batch: 79/224\n",
      "Batch loss: 0.08979416638612747 batch: 80/224\n",
      "Batch loss: 0.10944347828626633 batch: 81/224\n",
      "Batch loss: 0.10143978148698807 batch: 82/224\n",
      "Batch loss: 0.057909224182367325 batch: 83/224\n",
      "Batch loss: 0.05507788062095642 batch: 84/224\n",
      "Batch loss: 0.11619006097316742 batch: 85/224\n",
      "Batch loss: 0.07819554209709167 batch: 86/224\n",
      "Batch loss: 0.05445954576134682 batch: 87/224\n",
      "Batch loss: 0.07061094045639038 batch: 88/224\n",
      "Batch loss: 0.0640140250325203 batch: 89/224\n",
      "Batch loss: 0.06823477149009705 batch: 90/224\n",
      "Batch loss: 0.05453348904848099 batch: 91/224\n",
      "Batch loss: 0.09819678217172623 batch: 92/224\n",
      "Batch loss: 0.07353749871253967 batch: 93/224\n",
      "Batch loss: 0.08057822287082672 batch: 94/224\n",
      "Batch loss: 0.06906210631132126 batch: 95/224\n",
      "Batch loss: 0.08470273017883301 batch: 96/224\n",
      "Batch loss: 0.05879460647702217 batch: 97/224\n",
      "Batch loss: 0.05707626789808273 batch: 98/224\n",
      "Batch loss: 0.06867715716362 batch: 99/224\n",
      "Batch loss: 0.07178666442632675 batch: 100/224\n",
      "Batch loss: 0.08491519838571548 batch: 101/224\n",
      "Batch loss: 0.08206211030483246 batch: 102/224\n",
      "Batch loss: 0.08574232459068298 batch: 103/224\n",
      "Batch loss: 0.08262462168931961 batch: 104/224\n",
      "Batch loss: 0.06843268126249313 batch: 105/224\n",
      "Batch loss: 0.08557426929473877 batch: 106/224\n",
      "Batch loss: 0.07304777950048447 batch: 107/224\n",
      "Batch loss: 0.06309844553470612 batch: 108/224\n",
      "Batch loss: 0.05314819514751434 batch: 109/224\n",
      "Batch loss: 0.09138303250074387 batch: 110/224\n",
      "Batch loss: 0.06257329881191254 batch: 111/224\n",
      "Batch loss: 0.05919860675930977 batch: 112/224\n",
      "Batch loss: 0.05415644869208336 batch: 113/224\n",
      "Batch loss: 0.0644604042172432 batch: 114/224\n",
      "Batch loss: 0.08090796321630478 batch: 115/224\n",
      "Batch loss: 0.06649967283010483 batch: 116/224\n",
      "Batch loss: 0.031960081309080124 batch: 117/224\n",
      "Batch loss: 0.07573917508125305 batch: 118/224\n",
      "Batch loss: 0.0601714588701725 batch: 119/224\n",
      "Batch loss: 0.07425191253423691 batch: 120/224\n",
      "Batch loss: 0.061877887696027756 batch: 121/224\n",
      "Batch loss: 0.043912291526794434 batch: 122/224\n",
      "Batch loss: 0.07174542546272278 batch: 123/224\n",
      "Batch loss: 0.06502023339271545 batch: 124/224\n",
      "Batch loss: 0.10083252936601639 batch: 125/224\n",
      "Batch loss: 0.06316472589969635 batch: 126/224\n",
      "Batch loss: 0.11713762581348419 batch: 127/224\n",
      "Batch loss: 0.06385210156440735 batch: 128/224\n",
      "Batch loss: 0.048596590757369995 batch: 129/224\n",
      "Batch loss: 0.07926168292760849 batch: 130/224\n",
      "Batch loss: 0.07308406382799149 batch: 131/224\n",
      "Batch loss: 0.06777285784482956 batch: 132/224\n",
      "Batch loss: 0.08103074878454208 batch: 133/224\n",
      "Batch loss: 0.09123999625444412 batch: 134/224\n",
      "Batch loss: 0.08560837060213089 batch: 135/224\n",
      "Batch loss: 0.08275023102760315 batch: 136/224\n",
      "Batch loss: 0.04798978567123413 batch: 137/224\n",
      "Batch loss: 0.06283940374851227 batch: 138/224\n",
      "Batch loss: 0.07192802429199219 batch: 139/224\n",
      "Batch loss: 0.09287026524543762 batch: 140/224\n",
      "Batch loss: 0.07976946979761124 batch: 141/224\n",
      "Batch loss: 0.07660166919231415 batch: 142/224\n",
      "Batch loss: 0.05970901623368263 batch: 143/224\n",
      "Batch loss: 0.10623148828744888 batch: 144/224\n",
      "Batch loss: 0.0940384566783905 batch: 145/224\n",
      "Batch loss: 0.08793539553880692 batch: 146/224\n",
      "Batch loss: 0.07557971775531769 batch: 147/224\n",
      "Batch loss: 0.06302783638238907 batch: 148/224\n",
      "Batch loss: 0.1245701014995575 batch: 149/224\n",
      "Batch loss: 0.0812646746635437 batch: 150/224\n",
      "Batch loss: 0.08525918424129486 batch: 151/224\n",
      "Batch loss: 0.06513560563325882 batch: 152/224\n",
      "Batch loss: 0.0896463468670845 batch: 153/224\n",
      "Batch loss: 0.08350289613008499 batch: 154/224\n",
      "Batch loss: 0.08714994043111801 batch: 155/224\n",
      "Batch loss: 0.05844321474432945 batch: 156/224\n",
      "Batch loss: 0.05657125636935234 batch: 157/224\n",
      "Batch loss: 0.12788262963294983 batch: 158/224\n",
      "Batch loss: 0.07435264438390732 batch: 159/224\n",
      "Batch loss: 0.05971430614590645 batch: 160/224\n",
      "Batch loss: 0.0774805098772049 batch: 161/224\n",
      "Batch loss: 0.07113444060087204 batch: 162/224\n",
      "Batch loss: 0.053032249212265015 batch: 163/224\n",
      "Batch loss: 0.08227649331092834 batch: 164/224\n",
      "Batch loss: 0.09218013286590576 batch: 165/224\n",
      "Batch loss: 0.09590935707092285 batch: 166/224\n",
      "Batch loss: 0.07146752625703812 batch: 167/224\n",
      "Batch loss: 0.09700541943311691 batch: 168/224\n",
      "Batch loss: 0.08655475825071335 batch: 169/224\n",
      "Batch loss: 0.09083493798971176 batch: 170/224\n",
      "Batch loss: 0.06963685899972916 batch: 171/224\n",
      "Batch loss: 0.07175398617982864 batch: 172/224\n",
      "Batch loss: 0.07108188420534134 batch: 173/224\n",
      "Batch loss: 0.06115347146987915 batch: 174/224\n",
      "Batch loss: 0.07168170064687729 batch: 175/224\n",
      "Batch loss: 0.07453292608261108 batch: 176/224\n",
      "Batch loss: 0.061794254928827286 batch: 177/224\n",
      "Batch loss: 0.06138234585523605 batch: 178/224\n",
      "Batch loss: 0.07807260751724243 batch: 179/224\n",
      "Batch loss: 0.05855609476566315 batch: 180/224\n",
      "Batch loss: 0.05365896597504616 batch: 181/224\n",
      "Batch loss: 0.10659459233283997 batch: 182/224\n",
      "Batch loss: 0.09354773908853531 batch: 183/224\n",
      "Batch loss: 0.08783097565174103 batch: 184/224\n",
      "Batch loss: 0.07451417297124863 batch: 185/224\n",
      "Batch loss: 0.08306366205215454 batch: 186/224\n",
      "Batch loss: 0.07657523453235626 batch: 187/224\n",
      "Batch loss: 0.06015116721391678 batch: 188/224\n",
      "Batch loss: 0.09719821810722351 batch: 189/224\n",
      "Batch loss: 0.05682983621954918 batch: 190/224\n",
      "Batch loss: 0.04844295606017113 batch: 191/224\n",
      "Batch loss: 0.07818987965583801 batch: 192/224\n",
      "Batch loss: 0.06871609389781952 batch: 193/224\n",
      "Batch loss: 0.0681031122803688 batch: 194/224\n",
      "Batch loss: 0.0766439363360405 batch: 195/224\n",
      "Batch loss: 0.08864998072385788 batch: 196/224\n",
      "Batch loss: 0.07855097204446793 batch: 197/224\n",
      "Batch loss: 0.06422219425439835 batch: 198/224\n",
      "Batch loss: 0.06441913545131683 batch: 199/224\n",
      "Batch loss: 0.07805044949054718 batch: 200/224\n",
      "Batch loss: 0.09280896186828613 batch: 201/224\n",
      "Batch loss: 0.06422300636768341 batch: 202/224\n",
      "Batch loss: 0.05999957397580147 batch: 203/224\n",
      "Batch loss: 0.07763828337192535 batch: 204/224\n",
      "Batch loss: 0.08935597538948059 batch: 205/224\n",
      "Batch loss: 0.07161868363618851 batch: 206/224\n",
      "Batch loss: 0.08675997704267502 batch: 207/224\n",
      "Batch loss: 0.0772790014743805 batch: 208/224\n",
      "Batch loss: 0.08948560059070587 batch: 209/224\n",
      "Batch loss: 0.07645342499017715 batch: 210/224\n",
      "Batch loss: 0.0642845407128334 batch: 211/224\n",
      "Batch loss: 0.0742165744304657 batch: 212/224\n",
      "Batch loss: 0.0914212167263031 batch: 213/224\n",
      "Batch loss: 0.08834667503833771 batch: 214/224\n",
      "Batch loss: 0.09731690585613251 batch: 215/224\n",
      "Batch loss: 0.07742823660373688 batch: 216/224\n",
      "Batch loss: 0.07182344049215317 batch: 217/224\n",
      "Batch loss: 0.0868043452501297 batch: 218/224\n",
      "Batch loss: 0.06388356536626816 batch: 219/224\n",
      "Batch loss: 0.0722275972366333 batch: 220/224\n",
      "Batch loss: 0.08129637688398361 batch: 221/224\n",
      "Batch loss: 0.09330989420413971 batch: 222/224\n",
      "Batch loss: 0.0557706244289875 batch: 223/224\n",
      "Batch loss: 0.06713235378265381 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 53/75..  Training Loss: 0.00015..  Test Loss: 0.00097..  Test Accuracy: 0.89111\n",
      "Running epoch 54/75\n",
      "Batch loss: 0.04906586557626724 batch: 1/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.060230255126953125 batch: 2/224\n",
      "Batch loss: 0.07058583945035934 batch: 3/224\n",
      "Batch loss: 0.10445880144834518 batch: 4/224\n",
      "Batch loss: 0.07540813088417053 batch: 5/224\n",
      "Batch loss: 0.08753348141908646 batch: 6/224\n",
      "Batch loss: 0.07530619949102402 batch: 7/224\n",
      "Batch loss: 0.08230436593294144 batch: 8/224\n",
      "Batch loss: 0.07523492723703384 batch: 9/224\n",
      "Batch loss: 0.05209353566169739 batch: 10/224\n",
      "Batch loss: 0.1082235798239708 batch: 11/224\n",
      "Batch loss: 0.10576735436916351 batch: 12/224\n",
      "Batch loss: 0.048725586384534836 batch: 13/224\n",
      "Batch loss: 0.048243336379528046 batch: 14/224\n",
      "Batch loss: 0.06985624879598618 batch: 15/224\n",
      "Batch loss: 0.09496492147445679 batch: 16/224\n",
      "Batch loss: 0.06829240173101425 batch: 17/224\n",
      "Batch loss: 0.06151076406240463 batch: 18/224\n",
      "Batch loss: 0.09574688225984573 batch: 19/224\n",
      "Batch loss: 0.04973515123128891 batch: 20/224\n",
      "Batch loss: 0.08031237125396729 batch: 21/224\n",
      "Batch loss: 0.07286851853132248 batch: 22/224\n",
      "Batch loss: 0.07786954194307327 batch: 23/224\n",
      "Batch loss: 0.08445367962121964 batch: 24/224\n",
      "Batch loss: 0.06381399929523468 batch: 25/224\n",
      "Batch loss: 0.06928703188896179 batch: 26/224\n",
      "Batch loss: 0.09353866428136826 batch: 27/224\n",
      "Batch loss: 0.07079826295375824 batch: 28/224\n",
      "Batch loss: 0.096352219581604 batch: 29/224\n",
      "Batch loss: 0.06723669171333313 batch: 30/224\n",
      "Batch loss: 0.05909435451030731 batch: 31/224\n",
      "Batch loss: 0.08703093975782394 batch: 32/224\n",
      "Batch loss: 0.053175412118434906 batch: 33/224\n",
      "Batch loss: 0.08664276450872421 batch: 34/224\n",
      "Batch loss: 0.06553106755018234 batch: 35/224\n",
      "Batch loss: 0.0901142805814743 batch: 36/224\n",
      "Batch loss: 0.08486861735582352 batch: 37/224\n",
      "Batch loss: 0.07279757410287857 batch: 38/224\n",
      "Batch loss: 0.06454543024301529 batch: 39/224\n",
      "Batch loss: 0.07429640740156174 batch: 40/224\n",
      "Batch loss: 0.07500694692134857 batch: 41/224\n",
      "Batch loss: 0.062116190791130066 batch: 42/224\n",
      "Batch loss: 0.106014184653759 batch: 43/224\n",
      "Batch loss: 0.06870634853839874 batch: 44/224\n",
      "Batch loss: 0.05942380800843239 batch: 45/224\n",
      "Batch loss: 0.10368891060352325 batch: 46/224\n",
      "Batch loss: 0.06694934517145157 batch: 47/224\n",
      "Batch loss: 0.054329611361026764 batch: 48/224\n",
      "Batch loss: 0.04893910884857178 batch: 49/224\n",
      "Batch loss: 0.05239249765872955 batch: 50/224\n",
      "Batch loss: 0.05406654626131058 batch: 51/224\n",
      "Batch loss: 0.09285297989845276 batch: 52/224\n",
      "Batch loss: 0.09049852192401886 batch: 53/224\n",
      "Batch loss: 0.06144576147198677 batch: 54/224\n",
      "Batch loss: 0.05410752445459366 batch: 55/224\n",
      "Batch loss: 0.06898851692676544 batch: 56/224\n",
      "Batch loss: 0.10849260538816452 batch: 57/224\n",
      "Batch loss: 0.08857402950525284 batch: 58/224\n",
      "Batch loss: 0.11090441793203354 batch: 59/224\n",
      "Batch loss: 0.09064777195453644 batch: 60/224\n",
      "Batch loss: 0.06305939704179764 batch: 61/224\n",
      "Batch loss: 0.04262689873576164 batch: 62/224\n",
      "Batch loss: 0.06033125892281532 batch: 63/224\n",
      "Batch loss: 0.09146933257579803 batch: 64/224\n",
      "Batch loss: 0.06588907539844513 batch: 65/224\n",
      "Batch loss: 0.09954200685024261 batch: 66/224\n",
      "Batch loss: 0.04383532702922821 batch: 67/224\n",
      "Batch loss: 0.05652952194213867 batch: 68/224\n",
      "Batch loss: 0.05231773108243942 batch: 69/224\n",
      "Batch loss: 0.08606240898370743 batch: 70/224\n",
      "Batch loss: 0.07907842844724655 batch: 71/224\n",
      "Batch loss: 0.054869286715984344 batch: 72/224\n",
      "Batch loss: 0.07319857180118561 batch: 73/224\n",
      "Batch loss: 0.05647260323166847 batch: 74/224\n",
      "Batch loss: 0.09506609290838242 batch: 75/224\n",
      "Batch loss: 0.09769807010889053 batch: 76/224\n",
      "Batch loss: 0.06189534440636635 batch: 77/224\n",
      "Batch loss: 0.08494594693183899 batch: 78/224\n",
      "Batch loss: 0.10302352160215378 batch: 79/224\n",
      "Batch loss: 0.08121784776449203 batch: 80/224\n",
      "Batch loss: 0.11546000838279724 batch: 81/224\n",
      "Batch loss: 0.09550467133522034 batch: 82/224\n",
      "Batch loss: 0.07171926647424698 batch: 83/224\n",
      "Batch loss: 0.05625351890921593 batch: 84/224\n",
      "Batch loss: 0.06553153693675995 batch: 85/224\n",
      "Batch loss: 0.09636996686458588 batch: 86/224\n",
      "Batch loss: 0.07343967258930206 batch: 87/224\n",
      "Batch loss: 0.08915390074253082 batch: 88/224\n",
      "Batch loss: 0.06694851070642471 batch: 89/224\n",
      "Batch loss: 0.10029947757720947 batch: 90/224\n",
      "Batch loss: 0.08883517235517502 batch: 91/224\n",
      "Batch loss: 0.06108246371150017 batch: 92/224\n",
      "Batch loss: 0.05301028490066528 batch: 93/224\n",
      "Batch loss: 0.05682390183210373 batch: 94/224\n",
      "Batch loss: 0.05994103476405144 batch: 95/224\n",
      "Batch loss: 0.07979556918144226 batch: 96/224\n",
      "Batch loss: 0.08279471099376678 batch: 97/224\n",
      "Batch loss: 0.05571521818637848 batch: 98/224\n",
      "Batch loss: 0.08678150922060013 batch: 99/224\n",
      "Batch loss: 0.06560752540826797 batch: 100/224\n",
      "Batch loss: 0.10075923055410385 batch: 101/224\n",
      "Batch loss: 0.07253479957580566 batch: 102/224\n",
      "Batch loss: 0.0919785425066948 batch: 103/224\n",
      "Batch loss: 0.07716276496648788 batch: 104/224\n",
      "Batch loss: 0.0670204758644104 batch: 105/224\n",
      "Batch loss: 0.06604424864053726 batch: 106/224\n",
      "Batch loss: 0.0607004277408123 batch: 107/224\n",
      "Batch loss: 0.07425472140312195 batch: 108/224\n",
      "Batch loss: 0.0693264976143837 batch: 109/224\n",
      "Batch loss: 0.05073405057191849 batch: 110/224\n",
      "Batch loss: 0.09538731724023819 batch: 111/224\n",
      "Batch loss: 0.06400127708911896 batch: 112/224\n",
      "Batch loss: 0.047713752835989 batch: 113/224\n",
      "Batch loss: 0.06221891567111015 batch: 114/224\n",
      "Batch loss: 0.05530105531215668 batch: 115/224\n",
      "Batch loss: 0.0670660138130188 batch: 116/224\n",
      "Batch loss: 0.07301905006170273 batch: 117/224\n",
      "Batch loss: 0.10072832554578781 batch: 118/224\n",
      "Batch loss: 0.060018520802259445 batch: 119/224\n",
      "Batch loss: 0.060749202966690063 batch: 120/224\n",
      "Batch loss: 0.07175315916538239 batch: 121/224\n",
      "Batch loss: 0.05186777561903 batch: 122/224\n",
      "Batch loss: 0.04147692769765854 batch: 123/224\n",
      "Batch loss: 0.07939382642507553 batch: 124/224\n",
      "Batch loss: 0.08848533034324646 batch: 125/224\n",
      "Batch loss: 0.08600647747516632 batch: 126/224\n",
      "Batch loss: 0.08397872745990753 batch: 127/224\n",
      "Batch loss: 0.054887186735868454 batch: 128/224\n",
      "Batch loss: 0.07180839776992798 batch: 129/224\n",
      "Batch loss: 0.05000744387507439 batch: 130/224\n",
      "Batch loss: 0.05291476473212242 batch: 131/224\n",
      "Batch loss: 0.05700426548719406 batch: 132/224\n",
      "Batch loss: 0.06197521090507507 batch: 133/224\n",
      "Batch loss: 0.0744316503405571 batch: 134/224\n",
      "Batch loss: 0.0630694106221199 batch: 135/224\n",
      "Batch loss: 0.06355243921279907 batch: 136/224\n",
      "Batch loss: 0.06400246173143387 batch: 137/224\n",
      "Batch loss: 0.07229167968034744 batch: 138/224\n",
      "Batch loss: 0.08775142580270767 batch: 139/224\n",
      "Batch loss: 0.07524179667234421 batch: 140/224\n",
      "Batch loss: 0.049143895506858826 batch: 141/224\n",
      "Batch loss: 0.07447496056556702 batch: 142/224\n",
      "Batch loss: 0.043244294822216034 batch: 143/224\n",
      "Batch loss: 0.09476214647293091 batch: 144/224\n",
      "Batch loss: 0.045094627887010574 batch: 145/224\n",
      "Batch loss: 0.09681825339794159 batch: 146/224\n",
      "Batch loss: 0.08072308450937271 batch: 147/224\n",
      "Batch loss: 0.05886026844382286 batch: 148/224\n",
      "Batch loss: 0.06690473109483719 batch: 149/224\n",
      "Batch loss: 0.07071840018033981 batch: 150/224\n",
      "Batch loss: 0.07391893863677979 batch: 151/224\n",
      "Batch loss: 0.09132365882396698 batch: 152/224\n",
      "Batch loss: 0.0613909475505352 batch: 153/224\n",
      "Batch loss: 0.07446705549955368 batch: 154/224\n",
      "Batch loss: 0.056694503873586655 batch: 155/224\n",
      "Batch loss: 0.06510141491889954 batch: 156/224\n",
      "Batch loss: 0.08428241312503815 batch: 157/224\n",
      "Batch loss: 0.12237583100795746 batch: 158/224\n",
      "Batch loss: 0.08162952214479446 batch: 159/224\n",
      "Batch loss: 0.050421666353940964 batch: 160/224\n",
      "Batch loss: 0.05965384840965271 batch: 161/224\n",
      "Batch loss: 0.04256587103009224 batch: 162/224\n",
      "Batch loss: 0.049846798181533813 batch: 163/224\n",
      "Batch loss: 0.0570174939930439 batch: 164/224\n",
      "Batch loss: 0.06123719736933708 batch: 165/224\n",
      "Batch loss: 0.06786219030618668 batch: 166/224\n",
      "Batch loss: 0.0500711053609848 batch: 167/224\n",
      "Batch loss: 0.07452104985713959 batch: 168/224\n",
      "Batch loss: 0.1269349902868271 batch: 169/224\n",
      "Batch loss: 0.05693266913294792 batch: 170/224\n",
      "Batch loss: 0.05585222691297531 batch: 171/224\n",
      "Batch loss: 0.04997861012816429 batch: 172/224\n",
      "Batch loss: 0.10482095181941986 batch: 173/224\n",
      "Batch loss: 0.05899247154593468 batch: 174/224\n",
      "Batch loss: 0.07393721491098404 batch: 175/224\n",
      "Batch loss: 0.06301417946815491 batch: 176/224\n",
      "Batch loss: 0.08090247213840485 batch: 177/224\n",
      "Batch loss: 0.06097961962223053 batch: 178/224\n",
      "Batch loss: 0.11206216365098953 batch: 179/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.04159575700759888 batch: 180/224\n",
      "Batch loss: 0.06899794191122055 batch: 181/224\n",
      "Batch loss: 0.08707934617996216 batch: 182/224\n",
      "Batch loss: 0.09913574159145355 batch: 183/224\n",
      "Batch loss: 0.06077666953206062 batch: 184/224\n",
      "Batch loss: 0.07808680087327957 batch: 185/224\n",
      "Batch loss: 0.07448086887598038 batch: 186/224\n",
      "Batch loss: 0.09077167510986328 batch: 187/224\n",
      "Batch loss: 0.06408409029245377 batch: 188/224\n",
      "Batch loss: 0.07762090116739273 batch: 189/224\n",
      "Batch loss: 0.06692375242710114 batch: 190/224\n",
      "Batch loss: 0.0528220608830452 batch: 191/224\n",
      "Batch loss: 0.07661764323711395 batch: 192/224\n",
      "Batch loss: 0.08229412883520126 batch: 193/224\n",
      "Batch loss: 0.050569090992212296 batch: 194/224\n",
      "Batch loss: 0.0926520898938179 batch: 195/224\n",
      "Batch loss: 0.09055519849061966 batch: 196/224\n",
      "Batch loss: 0.06275469809770584 batch: 197/224\n",
      "Batch loss: 0.05733052268624306 batch: 198/224\n",
      "Batch loss: 0.07389909029006958 batch: 199/224\n",
      "Batch loss: 0.0806150734424591 batch: 200/224\n",
      "Batch loss: 0.1003577932715416 batch: 201/224\n",
      "Batch loss: 0.07810651510953903 batch: 202/224\n",
      "Batch loss: 0.079170361161232 batch: 203/224\n",
      "Batch loss: 0.06749238073825836 batch: 204/224\n",
      "Batch loss: 0.08232726156711578 batch: 205/224\n",
      "Batch loss: 0.0958709716796875 batch: 206/224\n",
      "Batch loss: 0.07259391993284225 batch: 207/224\n",
      "Batch loss: 0.053459636867046356 batch: 208/224\n",
      "Batch loss: 0.0852496549487114 batch: 209/224\n",
      "Batch loss: 0.05084318295121193 batch: 210/224\n",
      "Batch loss: 0.09613125771284103 batch: 211/224\n",
      "Batch loss: 0.0987972840666771 batch: 212/224\n",
      "Batch loss: 0.08638737350702286 batch: 213/224\n",
      "Batch loss: 0.08577736467123032 batch: 214/224\n",
      "Batch loss: 0.05229010060429573 batch: 215/224\n",
      "Batch loss: 0.07785878330469131 batch: 216/224\n",
      "Batch loss: 0.06400024145841599 batch: 217/224\n",
      "Batch loss: 0.08564483374357224 batch: 218/224\n",
      "Batch loss: 0.06748029589653015 batch: 219/224\n",
      "Batch loss: 0.07250894606113434 batch: 220/224\n",
      "Batch loss: 0.06772170960903168 batch: 221/224\n",
      "Batch loss: 0.08498439937829971 batch: 222/224\n",
      "Batch loss: 0.07012329995632172 batch: 223/224\n",
      "Batch loss: 0.07617823034524918 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 54/75..  Training Loss: 0.00015..  Test Loss: 0.00099..  Test Accuracy: 0.89100\n",
      "Running epoch 55/75\n",
      "Batch loss: 0.06463233381509781 batch: 1/224\n",
      "Batch loss: 0.08617250621318817 batch: 2/224\n",
      "Batch loss: 0.08413492888212204 batch: 3/224\n",
      "Batch loss: 0.08985062688589096 batch: 4/224\n",
      "Batch loss: 0.1031629741191864 batch: 5/224\n",
      "Batch loss: 0.07860126346349716 batch: 6/224\n",
      "Batch loss: 0.09554088115692139 batch: 7/224\n",
      "Batch loss: 0.09368806332349777 batch: 8/224\n",
      "Batch loss: 0.07380391657352448 batch: 9/224\n",
      "Batch loss: 0.05154724419116974 batch: 10/224\n",
      "Batch loss: 0.07249852269887924 batch: 11/224\n",
      "Batch loss: 0.06440780311822891 batch: 12/224\n",
      "Batch loss: 0.05274856835603714 batch: 13/224\n",
      "Batch loss: 0.08328497409820557 batch: 14/224\n",
      "Batch loss: 0.08930208534002304 batch: 15/224\n",
      "Batch loss: 0.06565359234809875 batch: 16/224\n",
      "Batch loss: 0.08553232997655869 batch: 17/224\n",
      "Batch loss: 0.05571717768907547 batch: 18/224\n",
      "Batch loss: 0.07455434650182724 batch: 19/224\n",
      "Batch loss: 0.07347718626260757 batch: 20/224\n",
      "Batch loss: 0.09557873010635376 batch: 21/224\n",
      "Batch loss: 0.06821299344301224 batch: 22/224\n",
      "Batch loss: 0.07866815477609634 batch: 23/224\n",
      "Batch loss: 0.0835513100028038 batch: 24/224\n",
      "Batch loss: 0.060933999717235565 batch: 25/224\n",
      "Batch loss: 0.06794983148574829 batch: 26/224\n",
      "Batch loss: 0.07380873709917068 batch: 27/224\n",
      "Batch loss: 0.09595690667629242 batch: 28/224\n",
      "Batch loss: 0.07566490769386292 batch: 29/224\n",
      "Batch loss: 0.06448584794998169 batch: 30/224\n",
      "Batch loss: 0.07866353541612625 batch: 31/224\n",
      "Batch loss: 0.059031467884778976 batch: 32/224\n",
      "Batch loss: 0.07997389882802963 batch: 33/224\n",
      "Batch loss: 0.06565932184457779 batch: 34/224\n",
      "Batch loss: 0.07761359959840775 batch: 35/224\n",
      "Batch loss: 0.0802517831325531 batch: 36/224\n",
      "Batch loss: 0.07755301892757416 batch: 37/224\n",
      "Batch loss: 0.10743574798107147 batch: 38/224\n",
      "Batch loss: 0.11131903529167175 batch: 39/224\n",
      "Batch loss: 0.05620982497930527 batch: 40/224\n",
      "Batch loss: 0.09175296127796173 batch: 41/224\n",
      "Batch loss: 0.08026985079050064 batch: 42/224\n",
      "Batch loss: 0.10178643465042114 batch: 43/224\n",
      "Batch loss: 0.05158918350934982 batch: 44/224\n",
      "Batch loss: 0.04454111307859421 batch: 45/224\n",
      "Batch loss: 0.08974385261535645 batch: 46/224\n",
      "Batch loss: 0.0728350356221199 batch: 47/224\n",
      "Batch loss: 0.06134866550564766 batch: 48/224\n",
      "Batch loss: 0.05252750217914581 batch: 49/224\n",
      "Batch loss: 0.05171816423535347 batch: 50/224\n",
      "Batch loss: 0.06686492264270782 batch: 51/224\n",
      "Batch loss: 0.07153783738613129 batch: 52/224\n",
      "Batch loss: 0.06472386419773102 batch: 53/224\n",
      "Batch loss: 0.054281994700431824 batch: 54/224\n",
      "Batch loss: 0.08075829595327377 batch: 55/224\n",
      "Batch loss: 0.06257074326276779 batch: 56/224\n",
      "Batch loss: 0.08587311208248138 batch: 57/224\n",
      "Batch loss: 0.10230617970228195 batch: 58/224\n",
      "Batch loss: 0.06313702464103699 batch: 59/224\n",
      "Batch loss: 0.09567583352327347 batch: 60/224\n",
      "Batch loss: 0.09787832945585251 batch: 61/224\n",
      "Batch loss: 0.04423930495977402 batch: 62/224\n",
      "Batch loss: 0.08976596593856812 batch: 63/224\n",
      "Batch loss: 0.06809165328741074 batch: 64/224\n",
      "Batch loss: 0.056259725242853165 batch: 65/224\n",
      "Batch loss: 0.09325016289949417 batch: 66/224\n",
      "Batch loss: 0.05806383490562439 batch: 67/224\n",
      "Batch loss: 0.06380417943000793 batch: 68/224\n",
      "Batch loss: 0.08287341147661209 batch: 69/224\n",
      "Batch loss: 0.07405433803796768 batch: 70/224\n",
      "Batch loss: 0.05413574352860451 batch: 71/224\n",
      "Batch loss: 0.05000609531998634 batch: 72/224\n",
      "Batch loss: 0.08975134789943695 batch: 73/224\n",
      "Batch loss: 0.0811237320303917 batch: 74/224\n",
      "Batch loss: 0.06607619673013687 batch: 75/224\n",
      "Batch loss: 0.08538507670164108 batch: 76/224\n",
      "Batch loss: 0.10136006772518158 batch: 77/224\n",
      "Batch loss: 0.07264291495084763 batch: 78/224\n",
      "Batch loss: 0.06970847398042679 batch: 79/224\n",
      "Batch loss: 0.07010303437709808 batch: 80/224\n",
      "Batch loss: 0.12267471849918365 batch: 81/224\n",
      "Batch loss: 0.0830467939376831 batch: 82/224\n",
      "Batch loss: 0.08694417774677277 batch: 83/224\n",
      "Batch loss: 0.06540897488594055 batch: 84/224\n",
      "Batch loss: 0.06199698522686958 batch: 85/224\n",
      "Batch loss: 0.06512539088726044 batch: 86/224\n",
      "Batch loss: 0.08447673916816711 batch: 87/224\n",
      "Batch loss: 0.08718045800924301 batch: 88/224\n",
      "Batch loss: 0.08914034813642502 batch: 89/224\n",
      "Batch loss: 0.07891800254583359 batch: 90/224\n",
      "Batch loss: 0.04774902015924454 batch: 91/224\n",
      "Batch loss: 0.0695510283112526 batch: 92/224\n",
      "Batch loss: 0.06175807863473892 batch: 93/224\n",
      "Batch loss: 0.08970575779676437 batch: 94/224\n",
      "Batch loss: 0.05660581588745117 batch: 95/224\n",
      "Batch loss: 0.05811874568462372 batch: 96/224\n",
      "Batch loss: 0.04701146110892296 batch: 97/224\n",
      "Batch loss: 0.04697982221841812 batch: 98/224\n",
      "Batch loss: 0.1002633348107338 batch: 99/224\n",
      "Batch loss: 0.05935763567686081 batch: 100/224\n",
      "Batch loss: 0.09564908593893051 batch: 101/224\n",
      "Batch loss: 0.08234383165836334 batch: 102/224\n",
      "Batch loss: 0.08875740319490433 batch: 103/224\n",
      "Batch loss: 0.05377059429883957 batch: 104/224\n",
      "Batch loss: 0.043753888458013535 batch: 105/224\n",
      "Batch loss: 0.08555376529693604 batch: 106/224\n",
      "Batch loss: 0.07335717976093292 batch: 107/224\n",
      "Batch loss: 0.11147350817918777 batch: 108/224\n",
      "Batch loss: 0.05486473813652992 batch: 109/224\n",
      "Batch loss: 0.07874779403209686 batch: 110/224\n",
      "Batch loss: 0.0855744481086731 batch: 111/224\n",
      "Batch loss: 0.0763234868645668 batch: 112/224\n",
      "Batch loss: 0.07001103460788727 batch: 113/224\n",
      "Batch loss: 0.08039503544569016 batch: 114/224\n",
      "Batch loss: 0.06446102261543274 batch: 115/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.0830463096499443 batch: 116/224\n",
      "Batch loss: 0.053560901433229446 batch: 117/224\n",
      "Batch loss: 0.06935925036668777 batch: 118/224\n",
      "Batch loss: 0.07432174682617188 batch: 119/224\n",
      "Batch loss: 0.07985664904117584 batch: 120/224\n",
      "Batch loss: 0.06434787064790726 batch: 121/224\n",
      "Batch loss: 0.08034569770097733 batch: 122/224\n",
      "Batch loss: 0.043235983699560165 batch: 123/224\n",
      "Batch loss: 0.052650149911642075 batch: 124/224\n",
      "Batch loss: 0.07681606709957123 batch: 125/224\n",
      "Batch loss: 0.0780886858701706 batch: 126/224\n",
      "Batch loss: 0.08598732203245163 batch: 127/224\n",
      "Batch loss: 0.06647633016109467 batch: 128/224\n",
      "Batch loss: 0.050154220312833786 batch: 129/224\n",
      "Batch loss: 0.08833662420511246 batch: 130/224\n",
      "Batch loss: 0.05527684465050697 batch: 131/224\n",
      "Batch loss: 0.08652032911777496 batch: 132/224\n",
      "Batch loss: 0.09080667048692703 batch: 133/224\n",
      "Batch loss: 0.06924714148044586 batch: 134/224\n",
      "Batch loss: 0.06897922605276108 batch: 135/224\n",
      "Batch loss: 0.07069066166877747 batch: 136/224\n",
      "Batch loss: 0.07882268726825714 batch: 137/224\n",
      "Batch loss: 0.04517687112092972 batch: 138/224\n",
      "Batch loss: 0.05363203212618828 batch: 139/224\n",
      "Batch loss: 0.08314840495586395 batch: 140/224\n",
      "Batch loss: 0.05168529227375984 batch: 141/224\n",
      "Batch loss: 0.07816427946090698 batch: 142/224\n",
      "Batch loss: 0.050405096262693405 batch: 143/224\n",
      "Batch loss: 0.05094918981194496 batch: 144/224\n",
      "Batch loss: 0.07928325235843658 batch: 145/224\n",
      "Batch loss: 0.08525805920362473 batch: 146/224\n",
      "Batch loss: 0.06116240844130516 batch: 147/224\n",
      "Batch loss: 0.07937389612197876 batch: 148/224\n",
      "Batch loss: 0.10674521327018738 batch: 149/224\n",
      "Batch loss: 0.08488687872886658 batch: 150/224\n",
      "Batch loss: 0.06877424567937851 batch: 151/224\n",
      "Batch loss: 0.061611197888851166 batch: 152/224\n",
      "Batch loss: 0.11085224151611328 batch: 153/224\n",
      "Batch loss: 0.08504920452833176 batch: 154/224\n",
      "Batch loss: 0.06520404666662216 batch: 155/224\n",
      "Batch loss: 0.06534161418676376 batch: 156/224\n",
      "Batch loss: 0.06911139190196991 batch: 157/224\n",
      "Batch loss: 0.1109187975525856 batch: 158/224\n",
      "Batch loss: 0.06819173693656921 batch: 159/224\n",
      "Batch loss: 0.055185604840517044 batch: 160/224\n",
      "Batch loss: 0.07422523945569992 batch: 161/224\n",
      "Batch loss: 0.0479847751557827 batch: 162/224\n",
      "Batch loss: 0.047097671777009964 batch: 163/224\n",
      "Batch loss: 0.06353476643562317 batch: 164/224\n",
      "Batch loss: 0.07847843319177628 batch: 165/224\n",
      "Batch loss: 0.09397251904010773 batch: 166/224\n",
      "Batch loss: 0.04406055808067322 batch: 167/224\n",
      "Batch loss: 0.08346984535455704 batch: 168/224\n",
      "Batch loss: 0.07384658604860306 batch: 169/224\n",
      "Batch loss: 0.05287305265665054 batch: 170/224\n",
      "Batch loss: 0.05770440027117729 batch: 171/224\n",
      "Batch loss: 0.07703731954097748 batch: 172/224\n",
      "Batch loss: 0.08010157942771912 batch: 173/224\n",
      "Batch loss: 0.041692815721035004 batch: 174/224\n",
      "Batch loss: 0.0749015286564827 batch: 175/224\n",
      "Batch loss: 0.06975562125444412 batch: 176/224\n",
      "Batch loss: 0.0826062336564064 batch: 177/224\n",
      "Batch loss: 0.09981415420770645 batch: 178/224\n",
      "Batch loss: 0.07445516437292099 batch: 179/224\n",
      "Batch loss: 0.03718286380171776 batch: 180/224\n",
      "Batch loss: 0.04697924852371216 batch: 181/224\n",
      "Batch loss: 0.07949420064687729 batch: 182/224\n",
      "Batch loss: 0.0822625532746315 batch: 183/224\n",
      "Batch loss: 0.07577118277549744 batch: 184/224\n",
      "Batch loss: 0.09781254827976227 batch: 185/224\n",
      "Batch loss: 0.04463551193475723 batch: 186/224\n",
      "Batch loss: 0.09154312312602997 batch: 187/224\n",
      "Batch loss: 0.06163353845477104 batch: 188/224\n",
      "Batch loss: 0.06337199360132217 batch: 189/224\n",
      "Batch loss: 0.08249135315418243 batch: 190/224\n",
      "Batch loss: 0.034082021564245224 batch: 191/224\n",
      "Batch loss: 0.06901032477617264 batch: 192/224\n",
      "Batch loss: 0.07702327519655228 batch: 193/224\n",
      "Batch loss: 0.07238788157701492 batch: 194/224\n",
      "Batch loss: 0.06477705389261246 batch: 195/224\n",
      "Batch loss: 0.09745291620492935 batch: 196/224\n",
      "Batch loss: 0.07616029679775238 batch: 197/224\n",
      "Batch loss: 0.04814569652080536 batch: 198/224\n",
      "Batch loss: 0.06522628664970398 batch: 199/224\n",
      "Batch loss: 0.09536130726337433 batch: 200/224\n",
      "Batch loss: 0.06878288090229034 batch: 201/224\n",
      "Batch loss: 0.05437783896923065 batch: 202/224\n",
      "Batch loss: 0.07330096513032913 batch: 203/224\n",
      "Batch loss: 0.09624462574720383 batch: 204/224\n",
      "Batch loss: 0.06186710298061371 batch: 205/224\n",
      "Batch loss: 0.06896376609802246 batch: 206/224\n",
      "Batch loss: 0.04017874225974083 batch: 207/224\n",
      "Batch loss: 0.053585637360811234 batch: 208/224\n",
      "Batch loss: 0.092852383852005 batch: 209/224\n",
      "Batch loss: 0.060254115611314774 batch: 210/224\n",
      "Batch loss: 0.06147245690226555 batch: 211/224\n",
      "Batch loss: 0.05322124436497688 batch: 212/224\n",
      "Batch loss: 0.09050807356834412 batch: 213/224\n",
      "Batch loss: 0.07756096869707108 batch: 214/224\n",
      "Batch loss: 0.08736373484134674 batch: 215/224\n",
      "Batch loss: 0.06558766961097717 batch: 216/224\n",
      "Batch loss: 0.09336153417825699 batch: 217/224\n",
      "Batch loss: 0.06396182626485825 batch: 218/224\n",
      "Batch loss: 0.07260450720787048 batch: 219/224\n",
      "Batch loss: 0.06778891384601593 batch: 220/224\n",
      "Batch loss: 0.08366833627223969 batch: 221/224\n",
      "Batch loss: 0.10875052213668823 batch: 222/224\n",
      "Batch loss: 0.05907222628593445 batch: 223/224\n",
      "Batch loss: 0.06765975058078766 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 55/75..  Training Loss: 0.00015..  Test Loss: 0.00102..  Test Accuracy: 0.89136\n",
      "Running epoch 56/75\n",
      "Batch loss: 0.061057787388563156 batch: 1/224\n",
      "Batch loss: 0.08839723467826843 batch: 2/224\n",
      "Batch loss: 0.06519518047571182 batch: 3/224\n",
      "Batch loss: 0.07205041497945786 batch: 4/224\n",
      "Batch loss: 0.056394606828689575 batch: 5/224\n",
      "Batch loss: 0.06773994117975235 batch: 6/224\n",
      "Batch loss: 0.058393750339746475 batch: 7/224\n",
      "Batch loss: 0.06500241160392761 batch: 8/224\n",
      "Batch loss: 0.05179859325289726 batch: 9/224\n",
      "Batch loss: 0.05381005257368088 batch: 10/224\n",
      "Batch loss: 0.06579069793224335 batch: 11/224\n",
      "Batch loss: 0.06366414576768875 batch: 12/224\n",
      "Batch loss: 0.05985149368643761 batch: 13/224\n",
      "Batch loss: 0.057979121804237366 batch: 14/224\n",
      "Batch loss: 0.07099901139736176 batch: 15/224\n",
      "Batch loss: 0.09157345443964005 batch: 16/224\n",
      "Batch loss: 0.06993807852268219 batch: 17/224\n",
      "Batch loss: 0.06445528566837311 batch: 18/224\n",
      "Batch loss: 0.0643048956990242 batch: 19/224\n",
      "Batch loss: 0.10111434757709503 batch: 20/224\n",
      "Batch loss: 0.04321066662669182 batch: 21/224\n",
      "Batch loss: 0.07081849873065948 batch: 22/224\n",
      "Batch loss: 0.07688195258378983 batch: 23/224\n",
      "Batch loss: 0.07936215400695801 batch: 24/224\n",
      "Batch loss: 0.060330167412757874 batch: 25/224\n",
      "Batch loss: 0.05215008184313774 batch: 26/224\n",
      "Batch loss: 0.08287549018859863 batch: 27/224\n",
      "Batch loss: 0.07499697804450989 batch: 28/224\n",
      "Batch loss: 0.06368248909711838 batch: 29/224\n",
      "Batch loss: 0.05188852176070213 batch: 30/224\n",
      "Batch loss: 0.05571543052792549 batch: 31/224\n",
      "Batch loss: 0.10148488730192184 batch: 32/224\n",
      "Batch loss: 0.06142621859908104 batch: 33/224\n",
      "Batch loss: 0.10609135776758194 batch: 34/224\n",
      "Batch loss: 0.09049265831708908 batch: 35/224\n",
      "Batch loss: 0.08927986770868301 batch: 36/224\n",
      "Batch loss: 0.07255642116069794 batch: 37/224\n",
      "Batch loss: 0.08110165596008301 batch: 38/224\n",
      "Batch loss: 0.08350441604852676 batch: 39/224\n",
      "Batch loss: 0.06546063721179962 batch: 40/224\n",
      "Batch loss: 0.09556639939546585 batch: 41/224\n",
      "Batch loss: 0.08117450028657913 batch: 42/224\n",
      "Batch loss: 0.04890710487961769 batch: 43/224\n",
      "Batch loss: 0.0413336344063282 batch: 44/224\n",
      "Batch loss: 0.035322848707437515 batch: 45/224\n",
      "Batch loss: 0.08420972526073456 batch: 46/224\n",
      "Batch loss: 0.08757703751325607 batch: 47/224\n",
      "Batch loss: 0.07754655182361603 batch: 48/224\n",
      "Batch loss: 0.07078862935304642 batch: 49/224\n",
      "Batch loss: 0.06815878301858902 batch: 50/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.06853871792554855 batch: 51/224\n",
      "Batch loss: 0.06330791860818863 batch: 52/224\n",
      "Batch loss: 0.09173711389303207 batch: 53/224\n",
      "Batch loss: 0.07503072172403336 batch: 54/224\n",
      "Batch loss: 0.06317339837551117 batch: 55/224\n",
      "Batch loss: 0.07745659351348877 batch: 56/224\n",
      "Batch loss: 0.08360994607210159 batch: 57/224\n",
      "Batch loss: 0.05935466289520264 batch: 58/224\n",
      "Batch loss: 0.03610442206263542 batch: 59/224\n",
      "Batch loss: 0.08213065564632416 batch: 60/224\n",
      "Batch loss: 0.0849587693810463 batch: 61/224\n",
      "Batch loss: 0.04459346458315849 batch: 62/224\n",
      "Batch loss: 0.09568388760089874 batch: 63/224\n",
      "Batch loss: 0.08277139067649841 batch: 64/224\n",
      "Batch loss: 0.09456294029951096 batch: 65/224\n",
      "Batch loss: 0.06110348179936409 batch: 66/224\n",
      "Batch loss: 0.041816964745521545 batch: 67/224\n",
      "Batch loss: 0.07525794208049774 batch: 68/224\n",
      "Batch loss: 0.06216147541999817 batch: 69/224\n",
      "Batch loss: 0.07238481193780899 batch: 70/224\n",
      "Batch loss: 0.08563324809074402 batch: 71/224\n",
      "Batch loss: 0.06696333736181259 batch: 72/224\n",
      "Batch loss: 0.07916868478059769 batch: 73/224\n",
      "Batch loss: 0.056197892874479294 batch: 74/224\n",
      "Batch loss: 0.07080067694187164 batch: 75/224\n",
      "Batch loss: 0.06275075674057007 batch: 76/224\n",
      "Batch loss: 0.07120415568351746 batch: 77/224\n",
      "Batch loss: 0.0734177753329277 batch: 78/224\n",
      "Batch loss: 0.08790379017591476 batch: 79/224\n",
      "Batch loss: 0.06395476311445236 batch: 80/224\n",
      "Batch loss: 0.08957169204950333 batch: 81/224\n",
      "Batch loss: 0.07357389479875565 batch: 82/224\n",
      "Batch loss: 0.06828754395246506 batch: 83/224\n",
      "Batch loss: 0.06365983188152313 batch: 84/224\n",
      "Batch loss: 0.08785027265548706 batch: 85/224\n",
      "Batch loss: 0.07940834760665894 batch: 86/224\n",
      "Batch loss: 0.08770705759525299 batch: 87/224\n",
      "Batch loss: 0.08316127210855484 batch: 88/224\n",
      "Batch loss: 0.07451985776424408 batch: 89/224\n",
      "Batch loss: 0.10091368854045868 batch: 90/224\n",
      "Batch loss: 0.033069804310798645 batch: 91/224\n",
      "Batch loss: 0.06399852782487869 batch: 92/224\n",
      "Batch loss: 0.037888091057538986 batch: 93/224\n",
      "Batch loss: 0.05682575702667236 batch: 94/224\n",
      "Batch loss: 0.058639802038669586 batch: 95/224\n",
      "Batch loss: 0.0751611590385437 batch: 96/224\n",
      "Batch loss: 0.056224312633275986 batch: 97/224\n",
      "Batch loss: 0.04090527445077896 batch: 98/224\n",
      "Batch loss: 0.07353093475103378 batch: 99/224\n",
      "Batch loss: 0.047335024923086166 batch: 100/224\n",
      "Batch loss: 0.08296603709459305 batch: 101/224\n",
      "Batch loss: 0.06536969542503357 batch: 102/224\n",
      "Batch loss: 0.08778190612792969 batch: 103/224\n",
      "Batch loss: 0.06535715609788895 batch: 104/224\n",
      "Batch loss: 0.07210034877061844 batch: 105/224\n",
      "Batch loss: 0.0743652880191803 batch: 106/224\n",
      "Batch loss: 0.06053227186203003 batch: 107/224\n",
      "Batch loss: 0.07221435755491257 batch: 108/224\n",
      "Batch loss: 0.036288511008024216 batch: 109/224\n",
      "Batch loss: 0.08405362814664841 batch: 110/224\n",
      "Batch loss: 0.04609696567058563 batch: 111/224\n",
      "Batch loss: 0.06917053461074829 batch: 112/224\n",
      "Batch loss: 0.07026106119155884 batch: 113/224\n",
      "Batch loss: 0.06242699921131134 batch: 114/224\n",
      "Batch loss: 0.06599061191082001 batch: 115/224\n",
      "Batch loss: 0.0857195034623146 batch: 116/224\n",
      "Batch loss: 0.07656082510948181 batch: 117/224\n",
      "Batch loss: 0.08415660262107849 batch: 118/224\n",
      "Batch loss: 0.04649503156542778 batch: 119/224\n",
      "Batch loss: 0.07936591655015945 batch: 120/224\n",
      "Batch loss: 0.0585232637822628 batch: 121/224\n",
      "Batch loss: 0.054648976773023605 batch: 122/224\n",
      "Batch loss: 0.03716425225138664 batch: 123/224\n",
      "Batch loss: 0.08028429001569748 batch: 124/224\n",
      "Batch loss: 0.08626866340637207 batch: 125/224\n",
      "Batch loss: 0.0697731301188469 batch: 126/224\n",
      "Batch loss: 0.08751720935106277 batch: 127/224\n",
      "Batch loss: 0.05454099178314209 batch: 128/224\n",
      "Batch loss: 0.06599727272987366 batch: 129/224\n",
      "Batch loss: 0.05551109090447426 batch: 130/224\n",
      "Batch loss: 0.061372797936201096 batch: 131/224\n",
      "Batch loss: 0.05889671668410301 batch: 132/224\n",
      "Batch loss: 0.07453688234090805 batch: 133/224\n",
      "Batch loss: 0.07947926968336105 batch: 134/224\n",
      "Batch loss: 0.07329536974430084 batch: 135/224\n",
      "Batch loss: 0.06062714383006096 batch: 136/224\n",
      "Batch loss: 0.04731070250272751 batch: 137/224\n",
      "Batch loss: 0.05690723657608032 batch: 138/224\n",
      "Batch loss: 0.1069292202591896 batch: 139/224\n",
      "Batch loss: 0.1190720647573471 batch: 140/224\n",
      "Batch loss: 0.0741347223520279 batch: 141/224\n",
      "Batch loss: 0.05623892694711685 batch: 142/224\n",
      "Batch loss: 0.049105849117040634 batch: 143/224\n",
      "Batch loss: 0.08276332914829254 batch: 144/224\n",
      "Batch loss: 0.054665230214595795 batch: 145/224\n",
      "Batch loss: 0.08963073045015335 batch: 146/224\n",
      "Batch loss: 0.054382868111133575 batch: 147/224\n",
      "Batch loss: 0.07391523569822311 batch: 148/224\n",
      "Batch loss: 0.11077984422445297 batch: 149/224\n",
      "Batch loss: 0.05857805535197258 batch: 150/224\n",
      "Batch loss: 0.0673985555768013 batch: 151/224\n",
      "Batch loss: 0.050549816340208054 batch: 152/224\n",
      "Batch loss: 0.08634548634290695 batch: 153/224\n",
      "Batch loss: 0.05443738028407097 batch: 154/224\n",
      "Batch loss: 0.06266263127326965 batch: 155/224\n",
      "Batch loss: 0.08281107991933823 batch: 156/224\n",
      "Batch loss: 0.0914619117975235 batch: 157/224\n",
      "Batch loss: 0.08421383798122406 batch: 158/224\n",
      "Batch loss: 0.07850910723209381 batch: 159/224\n",
      "Batch loss: 0.06831347942352295 batch: 160/224\n",
      "Batch loss: 0.03569627180695534 batch: 161/224\n",
      "Batch loss: 0.07818803936243057 batch: 162/224\n",
      "Batch loss: 0.05586545914411545 batch: 163/224\n",
      "Batch loss: 0.050304122269153595 batch: 164/224\n",
      "Batch loss: 0.07078798115253448 batch: 165/224\n",
      "Batch loss: 0.0812465101480484 batch: 166/224\n",
      "Batch loss: 0.0655345693230629 batch: 167/224\n",
      "Batch loss: 0.06179111823439598 batch: 168/224\n",
      "Batch loss: 0.07447309792041779 batch: 169/224\n",
      "Batch loss: 0.05481494590640068 batch: 170/224\n",
      "Batch loss: 0.07226106524467468 batch: 171/224\n",
      "Batch loss: 0.06608166545629501 batch: 172/224\n",
      "Batch loss: 0.06675269454717636 batch: 173/224\n",
      "Batch loss: 0.07204857468605042 batch: 174/224\n",
      "Batch loss: 0.0856853649020195 batch: 175/224\n",
      "Batch loss: 0.07394430041313171 batch: 176/224\n",
      "Batch loss: 0.06828302145004272 batch: 177/224\n",
      "Batch loss: 0.04790942370891571 batch: 178/224\n",
      "Batch loss: 0.0911409929394722 batch: 179/224\n",
      "Batch loss: 0.052421774715185165 batch: 180/224\n",
      "Batch loss: 0.06619076430797577 batch: 181/224\n",
      "Batch loss: 0.06769225001335144 batch: 182/224\n",
      "Batch loss: 0.10360937565565109 batch: 183/224\n",
      "Batch loss: 0.06408262997865677 batch: 184/224\n",
      "Batch loss: 0.06576389819383621 batch: 185/224\n",
      "Batch loss: 0.04317152500152588 batch: 186/224\n",
      "Batch loss: 0.06751007586717606 batch: 187/224\n",
      "Batch loss: 0.06585770100355148 batch: 188/224\n",
      "Batch loss: 0.05613616853952408 batch: 189/224\n",
      "Batch loss: 0.03776756674051285 batch: 190/224\n",
      "Batch loss: 0.06810542196035385 batch: 191/224\n",
      "Batch loss: 0.06294609606266022 batch: 192/224\n",
      "Batch loss: 0.09091457724571228 batch: 193/224\n",
      "Batch loss: 0.07549410313367844 batch: 194/224\n",
      "Batch loss: 0.08504798263311386 batch: 195/224\n",
      "Batch loss: 0.0897439643740654 batch: 196/224\n",
      "Batch loss: 0.07935196906328201 batch: 197/224\n",
      "Batch loss: 0.04046420753002167 batch: 198/224\n",
      "Batch loss: 0.07134418189525604 batch: 199/224\n",
      "Batch loss: 0.06747749447822571 batch: 200/224\n",
      "Batch loss: 0.07210492342710495 batch: 201/224\n",
      "Batch loss: 0.06896799802780151 batch: 202/224\n",
      "Batch loss: 0.08469142764806747 batch: 203/224\n",
      "Batch loss: 0.06117173284292221 batch: 204/224\n",
      "Batch loss: 0.0897163674235344 batch: 205/224\n",
      "Batch loss: 0.05845383182168007 batch: 206/224\n",
      "Batch loss: 0.0802917629480362 batch: 207/224\n",
      "Batch loss: 0.08422702550888062 batch: 208/224\n",
      "Batch loss: 0.03664195165038109 batch: 209/224\n",
      "Batch loss: 0.05474516749382019 batch: 210/224\n",
      "Batch loss: 0.04867413267493248 batch: 211/224\n",
      "Batch loss: 0.06520231813192368 batch: 212/224\n",
      "Batch loss: 0.09308136254549026 batch: 213/224\n",
      "Batch loss: 0.06359288841485977 batch: 214/224\n",
      "Batch loss: 0.10731234401464462 batch: 215/224\n",
      "Batch loss: 0.09104321151971817 batch: 216/224\n",
      "Batch loss: 0.09279903769493103 batch: 217/224\n",
      "Batch loss: 0.07530361413955688 batch: 218/224\n",
      "Batch loss: 0.06708033382892609 batch: 219/224\n",
      "Batch loss: 0.06377638876438141 batch: 220/224\n",
      "Batch loss: 0.08583283424377441 batch: 221/224\n",
      "Batch loss: 0.10737704485654831 batch: 222/224\n",
      "Batch loss: 0.03889409452676773 batch: 223/224\n",
      "Batch loss: 0.05719812214374542 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 56/75..  Training Loss: 0.00014..  Test Loss: 0.00102..  Test Accuracy: 0.89121\n",
      "Running epoch 57/75\n",
      "Batch loss: 0.05090741440653801 batch: 1/224\n",
      "Batch loss: 0.07666581124067307 batch: 2/224\n",
      "Batch loss: 0.06776808202266693 batch: 3/224\n",
      "Batch loss: 0.13131460547447205 batch: 4/224\n",
      "Batch loss: 0.07938728481531143 batch: 5/224\n",
      "Batch loss: 0.06729535758495331 batch: 6/224\n",
      "Batch loss: 0.08254677802324295 batch: 7/224\n",
      "Batch loss: 0.06809642165899277 batch: 8/224\n",
      "Batch loss: 0.08354652673006058 batch: 9/224\n",
      "Batch loss: 0.0602894201874733 batch: 10/224\n",
      "Batch loss: 0.06952010840177536 batch: 11/224\n",
      "Batch loss: 0.04857147857546806 batch: 12/224\n",
      "Batch loss: 0.059211365878582 batch: 13/224\n",
      "Batch loss: 0.0697762668132782 batch: 14/224\n",
      "Batch loss: 0.06965738534927368 batch: 15/224\n",
      "Batch loss: 0.08851811289787292 batch: 16/224\n",
      "Batch loss: 0.061478011310100555 batch: 17/224\n",
      "Batch loss: 0.07166583836078644 batch: 18/224\n",
      "Batch loss: 0.051285818219184875 batch: 19/224\n",
      "Batch loss: 0.06393785774707794 batch: 20/224\n",
      "Batch loss: 0.06542286276817322 batch: 21/224\n",
      "Batch loss: 0.05905330181121826 batch: 22/224\n",
      "Batch loss: 0.07650629431009293 batch: 23/224\n",
      "Batch loss: 0.08903704583644867 batch: 24/224\n",
      "Batch loss: 0.03330984711647034 batch: 25/224\n",
      "Batch loss: 0.04874787479639053 batch: 26/224\n",
      "Batch loss: 0.051310036331415176 batch: 27/224\n",
      "Batch loss: 0.06365274637937546 batch: 28/224\n",
      "Batch loss: 0.07918187230825424 batch: 29/224\n",
      "Batch loss: 0.06798235327005386 batch: 30/224\n",
      "Batch loss: 0.07508914917707443 batch: 31/224\n",
      "Batch loss: 0.05931458994746208 batch: 32/224\n",
      "Batch loss: 0.0651303082704544 batch: 33/224\n",
      "Batch loss: 0.05997695028781891 batch: 34/224\n",
      "Batch loss: 0.08763069659471512 batch: 35/224\n",
      "Batch loss: 0.07821977138519287 batch: 36/224\n",
      "Batch loss: 0.0631730705499649 batch: 37/224\n",
      "Batch loss: 0.0663544237613678 batch: 38/224\n",
      "Batch loss: 0.06264994293451309 batch: 39/224\n",
      "Batch loss: 0.09360083192586899 batch: 40/224\n",
      "Batch loss: 0.09908268600702286 batch: 41/224\n",
      "Batch loss: 0.05653844773769379 batch: 42/224\n",
      "Batch loss: 0.07377711683511734 batch: 43/224\n",
      "Batch loss: 0.07864804565906525 batch: 44/224\n",
      "Batch loss: 0.0551576167345047 batch: 45/224\n",
      "Batch loss: 0.06240878999233246 batch: 46/224\n",
      "Batch loss: 0.0961473360657692 batch: 47/224\n",
      "Batch loss: 0.05074520409107208 batch: 48/224\n",
      "Batch loss: 0.08272095769643784 batch: 49/224\n",
      "Batch loss: 0.05860162153840065 batch: 50/224\n",
      "Batch loss: 0.08392096310853958 batch: 51/224\n",
      "Batch loss: 0.03619171306490898 batch: 52/224\n",
      "Batch loss: 0.060989174991846085 batch: 53/224\n",
      "Batch loss: 0.07341265678405762 batch: 54/224\n",
      "Batch loss: 0.06096203252673149 batch: 55/224\n",
      "Batch loss: 0.07086897641420364 batch: 56/224\n",
      "Batch loss: 0.098837710916996 batch: 57/224\n",
      "Batch loss: 0.09502266347408295 batch: 58/224\n",
      "Batch loss: 0.06818310916423798 batch: 59/224\n",
      "Batch loss: 0.10763536393642426 batch: 60/224\n",
      "Batch loss: 0.0588969960808754 batch: 61/224\n",
      "Batch loss: 0.06193581968545914 batch: 62/224\n",
      "Batch loss: 0.08976807445287704 batch: 63/224\n",
      "Batch loss: 0.08048442751169205 batch: 64/224\n",
      "Batch loss: 0.05192695930600166 batch: 65/224\n",
      "Batch loss: 0.10752873122692108 batch: 66/224\n",
      "Batch loss: 0.09876414388418198 batch: 67/224\n",
      "Batch loss: 0.05613106116652489 batch: 68/224\n",
      "Batch loss: 0.09584280848503113 batch: 69/224\n",
      "Batch loss: 0.06653869897127151 batch: 70/224\n",
      "Batch loss: 0.06525272876024246 batch: 71/224\n",
      "Batch loss: 0.06876905262470245 batch: 72/224\n",
      "Batch loss: 0.08985781669616699 batch: 73/224\n",
      "Batch loss: 0.04272589460015297 batch: 74/224\n",
      "Batch loss: 0.06825355440378189 batch: 75/224\n",
      "Batch loss: 0.09022746980190277 batch: 76/224\n",
      "Batch loss: 0.07426033169031143 batch: 77/224\n",
      "Batch loss: 0.06879962235689163 batch: 78/224\n",
      "Batch loss: 0.08610453456640244 batch: 79/224\n",
      "Batch loss: 0.07202986627817154 batch: 80/224\n",
      "Batch loss: 0.09238916635513306 batch: 81/224\n",
      "Batch loss: 0.08151151239871979 batch: 82/224\n",
      "Batch loss: 0.06789591163396835 batch: 83/224\n",
      "Batch loss: 0.04560684412717819 batch: 84/224\n",
      "Batch loss: 0.05645960196852684 batch: 85/224\n",
      "Batch loss: 0.07747195661067963 batch: 86/224\n",
      "Batch loss: 0.053886234760284424 batch: 87/224\n",
      "Batch loss: 0.07835295051336288 batch: 88/224\n",
      "Batch loss: 0.07087130844593048 batch: 89/224\n",
      "Batch loss: 0.07048659026622772 batch: 90/224\n",
      "Batch loss: 0.05526299402117729 batch: 91/224\n",
      "Batch loss: 0.05554597079753876 batch: 92/224\n",
      "Batch loss: 0.02281246893107891 batch: 93/224\n",
      "Batch loss: 0.06370431929826736 batch: 94/224\n",
      "Batch loss: 0.05203556269407272 batch: 95/224\n",
      "Batch loss: 0.0995531901717186 batch: 96/224\n",
      "Batch loss: 0.051925066858530045 batch: 97/224\n",
      "Batch loss: 0.04704534262418747 batch: 98/224\n",
      "Batch loss: 0.09313623607158661 batch: 99/224\n",
      "Batch loss: 0.07453247159719467 batch: 100/224\n",
      "Batch loss: 0.07164649665355682 batch: 101/224\n",
      "Batch loss: 0.0649028867483139 batch: 102/224\n",
      "Batch loss: 0.08865641802549362 batch: 103/224\n",
      "Batch loss: 0.03708135336637497 batch: 104/224\n",
      "Batch loss: 0.0544184111058712 batch: 105/224\n",
      "Batch loss: 0.07859417796134949 batch: 106/224\n",
      "Batch loss: 0.07829149812459946 batch: 107/224\n",
      "Batch loss: 0.06928025931119919 batch: 108/224\n",
      "Batch loss: 0.03986722230911255 batch: 109/224\n",
      "Batch loss: 0.0473148413002491 batch: 110/224\n",
      "Batch loss: 0.09753774106502533 batch: 111/224\n",
      "Batch loss: 0.05999748408794403 batch: 112/224\n",
      "Batch loss: 0.06715115904808044 batch: 113/224\n",
      "Batch loss: 0.04099387302994728 batch: 114/224\n",
      "Batch loss: 0.07602374255657196 batch: 115/224\n",
      "Batch loss: 0.06277801841497421 batch: 116/224\n",
      "Batch loss: 0.03678320720791817 batch: 117/224\n",
      "Batch loss: 0.07073021680116653 batch: 118/224\n",
      "Batch loss: 0.06191891431808472 batch: 119/224\n",
      "Batch loss: 0.08573353290557861 batch: 120/224\n",
      "Batch loss: 0.06663194298744202 batch: 121/224\n",
      "Batch loss: 0.06822739541530609 batch: 122/224\n",
      "Batch loss: 0.07020597159862518 batch: 123/224\n",
      "Batch loss: 0.06073049083352089 batch: 124/224\n",
      "Batch loss: 0.0712442398071289 batch: 125/224\n",
      "Batch loss: 0.07602711766958237 batch: 126/224\n",
      "Batch loss: 0.09056233614683151 batch: 127/224\n",
      "Batch loss: 0.06910248100757599 batch: 128/224\n",
      "Batch loss: 0.05529583990573883 batch: 129/224\n",
      "Batch loss: 0.06494654715061188 batch: 130/224\n",
      "Batch loss: 0.0647834911942482 batch: 131/224\n",
      "Batch loss: 0.07041741162538528 batch: 132/224\n",
      "Batch loss: 0.07703570276498795 batch: 133/224\n",
      "Batch loss: 0.10059845447540283 batch: 134/224\n",
      "Batch loss: 0.05255967378616333 batch: 135/224\n",
      "Batch loss: 0.08452974259853363 batch: 136/224\n",
      "Batch loss: 0.08124969899654388 batch: 137/224\n",
      "Batch loss: 0.07405368238687515 batch: 138/224\n",
      "Batch loss: 0.06487984955310822 batch: 139/224\n",
      "Batch loss: 0.0729791522026062 batch: 140/224\n",
      "Batch loss: 0.04854762926697731 batch: 141/224\n",
      "Batch loss: 0.08585299551486969 batch: 142/224\n",
      "Batch loss: 0.06175117939710617 batch: 143/224\n",
      "Batch loss: 0.06946443766355515 batch: 144/224\n",
      "Batch loss: 0.0698874220252037 batch: 145/224\n",
      "Batch loss: 0.09531594812870026 batch: 146/224\n",
      "Batch loss: 0.07854586839675903 batch: 147/224\n",
      "Batch loss: 0.04798676446080208 batch: 148/224\n",
      "Batch loss: 0.09835085272789001 batch: 149/224\n",
      "Batch loss: 0.08050122112035751 batch: 150/224\n",
      "Batch loss: 0.05667232722043991 batch: 151/224\n",
      "Batch loss: 0.09374530613422394 batch: 152/224\n",
      "Batch loss: 0.0750504657626152 batch: 153/224\n",
      "Batch loss: 0.10426968336105347 batch: 154/224\n",
      "Batch loss: 0.042101651430130005 batch: 155/224\n",
      "Batch loss: 0.06129429116845131 batch: 156/224\n",
      "Batch loss: 0.07602128386497498 batch: 157/224\n",
      "Batch loss: 0.1200464740395546 batch: 158/224\n",
      "Batch loss: 0.06673590838909149 batch: 159/224\n",
      "Batch loss: 0.06126849725842476 batch: 160/224\n",
      "Batch loss: 0.05595572292804718 batch: 161/224\n",
      "Batch loss: 0.07311157137155533 batch: 162/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.08248168975114822 batch: 163/224\n",
      "Batch loss: 0.04160057380795479 batch: 164/224\n",
      "Batch loss: 0.0741167664527893 batch: 165/224\n",
      "Batch loss: 0.0669402927160263 batch: 166/224\n",
      "Batch loss: 0.06015263497829437 batch: 167/224\n",
      "Batch loss: 0.07159877568483353 batch: 168/224\n",
      "Batch loss: 0.08489076048135757 batch: 169/224\n",
      "Batch loss: 0.041495181620121 batch: 170/224\n",
      "Batch loss: 0.05680892989039421 batch: 171/224\n",
      "Batch loss: 0.08878249675035477 batch: 172/224\n",
      "Batch loss: 0.0826154500246048 batch: 173/224\n",
      "Batch loss: 0.04356008023023605 batch: 174/224\n",
      "Batch loss: 0.06203446537256241 batch: 175/224\n",
      "Batch loss: 0.08632561564445496 batch: 176/224\n",
      "Batch loss: 0.10292273014783859 batch: 177/224\n",
      "Batch loss: 0.04261494427919388 batch: 178/224\n",
      "Batch loss: 0.11778444796800613 batch: 179/224\n",
      "Batch loss: 0.04822002351284027 batch: 180/224\n",
      "Batch loss: 0.045954927802085876 batch: 181/224\n",
      "Batch loss: 0.0804273709654808 batch: 182/224\n",
      "Batch loss: 0.06970860064029694 batch: 183/224\n",
      "Batch loss: 0.07733722776174545 batch: 184/224\n",
      "Batch loss: 0.09166699647903442 batch: 185/224\n",
      "Batch loss: 0.055191077291965485 batch: 186/224\n",
      "Batch loss: 0.10049553215503693 batch: 187/224\n",
      "Batch loss: 0.06905683130025864 batch: 188/224\n",
      "Batch loss: 0.06203547865152359 batch: 189/224\n",
      "Batch loss: 0.059691719710826874 batch: 190/224\n",
      "Batch loss: 0.06446008384227753 batch: 191/224\n",
      "Batch loss: 0.06816405057907104 batch: 192/224\n",
      "Batch loss: 0.0716085359454155 batch: 193/224\n",
      "Batch loss: 0.07364089787006378 batch: 194/224\n",
      "Batch loss: 0.07518249750137329 batch: 195/224\n",
      "Batch loss: 0.054960109293460846 batch: 196/224\n",
      "Batch loss: 0.060590922832489014 batch: 197/224\n",
      "Batch loss: 0.07354161888360977 batch: 198/224\n",
      "Batch loss: 0.0853080227971077 batch: 199/224\n",
      "Batch loss: 0.07190874963998795 batch: 200/224\n",
      "Batch loss: 0.10302632302045822 batch: 201/224\n",
      "Batch loss: 0.05441422760486603 batch: 202/224\n",
      "Batch loss: 0.08513754606246948 batch: 203/224\n",
      "Batch loss: 0.1309090256690979 batch: 204/224\n",
      "Batch loss: 0.0406191386282444 batch: 205/224\n",
      "Batch loss: 0.08496060222387314 batch: 206/224\n",
      "Batch loss: 0.05647963285446167 batch: 207/224\n",
      "Batch loss: 0.05741173401474953 batch: 208/224\n",
      "Batch loss: 0.07220450788736343 batch: 209/224\n",
      "Batch loss: 0.07046609371900558 batch: 210/224\n",
      "Batch loss: 0.0774340108036995 batch: 211/224\n",
      "Batch loss: 0.07244516164064407 batch: 212/224\n",
      "Batch loss: 0.08087562769651413 batch: 213/224\n",
      "Batch loss: 0.07800856977701187 batch: 214/224\n",
      "Batch loss: 0.07376230508089066 batch: 215/224\n",
      "Batch loss: 0.05797785893082619 batch: 216/224\n",
      "Batch loss: 0.06640096753835678 batch: 217/224\n",
      "Batch loss: 0.0668087974190712 batch: 218/224\n",
      "Batch loss: 0.06561668962240219 batch: 219/224\n",
      "Batch loss: 0.06516644358634949 batch: 220/224\n",
      "Batch loss: 0.07115897536277771 batch: 221/224\n",
      "Batch loss: 0.056429654359817505 batch: 222/224\n",
      "Batch loss: 0.08013291656970978 batch: 223/224\n",
      "Batch loss: 0.060528747737407684 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 57/75..  Training Loss: 0.00014..  Test Loss: 0.00100..  Test Accuracy: 0.89264\n",
      "Running epoch 58/75\n",
      "Batch loss: 0.040795113891363144 batch: 1/224\n",
      "Batch loss: 0.06792455911636353 batch: 2/224\n",
      "Batch loss: 0.05391177907586098 batch: 3/224\n",
      "Batch loss: 0.11137420684099197 batch: 4/224\n",
      "Batch loss: 0.06251814216375351 batch: 5/224\n",
      "Batch loss: 0.07324206084012985 batch: 6/224\n",
      "Batch loss: 0.07814813405275345 batch: 7/224\n",
      "Batch loss: 0.06615538895130157 batch: 8/224\n",
      "Batch loss: 0.08051484823226929 batch: 9/224\n",
      "Batch loss: 0.04595891013741493 batch: 10/224\n",
      "Batch loss: 0.07624449580907822 batch: 11/224\n",
      "Batch loss: 0.06826949119567871 batch: 12/224\n",
      "Batch loss: 0.057903360575437546 batch: 13/224\n",
      "Batch loss: 0.05012934282422066 batch: 14/224\n",
      "Batch loss: 0.0603807270526886 batch: 15/224\n",
      "Batch loss: 0.07713501155376434 batch: 16/224\n",
      "Batch loss: 0.07084956765174866 batch: 17/224\n",
      "Batch loss: 0.05998057499527931 batch: 18/224\n",
      "Batch loss: 0.059676893055438995 batch: 19/224\n",
      "Batch loss: 0.04753740876913071 batch: 20/224\n",
      "Batch loss: 0.0646725669503212 batch: 21/224\n",
      "Batch loss: 0.076227568089962 batch: 22/224\n",
      "Batch loss: 0.09008017927408218 batch: 23/224\n",
      "Batch loss: 0.0854993388056755 batch: 24/224\n",
      "Batch loss: 0.05286320298910141 batch: 25/224\n",
      "Batch loss: 0.05008923262357712 batch: 26/224\n",
      "Batch loss: 0.07203138619661331 batch: 27/224\n",
      "Batch loss: 0.08847984671592712 batch: 28/224\n",
      "Batch loss: 0.06786888837814331 batch: 29/224\n",
      "Batch loss: 0.08984841406345367 batch: 30/224\n",
      "Batch loss: 0.06096237152814865 batch: 31/224\n",
      "Batch loss: 0.04766613990068436 batch: 32/224\n",
      "Batch loss: 0.04517289251089096 batch: 33/224\n",
      "Batch loss: 0.06710349023342133 batch: 34/224\n",
      "Batch loss: 0.04794677346944809 batch: 35/224\n",
      "Batch loss: 0.08212675154209137 batch: 36/224\n",
      "Batch loss: 0.07067065685987473 batch: 37/224\n",
      "Batch loss: 0.08620206266641617 batch: 38/224\n",
      "Batch loss: 0.08041704446077347 batch: 39/224\n",
      "Batch loss: 0.04880952462553978 batch: 40/224\n",
      "Batch loss: 0.0928359404206276 batch: 41/224\n",
      "Batch loss: 0.06107942759990692 batch: 42/224\n",
      "Batch loss: 0.07132921367883682 batch: 43/224\n",
      "Batch loss: 0.051110077649354935 batch: 44/224\n",
      "Batch loss: 0.07605133950710297 batch: 45/224\n",
      "Batch loss: 0.08732790499925613 batch: 46/224\n",
      "Batch loss: 0.039994604885578156 batch: 47/224\n",
      "Batch loss: 0.039411768317222595 batch: 48/224\n",
      "Batch loss: 0.04712012782692909 batch: 49/224\n",
      "Batch loss: 0.05501497536897659 batch: 50/224\n",
      "Batch loss: 0.0842539519071579 batch: 51/224\n",
      "Batch loss: 0.06525138020515442 batch: 52/224\n",
      "Batch loss: 0.08577706664800644 batch: 53/224\n",
      "Batch loss: 0.04860316589474678 batch: 54/224\n",
      "Batch loss: 0.03887814283370972 batch: 55/224\n",
      "Batch loss: 0.0673099011182785 batch: 56/224\n",
      "Batch loss: 0.09982877969741821 batch: 57/224\n",
      "Batch loss: 0.08417987823486328 batch: 58/224\n",
      "Batch loss: 0.04912012070417404 batch: 59/224\n",
      "Batch loss: 0.11347297579050064 batch: 60/224\n",
      "Batch loss: 0.06326370686292648 batch: 61/224\n",
      "Batch loss: 0.04176397994160652 batch: 62/224\n",
      "Batch loss: 0.07669084519147873 batch: 63/224\n",
      "Batch loss: 0.08866287767887115 batch: 64/224\n",
      "Batch loss: 0.04540644958615303 batch: 65/224\n",
      "Batch loss: 0.07876763492822647 batch: 66/224\n",
      "Batch loss: 0.057446833699941635 batch: 67/224\n",
      "Batch loss: 0.06356319785118103 batch: 68/224\n",
      "Batch loss: 0.0548226423561573 batch: 69/224\n",
      "Batch loss: 0.07587896287441254 batch: 70/224\n",
      "Batch loss: 0.07479740679264069 batch: 71/224\n",
      "Batch loss: 0.0380111001431942 batch: 72/224\n",
      "Batch loss: 0.08913061022758484 batch: 73/224\n",
      "Batch loss: 0.07030138373374939 batch: 74/224\n",
      "Batch loss: 0.11232588440179825 batch: 75/224\n",
      "Batch loss: 0.06083447113633156 batch: 76/224\n",
      "Batch loss: 0.09559128433465958 batch: 77/224\n",
      "Batch loss: 0.08967610448598862 batch: 78/224\n",
      "Batch loss: 0.06664469093084335 batch: 79/224\n",
      "Batch loss: 0.07720310240983963 batch: 80/224\n",
      "Batch loss: 0.09266043454408646 batch: 81/224\n",
      "Batch loss: 0.07401445508003235 batch: 82/224\n",
      "Batch loss: 0.059198666363954544 batch: 83/224\n",
      "Batch loss: 0.04495735466480255 batch: 84/224\n",
      "Batch loss: 0.0627567246556282 batch: 85/224\n",
      "Batch loss: 0.06601662933826447 batch: 86/224\n",
      "Batch loss: 0.10804199427366257 batch: 87/224\n",
      "Batch loss: 0.0850302129983902 batch: 88/224\n",
      "Batch loss: 0.05042331665754318 batch: 89/224\n",
      "Batch loss: 0.05341099575161934 batch: 90/224\n",
      "Batch loss: 0.05608872324228287 batch: 91/224\n",
      "Batch loss: 0.06784006208181381 batch: 92/224\n",
      "Batch loss: 0.048957888036966324 batch: 93/224\n",
      "Batch loss: 0.047630637884140015 batch: 94/224\n",
      "Batch loss: 0.03558215871453285 batch: 95/224\n",
      "Batch loss: 0.07043040543794632 batch: 96/224\n",
      "Batch loss: 0.05402389541268349 batch: 97/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.05191429704427719 batch: 98/224\n",
      "Batch loss: 0.06372793763875961 batch: 99/224\n",
      "Batch loss: 0.08493183553218842 batch: 100/224\n",
      "Batch loss: 0.07992468774318695 batch: 101/224\n",
      "Batch loss: 0.06371250003576279 batch: 102/224\n",
      "Batch loss: 0.07266920059919357 batch: 103/224\n",
      "Batch loss: 0.052047573029994965 batch: 104/224\n",
      "Batch loss: 0.06067967042326927 batch: 105/224\n",
      "Batch loss: 0.07650604099035263 batch: 106/224\n",
      "Batch loss: 0.054593127220869064 batch: 107/224\n",
      "Batch loss: 0.05881606787443161 batch: 108/224\n",
      "Batch loss: 0.03850862756371498 batch: 109/224\n",
      "Batch loss: 0.05306524038314819 batch: 110/224\n",
      "Batch loss: 0.05683961510658264 batch: 111/224\n",
      "Batch loss: 0.06758786737918854 batch: 112/224\n",
      "Batch loss: 0.05297306925058365 batch: 113/224\n",
      "Batch loss: 0.04226711392402649 batch: 114/224\n",
      "Batch loss: 0.054397787898778915 batch: 115/224\n",
      "Batch loss: 0.05517824739217758 batch: 116/224\n",
      "Batch loss: 0.049731794744729996 batch: 117/224\n",
      "Batch loss: 0.07434830069541931 batch: 118/224\n",
      "Batch loss: 0.06309068948030472 batch: 119/224\n",
      "Batch loss: 0.08012326061725616 batch: 120/224\n",
      "Batch loss: 0.05329340696334839 batch: 121/224\n",
      "Batch loss: 0.05169873684644699 batch: 122/224\n",
      "Batch loss: 0.0632544457912445 batch: 123/224\n",
      "Batch loss: 0.08383452892303467 batch: 124/224\n",
      "Batch loss: 0.08001197874546051 batch: 125/224\n",
      "Batch loss: 0.07061684131622314 batch: 126/224\n",
      "Batch loss: 0.08632451295852661 batch: 127/224\n",
      "Batch loss: 0.06495633721351624 batch: 128/224\n",
      "Batch loss: 0.061275362968444824 batch: 129/224\n",
      "Batch loss: 0.0753260925412178 batch: 130/224\n",
      "Batch loss: 0.05506962910294533 batch: 131/224\n",
      "Batch loss: 0.05373229458928108 batch: 132/224\n",
      "Batch loss: 0.06006148084998131 batch: 133/224\n",
      "Batch loss: 0.11536503583192825 batch: 134/224\n",
      "Batch loss: 0.07519282400608063 batch: 135/224\n",
      "Batch loss: 0.09817706048488617 batch: 136/224\n",
      "Batch loss: 0.046782348304986954 batch: 137/224\n",
      "Batch loss: 0.09597285091876984 batch: 138/224\n",
      "Batch loss: 0.08065979182720184 batch: 139/224\n",
      "Batch loss: 0.08296208828687668 batch: 140/224\n",
      "Batch loss: 0.06998559087514877 batch: 141/224\n",
      "Batch loss: 0.05619430169463158 batch: 142/224\n",
      "Batch loss: 0.06772229075431824 batch: 143/224\n",
      "Batch loss: 0.049252480268478394 batch: 144/224\n",
      "Batch loss: 0.07870709151029587 batch: 145/224\n",
      "Batch loss: 0.07432185113430023 batch: 146/224\n",
      "Batch loss: 0.05696464702486992 batch: 147/224\n",
      "Batch loss: 0.07901964336633682 batch: 148/224\n",
      "Batch loss: 0.08988453447818756 batch: 149/224\n",
      "Batch loss: 0.06861000508069992 batch: 150/224\n",
      "Batch loss: 0.06152130663394928 batch: 151/224\n",
      "Batch loss: 0.08380063623189926 batch: 152/224\n",
      "Batch loss: 0.0667143389582634 batch: 153/224\n",
      "Batch loss: 0.08426743000745773 batch: 154/224\n",
      "Batch loss: 0.04777119681239128 batch: 155/224\n",
      "Batch loss: 0.061477549374103546 batch: 156/224\n",
      "Batch loss: 0.10369212180376053 batch: 157/224\n",
      "Batch loss: 0.10250049084424973 batch: 158/224\n",
      "Batch loss: 0.04965308681130409 batch: 159/224\n",
      "Batch loss: 0.06597264111042023 batch: 160/224\n",
      "Batch loss: 0.07634550333023071 batch: 161/224\n",
      "Batch loss: 0.056054916232824326 batch: 162/224\n",
      "Batch loss: 0.054407067596912384 batch: 163/224\n",
      "Batch loss: 0.07363373786211014 batch: 164/224\n",
      "Batch loss: 0.07239210605621338 batch: 165/224\n",
      "Batch loss: 0.053501565009355545 batch: 166/224\n",
      "Batch loss: 0.04917541891336441 batch: 167/224\n",
      "Batch loss: 0.0591927245259285 batch: 168/224\n",
      "Batch loss: 0.0850958451628685 batch: 169/224\n",
      "Batch loss: 0.07236285507678986 batch: 170/224\n",
      "Batch loss: 0.06701107323169708 batch: 171/224\n",
      "Batch loss: 0.056698840111494064 batch: 172/224\n",
      "Batch loss: 0.09544418007135391 batch: 173/224\n",
      "Batch loss: 0.05647416412830353 batch: 174/224\n",
      "Batch loss: 0.0739305317401886 batch: 175/224\n",
      "Batch loss: 0.07443050295114517 batch: 176/224\n",
      "Batch loss: 0.07129216194152832 batch: 177/224\n",
      "Batch loss: 0.04784956946969032 batch: 178/224\n",
      "Batch loss: 0.06443053483963013 batch: 179/224\n",
      "Batch loss: 0.0453651025891304 batch: 180/224\n",
      "Batch loss: 0.046565208584070206 batch: 181/224\n",
      "Batch loss: 0.08904970437288284 batch: 182/224\n",
      "Batch loss: 0.059206075966358185 batch: 183/224\n",
      "Batch loss: 0.1028243824839592 batch: 184/224\n",
      "Batch loss: 0.06597458571195602 batch: 185/224\n",
      "Batch loss: 0.07624609023332596 batch: 186/224\n",
      "Batch loss: 0.07187467068433762 batch: 187/224\n",
      "Batch loss: 0.07783980667591095 batch: 188/224\n",
      "Batch loss: 0.0580127015709877 batch: 189/224\n",
      "Batch loss: 0.04679543152451515 batch: 190/224\n",
      "Batch loss: 0.06735555827617645 batch: 191/224\n",
      "Batch loss: 0.06306397914886475 batch: 192/224\n",
      "Batch loss: 0.07736164331436157 batch: 193/224\n",
      "Batch loss: 0.07277297973632812 batch: 194/224\n",
      "Batch loss: 0.07123985886573792 batch: 195/224\n",
      "Batch loss: 0.0838562622666359 batch: 196/224\n",
      "Batch loss: 0.07044775784015656 batch: 197/224\n",
      "Batch loss: 0.07466313987970352 batch: 198/224\n",
      "Batch loss: 0.055637143552303314 batch: 199/224\n",
      "Batch loss: 0.05814458802342415 batch: 200/224\n",
      "Batch loss: 0.06693246215581894 batch: 201/224\n",
      "Batch loss: 0.05572417378425598 batch: 202/224\n",
      "Batch loss: 0.07798219472169876 batch: 203/224\n",
      "Batch loss: 0.06004701927304268 batch: 204/224\n",
      "Batch loss: 0.08238683640956879 batch: 205/224\n",
      "Batch loss: 0.07902731001377106 batch: 206/224\n",
      "Batch loss: 0.06636358052492142 batch: 207/224\n",
      "Batch loss: 0.041578035801649094 batch: 208/224\n",
      "Batch loss: 0.056891124695539474 batch: 209/224\n",
      "Batch loss: 0.06593091785907745 batch: 210/224\n",
      "Batch loss: 0.06052879989147186 batch: 211/224\n",
      "Batch loss: 0.045509062707424164 batch: 212/224\n",
      "Batch loss: 0.1046641394495964 batch: 213/224\n",
      "Batch loss: 0.08490663021802902 batch: 214/224\n",
      "Batch loss: 0.07495661079883575 batch: 215/224\n",
      "Batch loss: 0.05612986162304878 batch: 216/224\n",
      "Batch loss: 0.08983571082353592 batch: 217/224\n",
      "Batch loss: 0.04729665815830231 batch: 218/224\n",
      "Batch loss: 0.06157834455370903 batch: 219/224\n",
      "Batch loss: 0.049540698528289795 batch: 220/224\n",
      "Batch loss: 0.09355425089597702 batch: 221/224\n",
      "Batch loss: 0.09331447631120682 batch: 222/224\n",
      "Batch loss: 0.054653462022542953 batch: 223/224\n",
      "Batch loss: 0.06607696413993835 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 58/75..  Training Loss: 0.00014..  Test Loss: 0.00103..  Test Accuracy: 0.89154\n",
      "Running epoch 59/75\n",
      "Batch loss: 0.06313563883304596 batch: 1/224\n",
      "Batch loss: 0.07520104199647903 batch: 2/224\n",
      "Batch loss: 0.07661004364490509 batch: 3/224\n",
      "Batch loss: 0.09058502316474915 batch: 4/224\n",
      "Batch loss: 0.05972824618220329 batch: 5/224\n",
      "Batch loss: 0.08016638457775116 batch: 6/224\n",
      "Batch loss: 0.0645630806684494 batch: 7/224\n",
      "Batch loss: 0.05622527003288269 batch: 8/224\n",
      "Batch loss: 0.057013239711523056 batch: 9/224\n",
      "Batch loss: 0.07715519517660141 batch: 10/224\n",
      "Batch loss: 0.09724671393632889 batch: 11/224\n",
      "Batch loss: 0.08633101731538773 batch: 12/224\n",
      "Batch loss: 0.05523008480668068 batch: 13/224\n",
      "Batch loss: 0.051549289375543594 batch: 14/224\n",
      "Batch loss: 0.07354989647865295 batch: 15/224\n",
      "Batch loss: 0.08072246611118317 batch: 16/224\n",
      "Batch loss: 0.05132921040058136 batch: 17/224\n",
      "Batch loss: 0.08024288713932037 batch: 18/224\n",
      "Batch loss: 0.05753548815846443 batch: 19/224\n",
      "Batch loss: 0.0614483579993248 batch: 20/224\n",
      "Batch loss: 0.05767587944865227 batch: 21/224\n",
      "Batch loss: 0.05333564430475235 batch: 22/224\n",
      "Batch loss: 0.0707111582159996 batch: 23/224\n",
      "Batch loss: 0.07390528917312622 batch: 24/224\n",
      "Batch loss: 0.06141244247555733 batch: 25/224\n",
      "Batch loss: 0.054095558822155 batch: 26/224\n",
      "Batch loss: 0.04199881851673126 batch: 27/224\n",
      "Batch loss: 0.07319951057434082 batch: 28/224\n",
      "Batch loss: 0.08273933827877045 batch: 29/224\n",
      "Batch loss: 0.04088698700070381 batch: 30/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.046857137233018875 batch: 31/224\n",
      "Batch loss: 0.06962726265192032 batch: 32/224\n",
      "Batch loss: 0.053390517830848694 batch: 33/224\n",
      "Batch loss: 0.07561414688825607 batch: 34/224\n",
      "Batch loss: 0.05555826053023338 batch: 35/224\n",
      "Batch loss: 0.05049114301800728 batch: 36/224\n",
      "Batch loss: 0.04941360652446747 batch: 37/224\n",
      "Batch loss: 0.09405007213354111 batch: 38/224\n",
      "Batch loss: 0.08297007530927658 batch: 39/224\n",
      "Batch loss: 0.06123366951942444 batch: 40/224\n",
      "Batch loss: 0.05911055579781532 batch: 41/224\n",
      "Batch loss: 0.047583553940057755 batch: 42/224\n",
      "Batch loss: 0.08890452235937119 batch: 43/224\n",
      "Batch loss: 0.058150071650743484 batch: 44/224\n",
      "Batch loss: 0.0842074304819107 batch: 45/224\n",
      "Batch loss: 0.10501068085432053 batch: 46/224\n",
      "Batch loss: 0.05598868429660797 batch: 47/224\n",
      "Batch loss: 0.04658976197242737 batch: 48/224\n",
      "Batch loss: 0.04000917077064514 batch: 49/224\n",
      "Batch loss: 0.06056785210967064 batch: 50/224\n",
      "Batch loss: 0.06328561156988144 batch: 51/224\n",
      "Batch loss: 0.06334902346134186 batch: 52/224\n",
      "Batch loss: 0.0662098303437233 batch: 53/224\n",
      "Batch loss: 0.05128396674990654 batch: 54/224\n",
      "Batch loss: 0.05877094715833664 batch: 55/224\n",
      "Batch loss: 0.07584008574485779 batch: 56/224\n",
      "Batch loss: 0.06480035930871964 batch: 57/224\n",
      "Batch loss: 0.08105387538671494 batch: 58/224\n",
      "Batch loss: 0.07161759585142136 batch: 59/224\n",
      "Batch loss: 0.08037255704402924 batch: 60/224\n",
      "Batch loss: 0.06393687427043915 batch: 61/224\n",
      "Batch loss: 0.06029197573661804 batch: 62/224\n",
      "Batch loss: 0.08462777733802795 batch: 63/224\n",
      "Batch loss: 0.09243012219667435 batch: 64/224\n",
      "Batch loss: 0.06175505369901657 batch: 65/224\n",
      "Batch loss: 0.06547679007053375 batch: 66/224\n",
      "Batch loss: 0.06814417243003845 batch: 67/224\n",
      "Batch loss: 0.06250666826963425 batch: 68/224\n",
      "Batch loss: 0.07878179848194122 batch: 69/224\n",
      "Batch loss: 0.07067744433879852 batch: 70/224\n",
      "Batch loss: 0.0893973782658577 batch: 71/224\n",
      "Batch loss: 0.050063882023096085 batch: 72/224\n",
      "Batch loss: 0.08547145128250122 batch: 73/224\n",
      "Batch loss: 0.06065317615866661 batch: 74/224\n",
      "Batch loss: 0.08892897516489029 batch: 75/224\n",
      "Batch loss: 0.09207873046398163 batch: 76/224\n",
      "Batch loss: 0.06401069462299347 batch: 77/224\n",
      "Batch loss: 0.07889958471059799 batch: 78/224\n",
      "Batch loss: 0.08423631638288498 batch: 79/224\n",
      "Batch loss: 0.06154702231287956 batch: 80/224\n",
      "Batch loss: 0.06462180614471436 batch: 81/224\n",
      "Batch loss: 0.057523321360349655 batch: 82/224\n",
      "Batch loss: 0.05301237106323242 batch: 83/224\n",
      "Batch loss: 0.06941767036914825 batch: 84/224\n",
      "Batch loss: 0.057436250150203705 batch: 85/224\n",
      "Batch loss: 0.07309655100107193 batch: 86/224\n",
      "Batch loss: 0.09041527658700943 batch: 87/224\n",
      "Batch loss: 0.07066243141889572 batch: 88/224\n",
      "Batch loss: 0.06489875912666321 batch: 89/224\n",
      "Batch loss: 0.06325147300958633 batch: 90/224\n",
      "Batch loss: 0.04058967903256416 batch: 91/224\n",
      "Batch loss: 0.05481165647506714 batch: 92/224\n",
      "Batch loss: 0.052473850548267365 batch: 93/224\n",
      "Batch loss: 0.06060592830181122 batch: 94/224\n",
      "Batch loss: 0.04351022467017174 batch: 95/224\n",
      "Batch loss: 0.08685131371021271 batch: 96/224\n",
      "Batch loss: 0.06315531581640244 batch: 97/224\n",
      "Batch loss: 0.0630330964922905 batch: 98/224\n",
      "Batch loss: 0.06982507556676865 batch: 99/224\n",
      "Batch loss: 0.05183316767215729 batch: 100/224\n",
      "Batch loss: 0.07508178800344467 batch: 101/224\n",
      "Batch loss: 0.05526904761791229 batch: 102/224\n",
      "Batch loss: 0.08155391365289688 batch: 103/224\n",
      "Batch loss: 0.05989224836230278 batch: 104/224\n",
      "Batch loss: 0.05427395552396774 batch: 105/224\n",
      "Batch loss: 0.06566596776247025 batch: 106/224\n",
      "Batch loss: 0.0395214669406414 batch: 107/224\n",
      "Batch loss: 0.07580742239952087 batch: 108/224\n",
      "Batch loss: 0.045811884105205536 batch: 109/224\n",
      "Batch loss: 0.07888634502887726 batch: 110/224\n",
      "Batch loss: 0.06680140644311905 batch: 111/224\n",
      "Batch loss: 0.08831797540187836 batch: 112/224\n",
      "Batch loss: 0.05871321260929108 batch: 113/224\n",
      "Batch loss: 0.0617787204682827 batch: 114/224\n",
      "Batch loss: 0.03731855750083923 batch: 115/224\n",
      "Batch loss: 0.058903757482767105 batch: 116/224\n",
      "Batch loss: 0.026400059461593628 batch: 117/224\n",
      "Batch loss: 0.06150702387094498 batch: 118/224\n",
      "Batch loss: 0.04396873712539673 batch: 119/224\n",
      "Batch loss: 0.09164926409721375 batch: 120/224\n",
      "Batch loss: 0.09490151703357697 batch: 121/224\n",
      "Batch loss: 0.07034686207771301 batch: 122/224\n",
      "Batch loss: 0.045036789029836655 batch: 123/224\n",
      "Batch loss: 0.052834223955869675 batch: 124/224\n",
      "Batch loss: 0.08560388535261154 batch: 125/224\n",
      "Batch loss: 0.09439466893672943 batch: 126/224\n",
      "Batch loss: 0.0999152883887291 batch: 127/224\n",
      "Batch loss: 0.05443163961172104 batch: 128/224\n",
      "Batch loss: 0.04262089356780052 batch: 129/224\n",
      "Batch loss: 0.07095944136381149 batch: 130/224\n",
      "Batch loss: 0.06854065507650375 batch: 131/224\n",
      "Batch loss: 0.05806618928909302 batch: 132/224\n",
      "Batch loss: 0.1243312880396843 batch: 133/224\n",
      "Batch loss: 0.06345423310995102 batch: 134/224\n",
      "Batch loss: 0.0632440596818924 batch: 135/224\n",
      "Batch loss: 0.0801694318652153 batch: 136/224\n",
      "Batch loss: 0.04729335010051727 batch: 137/224\n",
      "Batch loss: 0.06310529261827469 batch: 138/224\n",
      "Batch loss: 0.10426159203052521 batch: 139/224\n",
      "Batch loss: 0.0857340469956398 batch: 140/224\n",
      "Batch loss: 0.06758836656808853 batch: 141/224\n",
      "Batch loss: 0.06320016831159592 batch: 142/224\n",
      "Batch loss: 0.05436967313289642 batch: 143/224\n",
      "Batch loss: 0.043795567005872726 batch: 144/224\n",
      "Batch loss: 0.09691082686185837 batch: 145/224\n",
      "Batch loss: 0.08351446688175201 batch: 146/224\n",
      "Batch loss: 0.04808831587433815 batch: 147/224\n",
      "Batch loss: 0.0348883755505085 batch: 148/224\n",
      "Batch loss: 0.08214901387691498 batch: 149/224\n",
      "Batch loss: 0.07651317864656448 batch: 150/224\n",
      "Batch loss: 0.05911119282245636 batch: 151/224\n",
      "Batch loss: 0.05770374834537506 batch: 152/224\n",
      "Batch loss: 0.07134483009576797 batch: 153/224\n",
      "Batch loss: 0.07856878638267517 batch: 154/224\n",
      "Batch loss: 0.06013331934809685 batch: 155/224\n",
      "Batch loss: 0.06520893424749374 batch: 156/224\n",
      "Batch loss: 0.06411800533533096 batch: 157/224\n",
      "Batch loss: 0.09981029480695724 batch: 158/224\n",
      "Batch loss: 0.09011685103178024 batch: 159/224\n",
      "Batch loss: 0.06712117791175842 batch: 160/224\n",
      "Batch loss: 0.0605241134762764 batch: 161/224\n",
      "Batch loss: 0.06368663161993027 batch: 162/224\n",
      "Batch loss: 0.053528375923633575 batch: 163/224\n",
      "Batch loss: 0.04519105330109596 batch: 164/224\n",
      "Batch loss: 0.06476558744907379 batch: 165/224\n",
      "Batch loss: 0.07254073768854141 batch: 166/224\n",
      "Batch loss: 0.040246132761240005 batch: 167/224\n",
      "Batch loss: 0.041334059089422226 batch: 168/224\n",
      "Batch loss: 0.06587129086256027 batch: 169/224\n",
      "Batch loss: 0.0592922605574131 batch: 170/224\n",
      "Batch loss: 0.05032690614461899 batch: 171/224\n",
      "Batch loss: 0.05791107192635536 batch: 172/224\n",
      "Batch loss: 0.06843145936727524 batch: 173/224\n",
      "Batch loss: 0.06536640971899033 batch: 174/224\n",
      "Batch loss: 0.05979740247130394 batch: 175/224\n",
      "Batch loss: 0.06199456378817558 batch: 176/224\n",
      "Batch loss: 0.07552255690097809 batch: 177/224\n",
      "Batch loss: 0.0377749539911747 batch: 178/224\n",
      "Batch loss: 0.058406468480825424 batch: 179/224\n",
      "Batch loss: 0.03507944941520691 batch: 180/224\n",
      "Batch loss: 0.038830261677503586 batch: 181/224\n",
      "Batch loss: 0.08099908381700516 batch: 182/224\n",
      "Batch loss: 0.059509169310331345 batch: 183/224\n",
      "Batch loss: 0.08817854523658752 batch: 184/224\n",
      "Batch loss: 0.0857754796743393 batch: 185/224\n",
      "Batch loss: 0.06838218867778778 batch: 186/224\n",
      "Batch loss: 0.07992511987686157 batch: 187/224\n",
      "Batch loss: 0.03513871878385544 batch: 188/224\n",
      "Batch loss: 0.05993608012795448 batch: 189/224\n",
      "Batch loss: 0.054837632924318314 batch: 190/224\n",
      "Batch loss: 0.06503520160913467 batch: 191/224\n",
      "Batch loss: 0.06524281203746796 batch: 192/224\n",
      "Batch loss: 0.06680484861135483 batch: 193/224\n",
      "Batch loss: 0.06396003067493439 batch: 194/224\n",
      "Batch loss: 0.09724803268909454 batch: 195/224\n",
      "Batch loss: 0.0659426897764206 batch: 196/224\n",
      "Batch loss: 0.06753459572792053 batch: 197/224\n",
      "Batch loss: 0.08773697167634964 batch: 198/224\n",
      "Batch loss: 0.06290771067142487 batch: 199/224\n",
      "Batch loss: 0.06577545404434204 batch: 200/224\n",
      "Batch loss: 0.054714448750019073 batch: 201/224\n",
      "Batch loss: 0.08261775970458984 batch: 202/224\n",
      "Batch loss: 0.05677129700779915 batch: 203/224\n",
      "Batch loss: 0.08779600262641907 batch: 204/224\n",
      "Batch loss: 0.08271488547325134 batch: 205/224\n",
      "Batch loss: 0.07543118298053741 batch: 206/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.04232626035809517 batch: 207/224\n",
      "Batch loss: 0.07252930104732513 batch: 208/224\n",
      "Batch loss: 0.05887637659907341 batch: 209/224\n",
      "Batch loss: 0.07083349674940109 batch: 210/224\n",
      "Batch loss: 0.05033441632986069 batch: 211/224\n",
      "Batch loss: 0.08892782777547836 batch: 212/224\n",
      "Batch loss: 0.06563980132341385 batch: 213/224\n",
      "Batch loss: 0.05133579298853874 batch: 214/224\n",
      "Batch loss: 0.11720586568117142 batch: 215/224\n",
      "Batch loss: 0.05371776223182678 batch: 216/224\n",
      "Batch loss: 0.06209176406264305 batch: 217/224\n",
      "Batch loss: 0.07363694906234741 batch: 218/224\n",
      "Batch loss: 0.051781754940748215 batch: 219/224\n",
      "Batch loss: 0.06192426010966301 batch: 220/224\n",
      "Batch loss: 0.07679367065429688 batch: 221/224\n",
      "Batch loss: 0.06305462121963501 batch: 222/224\n",
      "Batch loss: 0.058711033314466476 batch: 223/224\n",
      "Batch loss: 0.06169191747903824 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 59/75..  Training Loss: 0.00013..  Test Loss: 0.00104..  Test Accuracy: 0.89271\n",
      "Running epoch 60/75\n",
      "Batch loss: 0.05470173805952072 batch: 1/224\n",
      "Batch loss: 0.07356707006692886 batch: 2/224\n",
      "Batch loss: 0.037249855697155 batch: 3/224\n",
      "Batch loss: 0.0837571769952774 batch: 4/224\n",
      "Batch loss: 0.07497856020927429 batch: 5/224\n",
      "Batch loss: 0.0592082217335701 batch: 6/224\n",
      "Batch loss: 0.037649672478437424 batch: 7/224\n",
      "Batch loss: 0.0708668902516365 batch: 8/224\n",
      "Batch loss: 0.034294791519641876 batch: 9/224\n",
      "Batch loss: 0.06178133189678192 batch: 10/224\n",
      "Batch loss: 0.06742459535598755 batch: 11/224\n",
      "Batch loss: 0.06465663015842438 batch: 12/224\n",
      "Batch loss: 0.041228339076042175 batch: 13/224\n",
      "Batch loss: 0.04747891053557396 batch: 14/224\n",
      "Batch loss: 0.06653904914855957 batch: 15/224\n",
      "Batch loss: 0.07729845494031906 batch: 16/224\n",
      "Batch loss: 0.05973795801401138 batch: 17/224\n",
      "Batch loss: 0.07395214587450027 batch: 18/224\n",
      "Batch loss: 0.059308163821697235 batch: 19/224\n",
      "Batch loss: 0.056327275931835175 batch: 20/224\n",
      "Batch loss: 0.07785078883171082 batch: 21/224\n",
      "Batch loss: 0.07028693705797195 batch: 22/224\n",
      "Batch loss: 0.08577822893857956 batch: 23/224\n",
      "Batch loss: 0.08936757594347 batch: 24/224\n",
      "Batch loss: 0.05080335587263107 batch: 25/224\n",
      "Batch loss: 0.060796961188316345 batch: 26/224\n",
      "Batch loss: 0.03672172874212265 batch: 27/224\n",
      "Batch loss: 0.08674832433462143 batch: 28/224\n",
      "Batch loss: 0.059974756091833115 batch: 29/224\n",
      "Batch loss: 0.07742413133382797 batch: 30/224\n",
      "Batch loss: 0.06896666437387466 batch: 31/224\n",
      "Batch loss: 0.07879827916622162 batch: 32/224\n",
      "Batch loss: 0.07810433208942413 batch: 33/224\n",
      "Batch loss: 0.0760897621512413 batch: 34/224\n",
      "Batch loss: 0.06485426425933838 batch: 35/224\n",
      "Batch loss: 0.09129489213228226 batch: 36/224\n",
      "Batch loss: 0.04561763256788254 batch: 37/224\n",
      "Batch loss: 0.05141975358128548 batch: 38/224\n",
      "Batch loss: 0.0753641128540039 batch: 39/224\n",
      "Batch loss: 0.06776397675275803 batch: 40/224\n",
      "Batch loss: 0.04517586529254913 batch: 41/224\n",
      "Batch loss: 0.0646994411945343 batch: 42/224\n",
      "Batch loss: 0.10802824050188065 batch: 43/224\n",
      "Batch loss: 0.0839042216539383 batch: 44/224\n",
      "Batch loss: 0.048749204725027084 batch: 45/224\n",
      "Batch loss: 0.06605874001979828 batch: 46/224\n",
      "Batch loss: 0.07388482987880707 batch: 47/224\n",
      "Batch loss: 0.043881479650735855 batch: 48/224\n",
      "Batch loss: 0.03648127615451813 batch: 49/224\n",
      "Batch loss: 0.04578375071287155 batch: 50/224\n",
      "Batch loss: 0.05404491350054741 batch: 51/224\n",
      "Batch loss: 0.04932429641485214 batch: 52/224\n",
      "Batch loss: 0.08334021270275116 batch: 53/224\n",
      "Batch loss: 0.055145345628261566 batch: 54/224\n",
      "Batch loss: 0.07898259162902832 batch: 55/224\n",
      "Batch loss: 0.07017720490694046 batch: 56/224\n",
      "Batch loss: 0.071319580078125 batch: 57/224\n",
      "Batch loss: 0.07849974930286407 batch: 58/224\n",
      "Batch loss: 0.03430212289094925 batch: 59/224\n",
      "Batch loss: 0.09031344950199127 batch: 60/224\n",
      "Batch loss: 0.04073712229728699 batch: 61/224\n",
      "Batch loss: 0.06577897816896439 batch: 62/224\n",
      "Batch loss: 0.1051301583647728 batch: 63/224\n",
      "Batch loss: 0.11618893593549728 batch: 64/224\n",
      "Batch loss: 0.0594356469810009 batch: 65/224\n",
      "Batch loss: 0.05882919207215309 batch: 66/224\n",
      "Batch loss: 0.06593302637338638 batch: 67/224\n",
      "Batch loss: 0.06097816303372383 batch: 68/224\n",
      "Batch loss: 0.0932527557015419 batch: 69/224\n",
      "Batch loss: 0.06393236666917801 batch: 70/224\n",
      "Batch loss: 0.08241935819387436 batch: 71/224\n",
      "Batch loss: 0.05495034158229828 batch: 72/224\n",
      "Batch loss: 0.05796850845217705 batch: 73/224\n",
      "Batch loss: 0.05807548388838768 batch: 74/224\n",
      "Batch loss: 0.07126867771148682 batch: 75/224\n",
      "Batch loss: 0.10271720588207245 batch: 76/224\n",
      "Batch loss: 0.054255418479442596 batch: 77/224\n",
      "Batch loss: 0.060994651168584824 batch: 78/224\n",
      "Batch loss: 0.0706043541431427 batch: 79/224\n",
      "Batch loss: 0.0471603088080883 batch: 80/224\n",
      "Batch loss: 0.0606999434530735 batch: 81/224\n",
      "Batch loss: 0.10200110077857971 batch: 82/224\n",
      "Batch loss: 0.06941519677639008 batch: 83/224\n",
      "Batch loss: 0.03564115986227989 batch: 84/224\n",
      "Batch loss: 0.0671459510922432 batch: 85/224\n",
      "Batch loss: 0.06512659788131714 batch: 86/224\n",
      "Batch loss: 0.07831854373216629 batch: 87/224\n",
      "Batch loss: 0.10235649347305298 batch: 88/224\n",
      "Batch loss: 0.06723041087388992 batch: 89/224\n",
      "Batch loss: 0.06855826824903488 batch: 90/224\n",
      "Batch loss: 0.04748791083693504 batch: 91/224\n",
      "Batch loss: 0.06935776025056839 batch: 92/224\n",
      "Batch loss: 0.053574543446302414 batch: 93/224\n",
      "Batch loss: 0.0708898976445198 batch: 94/224\n",
      "Batch loss: 0.061211954802274704 batch: 95/224\n",
      "Batch loss: 0.05987444147467613 batch: 96/224\n",
      "Batch loss: 0.0847686380147934 batch: 97/224\n",
      "Batch loss: 0.05029429495334625 batch: 98/224\n",
      "Batch loss: 0.05926789715886116 batch: 99/224\n",
      "Batch loss: 0.047474756836891174 batch: 100/224\n",
      "Batch loss: 0.08480346947908401 batch: 101/224\n",
      "Batch loss: 0.05801982805132866 batch: 102/224\n",
      "Batch loss: 0.08241249620914459 batch: 103/224\n",
      "Batch loss: 0.044125668704509735 batch: 104/224\n",
      "Batch loss: 0.04356742650270462 batch: 105/224\n",
      "Batch loss: 0.06617802381515503 batch: 106/224\n",
      "Batch loss: 0.060011181980371475 batch: 107/224\n",
      "Batch loss: 0.05173572897911072 batch: 108/224\n",
      "Batch loss: 0.03437748923897743 batch: 109/224\n",
      "Batch loss: 0.04608956351876259 batch: 110/224\n",
      "Batch loss: 0.053141724318265915 batch: 111/224\n",
      "Batch loss: 0.057022351771593094 batch: 112/224\n",
      "Batch loss: 0.0500095896422863 batch: 113/224\n",
      "Batch loss: 0.05390002578496933 batch: 114/224\n",
      "Batch loss: 0.060580816119909286 batch: 115/224\n",
      "Batch loss: 0.1012599915266037 batch: 116/224\n",
      "Batch loss: 0.029944205656647682 batch: 117/224\n",
      "Batch loss: 0.05923252925276756 batch: 118/224\n",
      "Batch loss: 0.07173305749893188 batch: 119/224\n",
      "Batch loss: 0.056517165154218674 batch: 120/224\n",
      "Batch loss: 0.03919480741024017 batch: 121/224\n",
      "Batch loss: 0.061771929264068604 batch: 122/224\n",
      "Batch loss: 0.057090502232313156 batch: 123/224\n",
      "Batch loss: 0.06997231394052505 batch: 124/224\n",
      "Batch loss: 0.06437616050243378 batch: 125/224\n",
      "Batch loss: 0.06046208739280701 batch: 126/224\n",
      "Batch loss: 0.11773130297660828 batch: 127/224\n",
      "Batch loss: 0.0692654624581337 batch: 128/224\n",
      "Batch loss: 0.06645922362804413 batch: 129/224\n",
      "Batch loss: 0.06018013507127762 batch: 130/224\n",
      "Batch loss: 0.05814601108431816 batch: 131/224\n",
      "Batch loss: 0.036467816680669785 batch: 132/224\n",
      "Batch loss: 0.05600238963961601 batch: 133/224\n",
      "Batch loss: 0.07202833890914917 batch: 134/224\n",
      "Batch loss: 0.06467816978693008 batch: 135/224\n",
      "Batch loss: 0.07335220277309418 batch: 136/224\n",
      "Batch loss: 0.07282912731170654 batch: 137/224\n",
      "Batch loss: 0.06668855249881744 batch: 138/224\n",
      "Batch loss: 0.08889588713645935 batch: 139/224\n",
      "Batch loss: 0.0653044581413269 batch: 140/224\n",
      "Batch loss: 0.04876363277435303 batch: 141/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.05000970885157585 batch: 142/224\n",
      "Batch loss: 0.03668661043047905 batch: 143/224\n",
      "Batch loss: 0.07449767738580704 batch: 144/224\n",
      "Batch loss: 0.05380646139383316 batch: 145/224\n",
      "Batch loss: 0.0866110771894455 batch: 146/224\n",
      "Batch loss: 0.03927168995141983 batch: 147/224\n",
      "Batch loss: 0.047492899000644684 batch: 148/224\n",
      "Batch loss: 0.0785708874464035 batch: 149/224\n",
      "Batch loss: 0.09895745664834976 batch: 150/224\n",
      "Batch loss: 0.055511027574539185 batch: 151/224\n",
      "Batch loss: 0.07156136631965637 batch: 152/224\n",
      "Batch loss: 0.06968169659376144 batch: 153/224\n",
      "Batch loss: 0.08453916013240814 batch: 154/224\n",
      "Batch loss: 0.04713517427444458 batch: 155/224\n",
      "Batch loss: 0.047095414251089096 batch: 156/224\n",
      "Batch loss: 0.062165506184101105 batch: 157/224\n",
      "Batch loss: 0.10933675616979599 batch: 158/224\n",
      "Batch loss: 0.06361140310764313 batch: 159/224\n",
      "Batch loss: 0.07065316289663315 batch: 160/224\n",
      "Batch loss: 0.0804450660943985 batch: 161/224\n",
      "Batch loss: 0.06426316499710083 batch: 162/224\n",
      "Batch loss: 0.03603683039546013 batch: 163/224\n",
      "Batch loss: 0.04570181295275688 batch: 164/224\n",
      "Batch loss: 0.0789603739976883 batch: 165/224\n",
      "Batch loss: 0.08417467772960663 batch: 166/224\n",
      "Batch loss: 0.04268458113074303 batch: 167/224\n",
      "Batch loss: 0.04782429710030556 batch: 168/224\n",
      "Batch loss: 0.06644628196954727 batch: 169/224\n",
      "Batch loss: 0.05641588568687439 batch: 170/224\n",
      "Batch loss: 0.043057769536972046 batch: 171/224\n",
      "Batch loss: 0.04611240327358246 batch: 172/224\n",
      "Batch loss: 0.06845758855342865 batch: 173/224\n",
      "Batch loss: 0.03518667444586754 batch: 174/224\n",
      "Batch loss: 0.04894343763589859 batch: 175/224\n",
      "Batch loss: 0.04653812199831009 batch: 176/224\n",
      "Batch loss: 0.062376782298088074 batch: 177/224\n",
      "Batch loss: 0.047918640077114105 batch: 178/224\n",
      "Batch loss: 0.07693900167942047 batch: 179/224\n",
      "Batch loss: 0.02288840338587761 batch: 180/224\n",
      "Batch loss: 0.06130438297986984 batch: 181/224\n",
      "Batch loss: 0.07077945023775101 batch: 182/224\n",
      "Batch loss: 0.08009407669305801 batch: 183/224\n",
      "Batch loss: 0.06471377611160278 batch: 184/224\n",
      "Batch loss: 0.0578710101544857 batch: 185/224\n",
      "Batch loss: 0.03390972316265106 batch: 186/224\n",
      "Batch loss: 0.04807352274656296 batch: 187/224\n",
      "Batch loss: 0.05419202148914337 batch: 188/224\n",
      "Batch loss: 0.051132168620824814 batch: 189/224\n",
      "Batch loss: 0.0624927394092083 batch: 190/224\n",
      "Batch loss: 0.047073815017938614 batch: 191/224\n",
      "Batch loss: 0.04762125760316849 batch: 192/224\n",
      "Batch loss: 0.07820948213338852 batch: 193/224\n",
      "Batch loss: 0.06344762444496155 batch: 194/224\n",
      "Batch loss: 0.09733841568231583 batch: 195/224\n",
      "Batch loss: 0.06615284085273743 batch: 196/224\n",
      "Batch loss: 0.06295376271009445 batch: 197/224\n",
      "Batch loss: 0.03097114898264408 batch: 198/224\n",
      "Batch loss: 0.038405224680900574 batch: 199/224\n",
      "Batch loss: 0.059436120092868805 batch: 200/224\n",
      "Batch loss: 0.043448541313409805 batch: 201/224\n",
      "Batch loss: 0.05098200961947441 batch: 202/224\n",
      "Batch loss: 0.03540554642677307 batch: 203/224\n",
      "Batch loss: 0.05952760577201843 batch: 204/224\n",
      "Batch loss: 0.06824339181184769 batch: 205/224\n",
      "Batch loss: 0.06228011101484299 batch: 206/224\n",
      "Batch loss: 0.09746655821800232 batch: 207/224\n",
      "Batch loss: 0.0524919256567955 batch: 208/224\n",
      "Batch loss: 0.07037322968244553 batch: 209/224\n",
      "Batch loss: 0.07366997003555298 batch: 210/224\n",
      "Batch loss: 0.03546885401010513 batch: 211/224\n",
      "Batch loss: 0.04933702200651169 batch: 212/224\n",
      "Batch loss: 0.07767462730407715 batch: 213/224\n",
      "Batch loss: 0.06474675983190536 batch: 214/224\n",
      "Batch loss: 0.08058442920446396 batch: 215/224\n",
      "Batch loss: 0.06604389101266861 batch: 216/224\n",
      "Batch loss: 0.08162244409322739 batch: 217/224\n",
      "Batch loss: 0.036120496690273285 batch: 218/224\n",
      "Batch loss: 0.03381733596324921 batch: 219/224\n",
      "Batch loss: 0.059760138392448425 batch: 220/224\n",
      "Batch loss: 0.07991597056388855 batch: 221/224\n",
      "Batch loss: 0.08866101503372192 batch: 222/224\n",
      "Batch loss: 0.06564294546842575 batch: 223/224\n",
      "Batch loss: 0.0641644150018692 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 60/75..  Training Loss: 0.00013..  Test Loss: 0.00108..  Test Accuracy: 0.89300\n",
      "Running epoch 61/75\n",
      "Batch loss: 0.0521424300968647 batch: 1/224\n",
      "Batch loss: 0.06635171175003052 batch: 2/224\n",
      "Batch loss: 0.09356991946697235 batch: 3/224\n",
      "Batch loss: 0.10691187530755997 batch: 4/224\n",
      "Batch loss: 0.05513439327478409 batch: 5/224\n",
      "Batch loss: 0.04600539058446884 batch: 6/224\n",
      "Batch loss: 0.05455765500664711 batch: 7/224\n",
      "Batch loss: 0.057875677943229675 batch: 8/224\n",
      "Batch loss: 0.04784318804740906 batch: 9/224\n",
      "Batch loss: 0.06689421087503433 batch: 10/224\n",
      "Batch loss: 0.05786142870783806 batch: 11/224\n",
      "Batch loss: 0.0711326077580452 batch: 12/224\n",
      "Batch loss: 0.04744163900613785 batch: 13/224\n",
      "Batch loss: 0.048521608114242554 batch: 14/224\n",
      "Batch loss: 0.05056263133883476 batch: 15/224\n",
      "Batch loss: 0.0929982140660286 batch: 16/224\n",
      "Batch loss: 0.047105271369218826 batch: 17/224\n",
      "Batch loss: 0.07732989639043808 batch: 18/224\n",
      "Batch loss: 0.07114173471927643 batch: 19/224\n",
      "Batch loss: 0.0628286823630333 batch: 20/224\n",
      "Batch loss: 0.0751621276140213 batch: 21/224\n",
      "Batch loss: 0.04230737313628197 batch: 22/224\n",
      "Batch loss: 0.08642397820949554 batch: 23/224\n",
      "Batch loss: 0.08331458270549774 batch: 24/224\n",
      "Batch loss: 0.04992230236530304 batch: 25/224\n",
      "Batch loss: 0.04768027365207672 batch: 26/224\n",
      "Batch loss: 0.04216407984495163 batch: 27/224\n",
      "Batch loss: 0.10026802867650986 batch: 28/224\n",
      "Batch loss: 0.0876198410987854 batch: 29/224\n",
      "Batch loss: 0.08334337174892426 batch: 30/224\n",
      "Batch loss: 0.05068393796682358 batch: 31/224\n",
      "Batch loss: 0.05761299282312393 batch: 32/224\n",
      "Batch loss: 0.05521787330508232 batch: 33/224\n",
      "Batch loss: 0.1160215511918068 batch: 34/224\n",
      "Batch loss: 0.08765769749879837 batch: 35/224\n",
      "Batch loss: 0.07219446450471878 batch: 36/224\n",
      "Batch loss: 0.05098606273531914 batch: 37/224\n",
      "Batch loss: 0.055810894817113876 batch: 38/224\n",
      "Batch loss: 0.04489549249410629 batch: 39/224\n",
      "Batch loss: 0.0437794104218483 batch: 40/224\n",
      "Batch loss: 0.07210361957550049 batch: 41/224\n",
      "Batch loss: 0.059103552252054214 batch: 42/224\n",
      "Batch loss: 0.056008026003837585 batch: 43/224\n",
      "Batch loss: 0.051139313727617264 batch: 44/224\n",
      "Batch loss: 0.06090879812836647 batch: 45/224\n",
      "Batch loss: 0.06331128627061844 batch: 46/224\n",
      "Batch loss: 0.06152050942182541 batch: 47/224\n",
      "Batch loss: 0.046812754124403 batch: 48/224\n",
      "Batch loss: 0.04677395150065422 batch: 49/224\n",
      "Batch loss: 0.07910975813865662 batch: 50/224\n",
      "Batch loss: 0.06319611519575119 batch: 51/224\n",
      "Batch loss: 0.036564137786626816 batch: 52/224\n",
      "Batch loss: 0.08233224600553513 batch: 53/224\n",
      "Batch loss: 0.03826538100838661 batch: 54/224\n",
      "Batch loss: 0.03708476945757866 batch: 55/224\n",
      "Batch loss: 0.059790391474962234 batch: 56/224\n",
      "Batch loss: 0.08145848661661148 batch: 57/224\n",
      "Batch loss: 0.0843416154384613 batch: 58/224\n",
      "Batch loss: 0.05243825539946556 batch: 59/224\n",
      "Batch loss: 0.07778052985668182 batch: 60/224\n",
      "Batch loss: 0.11802131682634354 batch: 61/224\n",
      "Batch loss: 0.061897482722997665 batch: 62/224\n",
      "Batch loss: 0.06629908829927444 batch: 63/224\n",
      "Batch loss: 0.06140979751944542 batch: 64/224\n",
      "Batch loss: 0.052038244903087616 batch: 65/224\n",
      "Batch loss: 0.0686979591846466 batch: 66/224\n",
      "Batch loss: 0.037015605717897415 batch: 67/224\n",
      "Batch loss: 0.05625837296247482 batch: 68/224\n",
      "Batch loss: 0.059379979968070984 batch: 69/224\n",
      "Batch loss: 0.07042812556028366 batch: 70/224\n",
      "Batch loss: 0.04846639558672905 batch: 71/224\n",
      "Batch loss: 0.05278228595852852 batch: 72/224\n",
      "Batch loss: 0.06629730015993118 batch: 73/224\n",
      "Batch loss: 0.053936272859573364 batch: 74/224\n",
      "Batch loss: 0.06357441842556 batch: 75/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.080792136490345 batch: 76/224\n",
      "Batch loss: 0.08653664588928223 batch: 77/224\n",
      "Batch loss: 0.07398995757102966 batch: 78/224\n",
      "Batch loss: 0.056008584797382355 batch: 79/224\n",
      "Batch loss: 0.05291123688220978 batch: 80/224\n",
      "Batch loss: 0.05872710421681404 batch: 81/224\n",
      "Batch loss: 0.096779003739357 batch: 82/224\n",
      "Batch loss: 0.07863379269838333 batch: 83/224\n",
      "Batch loss: 0.0405808724462986 batch: 84/224\n",
      "Batch loss: 0.07650705426931381 batch: 85/224\n",
      "Batch loss: 0.06701954454183578 batch: 86/224\n",
      "Batch loss: 0.03489634394645691 batch: 87/224\n",
      "Batch loss: 0.086944580078125 batch: 88/224\n",
      "Batch loss: 0.08194442093372345 batch: 89/224\n",
      "Batch loss: 0.06290912628173828 batch: 90/224\n",
      "Batch loss: 0.04575621709227562 batch: 91/224\n",
      "Batch loss: 0.06511957943439484 batch: 92/224\n",
      "Batch loss: 0.05060799419879913 batch: 93/224\n",
      "Batch loss: 0.047899842262268066 batch: 94/224\n",
      "Batch loss: 0.03341924771666527 batch: 95/224\n",
      "Batch loss: 0.07629116624593735 batch: 96/224\n",
      "Batch loss: 0.04926863685250282 batch: 97/224\n",
      "Batch loss: 0.044011447578668594 batch: 98/224\n",
      "Batch loss: 0.08265207707881927 batch: 99/224\n",
      "Batch loss: 0.050338003784418106 batch: 100/224\n",
      "Batch loss: 0.09771817922592163 batch: 101/224\n",
      "Batch loss: 0.059860751032829285 batch: 102/224\n",
      "Batch loss: 0.06740090250968933 batch: 103/224\n",
      "Batch loss: 0.04834223538637161 batch: 104/224\n",
      "Batch loss: 0.07304410636425018 batch: 105/224\n",
      "Batch loss: 0.08230038732290268 batch: 106/224\n",
      "Batch loss: 0.05618960037827492 batch: 107/224\n",
      "Batch loss: 0.06424257159233093 batch: 108/224\n",
      "Batch loss: 0.06188530847430229 batch: 109/224\n",
      "Batch loss: 0.04932413250207901 batch: 110/224\n",
      "Batch loss: 0.04453878104686737 batch: 111/224\n",
      "Batch loss: 0.03508874773979187 batch: 112/224\n",
      "Batch loss: 0.08171339333057404 batch: 113/224\n",
      "Batch loss: 0.043057721108198166 batch: 114/224\n",
      "Batch loss: 0.05257943645119667 batch: 115/224\n",
      "Batch loss: 0.05669322609901428 batch: 116/224\n",
      "Batch loss: 0.06926705688238144 batch: 117/224\n",
      "Batch loss: 0.06477837264537811 batch: 118/224\n",
      "Batch loss: 0.050068341195583344 batch: 119/224\n",
      "Batch loss: 0.07184366136789322 batch: 120/224\n",
      "Batch loss: 0.06907852739095688 batch: 121/224\n",
      "Batch loss: 0.03870764374732971 batch: 122/224\n",
      "Batch loss: 0.03624431788921356 batch: 123/224\n",
      "Batch loss: 0.050110068172216415 batch: 124/224\n",
      "Batch loss: 0.07886358350515366 batch: 125/224\n",
      "Batch loss: 0.07855869829654694 batch: 126/224\n",
      "Batch loss: 0.08522453904151917 batch: 127/224\n",
      "Batch loss: 0.0660027489066124 batch: 128/224\n",
      "Batch loss: 0.04273220896720886 batch: 129/224\n",
      "Batch loss: 0.04576466232538223 batch: 130/224\n",
      "Batch loss: 0.04225911200046539 batch: 131/224\n",
      "Batch loss: 0.0645347386598587 batch: 132/224\n",
      "Batch loss: 0.08085136115550995 batch: 133/224\n",
      "Batch loss: 0.05774736404418945 batch: 134/224\n",
      "Batch loss: 0.06501740962266922 batch: 135/224\n",
      "Batch loss: 0.0704471692442894 batch: 136/224\n",
      "Batch loss: 0.051029022783041 batch: 137/224\n",
      "Batch loss: 0.0703008696436882 batch: 138/224\n",
      "Batch loss: 0.060107551515102386 batch: 139/224\n",
      "Batch loss: 0.07145741581916809 batch: 140/224\n",
      "Batch loss: 0.08750186860561371 batch: 141/224\n",
      "Batch loss: 0.09256460517644882 batch: 142/224\n",
      "Batch loss: 0.05650021508336067 batch: 143/224\n",
      "Batch loss: 0.05225291848182678 batch: 144/224\n",
      "Batch loss: 0.06365102529525757 batch: 145/224\n",
      "Batch loss: 0.09776303172111511 batch: 146/224\n",
      "Batch loss: 0.055218059569597244 batch: 147/224\n",
      "Batch loss: 0.05415082722902298 batch: 148/224\n",
      "Batch loss: 0.11308013647794724 batch: 149/224\n",
      "Batch loss: 0.0807112455368042 batch: 150/224\n",
      "Batch loss: 0.04441032186150551 batch: 151/224\n",
      "Batch loss: 0.08363132178783417 batch: 152/224\n",
      "Batch loss: 0.08004608750343323 batch: 153/224\n",
      "Batch loss: 0.04895765706896782 batch: 154/224\n",
      "Batch loss: 0.07601113617420197 batch: 155/224\n",
      "Batch loss: 0.05527503043413162 batch: 156/224\n",
      "Batch loss: 0.06495534628629684 batch: 157/224\n",
      "Batch loss: 0.1096976175904274 batch: 158/224\n",
      "Batch loss: 0.07122109085321426 batch: 159/224\n",
      "Batch loss: 0.08842357993125916 batch: 160/224\n",
      "Batch loss: 0.07071316242218018 batch: 161/224\n",
      "Batch loss: 0.044825851917266846 batch: 162/224\n",
      "Batch loss: 0.04425392672419548 batch: 163/224\n",
      "Batch loss: 0.05843136087059975 batch: 164/224\n",
      "Batch loss: 0.07742749154567719 batch: 165/224\n",
      "Batch loss: 0.10522513836622238 batch: 166/224\n",
      "Batch loss: 0.04383260756731033 batch: 167/224\n",
      "Batch loss: 0.06426914781332016 batch: 168/224\n",
      "Batch loss: 0.0576048381626606 batch: 169/224\n",
      "Batch loss: 0.054524004459381104 batch: 170/224\n",
      "Batch loss: 0.06407991051673889 batch: 171/224\n",
      "Batch loss: 0.06229492276906967 batch: 172/224\n",
      "Batch loss: 0.08480315655469894 batch: 173/224\n",
      "Batch loss: 0.06541897356510162 batch: 174/224\n",
      "Batch loss: 0.08033561706542969 batch: 175/224\n",
      "Batch loss: 0.05794638395309448 batch: 176/224\n",
      "Batch loss: 0.09755878150463104 batch: 177/224\n",
      "Batch loss: 0.051527608186006546 batch: 178/224\n",
      "Batch loss: 0.07854125648736954 batch: 179/224\n",
      "Batch loss: 0.05353991687297821 batch: 180/224\n",
      "Batch loss: 0.053396206349134445 batch: 181/224\n",
      "Batch loss: 0.07670403271913528 batch: 182/224\n",
      "Batch loss: 0.08019541949033737 batch: 183/224\n",
      "Batch loss: 0.06771495938301086 batch: 184/224\n",
      "Batch loss: 0.06884989887475967 batch: 185/224\n",
      "Batch loss: 0.051047809422016144 batch: 186/224\n",
      "Batch loss: 0.06710878759622574 batch: 187/224\n",
      "Batch loss: 0.035657331347465515 batch: 188/224\n",
      "Batch loss: 0.05583006888628006 batch: 189/224\n",
      "Batch loss: 0.05715702474117279 batch: 190/224\n",
      "Batch loss: 0.03294334560632706 batch: 191/224\n",
      "Batch loss: 0.07993534952402115 batch: 192/224\n",
      "Batch loss: 0.06174794211983681 batch: 193/224\n",
      "Batch loss: 0.05143385007977486 batch: 194/224\n",
      "Batch loss: 0.04585716873407364 batch: 195/224\n",
      "Batch loss: 0.08214119076728821 batch: 196/224\n",
      "Batch loss: 0.04997018724679947 batch: 197/224\n",
      "Batch loss: 0.05233657360076904 batch: 198/224\n",
      "Batch loss: 0.05328723415732384 batch: 199/224\n",
      "Batch loss: 0.0638057291507721 batch: 200/224\n",
      "Batch loss: 0.06727975606918335 batch: 201/224\n",
      "Batch loss: 0.08225949108600616 batch: 202/224\n",
      "Batch loss: 0.06879854947328568 batch: 203/224\n",
      "Batch loss: 0.07166057080030441 batch: 204/224\n",
      "Batch loss: 0.09176034480333328 batch: 205/224\n",
      "Batch loss: 0.05708184838294983 batch: 206/224\n",
      "Batch loss: 0.05678892880678177 batch: 207/224\n",
      "Batch loss: 0.062362588942050934 batch: 208/224\n",
      "Batch loss: 0.07417796552181244 batch: 209/224\n",
      "Batch loss: 0.07797021418809891 batch: 210/224\n",
      "Batch loss: 0.06702814251184464 batch: 211/224\n",
      "Batch loss: 0.05047035589814186 batch: 212/224\n",
      "Batch loss: 0.08718130737543106 batch: 213/224\n",
      "Batch loss: 0.09755713492631912 batch: 214/224\n",
      "Batch loss: 0.0590369738638401 batch: 215/224\n",
      "Batch loss: 0.07388441264629364 batch: 216/224\n",
      "Batch loss: 0.0844390019774437 batch: 217/224\n",
      "Batch loss: 0.08543919771909714 batch: 218/224\n",
      "Batch loss: 0.03999572992324829 batch: 219/224\n",
      "Batch loss: 0.05856604501605034 batch: 220/224\n",
      "Batch loss: 0.09317198395729065 batch: 221/224\n",
      "Batch loss: 0.0769064873456955 batch: 222/224\n",
      "Batch loss: 0.07182231545448303 batch: 223/224\n",
      "Batch loss: 0.04929223656654358 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 61/75..  Training Loss: 0.00013..  Test Loss: 0.00102..  Test Accuracy: 0.89204\n",
      "Running epoch 62/75\n",
      "Batch loss: 0.06546463817358017 batch: 1/224\n",
      "Batch loss: 0.07571641355752945 batch: 2/224\n",
      "Batch loss: 0.07715069502592087 batch: 3/224\n",
      "Batch loss: 0.06524188816547394 batch: 4/224\n",
      "Batch loss: 0.06292109191417694 batch: 5/224\n",
      "Batch loss: 0.07952715456485748 batch: 6/224\n",
      "Batch loss: 0.04651891440153122 batch: 7/224\n",
      "Batch loss: 0.06261932104825974 batch: 8/224\n",
      "Batch loss: 0.05045650526881218 batch: 9/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.07153800874948502 batch: 10/224\n",
      "Batch loss: 0.04822538420557976 batch: 11/224\n",
      "Batch loss: 0.08119182288646698 batch: 12/224\n",
      "Batch loss: 0.05834689736366272 batch: 13/224\n",
      "Batch loss: 0.05425255745649338 batch: 14/224\n",
      "Batch loss: 0.08762093633413315 batch: 15/224\n",
      "Batch loss: 0.08364228159189224 batch: 16/224\n",
      "Batch loss: 0.07257252186536789 batch: 17/224\n",
      "Batch loss: 0.0405736081302166 batch: 18/224\n",
      "Batch loss: 0.06767954677343369 batch: 19/224\n",
      "Batch loss: 0.07484506815671921 batch: 20/224\n",
      "Batch loss: 0.05826708674430847 batch: 21/224\n",
      "Batch loss: 0.06975405663251877 batch: 22/224\n",
      "Batch loss: 0.07888443022966385 batch: 23/224\n",
      "Batch loss: 0.07398088276386261 batch: 24/224\n",
      "Batch loss: 0.06092940643429756 batch: 25/224\n",
      "Batch loss: 0.046018607914447784 batch: 26/224\n",
      "Batch loss: 0.07786338031291962 batch: 27/224\n",
      "Batch loss: 0.08256267756223679 batch: 28/224\n",
      "Batch loss: 0.08081714808940887 batch: 29/224\n",
      "Batch loss: 0.09176333248615265 batch: 30/224\n",
      "Batch loss: 0.0619635134935379 batch: 31/224\n",
      "Batch loss: 0.06500446796417236 batch: 32/224\n",
      "Batch loss: 0.06966084986925125 batch: 33/224\n",
      "Batch loss: 0.09375936537981033 batch: 34/224\n",
      "Batch loss: 0.0547339990735054 batch: 35/224\n",
      "Batch loss: 0.06489072740077972 batch: 36/224\n",
      "Batch loss: 0.0723164901137352 batch: 37/224\n",
      "Batch loss: 0.044151272624731064 batch: 38/224\n",
      "Batch loss: 0.07902749627828598 batch: 39/224\n",
      "Batch loss: 0.04686133936047554 batch: 40/224\n",
      "Batch loss: 0.06475216150283813 batch: 41/224\n",
      "Batch loss: 0.06479429453611374 batch: 42/224\n",
      "Batch loss: 0.10720572620630264 batch: 43/224\n",
      "Batch loss: 0.03383496031165123 batch: 44/224\n",
      "Batch loss: 0.0326230563223362 batch: 45/224\n",
      "Batch loss: 0.07763758301734924 batch: 46/224\n",
      "Batch loss: 0.05253861844539642 batch: 47/224\n",
      "Batch loss: 0.07368659228086472 batch: 48/224\n",
      "Batch loss: 0.09077589213848114 batch: 49/224\n",
      "Batch loss: 0.03318613022565842 batch: 50/224\n",
      "Batch loss: 0.07982362806797028 batch: 51/224\n",
      "Batch loss: 0.03456040844321251 batch: 52/224\n",
      "Batch loss: 0.07842284440994263 batch: 53/224\n",
      "Batch loss: 0.05237817391753197 batch: 54/224\n",
      "Batch loss: 0.060841888189315796 batch: 55/224\n",
      "Batch loss: 0.06611450016498566 batch: 56/224\n",
      "Batch loss: 0.07712198793888092 batch: 57/224\n",
      "Batch loss: 0.09338641911745071 batch: 58/224\n",
      "Batch loss: 0.04233028367161751 batch: 59/224\n",
      "Batch loss: 0.0721912831068039 batch: 60/224\n",
      "Batch loss: 0.06121901050209999 batch: 61/224\n",
      "Batch loss: 0.05752457305788994 batch: 62/224\n",
      "Batch loss: 0.04816530644893646 batch: 63/224\n",
      "Batch loss: 0.06635985523462296 batch: 64/224\n",
      "Batch loss: 0.08118196576833725 batch: 65/224\n",
      "Batch loss: 0.06860687583684921 batch: 66/224\n",
      "Batch loss: 0.049575041979551315 batch: 67/224\n",
      "Batch loss: 0.06404688209295273 batch: 68/224\n",
      "Batch loss: 0.07772187888622284 batch: 69/224\n",
      "Batch loss: 0.04946277663111687 batch: 70/224\n",
      "Batch loss: 0.06869624555110931 batch: 71/224\n",
      "Batch loss: 0.04094475507736206 batch: 72/224\n",
      "Batch loss: 0.07662034779787064 batch: 73/224\n",
      "Batch loss: 0.05987867712974548 batch: 74/224\n",
      "Batch loss: 0.06545157730579376 batch: 75/224\n",
      "Batch loss: 0.06521249562501907 batch: 76/224\n",
      "Batch loss: 0.0599762499332428 batch: 77/224\n",
      "Batch loss: 0.04681634530425072 batch: 78/224\n",
      "Batch loss: 0.09000145643949509 batch: 79/224\n",
      "Batch loss: 0.045747481286525726 batch: 80/224\n",
      "Batch loss: 0.07739252597093582 batch: 81/224\n",
      "Batch loss: 0.10632289201021194 batch: 82/224\n",
      "Batch loss: 0.046053674072027206 batch: 83/224\n",
      "Batch loss: 0.051633335649967194 batch: 84/224\n",
      "Batch loss: 0.060614313930273056 batch: 85/224\n",
      "Batch loss: 0.07551061362028122 batch: 86/224\n",
      "Batch loss: 0.05592002719640732 batch: 87/224\n",
      "Batch loss: 0.06537064164876938 batch: 88/224\n",
      "Batch loss: 0.0418613962829113 batch: 89/224\n",
      "Batch loss: 0.055193908512592316 batch: 90/224\n",
      "Batch loss: 0.06991971284151077 batch: 91/224\n",
      "Batch loss: 0.07687561959028244 batch: 92/224\n",
      "Batch loss: 0.03674820438027382 batch: 93/224\n",
      "Batch loss: 0.05610227957367897 batch: 94/224\n",
      "Batch loss: 0.07380244135856628 batch: 95/224\n",
      "Batch loss: 0.07143890857696533 batch: 96/224\n",
      "Batch loss: 0.049323927611112595 batch: 97/224\n",
      "Batch loss: 0.03749953210353851 batch: 98/224\n",
      "Batch loss: 0.05187855288386345 batch: 99/224\n",
      "Batch loss: 0.07020996510982513 batch: 100/224\n",
      "Batch loss: 0.07399468123912811 batch: 101/224\n",
      "Batch loss: 0.0756588727235794 batch: 102/224\n",
      "Batch loss: 0.09309601783752441 batch: 103/224\n",
      "Batch loss: 0.046077463775873184 batch: 104/224\n",
      "Batch loss: 0.055651113390922546 batch: 105/224\n",
      "Batch loss: 0.06652313470840454 batch: 106/224\n",
      "Batch loss: 0.06331738829612732 batch: 107/224\n",
      "Batch loss: 0.0639074444770813 batch: 108/224\n",
      "Batch loss: 0.03659453243017197 batch: 109/224\n",
      "Batch loss: 0.05719976872205734 batch: 110/224\n",
      "Batch loss: 0.07008194178342819 batch: 111/224\n",
      "Batch loss: 0.04828933998942375 batch: 112/224\n",
      "Batch loss: 0.050801776349544525 batch: 113/224\n",
      "Batch loss: 0.05692099407315254 batch: 114/224\n",
      "Batch loss: 0.049904003739356995 batch: 115/224\n",
      "Batch loss: 0.06067873537540436 batch: 116/224\n",
      "Batch loss: 0.04029634967446327 batch: 117/224\n",
      "Batch loss: 0.06026485562324524 batch: 118/224\n",
      "Batch loss: 0.06308646500110626 batch: 119/224\n",
      "Batch loss: 0.05155985802412033 batch: 120/224\n",
      "Batch loss: 0.03788989409804344 batch: 121/224\n",
      "Batch loss: 0.06424534320831299 batch: 122/224\n",
      "Batch loss: 0.03476036339998245 batch: 123/224\n",
      "Batch loss: 0.0550960972905159 batch: 124/224\n",
      "Batch loss: 0.049607958644628525 batch: 125/224\n",
      "Batch loss: 0.06207337975502014 batch: 126/224\n",
      "Batch loss: 0.06222882121801376 batch: 127/224\n",
      "Batch loss: 0.06398031115531921 batch: 128/224\n",
      "Batch loss: 0.062150612473487854 batch: 129/224\n",
      "Batch loss: 0.0807027518749237 batch: 130/224\n",
      "Batch loss: 0.0694083422422409 batch: 131/224\n",
      "Batch loss: 0.04974430799484253 batch: 132/224\n",
      "Batch loss: 0.07574689388275146 batch: 133/224\n",
      "Batch loss: 0.05594436824321747 batch: 134/224\n",
      "Batch loss: 0.03970661386847496 batch: 135/224\n",
      "Batch loss: 0.07199392467737198 batch: 136/224\n",
      "Batch loss: 0.06285892426967621 batch: 137/224\n",
      "Batch loss: 0.07304105907678604 batch: 138/224\n",
      "Batch loss: 0.07349034398794174 batch: 139/224\n",
      "Batch loss: 0.07893919944763184 batch: 140/224\n",
      "Batch loss: 0.06870514899492264 batch: 141/224\n",
      "Batch loss: 0.04981761425733566 batch: 142/224\n",
      "Batch loss: 0.05020078271627426 batch: 143/224\n",
      "Batch loss: 0.057855382561683655 batch: 144/224\n",
      "Batch loss: 0.0645536258816719 batch: 145/224\n",
      "Batch loss: 0.11496258527040482 batch: 146/224\n",
      "Batch loss: 0.06648967415094376 batch: 147/224\n",
      "Batch loss: 0.05739569291472435 batch: 148/224\n",
      "Batch loss: 0.08697305619716644 batch: 149/224\n",
      "Batch loss: 0.06987348943948746 batch: 150/224\n",
      "Batch loss: 0.06642556935548782 batch: 151/224\n",
      "Batch loss: 0.06227053701877594 batch: 152/224\n",
      "Batch loss: 0.07951521128416061 batch: 153/224\n",
      "Batch loss: 0.048355214297771454 batch: 154/224\n",
      "Batch loss: 0.05027664080262184 batch: 155/224\n",
      "Batch loss: 0.03950390964746475 batch: 156/224\n",
      "Batch loss: 0.06912925094366074 batch: 157/224\n",
      "Batch loss: 0.07607334107160568 batch: 158/224\n",
      "Batch loss: 0.058445192873477936 batch: 159/224\n",
      "Batch loss: 0.05765754356980324 batch: 160/224\n",
      "Batch loss: 0.04203452542424202 batch: 161/224\n",
      "Batch loss: 0.05243673920631409 batch: 162/224\n",
      "Batch loss: 0.049674130976200104 batch: 163/224\n",
      "Batch loss: 0.0808541402220726 batch: 164/224\n",
      "Batch loss: 0.07935497909784317 batch: 165/224\n",
      "Batch loss: 0.06044474616646767 batch: 166/224\n",
      "Batch loss: 0.060906749218702316 batch: 167/224\n",
      "Batch loss: 0.08475527167320251 batch: 168/224\n",
      "Batch loss: 0.07803035527467728 batch: 169/224\n",
      "Batch loss: 0.07242095470428467 batch: 170/224\n",
      "Batch loss: 0.07585331052541733 batch: 171/224\n",
      "Batch loss: 0.061847586184740067 batch: 172/224\n",
      "Batch loss: 0.06704532355070114 batch: 173/224\n",
      "Batch loss: 0.0772094801068306 batch: 174/224\n",
      "Batch loss: 0.06379920989274979 batch: 175/224\n",
      "Batch loss: 0.06201143562793732 batch: 176/224\n",
      "Batch loss: 0.08298114687204361 batch: 177/224\n",
      "Batch loss: 0.03184136748313904 batch: 178/224\n",
      "Batch loss: 0.05981387943029404 batch: 179/224\n",
      "Batch loss: 0.07144993543624878 batch: 180/224\n",
      "Batch loss: 0.06640475243330002 batch: 181/224\n",
      "Batch loss: 0.07836086302995682 batch: 182/224\n",
      "Batch loss: 0.06452217698097229 batch: 183/224\n",
      "Batch loss: 0.05319724604487419 batch: 184/224\n",
      "Batch loss: 0.06398853659629822 batch: 185/224\n",
      "Batch loss: 0.08056049793958664 batch: 186/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.05333128198981285 batch: 187/224\n",
      "Batch loss: 0.06810824573040009 batch: 188/224\n",
      "Batch loss: 0.05422242358326912 batch: 189/224\n",
      "Batch loss: 0.07198403030633926 batch: 190/224\n",
      "Batch loss: 0.055239856243133545 batch: 191/224\n",
      "Batch loss: 0.06794202327728271 batch: 192/224\n",
      "Batch loss: 0.06887780129909515 batch: 193/224\n",
      "Batch loss: 0.05772004649043083 batch: 194/224\n",
      "Batch loss: 0.0741468220949173 batch: 195/224\n",
      "Batch loss: 0.050391580909490585 batch: 196/224\n",
      "Batch loss: 0.06923048943281174 batch: 197/224\n",
      "Batch loss: 0.0538555271923542 batch: 198/224\n",
      "Batch loss: 0.04534061253070831 batch: 199/224\n",
      "Batch loss: 0.04545644670724869 batch: 200/224\n",
      "Batch loss: 0.049466572701931 batch: 201/224\n",
      "Batch loss: 0.08789251744747162 batch: 202/224\n",
      "Batch loss: 0.08510038256645203 batch: 203/224\n",
      "Batch loss: 0.06323309242725372 batch: 204/224\n",
      "Batch loss: 0.06816936284303665 batch: 205/224\n",
      "Batch loss: 0.060575783252716064 batch: 206/224\n",
      "Batch loss: 0.058586180210113525 batch: 207/224\n",
      "Batch loss: 0.04880242422223091 batch: 208/224\n",
      "Batch loss: 0.05062061920762062 batch: 209/224\n",
      "Batch loss: 0.03842490538954735 batch: 210/224\n",
      "Batch loss: 0.04716198518872261 batch: 211/224\n",
      "Batch loss: 0.08652346581220627 batch: 212/224\n",
      "Batch loss: 0.08663178235292435 batch: 213/224\n",
      "Batch loss: 0.08673777431249619 batch: 214/224\n",
      "Batch loss: 0.06630812585353851 batch: 215/224\n",
      "Batch loss: 0.059242572635412216 batch: 216/224\n",
      "Batch loss: 0.08635197579860687 batch: 217/224\n",
      "Batch loss: 0.057256367057561874 batch: 218/224\n",
      "Batch loss: 0.0664343535900116 batch: 219/224\n",
      "Batch loss: 0.0546913705766201 batch: 220/224\n",
      "Batch loss: 0.08861841261386871 batch: 221/224\n",
      "Batch loss: 0.09538610279560089 batch: 222/224\n",
      "Batch loss: 0.055261678993701935 batch: 223/224\n",
      "Batch loss: 0.04696144536137581 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 62/75..  Training Loss: 0.00013..  Test Loss: 0.00102..  Test Accuracy: 0.89196\n",
      "Running epoch 63/75\n",
      "Batch loss: 0.0452425554394722 batch: 1/224\n",
      "Batch loss: 0.06528840214014053 batch: 2/224\n",
      "Batch loss: 0.06091780960559845 batch: 3/224\n",
      "Batch loss: 0.09070809185504913 batch: 4/224\n",
      "Batch loss: 0.06485605984926224 batch: 5/224\n",
      "Batch loss: 0.05134045332670212 batch: 6/224\n",
      "Batch loss: 0.06582003086805344 batch: 7/224\n",
      "Batch loss: 0.06569577753543854 batch: 8/224\n",
      "Batch loss: 0.05408642441034317 batch: 9/224\n",
      "Batch loss: 0.06613434851169586 batch: 10/224\n",
      "Batch loss: 0.07013949751853943 batch: 11/224\n",
      "Batch loss: 0.05225791409611702 batch: 12/224\n",
      "Batch loss: 0.06689850986003876 batch: 13/224\n",
      "Batch loss: 0.03271102160215378 batch: 14/224\n",
      "Batch loss: 0.06272049248218536 batch: 15/224\n",
      "Batch loss: 0.07445109635591507 batch: 16/224\n",
      "Batch loss: 0.06405041366815567 batch: 17/224\n",
      "Batch loss: 0.04553542658686638 batch: 18/224\n",
      "Batch loss: 0.05816803500056267 batch: 19/224\n",
      "Batch loss: 0.047704197466373444 batch: 20/224\n",
      "Batch loss: 0.08257147669792175 batch: 21/224\n",
      "Batch loss: 0.053450219333171844 batch: 22/224\n",
      "Batch loss: 0.08104250580072403 batch: 23/224\n",
      "Batch loss: 0.07425031810998917 batch: 24/224\n",
      "Batch loss: 0.07212231308221817 batch: 25/224\n",
      "Batch loss: 0.028317337855696678 batch: 26/224\n",
      "Batch loss: 0.06750084459781647 batch: 27/224\n",
      "Batch loss: 0.06374198198318481 batch: 28/224\n",
      "Batch loss: 0.07299584150314331 batch: 29/224\n",
      "Batch loss: 0.06691940873861313 batch: 30/224\n",
      "Batch loss: 0.05148506909608841 batch: 31/224\n",
      "Batch loss: 0.04599558562040329 batch: 32/224\n",
      "Batch loss: 0.03450934961438179 batch: 33/224\n",
      "Batch loss: 0.06103498861193657 batch: 34/224\n",
      "Batch loss: 0.04089927673339844 batch: 35/224\n",
      "Batch loss: 0.07376793026924133 batch: 36/224\n",
      "Batch loss: 0.04854239150881767 batch: 37/224\n",
      "Batch loss: 0.05584622547030449 batch: 38/224\n",
      "Batch loss: 0.10536468029022217 batch: 39/224\n",
      "Batch loss: 0.06713174283504486 batch: 40/224\n",
      "Batch loss: 0.09083573520183563 batch: 41/224\n",
      "Batch loss: 0.06003052368760109 batch: 42/224\n",
      "Batch loss: 0.07384587824344635 batch: 43/224\n",
      "Batch loss: 0.053953394293785095 batch: 44/224\n",
      "Batch loss: 0.055537886917591095 batch: 45/224\n",
      "Batch loss: 0.11177068948745728 batch: 46/224\n",
      "Batch loss: 0.07156725972890854 batch: 47/224\n",
      "Batch loss: 0.04118168354034424 batch: 48/224\n",
      "Batch loss: 0.05477211996912956 batch: 49/224\n",
      "Batch loss: 0.05091138184070587 batch: 50/224\n",
      "Batch loss: 0.05189303681254387 batch: 51/224\n",
      "Batch loss: 0.037647828459739685 batch: 52/224\n",
      "Batch loss: 0.0773080438375473 batch: 53/224\n",
      "Batch loss: 0.02705022692680359 batch: 54/224\n",
      "Batch loss: 0.07276185601949692 batch: 55/224\n",
      "Batch loss: 0.057514771819114685 batch: 56/224\n",
      "Batch loss: 0.06869533658027649 batch: 57/224\n",
      "Batch loss: 0.07989975064992905 batch: 58/224\n",
      "Batch loss: 0.051051732152700424 batch: 59/224\n",
      "Batch loss: 0.09165927767753601 batch: 60/224\n",
      "Batch loss: 0.08200191706418991 batch: 61/224\n",
      "Batch loss: 0.03799847885966301 batch: 62/224\n",
      "Batch loss: 0.058608051389455795 batch: 63/224\n",
      "Batch loss: 0.07219113409519196 batch: 64/224\n",
      "Batch loss: 0.05818614736199379 batch: 65/224\n",
      "Batch loss: 0.07587052881717682 batch: 66/224\n",
      "Batch loss: 0.0461227111518383 batch: 67/224\n",
      "Batch loss: 0.05541456118226051 batch: 68/224\n",
      "Batch loss: 0.07100936770439148 batch: 69/224\n",
      "Batch loss: 0.06209418177604675 batch: 70/224\n",
      "Batch loss: 0.07305797934532166 batch: 71/224\n",
      "Batch loss: 0.0686606615781784 batch: 72/224\n",
      "Batch loss: 0.06775970757007599 batch: 73/224\n",
      "Batch loss: 0.04871155694127083 batch: 74/224\n",
      "Batch loss: 0.06995107978582382 batch: 75/224\n",
      "Batch loss: 0.04971423000097275 batch: 76/224\n",
      "Batch loss: 0.040109261870384216 batch: 77/224\n",
      "Batch loss: 0.05860874056816101 batch: 78/224\n",
      "Batch loss: 0.08071061968803406 batch: 79/224\n",
      "Batch loss: 0.052001938223838806 batch: 80/224\n",
      "Batch loss: 0.08539168536663055 batch: 81/224\n",
      "Batch loss: 0.05985640734434128 batch: 82/224\n",
      "Batch loss: 0.07461995631456375 batch: 83/224\n",
      "Batch loss: 0.033382609486579895 batch: 84/224\n",
      "Batch loss: 0.04971177875995636 batch: 85/224\n",
      "Batch loss: 0.0658939927816391 batch: 86/224\n",
      "Batch loss: 0.06433968991041183 batch: 87/224\n",
      "Batch loss: 0.07694130390882492 batch: 88/224\n",
      "Batch loss: 0.033737678080797195 batch: 89/224\n",
      "Batch loss: 0.06729266792535782 batch: 90/224\n",
      "Batch loss: 0.04691915586590767 batch: 91/224\n",
      "Batch loss: 0.0629734992980957 batch: 92/224\n",
      "Batch loss: 0.02381386049091816 batch: 93/224\n",
      "Batch loss: 0.034840527921915054 batch: 94/224\n",
      "Batch loss: 0.04267160966992378 batch: 95/224\n",
      "Batch loss: 0.07999121397733688 batch: 96/224\n",
      "Batch loss: 0.06184127554297447 batch: 97/224\n",
      "Batch loss: 0.025598619133234024 batch: 98/224\n",
      "Batch loss: 0.07617804408073425 batch: 99/224\n",
      "Batch loss: 0.05616868659853935 batch: 100/224\n",
      "Batch loss: 0.06901377439498901 batch: 101/224\n",
      "Batch loss: 0.09655767679214478 batch: 102/224\n",
      "Batch loss: 0.07323344051837921 batch: 103/224\n",
      "Batch loss: 0.05144432187080383 batch: 104/224\n",
      "Batch loss: 0.030468905344605446 batch: 105/224\n",
      "Batch loss: 0.0607895627617836 batch: 106/224\n",
      "Batch loss: 0.05519268661737442 batch: 107/224\n",
      "Batch loss: 0.07539048790931702 batch: 108/224\n",
      "Batch loss: 0.07475096732378006 batch: 109/224\n",
      "Batch loss: 0.06325599551200867 batch: 110/224\n",
      "Batch loss: 0.04383300244808197 batch: 111/224\n",
      "Batch loss: 0.030877240002155304 batch: 112/224\n",
      "Batch loss: 0.052315838634967804 batch: 113/224\n",
      "Batch loss: 0.04834366217255592 batch: 114/224\n",
      "Batch loss: 0.05476337671279907 batch: 115/224\n",
      "Batch loss: 0.03544345870614052 batch: 116/224\n",
      "Batch loss: 0.03893483430147171 batch: 117/224\n",
      "Batch loss: 0.0504392646253109 batch: 118/224\n",
      "Batch loss: 0.05892655625939369 batch: 119/224\n",
      "Batch loss: 0.06932319700717926 batch: 120/224\n",
      "Batch loss: 0.1069035604596138 batch: 121/224\n",
      "Batch loss: 0.054873790591955185 batch: 122/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.04269655793905258 batch: 123/224\n",
      "Batch loss: 0.05146848037838936 batch: 124/224\n",
      "Batch loss: 0.05919686332345009 batch: 125/224\n",
      "Batch loss: 0.0629287138581276 batch: 126/224\n",
      "Batch loss: 0.09311763197183609 batch: 127/224\n",
      "Batch loss: 0.05883624777197838 batch: 128/224\n",
      "Batch loss: 0.06323394924402237 batch: 129/224\n",
      "Batch loss: 0.04744994640350342 batch: 130/224\n",
      "Batch loss: 0.08961835503578186 batch: 131/224\n",
      "Batch loss: 0.07202626764774323 batch: 132/224\n",
      "Batch loss: 0.04804782569408417 batch: 133/224\n",
      "Batch loss: 0.08224650472402573 batch: 134/224\n",
      "Batch loss: 0.07108525186777115 batch: 135/224\n",
      "Batch loss: 0.06779920309782028 batch: 136/224\n",
      "Batch loss: 0.07432195544242859 batch: 137/224\n",
      "Batch loss: 0.06683775037527084 batch: 138/224\n",
      "Batch loss: 0.06376021355390549 batch: 139/224\n",
      "Batch loss: 0.10048601031303406 batch: 140/224\n",
      "Batch loss: 0.06369750201702118 batch: 141/224\n",
      "Batch loss: 0.07055190205574036 batch: 142/224\n",
      "Batch loss: 0.07716764509677887 batch: 143/224\n",
      "Batch loss: 0.06686340272426605 batch: 144/224\n",
      "Batch loss: 0.0658576637506485 batch: 145/224\n",
      "Batch loss: 0.11127530038356781 batch: 146/224\n",
      "Batch loss: 0.06233109533786774 batch: 147/224\n",
      "Batch loss: 0.061681441962718964 batch: 148/224\n",
      "Batch loss: 0.07668735831975937 batch: 149/224\n",
      "Batch loss: 0.09221285581588745 batch: 150/224\n",
      "Batch loss: 0.05841711536049843 batch: 151/224\n",
      "Batch loss: 0.06758879125118256 batch: 152/224\n",
      "Batch loss: 0.07351458817720413 batch: 153/224\n",
      "Batch loss: 0.05204126983880997 batch: 154/224\n",
      "Batch loss: 0.041740741580724716 batch: 155/224\n",
      "Batch loss: 0.06478781253099442 batch: 156/224\n",
      "Batch loss: 0.06785173714160919 batch: 157/224\n",
      "Batch loss: 0.10823215544223785 batch: 158/224\n",
      "Batch loss: 0.06316601485013962 batch: 159/224\n",
      "Batch loss: 0.05165315791964531 batch: 160/224\n",
      "Batch loss: 0.045694299042224884 batch: 161/224\n",
      "Batch loss: 0.06505633890628815 batch: 162/224\n",
      "Batch loss: 0.04535326361656189 batch: 163/224\n",
      "Batch loss: 0.07899276167154312 batch: 164/224\n",
      "Batch loss: 0.08924063295125961 batch: 165/224\n",
      "Batch loss: 0.07094951719045639 batch: 166/224\n",
      "Batch loss: 0.056540489196777344 batch: 167/224\n",
      "Batch loss: 0.06466446071863174 batch: 168/224\n",
      "Batch loss: 0.11168111115694046 batch: 169/224\n",
      "Batch loss: 0.0674382895231247 batch: 170/224\n",
      "Batch loss: 0.0514288991689682 batch: 171/224\n",
      "Batch loss: 0.045040346682071686 batch: 172/224\n",
      "Batch loss: 0.08023876696825027 batch: 173/224\n",
      "Batch loss: 0.05769263952970505 batch: 174/224\n",
      "Batch loss: 0.08669645339250565 batch: 175/224\n",
      "Batch loss: 0.05352100357413292 batch: 176/224\n",
      "Batch loss: 0.08802357316017151 batch: 177/224\n",
      "Batch loss: 0.0666445642709732 batch: 178/224\n",
      "Batch loss: 0.04891444370150566 batch: 179/224\n",
      "Batch loss: 0.023771293461322784 batch: 180/224\n",
      "Batch loss: 0.04841376841068268 batch: 181/224\n",
      "Batch loss: 0.06685741990804672 batch: 182/224\n",
      "Batch loss: 0.07186995446681976 batch: 183/224\n",
      "Batch loss: 0.06489825248718262 batch: 184/224\n",
      "Batch loss: 0.08436200022697449 batch: 185/224\n",
      "Batch loss: 0.060086362063884735 batch: 186/224\n",
      "Batch loss: 0.07624022662639618 batch: 187/224\n",
      "Batch loss: 0.04563678056001663 batch: 188/224\n",
      "Batch loss: 0.05843891203403473 batch: 189/224\n",
      "Batch loss: 0.05380222946405411 batch: 190/224\n",
      "Batch loss: 0.07270283252000809 batch: 191/224\n",
      "Batch loss: 0.0803346112370491 batch: 192/224\n",
      "Batch loss: 0.061752453446388245 batch: 193/224\n",
      "Batch loss: 0.06165910139679909 batch: 194/224\n",
      "Batch loss: 0.08624839782714844 batch: 195/224\n",
      "Batch loss: 0.05785840377211571 batch: 196/224\n",
      "Batch loss: 0.05302423983812332 batch: 197/224\n",
      "Batch loss: 0.0733155608177185 batch: 198/224\n",
      "Batch loss: 0.05246909707784653 batch: 199/224\n",
      "Batch loss: 0.06916701048612595 batch: 200/224\n",
      "Batch loss: 0.09128227829933167 batch: 201/224\n",
      "Batch loss: 0.059912294149398804 batch: 202/224\n",
      "Batch loss: 0.03888360410928726 batch: 203/224\n",
      "Batch loss: 0.06643855571746826 batch: 204/224\n",
      "Batch loss: 0.07574549317359924 batch: 205/224\n",
      "Batch loss: 0.08094967156648636 batch: 206/224\n",
      "Batch loss: 0.06186458095908165 batch: 207/224\n",
      "Batch loss: 0.03702212870121002 batch: 208/224\n",
      "Batch loss: 0.059326522052288055 batch: 209/224\n",
      "Batch loss: 0.04245789721608162 batch: 210/224\n",
      "Batch loss: 0.05367601662874222 batch: 211/224\n",
      "Batch loss: 0.09646312892436981 batch: 212/224\n",
      "Batch loss: 0.059035174548625946 batch: 213/224\n",
      "Batch loss: 0.09183654934167862 batch: 214/224\n",
      "Batch loss: 0.07232217490673065 batch: 215/224\n",
      "Batch loss: 0.04706908017396927 batch: 216/224\n",
      "Batch loss: 0.08211314678192139 batch: 217/224\n",
      "Batch loss: 0.06295382976531982 batch: 218/224\n",
      "Batch loss: 0.050828706473112106 batch: 219/224\n",
      "Batch loss: 0.07827284932136536 batch: 220/224\n",
      "Batch loss: 0.10108641535043716 batch: 221/224\n",
      "Batch loss: 0.06888221949338913 batch: 222/224\n",
      "Batch loss: 0.0373738631606102 batch: 223/224\n",
      "Batch loss: 0.05119086429476738 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 63/75..  Training Loss: 0.00013..  Test Loss: 0.00105..  Test Accuracy: 0.89225\n",
      "Running epoch 64/75\n",
      "Batch loss: 0.042798962444067 batch: 1/224\n",
      "Batch loss: 0.07008681446313858 batch: 2/224\n",
      "Batch loss: 0.07307525724172592 batch: 3/224\n",
      "Batch loss: 0.08542399108409882 batch: 4/224\n",
      "Batch loss: 0.059484802186489105 batch: 5/224\n",
      "Batch loss: 0.049051813781261444 batch: 6/224\n",
      "Batch loss: 0.06352004408836365 batch: 7/224\n",
      "Batch loss: 0.06801442801952362 batch: 8/224\n",
      "Batch loss: 0.04152192175388336 batch: 9/224\n",
      "Batch loss: 0.04369164630770683 batch: 10/224\n",
      "Batch loss: 0.09799687564373016 batch: 11/224\n",
      "Batch loss: 0.0683097094297409 batch: 12/224\n",
      "Batch loss: 0.039599813520908356 batch: 13/224\n",
      "Batch loss: 0.04717258736491203 batch: 14/224\n",
      "Batch loss: 0.05664873495697975 batch: 15/224\n",
      "Batch loss: 0.06290221214294434 batch: 16/224\n",
      "Batch loss: 0.05116984248161316 batch: 17/224\n",
      "Batch loss: 0.0484076626598835 batch: 18/224\n",
      "Batch loss: 0.060258716344833374 batch: 19/224\n",
      "Batch loss: 0.05729818344116211 batch: 20/224\n",
      "Batch loss: 0.06957103312015533 batch: 21/224\n",
      "Batch loss: 0.05149541795253754 batch: 22/224\n",
      "Batch loss: 0.07888975739479065 batch: 23/224\n",
      "Batch loss: 0.081996388733387 batch: 24/224\n",
      "Batch loss: 0.053735990077257156 batch: 25/224\n",
      "Batch loss: 0.04464995861053467 batch: 26/224\n",
      "Batch loss: 0.046837370842695236 batch: 27/224\n",
      "Batch loss: 0.06176698952913284 batch: 28/224\n",
      "Batch loss: 0.056021690368652344 batch: 29/224\n",
      "Batch loss: 0.03731309995055199 batch: 30/224\n",
      "Batch loss: 0.04836824908852577 batch: 31/224\n",
      "Batch loss: 0.07510555535554886 batch: 32/224\n",
      "Batch loss: 0.04487192630767822 batch: 33/224\n",
      "Batch loss: 0.08664145320653915 batch: 34/224\n",
      "Batch loss: 0.09248566627502441 batch: 35/224\n",
      "Batch loss: 0.05357835441827774 batch: 36/224\n",
      "Batch loss: 0.05182061344385147 batch: 37/224\n",
      "Batch loss: 0.06916738301515579 batch: 38/224\n",
      "Batch loss: 0.06091879680752754 batch: 39/224\n",
      "Batch loss: 0.04768446087837219 batch: 40/224\n",
      "Batch loss: 0.060470614582300186 batch: 41/224\n",
      "Batch loss: 0.07833828777074814 batch: 42/224\n",
      "Batch loss: 0.08176521211862564 batch: 43/224\n",
      "Batch loss: 0.05738982930779457 batch: 44/224\n",
      "Batch loss: 0.07809149473905563 batch: 45/224\n",
      "Batch loss: 0.11945465207099915 batch: 46/224\n",
      "Batch loss: 0.06477141380310059 batch: 47/224\n",
      "Batch loss: 0.0695626512169838 batch: 48/224\n",
      "Batch loss: 0.03619043901562691 batch: 49/224\n",
      "Batch loss: 0.058232199400663376 batch: 50/224\n",
      "Batch loss: 0.07112956792116165 batch: 51/224\n",
      "Batch loss: 0.04794222488999367 batch: 52/224\n",
      "Batch loss: 0.07459086924791336 batch: 53/224\n",
      "Batch loss: 0.05219729617238045 batch: 54/224\n",
      "Batch loss: 0.055097684264183044 batch: 55/224\n",
      "Batch loss: 0.06923168897628784 batch: 56/224\n",
      "Batch loss: 0.08332537859678268 batch: 57/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.07746061682701111 batch: 58/224\n",
      "Batch loss: 0.07796590030193329 batch: 59/224\n",
      "Batch loss: 0.08258070796728134 batch: 60/224\n",
      "Batch loss: 0.06152215972542763 batch: 61/224\n",
      "Batch loss: 0.03680506348609924 batch: 62/224\n",
      "Batch loss: 0.060348935425281525 batch: 63/224\n",
      "Batch loss: 0.06304826587438583 batch: 64/224\n",
      "Batch loss: 0.03546953946352005 batch: 65/224\n",
      "Batch loss: 0.06600431352853775 batch: 66/224\n",
      "Batch loss: 0.06196369230747223 batch: 67/224\n",
      "Batch loss: 0.02848050370812416 batch: 68/224\n",
      "Batch loss: 0.06546836346387863 batch: 69/224\n",
      "Batch loss: 0.08776026219129562 batch: 70/224\n",
      "Batch loss: 0.03394768387079239 batch: 71/224\n",
      "Batch loss: 0.049061231315135956 batch: 72/224\n",
      "Batch loss: 0.05209207162261009 batch: 73/224\n",
      "Batch loss: 0.045811112970113754 batch: 74/224\n",
      "Batch loss: 0.06017284467816353 batch: 75/224\n",
      "Batch loss: 0.05943821743130684 batch: 76/224\n",
      "Batch loss: 0.06179350987076759 batch: 77/224\n",
      "Batch loss: 0.07042967528104782 batch: 78/224\n",
      "Batch loss: 0.049255404621362686 batch: 79/224\n",
      "Batch loss: 0.04417194053530693 batch: 80/224\n",
      "Batch loss: 0.09854750335216522 batch: 81/224\n",
      "Batch loss: 0.08601300418376923 batch: 82/224\n",
      "Batch loss: 0.05352449044585228 batch: 83/224\n",
      "Batch loss: 0.03802137449383736 batch: 84/224\n",
      "Batch loss: 0.06361117213964462 batch: 85/224\n",
      "Batch loss: 0.06664935499429703 batch: 86/224\n",
      "Batch loss: 0.06527134031057358 batch: 87/224\n",
      "Batch loss: 0.0844084769487381 batch: 88/224\n",
      "Batch loss: 0.04254132881760597 batch: 89/224\n",
      "Batch loss: 0.045738060027360916 batch: 90/224\n",
      "Batch loss: 0.0290609672665596 batch: 91/224\n",
      "Batch loss: 0.060679249465465546 batch: 92/224\n",
      "Batch loss: 0.041693318635225296 batch: 93/224\n",
      "Batch loss: 0.060130245983600616 batch: 94/224\n",
      "Batch loss: 0.060377705842256546 batch: 95/224\n",
      "Batch loss: 0.08275530487298965 batch: 96/224\n",
      "Batch loss: 0.040719322860240936 batch: 97/224\n",
      "Batch loss: 0.03433898091316223 batch: 98/224\n",
      "Batch loss: 0.06631585210561752 batch: 99/224\n",
      "Batch loss: 0.0694374069571495 batch: 100/224\n",
      "Batch loss: 0.07852499932050705 batch: 101/224\n",
      "Batch loss: 0.07314000278711319 batch: 102/224\n",
      "Batch loss: 0.09063902497291565 batch: 103/224\n",
      "Batch loss: 0.033234190195798874 batch: 104/224\n",
      "Batch loss: 0.07985170185565948 batch: 105/224\n",
      "Batch loss: 0.05961131677031517 batch: 106/224\n",
      "Batch loss: 0.06421972066164017 batch: 107/224\n",
      "Batch loss: 0.047939494252204895 batch: 108/224\n",
      "Batch loss: 0.06369686126708984 batch: 109/224\n",
      "Batch loss: 0.06061452999711037 batch: 110/224\n",
      "Batch loss: 0.06398353725671768 batch: 111/224\n",
      "Batch loss: 0.062258120626211166 batch: 112/224\n",
      "Batch loss: 0.05806452035903931 batch: 113/224\n",
      "Batch loss: 0.050580091774463654 batch: 114/224\n",
      "Batch loss: 0.03832389414310455 batch: 115/224\n",
      "Batch loss: 0.07436125725507736 batch: 116/224\n",
      "Batch loss: 0.04902932792901993 batch: 117/224\n",
      "Batch loss: 0.03710325062274933 batch: 118/224\n",
      "Batch loss: 0.04811696335673332 batch: 119/224\n",
      "Batch loss: 0.05806276574730873 batch: 120/224\n",
      "Batch loss: 0.04754031449556351 batch: 121/224\n",
      "Batch loss: 0.06137882545590401 batch: 122/224\n",
      "Batch loss: 0.034742191433906555 batch: 123/224\n",
      "Batch loss: 0.05511655658483505 batch: 124/224\n",
      "Batch loss: 0.07590153813362122 batch: 125/224\n",
      "Batch loss: 0.051806408911943436 batch: 126/224\n",
      "Batch loss: 0.08155262470245361 batch: 127/224\n",
      "Batch loss: 0.03859035670757294 batch: 128/224\n",
      "Batch loss: 0.0648413598537445 batch: 129/224\n",
      "Batch loss: 0.05992349982261658 batch: 130/224\n",
      "Batch loss: 0.02422844059765339 batch: 131/224\n",
      "Batch loss: 0.09374628216028214 batch: 132/224\n",
      "Batch loss: 0.07924412190914154 batch: 133/224\n",
      "Batch loss: 0.05926100164651871 batch: 134/224\n",
      "Batch loss: 0.03680667281150818 batch: 135/224\n",
      "Batch loss: 0.06598354130983353 batch: 136/224\n",
      "Batch loss: 0.05746737867593765 batch: 137/224\n",
      "Batch loss: 0.054601699113845825 batch: 138/224\n",
      "Batch loss: 0.044423043727874756 batch: 139/224\n",
      "Batch loss: 0.08325742930173874 batch: 140/224\n",
      "Batch loss: 0.062369782477617264 batch: 141/224\n",
      "Batch loss: 0.08464919775724411 batch: 142/224\n",
      "Batch loss: 0.044162552803754807 batch: 143/224\n",
      "Batch loss: 0.05108901113271713 batch: 144/224\n",
      "Batch loss: 0.053069476038217545 batch: 145/224\n",
      "Batch loss: 0.08023513853549957 batch: 146/224\n",
      "Batch loss: 0.061587829142808914 batch: 147/224\n",
      "Batch loss: 0.06563150137662888 batch: 148/224\n",
      "Batch loss: 0.07728084921836853 batch: 149/224\n",
      "Batch loss: 0.08333055675029755 batch: 150/224\n",
      "Batch loss: 0.05817272886633873 batch: 151/224\n",
      "Batch loss: 0.07286591827869415 batch: 152/224\n",
      "Batch loss: 0.0731193870306015 batch: 153/224\n",
      "Batch loss: 0.06734665483236313 batch: 154/224\n",
      "Batch loss: 0.05347014218568802 batch: 155/224\n",
      "Batch loss: 0.06851296126842499 batch: 156/224\n",
      "Batch loss: 0.04097557067871094 batch: 157/224\n",
      "Batch loss: 0.074400395154953 batch: 158/224\n",
      "Batch loss: 0.05613251402974129 batch: 159/224\n",
      "Batch loss: 0.04787061735987663 batch: 160/224\n",
      "Batch loss: 0.07093121856451035 batch: 161/224\n",
      "Batch loss: 0.06253410875797272 batch: 162/224\n",
      "Batch loss: 0.04784684255719185 batch: 163/224\n",
      "Batch loss: 0.04803800955414772 batch: 164/224\n",
      "Batch loss: 0.050356291234493256 batch: 165/224\n",
      "Batch loss: 0.0745878592133522 batch: 166/224\n",
      "Batch loss: 0.045969441533088684 batch: 167/224\n",
      "Batch loss: 0.05440283194184303 batch: 168/224\n",
      "Batch loss: 0.0664733275771141 batch: 169/224\n",
      "Batch loss: 0.06629423052072525 batch: 170/224\n",
      "Batch loss: 0.05651065707206726 batch: 171/224\n",
      "Batch loss: 0.04503870755434036 batch: 172/224\n",
      "Batch loss: 0.057537660002708435 batch: 173/224\n",
      "Batch loss: 0.03579266369342804 batch: 174/224\n",
      "Batch loss: 0.03795015066862106 batch: 175/224\n",
      "Batch loss: 0.04434594884514809 batch: 176/224\n",
      "Batch loss: 0.07228770107030869 batch: 177/224\n",
      "Batch loss: 0.04028293490409851 batch: 178/224\n",
      "Batch loss: 0.09248624742031097 batch: 179/224\n",
      "Batch loss: 0.06597557663917542 batch: 180/224\n",
      "Batch loss: 0.04375409707427025 batch: 181/224\n",
      "Batch loss: 0.057389996945858 batch: 182/224\n",
      "Batch loss: 0.07949237525463104 batch: 183/224\n",
      "Batch loss: 0.06818947196006775 batch: 184/224\n",
      "Batch loss: 0.07400687038898468 batch: 185/224\n",
      "Batch loss: 0.04169825464487076 batch: 186/224\n",
      "Batch loss: 0.06490223109722137 batch: 187/224\n",
      "Batch loss: 0.044893939048051834 batch: 188/224\n",
      "Batch loss: 0.051684897392988205 batch: 189/224\n",
      "Batch loss: 0.0694124773144722 batch: 190/224\n",
      "Batch loss: 0.07686781883239746 batch: 191/224\n",
      "Batch loss: 0.07159008085727692 batch: 192/224\n",
      "Batch loss: 0.08025293797254562 batch: 193/224\n",
      "Batch loss: 0.06984265893697739 batch: 194/224\n",
      "Batch loss: 0.05116622522473335 batch: 195/224\n",
      "Batch loss: 0.08098749816417694 batch: 196/224\n",
      "Batch loss: 0.035533685237169266 batch: 197/224\n",
      "Batch loss: 0.04387455806136131 batch: 198/224\n",
      "Batch loss: 0.04315831884741783 batch: 199/224\n",
      "Batch loss: 0.04954126477241516 batch: 200/224\n",
      "Batch loss: 0.05952933803200722 batch: 201/224\n",
      "Batch loss: 0.07326462864875793 batch: 202/224\n",
      "Batch loss: 0.058658234775066376 batch: 203/224\n",
      "Batch loss: 0.054976314306259155 batch: 204/224\n",
      "Batch loss: 0.08660178631544113 batch: 205/224\n",
      "Batch loss: 0.06265316903591156 batch: 206/224\n",
      "Batch loss: 0.04996921494603157 batch: 207/224\n",
      "Batch loss: 0.06723666191101074 batch: 208/224\n",
      "Batch loss: 0.07169125974178314 batch: 209/224\n",
      "Batch loss: 0.05666196718811989 batch: 210/224\n",
      "Batch loss: 0.06181154400110245 batch: 211/224\n",
      "Batch loss: 0.03789655119180679 batch: 212/224\n",
      "Batch loss: 0.08894664794206619 batch: 213/224\n",
      "Batch loss: 0.046924229711294174 batch: 214/224\n",
      "Batch loss: 0.06421272456645966 batch: 215/224\n",
      "Batch loss: 0.0534236840903759 batch: 216/224\n",
      "Batch loss: 0.06572005152702332 batch: 217/224\n",
      "Batch loss: 0.05241112783551216 batch: 218/224\n",
      "Batch loss: 0.04003657400608063 batch: 219/224\n",
      "Batch loss: 0.04734547436237335 batch: 220/224\n",
      "Batch loss: 0.06312589347362518 batch: 221/224\n",
      "Batch loss: 0.10286267101764679 batch: 222/224\n",
      "Batch loss: 0.04610626772046089 batch: 223/224\n",
      "Batch loss: 0.06835825741291046 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 64/75..  Training Loss: 0.00012..  Test Loss: 0.00104..  Test Accuracy: 0.89200\n",
      "Running epoch 65/75\n",
      "Batch loss: 0.05223039165139198 batch: 1/224\n",
      "Batch loss: 0.06272097676992416 batch: 2/224\n",
      "Batch loss: 0.06064828485250473 batch: 3/224\n",
      "Batch loss: 0.08681149780750275 batch: 4/224\n",
      "Batch loss: 0.0770762637257576 batch: 5/224\n",
      "Batch loss: 0.07431208342313766 batch: 6/224\n",
      "Batch loss: 0.056455887854099274 batch: 7/224\n",
      "Batch loss: 0.07875345647335052 batch: 8/224\n",
      "Batch loss: 0.062152452766895294 batch: 9/224\n",
      "Batch loss: 0.04315301030874252 batch: 10/224\n",
      "Batch loss: 0.0818936750292778 batch: 11/224\n",
      "Batch loss: 0.05886472761631012 batch: 12/224\n",
      "Batch loss: 0.051129650324583054 batch: 13/224\n",
      "Batch loss: 0.03856933116912842 batch: 14/224\n",
      "Batch loss: 0.059220537543296814 batch: 15/224\n",
      "Batch loss: 0.06957647204399109 batch: 16/224\n",
      "Batch loss: 0.058801017701625824 batch: 17/224\n",
      "Batch loss: 0.0606558583676815 batch: 18/224\n",
      "Batch loss: 0.03698712959885597 batch: 19/224\n",
      "Batch loss: 0.05127910524606705 batch: 20/224\n",
      "Batch loss: 0.05288957431912422 batch: 21/224\n",
      "Batch loss: 0.04832520708441734 batch: 22/224\n",
      "Batch loss: 0.07571747153997421 batch: 23/224\n",
      "Batch loss: 0.06543255597352982 batch: 24/224\n",
      "Batch loss: 0.044653747230768204 batch: 25/224\n",
      "Batch loss: 0.05321196839213371 batch: 26/224\n",
      "Batch loss: 0.05754358321428299 batch: 27/224\n",
      "Batch loss: 0.08485879004001617 batch: 28/224\n",
      "Batch loss: 0.0494777113199234 batch: 29/224\n",
      "Batch loss: 0.055948227643966675 batch: 30/224\n",
      "Batch loss: 0.04409307241439819 batch: 31/224\n",
      "Batch loss: 0.043259397149086 batch: 32/224\n",
      "Batch loss: 0.061956536024808884 batch: 33/224\n",
      "Batch loss: 0.08093440532684326 batch: 34/224\n",
      "Batch loss: 0.07191718369722366 batch: 35/224\n",
      "Batch loss: 0.05673176795244217 batch: 36/224\n",
      "Batch loss: 0.05769900977611542 batch: 37/224\n",
      "Batch loss: 0.055453065782785416 batch: 38/224\n",
      "Batch loss: 0.08514037728309631 batch: 39/224\n",
      "Batch loss: 0.05276305228471756 batch: 40/224\n",
      "Batch loss: 0.062382012605667114 batch: 41/224\n",
      "Batch loss: 0.06184393912553787 batch: 42/224\n",
      "Batch loss: 0.09252817928791046 batch: 43/224\n",
      "Batch loss: 0.06739166378974915 batch: 44/224\n",
      "Batch loss: 0.06348211318254471 batch: 45/224\n",
      "Batch loss: 0.06565533578395844 batch: 46/224\n",
      "Batch loss: 0.062185727059841156 batch: 47/224\n",
      "Batch loss: 0.04810294136404991 batch: 48/224\n",
      "Batch loss: 0.06491278111934662 batch: 49/224\n",
      "Batch loss: 0.0335199199616909 batch: 50/224\n",
      "Batch loss: 0.03553834930062294 batch: 51/224\n",
      "Batch loss: 0.08212368190288544 batch: 52/224\n",
      "Batch loss: 0.07035503536462784 batch: 53/224\n",
      "Batch loss: 0.043826065957546234 batch: 54/224\n",
      "Batch loss: 0.06529971212148666 batch: 55/224\n",
      "Batch loss: 0.024951022118330002 batch: 56/224\n",
      "Batch loss: 0.07068589329719543 batch: 57/224\n",
      "Batch loss: 0.06030403822660446 batch: 58/224\n",
      "Batch loss: 0.04121801629662514 batch: 59/224\n",
      "Batch loss: 0.09369073063135147 batch: 60/224\n",
      "Batch loss: 0.07071392983198166 batch: 61/224\n",
      "Batch loss: 0.03470604121685028 batch: 62/224\n",
      "Batch loss: 0.07518139481544495 batch: 63/224\n",
      "Batch loss: 0.07587691396474838 batch: 64/224\n",
      "Batch loss: 0.05826619267463684 batch: 65/224\n",
      "Batch loss: 0.050089020282030106 batch: 66/224\n",
      "Batch loss: 0.052715983241796494 batch: 67/224\n",
      "Batch loss: 0.04182954132556915 batch: 68/224\n",
      "Batch loss: 0.049371954053640366 batch: 69/224\n",
      "Batch loss: 0.07143668085336685 batch: 70/224\n",
      "Batch loss: 0.06556728482246399 batch: 71/224\n",
      "Batch loss: 0.04866831749677658 batch: 72/224\n",
      "Batch loss: 0.0790981650352478 batch: 73/224\n",
      "Batch loss: 0.0809096246957779 batch: 74/224\n",
      "Batch loss: 0.04206429049372673 batch: 75/224\n",
      "Batch loss: 0.07802331447601318 batch: 76/224\n",
      "Batch loss: 0.054224610328674316 batch: 77/224\n",
      "Batch loss: 0.06247666850686073 batch: 78/224\n",
      "Batch loss: 0.079266257584095 batch: 79/224\n",
      "Batch loss: 0.05336150899529457 batch: 80/224\n",
      "Batch loss: 0.09525330364704132 batch: 81/224\n",
      "Batch loss: 0.10137908905744553 batch: 82/224\n",
      "Batch loss: 0.06177455931901932 batch: 83/224\n",
      "Batch loss: 0.03647032007575035 batch: 84/224\n",
      "Batch loss: 0.0674256831407547 batch: 85/224\n",
      "Batch loss: 0.06322836875915527 batch: 86/224\n",
      "Batch loss: 0.06440329551696777 batch: 87/224\n",
      "Batch loss: 0.05035281553864479 batch: 88/224\n",
      "Batch loss: 0.05058779940009117 batch: 89/224\n",
      "Batch loss: 0.05581963062286377 batch: 90/224\n",
      "Batch loss: 0.053996741771698 batch: 91/224\n",
      "Batch loss: 0.051940858364105225 batch: 92/224\n",
      "Batch loss: 0.03636661544442177 batch: 93/224\n",
      "Batch loss: 0.06134488806128502 batch: 94/224\n",
      "Batch loss: 0.03390084207057953 batch: 95/224\n",
      "Batch loss: 0.04393431171774864 batch: 96/224\n",
      "Batch loss: 0.04989587515592575 batch: 97/224\n",
      "Batch loss: 0.026441896334290504 batch: 98/224\n",
      "Batch loss: 0.06601087749004364 batch: 99/224\n",
      "Batch loss: 0.04620286077260971 batch: 100/224\n",
      "Batch loss: 0.11683403700590134 batch: 101/224\n",
      "Batch loss: 0.0688248947262764 batch: 102/224\n",
      "Batch loss: 0.08283182978630066 batch: 103/224\n",
      "Batch loss: 0.04656139388680458 batch: 104/224\n",
      "Batch loss: 0.05072103068232536 batch: 105/224\n",
      "Batch loss: 0.06441086530685425 batch: 106/224\n",
      "Batch loss: 0.06289411336183548 batch: 107/224\n",
      "Batch loss: 0.06540029495954514 batch: 108/224\n",
      "Batch loss: 0.05411265417933464 batch: 109/224\n",
      "Batch loss: 0.04216485470533371 batch: 110/224\n",
      "Batch loss: 0.047886356711387634 batch: 111/224\n",
      "Batch loss: 0.05587927997112274 batch: 112/224\n",
      "Batch loss: 0.04746139794588089 batch: 113/224\n",
      "Batch loss: 0.061484094709157944 batch: 114/224\n",
      "Batch loss: 0.05144659802317619 batch: 115/224\n",
      "Batch loss: 0.06178915500640869 batch: 116/224\n",
      "Batch loss: 0.04575269669294357 batch: 117/224\n",
      "Batch loss: 0.04030163586139679 batch: 118/224\n",
      "Batch loss: 0.05109916627407074 batch: 119/224\n",
      "Batch loss: 0.05353488773107529 batch: 120/224\n",
      "Batch loss: 0.03888086602091789 batch: 121/224\n",
      "Batch loss: 0.044534776359796524 batch: 122/224\n",
      "Batch loss: 0.05032708868384361 batch: 123/224\n",
      "Batch loss: 0.05574079230427742 batch: 124/224\n",
      "Batch loss: 0.09613844007253647 batch: 125/224\n",
      "Batch loss: 0.07559797912836075 batch: 126/224\n",
      "Batch loss: 0.09686320275068283 batch: 127/224\n",
      "Batch loss: 0.06664111465215683 batch: 128/224\n",
      "Batch loss: 0.046947937458753586 batch: 129/224\n",
      "Batch loss: 0.07495098561048508 batch: 130/224\n",
      "Batch loss: 0.03482011333107948 batch: 131/224\n",
      "Batch loss: 0.054109543561935425 batch: 132/224\n",
      "Batch loss: 0.08974537998437881 batch: 133/224\n",
      "Batch loss: 0.06396925449371338 batch: 134/224\n",
      "Batch loss: 0.05710531771183014 batch: 135/224\n",
      "Batch loss: 0.05132346972823143 batch: 136/224\n",
      "Batch loss: 0.03261776641011238 batch: 137/224\n",
      "Batch loss: 0.07940617203712463 batch: 138/224\n",
      "Batch loss: 0.08324891328811646 batch: 139/224\n",
      "Batch loss: 0.06739272177219391 batch: 140/224\n",
      "Batch loss: 0.06566072255373001 batch: 141/224\n",
      "Batch loss: 0.05178770795464516 batch: 142/224\n",
      "Batch loss: 0.04948492348194122 batch: 143/224\n",
      "Batch loss: 0.04980052635073662 batch: 144/224\n",
      "Batch loss: 0.05274387076497078 batch: 145/224\n",
      "Batch loss: 0.10251768678426743 batch: 146/224\n",
      "Batch loss: 0.08029121160507202 batch: 147/224\n",
      "Batch loss: 0.07503877580165863 batch: 148/224\n",
      "Batch loss: 0.08288726210594177 batch: 149/224\n",
      "Batch loss: 0.07448296993970871 batch: 150/224\n",
      "Batch loss: 0.054510533809661865 batch: 151/224\n",
      "Batch loss: 0.0707709863781929 batch: 152/224\n",
      "Batch loss: 0.07529202103614807 batch: 153/224\n",
      "Batch loss: 0.036036647856235504 batch: 154/224\n",
      "Batch loss: 0.044884778559207916 batch: 155/224\n",
      "Batch loss: 0.08549754321575165 batch: 156/224\n",
      "Batch loss: 0.0882042944431305 batch: 157/224\n",
      "Batch loss: 0.09660396724939346 batch: 158/224\n",
      "Batch loss: 0.04009826481342316 batch: 159/224\n",
      "Batch loss: 0.05409782752394676 batch: 160/224\n",
      "Batch loss: 0.035141557455062866 batch: 161/224\n",
      "Batch loss: 0.06539270281791687 batch: 162/224\n",
      "Batch loss: 0.04107063263654709 batch: 163/224\n",
      "Batch loss: 0.043861061334609985 batch: 164/224\n",
      "Batch loss: 0.07326507568359375 batch: 165/224\n",
      "Batch loss: 0.07337647676467896 batch: 166/224\n",
      "Batch loss: 0.05224050208926201 batch: 167/224\n",
      "Batch loss: 0.04640522599220276 batch: 168/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.06382985413074493 batch: 169/224\n",
      "Batch loss: 0.048202574253082275 batch: 170/224\n",
      "Batch loss: 0.056329503655433655 batch: 171/224\n",
      "Batch loss: 0.03253771364688873 batch: 172/224\n",
      "Batch loss: 0.09420071542263031 batch: 173/224\n",
      "Batch loss: 0.053468007594347 batch: 174/224\n",
      "Batch loss: 0.0478440560400486 batch: 175/224\n",
      "Batch loss: 0.052940838038921356 batch: 176/224\n",
      "Batch loss: 0.07651205360889435 batch: 177/224\n",
      "Batch loss: 0.06042355298995972 batch: 178/224\n",
      "Batch loss: 0.08088542520999908 batch: 179/224\n",
      "Batch loss: 0.04934040829539299 batch: 180/224\n",
      "Batch loss: 0.04113654047250748 batch: 181/224\n",
      "Batch loss: 0.07860694825649261 batch: 182/224\n",
      "Batch loss: 0.08029837161302567 batch: 183/224\n",
      "Batch loss: 0.06359944492578506 batch: 184/224\n",
      "Batch loss: 0.06425211578607559 batch: 185/224\n",
      "Batch loss: 0.04978928714990616 batch: 186/224\n",
      "Batch loss: 0.07347433269023895 batch: 187/224\n",
      "Batch loss: 0.054706424474716187 batch: 188/224\n",
      "Batch loss: 0.05470253527164459 batch: 189/224\n",
      "Batch loss: 0.06150761991739273 batch: 190/224\n",
      "Batch loss: 0.03756476566195488 batch: 191/224\n",
      "Batch loss: 0.05949172377586365 batch: 192/224\n",
      "Batch loss: 0.06253162026405334 batch: 193/224\n",
      "Batch loss: 0.06883536279201508 batch: 194/224\n",
      "Batch loss: 0.0793672576546669 batch: 195/224\n",
      "Batch loss: 0.05098849907517433 batch: 196/224\n",
      "Batch loss: 0.06233249977231026 batch: 197/224\n",
      "Batch loss: 0.055265914648771286 batch: 198/224\n",
      "Batch loss: 0.04222376272082329 batch: 199/224\n",
      "Batch loss: 0.04185618460178375 batch: 200/224\n",
      "Batch loss: 0.057894304394721985 batch: 201/224\n",
      "Batch loss: 0.07851486653089523 batch: 202/224\n",
      "Batch loss: 0.058022767305374146 batch: 203/224\n",
      "Batch loss: 0.07337384670972824 batch: 204/224\n",
      "Batch loss: 0.08271293342113495 batch: 205/224\n",
      "Batch loss: 0.049473267048597336 batch: 206/224\n",
      "Batch loss: 0.0546797513961792 batch: 207/224\n",
      "Batch loss: 0.05254248157143593 batch: 208/224\n",
      "Batch loss: 0.07469525188207626 batch: 209/224\n",
      "Batch loss: 0.07170862704515457 batch: 210/224\n",
      "Batch loss: 0.05858594551682472 batch: 211/224\n",
      "Batch loss: 0.04415680468082428 batch: 212/224\n",
      "Batch loss: 0.08567864447832108 batch: 213/224\n",
      "Batch loss: 0.05673231929540634 batch: 214/224\n",
      "Batch loss: 0.07469518482685089 batch: 215/224\n",
      "Batch loss: 0.07653144747018814 batch: 216/224\n",
      "Batch loss: 0.06484156847000122 batch: 217/224\n",
      "Batch loss: 0.03689120337367058 batch: 218/224\n",
      "Batch loss: 0.03627639263868332 batch: 219/224\n",
      "Batch loss: 0.04109124466776848 batch: 220/224\n",
      "Batch loss: 0.07494951039552689 batch: 221/224\n",
      "Batch loss: 0.08796142786741257 batch: 222/224\n",
      "Batch loss: 0.03271029517054558 batch: 223/224\n",
      "Batch loss: 0.05212269723415375 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 65/75..  Training Loss: 0.00012..  Test Loss: 0.00107..  Test Accuracy: 0.89021\n",
      "Running epoch 66/75\n",
      "Batch loss: 0.06637070327997208 batch: 1/224\n",
      "Batch loss: 0.07261700928211212 batch: 2/224\n",
      "Batch loss: 0.056892361491918564 batch: 3/224\n",
      "Batch loss: 0.0766269639134407 batch: 4/224\n",
      "Batch loss: 0.05576970800757408 batch: 5/224\n",
      "Batch loss: 0.1064290925860405 batch: 6/224\n",
      "Batch loss: 0.04226486757397652 batch: 7/224\n",
      "Batch loss: 0.05395204573869705 batch: 8/224\n",
      "Batch loss: 0.06504955142736435 batch: 9/224\n",
      "Batch loss: 0.04934149980545044 batch: 10/224\n",
      "Batch loss: 0.06095562130212784 batch: 11/224\n",
      "Batch loss: 0.05418414622545242 batch: 12/224\n",
      "Batch loss: 0.07552587985992432 batch: 13/224\n",
      "Batch loss: 0.0268847718834877 batch: 14/224\n",
      "Batch loss: 0.060388337820768356 batch: 15/224\n",
      "Batch loss: 0.07779626548290253 batch: 16/224\n",
      "Batch loss: 0.05523013696074486 batch: 17/224\n",
      "Batch loss: 0.05399566888809204 batch: 18/224\n",
      "Batch loss: 0.06903909891843796 batch: 19/224\n",
      "Batch loss: 0.06888153403997421 batch: 20/224\n",
      "Batch loss: 0.07093673944473267 batch: 21/224\n",
      "Batch loss: 0.047444865107536316 batch: 22/224\n",
      "Batch loss: 0.06065526604652405 batch: 23/224\n",
      "Batch loss: 0.0826607197523117 batch: 24/224\n",
      "Batch loss: 0.044916536659002304 batch: 25/224\n",
      "Batch loss: 0.06401441246271133 batch: 26/224\n",
      "Batch loss: 0.07088792324066162 batch: 27/224\n",
      "Batch loss: 0.07204026728868484 batch: 28/224\n",
      "Batch loss: 0.060816142708063126 batch: 29/224\n",
      "Batch loss: 0.04024689272046089 batch: 30/224\n",
      "Batch loss: 0.04783257842063904 batch: 31/224\n",
      "Batch loss: 0.03471355140209198 batch: 32/224\n",
      "Batch loss: 0.05500955879688263 batch: 33/224\n",
      "Batch loss: 0.06325177848339081 batch: 34/224\n",
      "Batch loss: 0.07421480864286423 batch: 35/224\n",
      "Batch loss: 0.07078256458044052 batch: 36/224\n",
      "Batch loss: 0.06555596739053726 batch: 37/224\n",
      "Batch loss: 0.05507062375545502 batch: 38/224\n",
      "Batch loss: 0.06042046099901199 batch: 39/224\n",
      "Batch loss: 0.04467710852622986 batch: 40/224\n",
      "Batch loss: 0.07055922597646713 batch: 41/224\n",
      "Batch loss: 0.06416787952184677 batch: 42/224\n",
      "Batch loss: 0.06173217296600342 batch: 43/224\n",
      "Batch loss: 0.05754644796252251 batch: 44/224\n",
      "Batch loss: 0.06066790595650673 batch: 45/224\n",
      "Batch loss: 0.09361830353736877 batch: 46/224\n",
      "Batch loss: 0.05130956321954727 batch: 47/224\n",
      "Batch loss: 0.03919919580221176 batch: 48/224\n",
      "Batch loss: 0.06490355730056763 batch: 49/224\n",
      "Batch loss: 0.04638969525694847 batch: 50/224\n",
      "Batch loss: 0.038461074233055115 batch: 51/224\n",
      "Batch loss: 0.06453662365674973 batch: 52/224\n",
      "Batch loss: 0.07114779204130173 batch: 53/224\n",
      "Batch loss: 0.0521111823618412 batch: 54/224\n",
      "Batch loss: 0.03871266171336174 batch: 55/224\n",
      "Batch loss: 0.045120853930711746 batch: 56/224\n",
      "Batch loss: 0.05877673625946045 batch: 57/224\n",
      "Batch loss: 0.06608320027589798 batch: 58/224\n",
      "Batch loss: 0.056515056639909744 batch: 59/224\n",
      "Batch loss: 0.07362978160381317 batch: 60/224\n",
      "Batch loss: 0.09038487821817398 batch: 61/224\n",
      "Batch loss: 0.044599421322345734 batch: 62/224\n",
      "Batch loss: 0.06522558629512787 batch: 63/224\n",
      "Batch loss: 0.05900662764906883 batch: 64/224\n",
      "Batch loss: 0.04670950770378113 batch: 65/224\n",
      "Batch loss: 0.04550529643893242 batch: 66/224\n",
      "Batch loss: 0.04538966715335846 batch: 67/224\n",
      "Batch loss: 0.04300064221024513 batch: 68/224\n",
      "Batch loss: 0.06703032553195953 batch: 69/224\n",
      "Batch loss: 0.07047580927610397 batch: 70/224\n",
      "Batch loss: 0.057996515184640884 batch: 71/224\n",
      "Batch loss: 0.051099300384521484 batch: 72/224\n",
      "Batch loss: 0.06150099262595177 batch: 73/224\n",
      "Batch loss: 0.05540599301457405 batch: 74/224\n",
      "Batch loss: 0.05334784835577011 batch: 75/224\n",
      "Batch loss: 0.05604945868253708 batch: 76/224\n",
      "Batch loss: 0.05297353118658066 batch: 77/224\n",
      "Batch loss: 0.04979662224650383 batch: 78/224\n",
      "Batch loss: 0.06090082973241806 batch: 79/224\n",
      "Batch loss: 0.04572174325585365 batch: 80/224\n",
      "Batch loss: 0.09095506370067596 batch: 81/224\n",
      "Batch loss: 0.0789417028427124 batch: 82/224\n",
      "Batch loss: 0.044417478144168854 batch: 83/224\n",
      "Batch loss: 0.04903189465403557 batch: 84/224\n",
      "Batch loss: 0.05030377581715584 batch: 85/224\n",
      "Batch loss: 0.055176474153995514 batch: 86/224\n",
      "Batch loss: 0.05682377517223358 batch: 87/224\n",
      "Batch loss: 0.08651263266801834 batch: 88/224\n",
      "Batch loss: 0.06984683126211166 batch: 89/224\n",
      "Batch loss: 0.04465581849217415 batch: 90/224\n",
      "Batch loss: 0.049607910215854645 batch: 91/224\n",
      "Batch loss: 0.08523524552583694 batch: 92/224\n",
      "Batch loss: 0.04099608212709427 batch: 93/224\n",
      "Batch loss: 0.0610826313495636 batch: 94/224\n",
      "Batch loss: 0.07862559705972672 batch: 95/224\n",
      "Batch loss: 0.06176379695534706 batch: 96/224\n",
      "Batch loss: 0.05146440118551254 batch: 97/224\n",
      "Batch loss: 0.05301583930850029 batch: 98/224\n",
      "Batch loss: 0.05332133173942566 batch: 99/224\n",
      "Batch loss: 0.04919622465968132 batch: 100/224\n",
      "Batch loss: 0.06757717579603195 batch: 101/224\n",
      "Batch loss: 0.06088848039507866 batch: 102/224\n",
      "Batch loss: 0.06211443617939949 batch: 103/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.059421200305223465 batch: 104/224\n",
      "Batch loss: 0.07810191810131073 batch: 105/224\n",
      "Batch loss: 0.08926951140165329 batch: 106/224\n",
      "Batch loss: 0.043454430997371674 batch: 107/224\n",
      "Batch loss: 0.05653969943523407 batch: 108/224\n",
      "Batch loss: 0.038937900215387344 batch: 109/224\n",
      "Batch loss: 0.035524386912584305 batch: 110/224\n",
      "Batch loss: 0.07067380845546722 batch: 111/224\n",
      "Batch loss: 0.0368838906288147 batch: 112/224\n",
      "Batch loss: 0.05190236493945122 batch: 113/224\n",
      "Batch loss: 0.03599262237548828 batch: 114/224\n",
      "Batch loss: 0.057642046362161636 batch: 115/224\n",
      "Batch loss: 0.05118235573172569 batch: 116/224\n",
      "Batch loss: 0.06639816612005234 batch: 117/224\n",
      "Batch loss: 0.08425222337245941 batch: 118/224\n",
      "Batch loss: 0.03642814978957176 batch: 119/224\n",
      "Batch loss: 0.055621709674596786 batch: 120/224\n",
      "Batch loss: 0.04567857086658478 batch: 121/224\n",
      "Batch loss: 0.050172269344329834 batch: 122/224\n",
      "Batch loss: 0.06453690677881241 batch: 123/224\n",
      "Batch loss: 0.04988418146967888 batch: 124/224\n",
      "Batch loss: 0.052887726575136185 batch: 125/224\n",
      "Batch loss: 0.055946704000234604 batch: 126/224\n",
      "Batch loss: 0.09108920395374298 batch: 127/224\n",
      "Batch loss: 0.04634919390082359 batch: 128/224\n",
      "Batch loss: 0.048777882009744644 batch: 129/224\n",
      "Batch loss: 0.06179670989513397 batch: 130/224\n",
      "Batch loss: 0.05068608373403549 batch: 131/224\n",
      "Batch loss: 0.0795501098036766 batch: 132/224\n",
      "Batch loss: 0.0832730159163475 batch: 133/224\n",
      "Batch loss: 0.07332376390695572 batch: 134/224\n",
      "Batch loss: 0.05333966761827469 batch: 135/224\n",
      "Batch loss: 0.0680733323097229 batch: 136/224\n",
      "Batch loss: 0.047959741204977036 batch: 137/224\n",
      "Batch loss: 0.05569770187139511 batch: 138/224\n",
      "Batch loss: 0.07955391705036163 batch: 139/224\n",
      "Batch loss: 0.09246712923049927 batch: 140/224\n",
      "Batch loss: 0.041809409856796265 batch: 141/224\n",
      "Batch loss: 0.05926245078444481 batch: 142/224\n",
      "Batch loss: 0.045490045100450516 batch: 143/224\n",
      "Batch loss: 0.05479136481881142 batch: 144/224\n",
      "Batch loss: 0.06668534129858017 batch: 145/224\n",
      "Batch loss: 0.0754372775554657 batch: 146/224\n",
      "Batch loss: 0.052790336310863495 batch: 147/224\n",
      "Batch loss: 0.03006073459982872 batch: 148/224\n",
      "Batch loss: 0.07511147856712341 batch: 149/224\n",
      "Batch loss: 0.06006088852882385 batch: 150/224\n",
      "Batch loss: 0.05081092566251755 batch: 151/224\n",
      "Batch loss: 0.056781888008117676 batch: 152/224\n",
      "Batch loss: 0.08700255304574966 batch: 153/224\n",
      "Batch loss: 0.0734618753194809 batch: 154/224\n",
      "Batch loss: 0.09567955881357193 batch: 155/224\n",
      "Batch loss: 0.05115732550621033 batch: 156/224\n",
      "Batch loss: 0.07565099745988846 batch: 157/224\n",
      "Batch loss: 0.08732368052005768 batch: 158/224\n",
      "Batch loss: 0.04559333249926567 batch: 159/224\n",
      "Batch loss: 0.06302209943532944 batch: 160/224\n",
      "Batch loss: 0.04329180344939232 batch: 161/224\n",
      "Batch loss: 0.03910146653652191 batch: 162/224\n",
      "Batch loss: 0.039127908647060394 batch: 163/224\n",
      "Batch loss: 0.05895351618528366 batch: 164/224\n",
      "Batch loss: 0.0836581215262413 batch: 165/224\n",
      "Batch loss: 0.07697723060846329 batch: 166/224\n",
      "Batch loss: 0.08680706471204758 batch: 167/224\n",
      "Batch loss: 0.05378758907318115 batch: 168/224\n",
      "Batch loss: 0.06204484403133392 batch: 169/224\n",
      "Batch loss: 0.027422212064266205 batch: 170/224\n",
      "Batch loss: 0.07703417539596558 batch: 171/224\n",
      "Batch loss: 0.03659724444150925 batch: 172/224\n",
      "Batch loss: 0.06444838643074036 batch: 173/224\n",
      "Batch loss: 0.03596300631761551 batch: 174/224\n",
      "Batch loss: 0.06985300779342651 batch: 175/224\n",
      "Batch loss: 0.05722653493285179 batch: 176/224\n",
      "Batch loss: 0.050997886806726456 batch: 177/224\n",
      "Batch loss: 0.04267703741788864 batch: 178/224\n",
      "Batch loss: 0.056945279240608215 batch: 179/224\n",
      "Batch loss: 0.05756044387817383 batch: 180/224\n",
      "Batch loss: 0.0742611289024353 batch: 181/224\n",
      "Batch loss: 0.05732567608356476 batch: 182/224\n",
      "Batch loss: 0.06575947254896164 batch: 183/224\n",
      "Batch loss: 0.0720846951007843 batch: 184/224\n",
      "Batch loss: 0.06530995666980743 batch: 185/224\n",
      "Batch loss: 0.06116781383752823 batch: 186/224\n",
      "Batch loss: 0.06021446734666824 batch: 187/224\n",
      "Batch loss: 0.05052874609827995 batch: 188/224\n",
      "Batch loss: 0.06802012026309967 batch: 189/224\n",
      "Batch loss: 0.05151683837175369 batch: 190/224\n",
      "Batch loss: 0.05770740658044815 batch: 191/224\n",
      "Batch loss: 0.07982418686151505 batch: 192/224\n",
      "Batch loss: 0.06330494582653046 batch: 193/224\n",
      "Batch loss: 0.0322539359331131 batch: 194/224\n",
      "Batch loss: 0.0513419471681118 batch: 195/224\n",
      "Batch loss: 0.0629456639289856 batch: 196/224\n",
      "Batch loss: 0.04955805465579033 batch: 197/224\n",
      "Batch loss: 0.06982976198196411 batch: 198/224\n",
      "Batch loss: 0.026348525658249855 batch: 199/224\n",
      "Batch loss: 0.04182722046971321 batch: 200/224\n",
      "Batch loss: 0.04552934691309929 batch: 201/224\n",
      "Batch loss: 0.06610491126775742 batch: 202/224\n",
      "Batch loss: 0.04993382841348648 batch: 203/224\n",
      "Batch loss: 0.07427702099084854 batch: 204/224\n",
      "Batch loss: 0.07852314412593842 batch: 205/224\n",
      "Batch loss: 0.055949654430150986 batch: 206/224\n",
      "Batch loss: 0.055781032890081406 batch: 207/224\n",
      "Batch loss: 0.037820808589458466 batch: 208/224\n",
      "Batch loss: 0.0723862424492836 batch: 209/224\n",
      "Batch loss: 0.05420053005218506 batch: 210/224\n",
      "Batch loss: 0.05647575482726097 batch: 211/224\n",
      "Batch loss: 0.04052518680691719 batch: 212/224\n",
      "Batch loss: 0.0766197144985199 batch: 213/224\n",
      "Batch loss: 0.04130003973841667 batch: 214/224\n",
      "Batch loss: 0.06980922818183899 batch: 215/224\n",
      "Batch loss: 0.08245740085840225 batch: 216/224\n",
      "Batch loss: 0.06302361190319061 batch: 217/224\n",
      "Batch loss: 0.06384606659412384 batch: 218/224\n",
      "Batch loss: 0.033505551517009735 batch: 219/224\n",
      "Batch loss: 0.058862682431936264 batch: 220/224\n",
      "Batch loss: 0.06534504890441895 batch: 221/224\n",
      "Batch loss: 0.05681603029370308 batch: 222/224\n",
      "Batch loss: 0.0749317854642868 batch: 223/224\n",
      "Batch loss: 0.03939565271139145 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 66/75..  Training Loss: 0.00012..  Test Loss: 0.00110..  Test Accuracy: 0.89175\n",
      "Running epoch 67/75\n",
      "Batch loss: 0.041303083300590515 batch: 1/224\n",
      "Batch loss: 0.07631782442331314 batch: 2/224\n",
      "Batch loss: 0.07575614750385284 batch: 3/224\n",
      "Batch loss: 0.0475747287273407 batch: 4/224\n",
      "Batch loss: 0.10413335263729095 batch: 5/224\n",
      "Batch loss: 0.07554078102111816 batch: 6/224\n",
      "Batch loss: 0.05154574289917946 batch: 7/224\n",
      "Batch loss: 0.06380029767751694 batch: 8/224\n",
      "Batch loss: 0.05243156477808952 batch: 9/224\n",
      "Batch loss: 0.04238927736878395 batch: 10/224\n",
      "Batch loss: 0.09068205207586288 batch: 11/224\n",
      "Batch loss: 0.06138700991868973 batch: 12/224\n",
      "Batch loss: 0.040214430540800095 batch: 13/224\n",
      "Batch loss: 0.07111793756484985 batch: 14/224\n",
      "Batch loss: 0.040296513587236404 batch: 15/224\n",
      "Batch loss: 0.08981849998235703 batch: 16/224\n",
      "Batch loss: 0.049873918294906616 batch: 17/224\n",
      "Batch loss: 0.07837078720331192 batch: 18/224\n",
      "Batch loss: 0.058137036859989166 batch: 19/224\n",
      "Batch loss: 0.04638155922293663 batch: 20/224\n",
      "Batch loss: 0.06194722652435303 batch: 21/224\n",
      "Batch loss: 0.055720504373311996 batch: 22/224\n",
      "Batch loss: 0.07298506051301956 batch: 23/224\n",
      "Batch loss: 0.06528899818658829 batch: 24/224\n",
      "Batch loss: 0.0704488754272461 batch: 25/224\n",
      "Batch loss: 0.04359843581914902 batch: 26/224\n",
      "Batch loss: 0.04513588920235634 batch: 27/224\n",
      "Batch loss: 0.07478020340204239 batch: 28/224\n",
      "Batch loss: 0.07994453608989716 batch: 29/224\n",
      "Batch loss: 0.04334090277552605 batch: 30/224\n",
      "Batch loss: 0.06667091697454453 batch: 31/224\n",
      "Batch loss: 0.055430930107831955 batch: 32/224\n",
      "Batch loss: 0.07925395667552948 batch: 33/224\n",
      "Batch loss: 0.08480051904916763 batch: 34/224\n",
      "Batch loss: 0.09085971117019653 batch: 35/224\n",
      "Batch loss: 0.058130715042352676 batch: 36/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.04343554750084877 batch: 37/224\n",
      "Batch loss: 0.044851262122392654 batch: 38/224\n",
      "Batch loss: 0.07223451137542725 batch: 39/224\n",
      "Batch loss: 0.04415994882583618 batch: 40/224\n",
      "Batch loss: 0.045612674206495285 batch: 41/224\n",
      "Batch loss: 0.04898759350180626 batch: 42/224\n",
      "Batch loss: 0.08559110015630722 batch: 43/224\n",
      "Batch loss: 0.05883447825908661 batch: 44/224\n",
      "Batch loss: 0.038102585822343826 batch: 45/224\n",
      "Batch loss: 0.06032118201255798 batch: 46/224\n",
      "Batch loss: 0.05144045874476433 batch: 47/224\n",
      "Batch loss: 0.0620364211499691 batch: 48/224\n",
      "Batch loss: 0.05123116821050644 batch: 49/224\n",
      "Batch loss: 0.03138798102736473 batch: 50/224\n",
      "Batch loss: 0.062459804117679596 batch: 51/224\n",
      "Batch loss: 0.04075448587536812 batch: 52/224\n",
      "Batch loss: 0.06416315585374832 batch: 53/224\n",
      "Batch loss: 0.0420096218585968 batch: 54/224\n",
      "Batch loss: 0.0522645078599453 batch: 55/224\n",
      "Batch loss: 0.05994386598467827 batch: 56/224\n",
      "Batch loss: 0.061600249260663986 batch: 57/224\n",
      "Batch loss: 0.08778222650289536 batch: 58/224\n",
      "Batch loss: 0.06129808723926544 batch: 59/224\n",
      "Batch loss: 0.08590339124202728 batch: 60/224\n",
      "Batch loss: 0.04488203302025795 batch: 61/224\n",
      "Batch loss: 0.035778384655714035 batch: 62/224\n",
      "Batch loss: 0.05012362822890282 batch: 63/224\n",
      "Batch loss: 0.056668322533369064 batch: 64/224\n",
      "Batch loss: 0.05108676105737686 batch: 65/224\n",
      "Batch loss: 0.053533487021923065 batch: 66/224\n",
      "Batch loss: 0.07207723706960678 batch: 67/224\n",
      "Batch loss: 0.053492333739995956 batch: 68/224\n",
      "Batch loss: 0.05694718286395073 batch: 69/224\n",
      "Batch loss: 0.062313199043273926 batch: 70/224\n",
      "Batch loss: 0.05958022549748421 batch: 71/224\n",
      "Batch loss: 0.030172696337103844 batch: 72/224\n",
      "Batch loss: 0.0792667418718338 batch: 73/224\n",
      "Batch loss: 0.07267014682292938 batch: 74/224\n",
      "Batch loss: 0.04463068023324013 batch: 75/224\n",
      "Batch loss: 0.057352423667907715 batch: 76/224\n",
      "Batch loss: 0.061318427324295044 batch: 77/224\n",
      "Batch loss: 0.06311715394258499 batch: 78/224\n",
      "Batch loss: 0.05932300165295601 batch: 79/224\n",
      "Batch loss: 0.03160572797060013 batch: 80/224\n",
      "Batch loss: 0.07343654334545135 batch: 81/224\n",
      "Batch loss: 0.07640371471643448 batch: 82/224\n",
      "Batch loss: 0.0874321460723877 batch: 83/224\n",
      "Batch loss: 0.03162050619721413 batch: 84/224\n",
      "Batch loss: 0.05301748588681221 batch: 85/224\n",
      "Batch loss: 0.0866813212633133 batch: 86/224\n",
      "Batch loss: 0.07979018241167068 batch: 87/224\n",
      "Batch loss: 0.07662694156169891 batch: 88/224\n",
      "Batch loss: 0.08219745755195618 batch: 89/224\n",
      "Batch loss: 0.06275012344121933 batch: 90/224\n",
      "Batch loss: 0.06345954537391663 batch: 91/224\n",
      "Batch loss: 0.06916192919015884 batch: 92/224\n",
      "Batch loss: 0.04802557826042175 batch: 93/224\n",
      "Batch loss: 0.05268565192818642 batch: 94/224\n",
      "Batch loss: 0.055622316896915436 batch: 95/224\n",
      "Batch loss: 0.05341891571879387 batch: 96/224\n",
      "Batch loss: 0.05181737244129181 batch: 97/224\n",
      "Batch loss: 0.04211430996656418 batch: 98/224\n",
      "Batch loss: 0.07243291288614273 batch: 99/224\n",
      "Batch loss: 0.04892195016145706 batch: 100/224\n",
      "Batch loss: 0.06712841987609863 batch: 101/224\n",
      "Batch loss: 0.05687873438000679 batch: 102/224\n",
      "Batch loss: 0.06259261071681976 batch: 103/224\n",
      "Batch loss: 0.06308141350746155 batch: 104/224\n",
      "Batch loss: 0.06041479483246803 batch: 105/224\n",
      "Batch loss: 0.04379839077591896 batch: 106/224\n",
      "Batch loss: 0.07908506691455841 batch: 107/224\n",
      "Batch loss: 0.04964173212647438 batch: 108/224\n",
      "Batch loss: 0.04046403616666794 batch: 109/224\n",
      "Batch loss: 0.03034195676445961 batch: 110/224\n",
      "Batch loss: 0.08905836939811707 batch: 111/224\n",
      "Batch loss: 0.07269050180912018 batch: 112/224\n",
      "Batch loss: 0.048293109983205795 batch: 113/224\n",
      "Batch loss: 0.05397452786564827 batch: 114/224\n",
      "Batch loss: 0.06754550337791443 batch: 115/224\n",
      "Batch loss: 0.07050031423568726 batch: 116/224\n",
      "Batch loss: 0.06177875027060509 batch: 117/224\n",
      "Batch loss: 0.04170665517449379 batch: 118/224\n",
      "Batch loss: 0.042349666357040405 batch: 119/224\n",
      "Batch loss: 0.053080324083566666 batch: 120/224\n",
      "Batch loss: 0.046784527599811554 batch: 121/224\n",
      "Batch loss: 0.05191155895590782 batch: 122/224\n",
      "Batch loss: 0.051645055413246155 batch: 123/224\n",
      "Batch loss: 0.05779643356800079 batch: 124/224\n",
      "Batch loss: 0.07260966300964355 batch: 125/224\n",
      "Batch loss: 0.08041712641716003 batch: 126/224\n",
      "Batch loss: 0.07052924484014511 batch: 127/224\n",
      "Batch loss: 0.04036380350589752 batch: 128/224\n",
      "Batch loss: 0.04097197949886322 batch: 129/224\n",
      "Batch loss: 0.061409998685121536 batch: 130/224\n",
      "Batch loss: 0.04416470229625702 batch: 131/224\n",
      "Batch loss: 0.04993767663836479 batch: 132/224\n",
      "Batch loss: 0.0717657133936882 batch: 133/224\n",
      "Batch loss: 0.0716959536075592 batch: 134/224\n",
      "Batch loss: 0.07825925946235657 batch: 135/224\n",
      "Batch loss: 0.0596822053194046 batch: 136/224\n",
      "Batch loss: 0.039271675050258636 batch: 137/224\n",
      "Batch loss: 0.061950430274009705 batch: 138/224\n",
      "Batch loss: 0.05550674349069595 batch: 139/224\n",
      "Batch loss: 0.07398662716150284 batch: 140/224\n",
      "Batch loss: 0.061650484800338745 batch: 141/224\n",
      "Batch loss: 0.03664054349064827 batch: 142/224\n",
      "Batch loss: 0.06637169420719147 batch: 143/224\n",
      "Batch loss: 0.04766279458999634 batch: 144/224\n",
      "Batch loss: 0.05306195467710495 batch: 145/224\n",
      "Batch loss: 0.0752604678273201 batch: 146/224\n",
      "Batch loss: 0.08371780067682266 batch: 147/224\n",
      "Batch loss: 0.04786469787359238 batch: 148/224\n",
      "Batch loss: 0.08694323152303696 batch: 149/224\n",
      "Batch loss: 0.06013369932770729 batch: 150/224\n",
      "Batch loss: 0.03829473257064819 batch: 151/224\n",
      "Batch loss: 0.058706134557724 batch: 152/224\n",
      "Batch loss: 0.05055543780326843 batch: 153/224\n",
      "Batch loss: 0.04111752286553383 batch: 154/224\n",
      "Batch loss: 0.04033434018492699 batch: 155/224\n",
      "Batch loss: 0.06359676271677017 batch: 156/224\n",
      "Batch loss: 0.06370086222887039 batch: 157/224\n",
      "Batch loss: 0.10169851779937744 batch: 158/224\n",
      "Batch loss: 0.05519570782780647 batch: 159/224\n",
      "Batch loss: 0.0695064440369606 batch: 160/224\n",
      "Batch loss: 0.051035284996032715 batch: 161/224\n",
      "Batch loss: 0.05296802520751953 batch: 162/224\n",
      "Batch loss: 0.02786463126540184 batch: 163/224\n",
      "Batch loss: 0.03547656163573265 batch: 164/224\n",
      "Batch loss: 0.06810075789690018 batch: 165/224\n",
      "Batch loss: 0.07862523198127747 batch: 166/224\n",
      "Batch loss: 0.05768826976418495 batch: 167/224\n",
      "Batch loss: 0.059367481619119644 batch: 168/224\n",
      "Batch loss: 0.08255982398986816 batch: 169/224\n",
      "Batch loss: 0.04964081197977066 batch: 170/224\n",
      "Batch loss: 0.03587430715560913 batch: 171/224\n",
      "Batch loss: 0.059556469321250916 batch: 172/224\n",
      "Batch loss: 0.061504341661930084 batch: 173/224\n",
      "Batch loss: 0.04196961969137192 batch: 174/224\n",
      "Batch loss: 0.05187853425741196 batch: 175/224\n",
      "Batch loss: 0.0412864126265049 batch: 176/224\n",
      "Batch loss: 0.051769357174634933 batch: 177/224\n",
      "Batch loss: 0.05117347091436386 batch: 178/224\n",
      "Batch loss: 0.07648314535617828 batch: 179/224\n",
      "Batch loss: 0.029713623225688934 batch: 180/224\n",
      "Batch loss: 0.03799506649374962 batch: 181/224\n",
      "Batch loss: 0.08586397022008896 batch: 182/224\n",
      "Batch loss: 0.08445028215646744 batch: 183/224\n",
      "Batch loss: 0.06068817153573036 batch: 184/224\n",
      "Batch loss: 0.08406173437833786 batch: 185/224\n",
      "Batch loss: 0.04785500466823578 batch: 186/224\n",
      "Batch loss: 0.061965666711330414 batch: 187/224\n",
      "Batch loss: 0.03925288841128349 batch: 188/224\n",
      "Batch loss: 0.06372763961553574 batch: 189/224\n",
      "Batch loss: 0.0725204274058342 batch: 190/224\n",
      "Batch loss: 0.053588513284921646 batch: 191/224\n",
      "Batch loss: 0.061831336468458176 batch: 192/224\n",
      "Batch loss: 0.05523058772087097 batch: 193/224\n",
      "Batch loss: 0.06809820979833603 batch: 194/224\n",
      "Batch loss: 0.06360889226198196 batch: 195/224\n",
      "Batch loss: 0.09394386410713196 batch: 196/224\n",
      "Batch loss: 0.05190981924533844 batch: 197/224\n",
      "Batch loss: 0.045698169618844986 batch: 198/224\n",
      "Batch loss: 0.048432767391204834 batch: 199/224\n",
      "Batch loss: 0.05121752992272377 batch: 200/224\n",
      "Batch loss: 0.07149327546358109 batch: 201/224\n",
      "Batch loss: 0.07273591309785843 batch: 202/224\n",
      "Batch loss: 0.0444176122546196 batch: 203/224\n",
      "Batch loss: 0.03935105726122856 batch: 204/224\n",
      "Batch loss: 0.05597614124417305 batch: 205/224\n",
      "Batch loss: 0.06191180273890495 batch: 206/224\n",
      "Batch loss: 0.07299298793077469 batch: 207/224\n",
      "Batch loss: 0.05094632878899574 batch: 208/224\n",
      "Batch loss: 0.05063594877719879 batch: 209/224\n",
      "Batch loss: 0.053792402148246765 batch: 210/224\n",
      "Batch loss: 0.048850320279598236 batch: 211/224\n",
      "Batch loss: 0.060001350939273834 batch: 212/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.07912132143974304 batch: 213/224\n",
      "Batch loss: 0.0680064931511879 batch: 214/224\n",
      "Batch loss: 0.06371363252401352 batch: 215/224\n",
      "Batch loss: 0.037942446768283844 batch: 216/224\n",
      "Batch loss: 0.07012799382209778 batch: 217/224\n",
      "Batch loss: 0.05225776135921478 batch: 218/224\n",
      "Batch loss: 0.05580543726682663 batch: 219/224\n",
      "Batch loss: 0.052711762487888336 batch: 220/224\n",
      "Batch loss: 0.06173430383205414 batch: 221/224\n",
      "Batch loss: 0.08391409367322922 batch: 222/224\n",
      "Batch loss: 0.0445612408220768 batch: 223/224\n",
      "Batch loss: 0.02879689633846283 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 67/75..  Training Loss: 0.00012..  Test Loss: 0.00107..  Test Accuracy: 0.89075\n",
      "Running epoch 68/75\n",
      "Batch loss: 0.0422065332531929 batch: 1/224\n",
      "Batch loss: 0.06855170428752899 batch: 2/224\n",
      "Batch loss: 0.05072849243879318 batch: 3/224\n",
      "Batch loss: 0.05770248547196388 batch: 4/224\n",
      "Batch loss: 0.06582024693489075 batch: 5/224\n",
      "Batch loss: 0.07812318950891495 batch: 6/224\n",
      "Batch loss: 0.04269387573003769 batch: 7/224\n",
      "Batch loss: 0.05987416207790375 batch: 8/224\n",
      "Batch loss: 0.048590198159217834 batch: 9/224\n",
      "Batch loss: 0.04447963088750839 batch: 10/224\n",
      "Batch loss: 0.07325597107410431 batch: 11/224\n",
      "Batch loss: 0.060149531811475754 batch: 12/224\n",
      "Batch loss: 0.037208814173936844 batch: 13/224\n",
      "Batch loss: 0.06191597878932953 batch: 14/224\n",
      "Batch loss: 0.05837489292025566 batch: 15/224\n",
      "Batch loss: 0.0693998634815216 batch: 16/224\n",
      "Batch loss: 0.039085227996110916 batch: 17/224\n",
      "Batch loss: 0.0659637302160263 batch: 18/224\n",
      "Batch loss: 0.07461518794298172 batch: 19/224\n",
      "Batch loss: 0.05426856875419617 batch: 20/224\n",
      "Batch loss: 0.04835061356425285 batch: 21/224\n",
      "Batch loss: 0.08184105902910233 batch: 22/224\n",
      "Batch loss: 0.07987937331199646 batch: 23/224\n",
      "Batch loss: 0.06385160982608795 batch: 24/224\n",
      "Batch loss: 0.05062782019376755 batch: 25/224\n",
      "Batch loss: 0.04238453879952431 batch: 26/224\n",
      "Batch loss: 0.06764490157365799 batch: 27/224\n",
      "Batch loss: 0.04841715097427368 batch: 28/224\n",
      "Batch loss: 0.050834931433200836 batch: 29/224\n",
      "Batch loss: 0.04293179139494896 batch: 30/224\n",
      "Batch loss: 0.05628080666065216 batch: 31/224\n",
      "Batch loss: 0.03708783537149429 batch: 32/224\n",
      "Batch loss: 0.07444025576114655 batch: 33/224\n",
      "Batch loss: 0.065482959151268 batch: 34/224\n",
      "Batch loss: 0.07056567072868347 batch: 35/224\n",
      "Batch loss: 0.07181788980960846 batch: 36/224\n",
      "Batch loss: 0.05355279520153999 batch: 37/224\n",
      "Batch loss: 0.0512034110724926 batch: 38/224\n",
      "Batch loss: 0.0668264701962471 batch: 39/224\n",
      "Batch loss: 0.037485282868146896 batch: 40/224\n",
      "Batch loss: 0.0688176304101944 batch: 41/224\n",
      "Batch loss: 0.0665331706404686 batch: 42/224\n",
      "Batch loss: 0.04752295836806297 batch: 43/224\n",
      "Batch loss: 0.03862560912966728 batch: 44/224\n",
      "Batch loss: 0.044715866446495056 batch: 45/224\n",
      "Batch loss: 0.07495565712451935 batch: 46/224\n",
      "Batch loss: 0.07059691101312637 batch: 47/224\n",
      "Batch loss: 0.05991602689027786 batch: 48/224\n",
      "Batch loss: 0.03839535638689995 batch: 49/224\n",
      "Batch loss: 0.031014274805784225 batch: 50/224\n",
      "Batch loss: 0.061289310455322266 batch: 51/224\n",
      "Batch loss: 0.04281514510512352 batch: 52/224\n",
      "Batch loss: 0.036793358623981476 batch: 53/224\n",
      "Batch loss: 0.04049332067370415 batch: 54/224\n",
      "Batch loss: 0.03780307620763779 batch: 55/224\n",
      "Batch loss: 0.03947349265217781 batch: 56/224\n",
      "Batch loss: 0.0507691465318203 batch: 57/224\n",
      "Batch loss: 0.04687259718775749 batch: 58/224\n",
      "Batch loss: 0.04564349725842476 batch: 59/224\n",
      "Batch loss: 0.07065168023109436 batch: 60/224\n",
      "Batch loss: 0.04812721908092499 batch: 61/224\n",
      "Batch loss: 0.02688070386648178 batch: 62/224\n",
      "Batch loss: 0.041656412184238434 batch: 63/224\n",
      "Batch loss: 0.05912311002612114 batch: 64/224\n",
      "Batch loss: 0.06630834937095642 batch: 65/224\n",
      "Batch loss: 0.06683197617530823 batch: 66/224\n",
      "Batch loss: 0.03411202132701874 batch: 67/224\n",
      "Batch loss: 0.04130879417061806 batch: 68/224\n",
      "Batch loss: 0.056503552943468094 batch: 69/224\n",
      "Batch loss: 0.04022584483027458 batch: 70/224\n",
      "Batch loss: 0.08067625761032104 batch: 71/224\n",
      "Batch loss: 0.029097076505422592 batch: 72/224\n",
      "Batch loss: 0.06657295674085617 batch: 73/224\n",
      "Batch loss: 0.07174883782863617 batch: 74/224\n",
      "Batch loss: 0.044782403856515884 batch: 75/224\n",
      "Batch loss: 0.06430847942829132 batch: 76/224\n",
      "Batch loss: 0.07364501059055328 batch: 77/224\n",
      "Batch loss: 0.05952126905322075 batch: 78/224\n",
      "Batch loss: 0.05513836070895195 batch: 79/224\n",
      "Batch loss: 0.03750966861844063 batch: 80/224\n",
      "Batch loss: 0.06800379604101181 batch: 81/224\n",
      "Batch loss: 0.08492321521043777 batch: 82/224\n",
      "Batch loss: 0.05853355675935745 batch: 83/224\n",
      "Batch loss: 0.03837938979268074 batch: 84/224\n",
      "Batch loss: 0.05410001054406166 batch: 85/224\n",
      "Batch loss: 0.06629588454961777 batch: 86/224\n",
      "Batch loss: 0.03608379885554314 batch: 87/224\n",
      "Batch loss: 0.06475696712732315 batch: 88/224\n",
      "Batch loss: 0.046408701688051224 batch: 89/224\n",
      "Batch loss: 0.06680835038423538 batch: 90/224\n",
      "Batch loss: 0.050437673926353455 batch: 91/224\n",
      "Batch loss: 0.056360095739364624 batch: 92/224\n",
      "Batch loss: 0.03142068535089493 batch: 93/224\n",
      "Batch loss: 0.054837666451931 batch: 94/224\n",
      "Batch loss: 0.031124692410230637 batch: 95/224\n",
      "Batch loss: 0.04019051045179367 batch: 96/224\n",
      "Batch loss: 0.03389272093772888 batch: 97/224\n",
      "Batch loss: 0.04411047324538231 batch: 98/224\n",
      "Batch loss: 0.047993727028369904 batch: 99/224\n",
      "Batch loss: 0.03357875347137451 batch: 100/224\n",
      "Batch loss: 0.0417025163769722 batch: 101/224\n",
      "Batch loss: 0.04944048449397087 batch: 102/224\n",
      "Batch loss: 0.07012347877025604 batch: 103/224\n",
      "Batch loss: 0.03444746881723404 batch: 104/224\n",
      "Batch loss: 0.07367908954620361 batch: 105/224\n",
      "Batch loss: 0.08164365589618683 batch: 106/224\n",
      "Batch loss: 0.07376070320606232 batch: 107/224\n",
      "Batch loss: 0.06177690997719765 batch: 108/224\n",
      "Batch loss: 0.030932210385799408 batch: 109/224\n",
      "Batch loss: 0.03871939703822136 batch: 110/224\n",
      "Batch loss: 0.05822525545954704 batch: 111/224\n",
      "Batch loss: 0.042621780186891556 batch: 112/224\n",
      "Batch loss: 0.051459599286317825 batch: 113/224\n",
      "Batch loss: 0.038159627467393875 batch: 114/224\n",
      "Batch loss: 0.03658308461308479 batch: 115/224\n",
      "Batch loss: 0.046595118939876556 batch: 116/224\n",
      "Batch loss: 0.028000542894005775 batch: 117/224\n",
      "Batch loss: 0.08254346996545792 batch: 118/224\n",
      "Batch loss: 0.04898795485496521 batch: 119/224\n",
      "Batch loss: 0.03908809646964073 batch: 120/224\n",
      "Batch loss: 0.05197124928236008 batch: 121/224\n",
      "Batch loss: 0.053684402257204056 batch: 122/224\n",
      "Batch loss: 0.021168427541851997 batch: 123/224\n",
      "Batch loss: 0.05147896707057953 batch: 124/224\n",
      "Batch loss: 0.0573631189763546 batch: 125/224\n",
      "Batch loss: 0.08148524165153503 batch: 126/224\n",
      "Batch loss: 0.08299015462398529 batch: 127/224\n",
      "Batch loss: 0.059087373316287994 batch: 128/224\n",
      "Batch loss: 0.059813424944877625 batch: 129/224\n",
      "Batch loss: 0.05519002676010132 batch: 130/224\n",
      "Batch loss: 0.05711681395769119 batch: 131/224\n",
      "Batch loss: 0.05651179701089859 batch: 132/224\n",
      "Batch loss: 0.04703545942902565 batch: 133/224\n",
      "Batch loss: 0.05714225396513939 batch: 134/224\n",
      "Batch loss: 0.05862680822610855 batch: 135/224\n",
      "Batch loss: 0.055933352559804916 batch: 136/224\n",
      "Batch loss: 0.051994092762470245 batch: 137/224\n",
      "Batch loss: 0.05145741254091263 batch: 138/224\n",
      "Batch loss: 0.07633083313703537 batch: 139/224\n",
      "Batch loss: 0.08823376148939133 batch: 140/224\n",
      "Batch loss: 0.04429452121257782 batch: 141/224\n",
      "Batch loss: 0.04234180971980095 batch: 142/224\n",
      "Batch loss: 0.0559852197766304 batch: 143/224\n",
      "Batch loss: 0.0459488146007061 batch: 144/224\n",
      "Batch loss: 0.03617650270462036 batch: 145/224\n",
      "Batch loss: 0.10477808862924576 batch: 146/224\n",
      "Batch loss: 0.04168001934885979 batch: 147/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.027051042765378952 batch: 148/224\n",
      "Batch loss: 0.08085420727729797 batch: 149/224\n",
      "Batch loss: 0.05889502540230751 batch: 150/224\n",
      "Batch loss: 0.028854675590991974 batch: 151/224\n",
      "Batch loss: 0.06351319700479507 batch: 152/224\n",
      "Batch loss: 0.05982942879199982 batch: 153/224\n",
      "Batch loss: 0.04371585696935654 batch: 154/224\n",
      "Batch loss: 0.036100924015045166 batch: 155/224\n",
      "Batch loss: 0.03370068222284317 batch: 156/224\n",
      "Batch loss: 0.06281645596027374 batch: 157/224\n",
      "Batch loss: 0.09565801918506622 batch: 158/224\n",
      "Batch loss: 0.05288161709904671 batch: 159/224\n",
      "Batch loss: 0.03721717372536659 batch: 160/224\n",
      "Batch loss: 0.054051388055086136 batch: 161/224\n",
      "Batch loss: 0.06710261106491089 batch: 162/224\n",
      "Batch loss: 0.025621648877859116 batch: 163/224\n",
      "Batch loss: 0.03447030112147331 batch: 164/224\n",
      "Batch loss: 0.08446686714887619 batch: 165/224\n",
      "Batch loss: 0.07314605265855789 batch: 166/224\n",
      "Batch loss: 0.047837819904088974 batch: 167/224\n",
      "Batch loss: 0.05259346589446068 batch: 168/224\n",
      "Batch loss: 0.05804217979311943 batch: 169/224\n",
      "Batch loss: 0.03367041423916817 batch: 170/224\n",
      "Batch loss: 0.06071136146783829 batch: 171/224\n",
      "Batch loss: 0.034786198288202286 batch: 172/224\n",
      "Batch loss: 0.03789839148521423 batch: 173/224\n",
      "Batch loss: 0.03281240537762642 batch: 174/224\n",
      "Batch loss: 0.057952675968408585 batch: 175/224\n",
      "Batch loss: 0.04385215789079666 batch: 176/224\n",
      "Batch loss: 0.07846907526254654 batch: 177/224\n",
      "Batch loss: 0.050432249903678894 batch: 178/224\n",
      "Batch loss: 0.06107552722096443 batch: 179/224\n",
      "Batch loss: 0.05232187360525131 batch: 180/224\n",
      "Batch loss: 0.06037362292408943 batch: 181/224\n",
      "Batch loss: 0.045532118529081345 batch: 182/224\n",
      "Batch loss: 0.09087711572647095 batch: 183/224\n",
      "Batch loss: 0.06645018607378006 batch: 184/224\n",
      "Batch loss: 0.04148543253540993 batch: 185/224\n",
      "Batch loss: 0.0363876037299633 batch: 186/224\n",
      "Batch loss: 0.054303623735904694 batch: 187/224\n",
      "Batch loss: 0.03459560126066208 batch: 188/224\n",
      "Batch loss: 0.04239707440137863 batch: 189/224\n",
      "Batch loss: 0.06763362884521484 batch: 190/224\n",
      "Batch loss: 0.03821168094873428 batch: 191/224\n",
      "Batch loss: 0.06222403421998024 batch: 192/224\n",
      "Batch loss: 0.05605613440275192 batch: 193/224\n",
      "Batch loss: 0.04756975546479225 batch: 194/224\n",
      "Batch loss: 0.04849746823310852 batch: 195/224\n",
      "Batch loss: 0.06301666796207428 batch: 196/224\n",
      "Batch loss: 0.053800854831933975 batch: 197/224\n",
      "Batch loss: 0.08529768139123917 batch: 198/224\n",
      "Batch loss: 0.03912988305091858 batch: 199/224\n",
      "Batch loss: 0.05111163482069969 batch: 200/224\n",
      "Batch loss: 0.05198198929429054 batch: 201/224\n",
      "Batch loss: 0.07909984141588211 batch: 202/224\n",
      "Batch loss: 0.045395273715257645 batch: 203/224\n",
      "Batch loss: 0.05689841881394386 batch: 204/224\n",
      "Batch loss: 0.05091315880417824 batch: 205/224\n",
      "Batch loss: 0.04794090986251831 batch: 206/224\n",
      "Batch loss: 0.04451815411448479 batch: 207/224\n",
      "Batch loss: 0.031103035435080528 batch: 208/224\n",
      "Batch loss: 0.05517977848649025 batch: 209/224\n",
      "Batch loss: 0.05354256555438042 batch: 210/224\n",
      "Batch loss: 0.04790351912379265 batch: 211/224\n",
      "Batch loss: 0.048322901129722595 batch: 212/224\n",
      "Batch loss: 0.08171281218528748 batch: 213/224\n",
      "Batch loss: 0.07664074003696442 batch: 214/224\n",
      "Batch loss: 0.06018852815032005 batch: 215/224\n",
      "Batch loss: 0.056128956377506256 batch: 216/224\n",
      "Batch loss: 0.04587167128920555 batch: 217/224\n",
      "Batch loss: 0.05340550094842911 batch: 218/224\n",
      "Batch loss: 0.021954115480184555 batch: 219/224\n",
      "Batch loss: 0.03989551588892937 batch: 220/224\n",
      "Batch loss: 0.0669972226023674 batch: 221/224\n",
      "Batch loss: 0.05031377449631691 batch: 222/224\n",
      "Batch loss: 0.04600679501891136 batch: 223/224\n",
      "Batch loss: 0.07425912469625473 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 68/75..  Training Loss: 0.00011..  Test Loss: 0.00111..  Test Accuracy: 0.89325\n",
      "Running epoch 69/75\n",
      "Batch loss: 0.030177157372236252 batch: 1/224\n",
      "Batch loss: 0.08613546192646027 batch: 2/224\n",
      "Batch loss: 0.07283823937177658 batch: 3/224\n",
      "Batch loss: 0.0787414014339447 batch: 4/224\n",
      "Batch loss: 0.06857112795114517 batch: 5/224\n",
      "Batch loss: 0.0704672783613205 batch: 6/224\n",
      "Batch loss: 0.04468930512666702 batch: 7/224\n",
      "Batch loss: 0.052528489381074905 batch: 8/224\n",
      "Batch loss: 0.035725437104701996 batch: 9/224\n",
      "Batch loss: 0.06494472175836563 batch: 10/224\n",
      "Batch loss: 0.0857386440038681 batch: 11/224\n",
      "Batch loss: 0.04857395589351654 batch: 12/224\n",
      "Batch loss: 0.06868647783994675 batch: 13/224\n",
      "Batch loss: 0.06092815473675728 batch: 14/224\n",
      "Batch loss: 0.047846753150224686 batch: 15/224\n",
      "Batch loss: 0.06480579078197479 batch: 16/224\n",
      "Batch loss: 0.05108225718140602 batch: 17/224\n",
      "Batch loss: 0.05731901898980141 batch: 18/224\n",
      "Batch loss: 0.02996952272951603 batch: 19/224\n",
      "Batch loss: 0.041004255414009094 batch: 20/224\n",
      "Batch loss: 0.05378449335694313 batch: 21/224\n",
      "Batch loss: 0.06372535973787308 batch: 22/224\n",
      "Batch loss: 0.053370729088783264 batch: 23/224\n",
      "Batch loss: 0.10067185759544373 batch: 24/224\n",
      "Batch loss: 0.04490286856889725 batch: 25/224\n",
      "Batch loss: 0.03430628404021263 batch: 26/224\n",
      "Batch loss: 0.041921596974134445 batch: 27/224\n",
      "Batch loss: 0.09410839527845383 batch: 28/224\n",
      "Batch loss: 0.05752528831362724 batch: 29/224\n",
      "Batch loss: 0.0622490718960762 batch: 30/224\n",
      "Batch loss: 0.05470350757241249 batch: 31/224\n",
      "Batch loss: 0.0421556755900383 batch: 32/224\n",
      "Batch loss: 0.04232893884181976 batch: 33/224\n",
      "Batch loss: 0.07470082491636276 batch: 34/224\n",
      "Batch loss: 0.0542752668261528 batch: 35/224\n",
      "Batch loss: 0.05829806253314018 batch: 36/224\n",
      "Batch loss: 0.0656297504901886 batch: 37/224\n",
      "Batch loss: 0.04544784873723984 batch: 38/224\n",
      "Batch loss: 0.06891702115535736 batch: 39/224\n",
      "Batch loss: 0.03999558836221695 batch: 40/224\n",
      "Batch loss: 0.06500396132469177 batch: 41/224\n",
      "Batch loss: 0.06174010410904884 batch: 42/224\n",
      "Batch loss: 0.05678078159689903 batch: 43/224\n",
      "Batch loss: 0.03875287249684334 batch: 44/224\n",
      "Batch loss: 0.03528093919157982 batch: 45/224\n",
      "Batch loss: 0.09296124428510666 batch: 46/224\n",
      "Batch loss: 0.05762156844139099 batch: 47/224\n",
      "Batch loss: 0.037707649171352386 batch: 48/224\n",
      "Batch loss: 0.03959078714251518 batch: 49/224\n",
      "Batch loss: 0.05150029435753822 batch: 50/224\n",
      "Batch loss: 0.05290811508893967 batch: 51/224\n",
      "Batch loss: 0.04464684799313545 batch: 52/224\n",
      "Batch loss: 0.06530159711837769 batch: 53/224\n",
      "Batch loss: 0.03196438401937485 batch: 54/224\n",
      "Batch loss: 0.05516723915934563 batch: 55/224\n",
      "Batch loss: 0.05633297935128212 batch: 56/224\n",
      "Batch loss: 0.0654156431555748 batch: 57/224\n",
      "Batch loss: 0.08621484041213989 batch: 58/224\n",
      "Batch loss: 0.06076771020889282 batch: 59/224\n",
      "Batch loss: 0.08741052448749542 batch: 60/224\n",
      "Batch loss: 0.05541561543941498 batch: 61/224\n",
      "Batch loss: 0.03432757407426834 batch: 62/224\n",
      "Batch loss: 0.06551877409219742 batch: 63/224\n",
      "Batch loss: 0.07878647744655609 batch: 64/224\n",
      "Batch loss: 0.07939330488443375 batch: 65/224\n",
      "Batch loss: 0.0668223574757576 batch: 66/224\n",
      "Batch loss: 0.05629533901810646 batch: 67/224\n",
      "Batch loss: 0.03488795831799507 batch: 68/224\n",
      "Batch loss: 0.05438623204827309 batch: 69/224\n",
      "Batch loss: 0.05206810683012009 batch: 70/224\n",
      "Batch loss: 0.029141943901777267 batch: 71/224\n",
      "Batch loss: 0.03852277621626854 batch: 72/224\n",
      "Batch loss: 0.06904897093772888 batch: 73/224\n",
      "Batch loss: 0.04699914902448654 batch: 74/224\n",
      "Batch loss: 0.07537414133548737 batch: 75/224\n",
      "Batch loss: 0.08853678405284882 batch: 76/224\n",
      "Batch loss: 0.05874118581414223 batch: 77/224\n",
      "Batch loss: 0.06250450015068054 batch: 78/224\n",
      "Batch loss: 0.049130216240882874 batch: 79/224\n",
      "Batch loss: 0.039136774837970734 batch: 80/224\n",
      "Batch loss: 0.05534866079688072 batch: 81/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.05757569149136543 batch: 82/224\n",
      "Batch loss: 0.0701514333486557 batch: 83/224\n",
      "Batch loss: 0.022868799045681953 batch: 84/224\n",
      "Batch loss: 0.04726110026240349 batch: 85/224\n",
      "Batch loss: 0.055818576365709305 batch: 86/224\n",
      "Batch loss: 0.0435342863202095 batch: 87/224\n",
      "Batch loss: 0.06312153488397598 batch: 88/224\n",
      "Batch loss: 0.04192032292485237 batch: 89/224\n",
      "Batch loss: 0.05375611409544945 batch: 90/224\n",
      "Batch loss: 0.06649729609489441 batch: 91/224\n",
      "Batch loss: 0.0721343457698822 batch: 92/224\n",
      "Batch loss: 0.04005759581923485 batch: 93/224\n",
      "Batch loss: 0.0640803650021553 batch: 94/224\n",
      "Batch loss: 0.035903140902519226 batch: 95/224\n",
      "Batch loss: 0.05475010350346565 batch: 96/224\n",
      "Batch loss: 0.038273029029369354 batch: 97/224\n",
      "Batch loss: 0.03359273076057434 batch: 98/224\n",
      "Batch loss: 0.06644444167613983 batch: 99/224\n",
      "Batch loss: 0.05116713419556618 batch: 100/224\n",
      "Batch loss: 0.0758160725235939 batch: 101/224\n",
      "Batch loss: 0.05828256160020828 batch: 102/224\n",
      "Batch loss: 0.10048375278711319 batch: 103/224\n",
      "Batch loss: 0.03874422237277031 batch: 104/224\n",
      "Batch loss: 0.029206227511167526 batch: 105/224\n",
      "Batch loss: 0.06007494032382965 batch: 106/224\n",
      "Batch loss: 0.047617074102163315 batch: 107/224\n",
      "Batch loss: 0.04458246007561684 batch: 108/224\n",
      "Batch loss: 0.03292328491806984 batch: 109/224\n",
      "Batch loss: 0.042964592576026917 batch: 110/224\n",
      "Batch loss: 0.031491417437791824 batch: 111/224\n",
      "Batch loss: 0.025258682668209076 batch: 112/224\n",
      "Batch loss: 0.03978961706161499 batch: 113/224\n",
      "Batch loss: 0.030915213748812675 batch: 114/224\n",
      "Batch loss: 0.043794453144073486 batch: 115/224\n",
      "Batch loss: 0.05324520170688629 batch: 116/224\n",
      "Batch loss: 0.05515884980559349 batch: 117/224\n",
      "Batch loss: 0.0418580062687397 batch: 118/224\n",
      "Batch loss: 0.039649445563554764 batch: 119/224\n",
      "Batch loss: 0.062462784349918365 batch: 120/224\n",
      "Batch loss: 0.04752347990870476 batch: 121/224\n",
      "Batch loss: 0.057282496243715286 batch: 122/224\n",
      "Batch loss: 0.039557941257953644 batch: 123/224\n",
      "Batch loss: 0.03251262381672859 batch: 124/224\n",
      "Batch loss: 0.0595463402569294 batch: 125/224\n",
      "Batch loss: 0.09145845472812653 batch: 126/224\n",
      "Batch loss: 0.04419894143939018 batch: 127/224\n",
      "Batch loss: 0.0416962094604969 batch: 128/224\n",
      "Batch loss: 0.04270618408918381 batch: 129/224\n",
      "Batch loss: 0.03427497297525406 batch: 130/224\n",
      "Batch loss: 0.02510898932814598 batch: 131/224\n",
      "Batch loss: 0.07241521775722504 batch: 132/224\n",
      "Batch loss: 0.04658235237002373 batch: 133/224\n",
      "Batch loss: 0.04422552138566971 batch: 134/224\n",
      "Batch loss: 0.03512881323695183 batch: 135/224\n",
      "Batch loss: 0.05789732560515404 batch: 136/224\n",
      "Batch loss: 0.04260920360684395 batch: 137/224\n",
      "Batch loss: 0.047996655106544495 batch: 138/224\n",
      "Batch loss: 0.0605350024998188 batch: 139/224\n",
      "Batch loss: 0.05599376559257507 batch: 140/224\n",
      "Batch loss: 0.05837688967585564 batch: 141/224\n",
      "Batch loss: 0.03148118779063225 batch: 142/224\n",
      "Batch loss: 0.05226881057024002 batch: 143/224\n",
      "Batch loss: 0.03637751191854477 batch: 144/224\n",
      "Batch loss: 0.05306720733642578 batch: 145/224\n",
      "Batch loss: 0.0799422487616539 batch: 146/224\n",
      "Batch loss: 0.07545080780982971 batch: 147/224\n",
      "Batch loss: 0.038136109709739685 batch: 148/224\n",
      "Batch loss: 0.06378301233053207 batch: 149/224\n",
      "Batch loss: 0.04585644230246544 batch: 150/224\n",
      "Batch loss: 0.03428565338253975 batch: 151/224\n",
      "Batch loss: 0.06117123365402222 batch: 152/224\n",
      "Batch loss: 0.07239150255918503 batch: 153/224\n",
      "Batch loss: 0.055831968784332275 batch: 154/224\n",
      "Batch loss: 0.04432918131351471 batch: 155/224\n",
      "Batch loss: 0.05457767844200134 batch: 156/224\n",
      "Batch loss: 0.04829837754368782 batch: 157/224\n",
      "Batch loss: 0.09421998262405396 batch: 158/224\n",
      "Batch loss: 0.04593687132000923 batch: 159/224\n",
      "Batch loss: 0.04997990280389786 batch: 160/224\n",
      "Batch loss: 0.03814874216914177 batch: 161/224\n",
      "Batch loss: 0.0483049601316452 batch: 162/224\n",
      "Batch loss: 0.05193621292710304 batch: 163/224\n",
      "Batch loss: 0.0328189916908741 batch: 164/224\n",
      "Batch loss: 0.06563908606767654 batch: 165/224\n",
      "Batch loss: 0.07051046937704086 batch: 166/224\n",
      "Batch loss: 0.03229965642094612 batch: 167/224\n",
      "Batch loss: 0.05717768892645836 batch: 168/224\n",
      "Batch loss: 0.042577628046274185 batch: 169/224\n",
      "Batch loss: 0.048917192965745926 batch: 170/224\n",
      "Batch loss: 0.0429101400077343 batch: 171/224\n",
      "Batch loss: 0.03097458928823471 batch: 172/224\n",
      "Batch loss: 0.05584048852324486 batch: 173/224\n",
      "Batch loss: 0.03464192524552345 batch: 174/224\n",
      "Batch loss: 0.05400150269269943 batch: 175/224\n",
      "Batch loss: 0.05949602648615837 batch: 176/224\n",
      "Batch loss: 0.06780984997749329 batch: 177/224\n",
      "Batch loss: 0.03701400384306908 batch: 178/224\n",
      "Batch loss: 0.08479063957929611 batch: 179/224\n",
      "Batch loss: 0.015411359257996082 batch: 180/224\n",
      "Batch loss: 0.07608135789632797 batch: 181/224\n",
      "Batch loss: 0.033656224608421326 batch: 182/224\n",
      "Batch loss: 0.06697265058755875 batch: 183/224\n",
      "Batch loss: 0.04437585175037384 batch: 184/224\n",
      "Batch loss: 0.060801152139902115 batch: 185/224\n",
      "Batch loss: 0.03595270216464996 batch: 186/224\n",
      "Batch loss: 0.05440974608063698 batch: 187/224\n",
      "Batch loss: 0.05899447202682495 batch: 188/224\n",
      "Batch loss: 0.04762682691216469 batch: 189/224\n",
      "Batch loss: 0.02595621719956398 batch: 190/224\n",
      "Batch loss: 0.06895380467176437 batch: 191/224\n",
      "Batch loss: 0.04725737124681473 batch: 192/224\n",
      "Batch loss: 0.05333883687853813 batch: 193/224\n",
      "Batch loss: 0.04850199073553085 batch: 194/224\n",
      "Batch loss: 0.042167190462350845 batch: 195/224\n",
      "Batch loss: 0.05206012353301048 batch: 196/224\n",
      "Batch loss: 0.05560033768415451 batch: 197/224\n",
      "Batch loss: 0.03284734487533569 batch: 198/224\n",
      "Batch loss: 0.03351113945245743 batch: 199/224\n",
      "Batch loss: 0.04832063987851143 batch: 200/224\n",
      "Batch loss: 0.04842313379049301 batch: 201/224\n",
      "Batch loss: 0.057252343744039536 batch: 202/224\n",
      "Batch loss: 0.040998317301273346 batch: 203/224\n",
      "Batch loss: 0.06069966033101082 batch: 204/224\n",
      "Batch loss: 0.06485576182603836 batch: 205/224\n",
      "Batch loss: 0.05106770992279053 batch: 206/224\n",
      "Batch loss: 0.046960972249507904 batch: 207/224\n",
      "Batch loss: 0.054050009697675705 batch: 208/224\n",
      "Batch loss: 0.04400041699409485 batch: 209/224\n",
      "Batch loss: 0.051573947072029114 batch: 210/224\n",
      "Batch loss: 0.038736939430236816 batch: 211/224\n",
      "Batch loss: 0.05739814043045044 batch: 212/224\n",
      "Batch loss: 0.06742789596319199 batch: 213/224\n",
      "Batch loss: 0.05750202015042305 batch: 214/224\n",
      "Batch loss: 0.13265904784202576 batch: 215/224\n",
      "Batch loss: 0.0382620207965374 batch: 216/224\n",
      "Batch loss: 0.05630292370915413 batch: 217/224\n",
      "Batch loss: 0.08353697508573532 batch: 218/224\n",
      "Batch loss: 0.04517623409628868 batch: 219/224\n",
      "Batch loss: 0.04264168441295624 batch: 220/224\n",
      "Batch loss: 0.060093387961387634 batch: 221/224\n",
      "Batch loss: 0.09522589296102524 batch: 222/224\n",
      "Batch loss: 0.027807530015707016 batch: 223/224\n",
      "Batch loss: 0.05247799679636955 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 69/75..  Training Loss: 0.00011..  Test Loss: 0.00115..  Test Accuracy: 0.89100\n",
      "Running epoch 70/75\n",
      "Batch loss: 0.05355977267026901 batch: 1/224\n",
      "Batch loss: 0.06119708716869354 batch: 2/224\n",
      "Batch loss: 0.06940152496099472 batch: 3/224\n",
      "Batch loss: 0.0697406530380249 batch: 4/224\n",
      "Batch loss: 0.05028896406292915 batch: 5/224\n",
      "Batch loss: 0.05432562902569771 batch: 6/224\n",
      "Batch loss: 0.04103577136993408 batch: 7/224\n",
      "Batch loss: 0.05465224012732506 batch: 8/224\n",
      "Batch loss: 0.045605357736349106 batch: 9/224\n",
      "Batch loss: 0.03522934764623642 batch: 10/224\n",
      "Batch loss: 0.05480886995792389 batch: 11/224\n",
      "Batch loss: 0.04948113113641739 batch: 12/224\n",
      "Batch loss: 0.03330812603235245 batch: 13/224\n",
      "Batch loss: 0.04308309033513069 batch: 14/224\n",
      "Batch loss: 0.03976382687687874 batch: 15/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.0783601775765419 batch: 16/224\n",
      "Batch loss: 0.04961186274886131 batch: 17/224\n",
      "Batch loss: 0.06306149810552597 batch: 18/224\n",
      "Batch loss: 0.05476337671279907 batch: 19/224\n",
      "Batch loss: 0.05110862851142883 batch: 20/224\n",
      "Batch loss: 0.0578727088868618 batch: 21/224\n",
      "Batch loss: 0.05106465518474579 batch: 22/224\n",
      "Batch loss: 0.0610433928668499 batch: 23/224\n",
      "Batch loss: 0.05148640647530556 batch: 24/224\n",
      "Batch loss: 0.0535007044672966 batch: 25/224\n",
      "Batch loss: 0.05801670253276825 batch: 26/224\n",
      "Batch loss: 0.0720827654004097 batch: 27/224\n",
      "Batch loss: 0.057627540081739426 batch: 28/224\n",
      "Batch loss: 0.04935204237699509 batch: 29/224\n",
      "Batch loss: 0.052794910967350006 batch: 30/224\n",
      "Batch loss: 0.036154601722955704 batch: 31/224\n",
      "Batch loss: 0.025647476315498352 batch: 32/224\n",
      "Batch loss: 0.04874143376946449 batch: 33/224\n",
      "Batch loss: 0.051270339637994766 batch: 34/224\n",
      "Batch loss: 0.056891486048698425 batch: 35/224\n",
      "Batch loss: 0.04852920398116112 batch: 36/224\n",
      "Batch loss: 0.04055263474583626 batch: 37/224\n",
      "Batch loss: 0.046432383358478546 batch: 38/224\n",
      "Batch loss: 0.06211243197321892 batch: 39/224\n",
      "Batch loss: 0.05380438640713692 batch: 40/224\n",
      "Batch loss: 0.05234343186020851 batch: 41/224\n",
      "Batch loss: 0.048020362854003906 batch: 42/224\n",
      "Batch loss: 0.07627012580633163 batch: 43/224\n",
      "Batch loss: 0.06516881287097931 batch: 44/224\n",
      "Batch loss: 0.03246506676077843 batch: 45/224\n",
      "Batch loss: 0.04831133782863617 batch: 46/224\n",
      "Batch loss: 0.03793611750006676 batch: 47/224\n",
      "Batch loss: 0.02845842018723488 batch: 48/224\n",
      "Batch loss: 0.044040288776159286 batch: 49/224\n",
      "Batch loss: 0.049611568450927734 batch: 50/224\n",
      "Batch loss: 0.06280794739723206 batch: 51/224\n",
      "Batch loss: 0.03784211352467537 batch: 52/224\n",
      "Batch loss: 0.07475770264863968 batch: 53/224\n",
      "Batch loss: 0.029412753880023956 batch: 54/224\n",
      "Batch loss: 0.044774968177080154 batch: 55/224\n",
      "Batch loss: 0.03827296942472458 batch: 56/224\n",
      "Batch loss: 0.06184675917029381 batch: 57/224\n",
      "Batch loss: 0.08012691140174866 batch: 58/224\n",
      "Batch loss: 0.06766244024038315 batch: 59/224\n",
      "Batch loss: 0.04747987538576126 batch: 60/224\n",
      "Batch loss: 0.061950504779815674 batch: 61/224\n",
      "Batch loss: 0.05325043201446533 batch: 62/224\n",
      "Batch loss: 0.03632783889770508 batch: 63/224\n",
      "Batch loss: 0.0676509216427803 batch: 64/224\n",
      "Batch loss: 0.033474504947662354 batch: 65/224\n",
      "Batch loss: 0.06386066228151321 batch: 66/224\n",
      "Batch loss: 0.04520212113857269 batch: 67/224\n",
      "Batch loss: 0.04952554032206535 batch: 68/224\n",
      "Batch loss: 0.12736913561820984 batch: 69/224\n",
      "Batch loss: 0.08075899630784988 batch: 70/224\n",
      "Batch loss: 0.050607770681381226 batch: 71/224\n",
      "Batch loss: 0.045374371111392975 batch: 72/224\n",
      "Batch loss: 0.05898386240005493 batch: 73/224\n",
      "Batch loss: 0.047385863959789276 batch: 74/224\n",
      "Batch loss: 0.03958228603005409 batch: 75/224\n",
      "Batch loss: 0.050316423177719116 batch: 76/224\n",
      "Batch loss: 0.04993884265422821 batch: 77/224\n",
      "Batch loss: 0.059501420706510544 batch: 78/224\n",
      "Batch loss: 0.06118710711598396 batch: 79/224\n",
      "Batch loss: 0.053269144147634506 batch: 80/224\n",
      "Batch loss: 0.05916335806250572 batch: 81/224\n",
      "Batch loss: 0.06779942661523819 batch: 82/224\n",
      "Batch loss: 0.04804622009396553 batch: 83/224\n",
      "Batch loss: 0.04573005065321922 batch: 84/224\n",
      "Batch loss: 0.057699237018823624 batch: 85/224\n",
      "Batch loss: 0.05692039802670479 batch: 86/224\n",
      "Batch loss: 0.05816523730754852 batch: 87/224\n",
      "Batch loss: 0.06087537482380867 batch: 88/224\n",
      "Batch loss: 0.030286293476819992 batch: 89/224\n",
      "Batch loss: 0.05602799728512764 batch: 90/224\n",
      "Batch loss: 0.037375032901763916 batch: 91/224\n",
      "Batch loss: 0.043627381324768066 batch: 92/224\n",
      "Batch loss: 0.04629463329911232 batch: 93/224\n",
      "Batch loss: 0.05087028816342354 batch: 94/224\n",
      "Batch loss: 0.0394110307097435 batch: 95/224\n",
      "Batch loss: 0.0668943002820015 batch: 96/224\n",
      "Batch loss: 0.030127249658107758 batch: 97/224\n",
      "Batch loss: 0.025867920368909836 batch: 98/224\n",
      "Batch loss: 0.07066676765680313 batch: 99/224\n",
      "Batch loss: 0.035758864134550095 batch: 100/224\n",
      "Batch loss: 0.07282661646604538 batch: 101/224\n",
      "Batch loss: 0.053376950323581696 batch: 102/224\n",
      "Batch loss: 0.07504698634147644 batch: 103/224\n",
      "Batch loss: 0.032966554164886475 batch: 104/224\n",
      "Batch loss: 0.04989580810070038 batch: 105/224\n",
      "Batch loss: 0.044390127062797546 batch: 106/224\n",
      "Batch loss: 0.032775331288576126 batch: 107/224\n",
      "Batch loss: 0.06967361271381378 batch: 108/224\n",
      "Batch loss: 0.03102092258632183 batch: 109/224\n",
      "Batch loss: 0.04373243451118469 batch: 110/224\n",
      "Batch loss: 0.04023148491978645 batch: 111/224\n",
      "Batch loss: 0.07040169835090637 batch: 112/224\n",
      "Batch loss: 0.08573013544082642 batch: 113/224\n",
      "Batch loss: 0.04209056496620178 batch: 114/224\n",
      "Batch loss: 0.04259934648871422 batch: 115/224\n",
      "Batch loss: 0.05621371045708656 batch: 116/224\n",
      "Batch loss: 0.015510342083871365 batch: 117/224\n",
      "Batch loss: 0.05474062263965607 batch: 118/224\n",
      "Batch loss: 0.06495922058820724 batch: 119/224\n",
      "Batch loss: 0.04771745949983597 batch: 120/224\n",
      "Batch loss: 0.054608382284641266 batch: 121/224\n",
      "Batch loss: 0.07959078252315521 batch: 122/224\n",
      "Batch loss: 0.04185541719198227 batch: 123/224\n",
      "Batch loss: 0.039851151406764984 batch: 124/224\n",
      "Batch loss: 0.0681319609284401 batch: 125/224\n",
      "Batch loss: 0.05950186774134636 batch: 126/224\n",
      "Batch loss: 0.08787496387958527 batch: 127/224\n",
      "Batch loss: 0.04629232734441757 batch: 128/224\n",
      "Batch loss: 0.03616807982325554 batch: 129/224\n",
      "Batch loss: 0.05833881348371506 batch: 130/224\n",
      "Batch loss: 0.05118277296423912 batch: 131/224\n",
      "Batch loss: 0.054628886282444 batch: 132/224\n",
      "Batch loss: 0.0689823180437088 batch: 133/224\n",
      "Batch loss: 0.051314521580934525 batch: 134/224\n",
      "Batch loss: 0.06018940359354019 batch: 135/224\n",
      "Batch loss: 0.0764840692281723 batch: 136/224\n",
      "Batch loss: 0.03707139566540718 batch: 137/224\n",
      "Batch loss: 0.07038189470767975 batch: 138/224\n",
      "Batch loss: 0.09534285217523575 batch: 139/224\n",
      "Batch loss: 0.06496434658765793 batch: 140/224\n",
      "Batch loss: 0.06123802438378334 batch: 141/224\n",
      "Batch loss: 0.051840316504240036 batch: 142/224\n",
      "Batch loss: 0.048044003546237946 batch: 143/224\n",
      "Batch loss: 0.05609752610325813 batch: 144/224\n",
      "Batch loss: 0.05422485992312431 batch: 145/224\n",
      "Batch loss: 0.07148640602827072 batch: 146/224\n",
      "Batch loss: 0.0635886937379837 batch: 147/224\n",
      "Batch loss: 0.048644859343767166 batch: 148/224\n",
      "Batch loss: 0.09166253358125687 batch: 149/224\n",
      "Batch loss: 0.07688920199871063 batch: 150/224\n",
      "Batch loss: 0.034946441650390625 batch: 151/224\n",
      "Batch loss: 0.06808161735534668 batch: 152/224\n",
      "Batch loss: 0.053208205848932266 batch: 153/224\n",
      "Batch loss: 0.060813058167696 batch: 154/224\n",
      "Batch loss: 0.043047092854976654 batch: 155/224\n",
      "Batch loss: 0.050653550773859024 batch: 156/224\n",
      "Batch loss: 0.03281693533062935 batch: 157/224\n",
      "Batch loss: 0.07486078143119812 batch: 158/224\n",
      "Batch loss: 0.06000232324004173 batch: 159/224\n",
      "Batch loss: 0.08979333192110062 batch: 160/224\n",
      "Batch loss: 0.04649842903017998 batch: 161/224\n",
      "Batch loss: 0.07587774097919464 batch: 162/224\n",
      "Batch loss: 0.05690525099635124 batch: 163/224\n",
      "Batch loss: 0.03901970759034157 batch: 164/224\n",
      "Batch loss: 0.04819152131676674 batch: 165/224\n",
      "Batch loss: 0.0691775530576706 batch: 166/224\n",
      "Batch loss: 0.06218891590833664 batch: 167/224\n",
      "Batch loss: 0.029117988422513008 batch: 168/224\n",
      "Batch loss: 0.06385794281959534 batch: 169/224\n",
      "Batch loss: 0.046241890639066696 batch: 170/224\n",
      "Batch loss: 0.03935804218053818 batch: 171/224\n",
      "Batch loss: 0.07048702985048294 batch: 172/224\n",
      "Batch loss: 0.06822503358125687 batch: 173/224\n",
      "Batch loss: 0.029449354857206345 batch: 174/224\n",
      "Batch loss: 0.040242668241262436 batch: 175/224\n",
      "Batch loss: 0.05209041014313698 batch: 176/224\n",
      "Batch loss: 0.10517987608909607 batch: 177/224\n",
      "Batch loss: 0.04732732102274895 batch: 178/224\n",
      "Batch loss: 0.06347868591547012 batch: 179/224\n",
      "Batch loss: 0.027216223999857903 batch: 180/224\n",
      "Batch loss: 0.044782452285289764 batch: 181/224\n",
      "Batch loss: 0.05722051113843918 batch: 182/224\n",
      "Batch loss: 0.052622247487306595 batch: 183/224\n",
      "Batch loss: 0.08013388514518738 batch: 184/224\n",
      "Batch loss: 0.048182640224695206 batch: 185/224\n",
      "Batch loss: 0.036378417164087296 batch: 186/224\n",
      "Batch loss: 0.07248740643262863 batch: 187/224\n",
      "Batch loss: 0.044549085199832916 batch: 188/224\n",
      "Batch loss: 0.04835250601172447 batch: 189/224\n",
      "Batch loss: 0.04990573599934578 batch: 190/224\n",
      "Batch loss: 0.061188604682683945 batch: 191/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.041199468076229095 batch: 192/224\n",
      "Batch loss: 0.06567110866308212 batch: 193/224\n",
      "Batch loss: 0.04345637559890747 batch: 194/224\n",
      "Batch loss: 0.06293884664773941 batch: 195/224\n",
      "Batch loss: 0.05042660981416702 batch: 196/224\n",
      "Batch loss: 0.050114285200834274 batch: 197/224\n",
      "Batch loss: 0.049336761236190796 batch: 198/224\n",
      "Batch loss: 0.03606036305427551 batch: 199/224\n",
      "Batch loss: 0.06398776173591614 batch: 200/224\n",
      "Batch loss: 0.03323248401284218 batch: 201/224\n",
      "Batch loss: 0.05517319217324257 batch: 202/224\n",
      "Batch loss: 0.05474115163087845 batch: 203/224\n",
      "Batch loss: 0.07072068750858307 batch: 204/224\n",
      "Batch loss: 0.06315424293279648 batch: 205/224\n",
      "Batch loss: 0.04938412457704544 batch: 206/224\n",
      "Batch loss: 0.04495583102107048 batch: 207/224\n",
      "Batch loss: 0.04600472375750542 batch: 208/224\n",
      "Batch loss: 0.048281244933605194 batch: 209/224\n",
      "Batch loss: 0.08364350348711014 batch: 210/224\n",
      "Batch loss: 0.02972094900906086 batch: 211/224\n",
      "Batch loss: 0.08058524876832962 batch: 212/224\n",
      "Batch loss: 0.05964787304401398 batch: 213/224\n",
      "Batch loss: 0.04650086909532547 batch: 214/224\n",
      "Batch loss: 0.046750929206609726 batch: 215/224\n",
      "Batch loss: 0.04809882491827011 batch: 216/224\n",
      "Batch loss: 0.07170546054840088 batch: 217/224\n",
      "Batch loss: 0.03886676952242851 batch: 218/224\n",
      "Batch loss: 0.04544742405414581 batch: 219/224\n",
      "Batch loss: 0.04478844255208969 batch: 220/224\n",
      "Batch loss: 0.08021054416894913 batch: 221/224\n",
      "Batch loss: 0.04479726031422615 batch: 222/224\n",
      "Batch loss: 0.04633742943406105 batch: 223/224\n",
      "Batch loss: 0.03275282680988312 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 70/75..  Training Loss: 0.00011..  Test Loss: 0.00111..  Test Accuracy: 0.89246\n",
      "Running epoch 71/75\n",
      "Batch loss: 0.03563261404633522 batch: 1/224\n",
      "Batch loss: 0.042548730969429016 batch: 2/224\n",
      "Batch loss: 0.050382573157548904 batch: 3/224\n",
      "Batch loss: 0.06486667692661285 batch: 4/224\n",
      "Batch loss: 0.042120445519685745 batch: 5/224\n",
      "Batch loss: 0.039873309433460236 batch: 6/224\n",
      "Batch loss: 0.04718182608485222 batch: 7/224\n",
      "Batch loss: 0.05765466392040253 batch: 8/224\n",
      "Batch loss: 0.02829054184257984 batch: 9/224\n",
      "Batch loss: 0.06577204912900925 batch: 10/224\n",
      "Batch loss: 0.06704004108905792 batch: 11/224\n",
      "Batch loss: 0.05402276664972305 batch: 12/224\n",
      "Batch loss: 0.03477392718195915 batch: 13/224\n",
      "Batch loss: 0.03554397448897362 batch: 14/224\n",
      "Batch loss: 0.0342000275850296 batch: 15/224\n",
      "Batch loss: 0.08879909664392471 batch: 16/224\n",
      "Batch loss: 0.05066623538732529 batch: 17/224\n",
      "Batch loss: 0.03800084441900253 batch: 18/224\n",
      "Batch loss: 0.04585190862417221 batch: 19/224\n",
      "Batch loss: 0.06131623685359955 batch: 20/224\n",
      "Batch loss: 0.059933342039585114 batch: 21/224\n",
      "Batch loss: 0.055001989006996155 batch: 22/224\n",
      "Batch loss: 0.062454454600811005 batch: 23/224\n",
      "Batch loss: 0.04617508873343468 batch: 24/224\n",
      "Batch loss: 0.051087480038404465 batch: 25/224\n",
      "Batch loss: 0.05067922919988632 batch: 26/224\n",
      "Batch loss: 0.05201670899987221 batch: 27/224\n",
      "Batch loss: 0.049559302628040314 batch: 28/224\n",
      "Batch loss: 0.06360211223363876 batch: 29/224\n",
      "Batch loss: 0.0708233118057251 batch: 30/224\n",
      "Batch loss: 0.03301373869180679 batch: 31/224\n",
      "Batch loss: 0.04376809298992157 batch: 32/224\n",
      "Batch loss: 0.0237845778465271 batch: 33/224\n",
      "Batch loss: 0.050430089235305786 batch: 34/224\n",
      "Batch loss: 0.08073366433382034 batch: 35/224\n",
      "Batch loss: 0.05876532942056656 batch: 36/224\n",
      "Batch loss: 0.05475356802344322 batch: 37/224\n",
      "Batch loss: 0.05212428420782089 batch: 38/224\n",
      "Batch loss: 0.05244549736380577 batch: 39/224\n",
      "Batch loss: 0.046343520283699036 batch: 40/224\n",
      "Batch loss: 0.05249801278114319 batch: 41/224\n",
      "Batch loss: 0.04494570195674896 batch: 42/224\n",
      "Batch loss: 0.07883450388908386 batch: 43/224\n",
      "Batch loss: 0.04768586531281471 batch: 44/224\n",
      "Batch loss: 0.0404696948826313 batch: 45/224\n",
      "Batch loss: 0.07586606591939926 batch: 46/224\n",
      "Batch loss: 0.03726266324520111 batch: 47/224\n",
      "Batch loss: 0.03718067705631256 batch: 48/224\n",
      "Batch loss: 0.05604325607419014 batch: 49/224\n",
      "Batch loss: 0.043455109000205994 batch: 50/224\n",
      "Batch loss: 0.07330434024333954 batch: 51/224\n",
      "Batch loss: 0.052906379103660583 batch: 52/224\n",
      "Batch loss: 0.06265384703874588 batch: 53/224\n",
      "Batch loss: 0.06026139855384827 batch: 54/224\n",
      "Batch loss: 0.04814218729734421 batch: 55/224\n",
      "Batch loss: 0.07445459812879562 batch: 56/224\n",
      "Batch loss: 0.0636204332113266 batch: 57/224\n",
      "Batch loss: 0.0555642768740654 batch: 58/224\n",
      "Batch loss: 0.0303640253841877 batch: 59/224\n",
      "Batch loss: 0.0558624342083931 batch: 60/224\n",
      "Batch loss: 0.03671254217624664 batch: 61/224\n",
      "Batch loss: 0.07065999507904053 batch: 62/224\n",
      "Batch loss: 0.07359741628170013 batch: 63/224\n",
      "Batch loss: 0.06819137930870056 batch: 64/224\n",
      "Batch loss: 0.04295845702290535 batch: 65/224\n",
      "Batch loss: 0.07127425074577332 batch: 66/224\n",
      "Batch loss: 0.044505588710308075 batch: 67/224\n",
      "Batch loss: 0.06954295188188553 batch: 68/224\n",
      "Batch loss: 0.0559842549264431 batch: 69/224\n",
      "Batch loss: 0.07019920647144318 batch: 70/224\n",
      "Batch loss: 0.0682259127497673 batch: 71/224\n",
      "Batch loss: 0.037665411829948425 batch: 72/224\n",
      "Batch loss: 0.08382392674684525 batch: 73/224\n",
      "Batch loss: 0.03136665001511574 batch: 74/224\n",
      "Batch loss: 0.05989189073443413 batch: 75/224\n",
      "Batch loss: 0.07290160655975342 batch: 76/224\n",
      "Batch loss: 0.07465559989213943 batch: 77/224\n",
      "Batch loss: 0.06447751820087433 batch: 78/224\n",
      "Batch loss: 0.06533516943454742 batch: 79/224\n",
      "Batch loss: 0.05145300552248955 batch: 80/224\n",
      "Batch loss: 0.07972434908151627 batch: 81/224\n",
      "Batch loss: 0.04205557703971863 batch: 82/224\n",
      "Batch loss: 0.050505563616752625 batch: 83/224\n",
      "Batch loss: 0.03250585123896599 batch: 84/224\n",
      "Batch loss: 0.07603286951780319 batch: 85/224\n",
      "Batch loss: 0.04173903167247772 batch: 86/224\n",
      "Batch loss: 0.06770794838666916 batch: 87/224\n",
      "Batch loss: 0.06845453381538391 batch: 88/224\n",
      "Batch loss: 0.07095509767532349 batch: 89/224\n",
      "Batch loss: 0.07200329750776291 batch: 90/224\n",
      "Batch loss: 0.041382890194654465 batch: 91/224\n",
      "Batch loss: 0.042341627180576324 batch: 92/224\n",
      "Batch loss: 0.04442073032259941 batch: 93/224\n",
      "Batch loss: 0.051914773881435394 batch: 94/224\n",
      "Batch loss: 0.0537237711250782 batch: 95/224\n",
      "Batch loss: 0.06148738041520119 batch: 96/224\n",
      "Batch loss: 0.03258354589343071 batch: 97/224\n",
      "Batch loss: 0.041980158537626266 batch: 98/224\n",
      "Batch loss: 0.07861215621232986 batch: 99/224\n",
      "Batch loss: 0.03729643300175667 batch: 100/224\n",
      "Batch loss: 0.05819855257868767 batch: 101/224\n",
      "Batch loss: 0.04366389289498329 batch: 102/224\n",
      "Batch loss: 0.09190764278173447 batch: 103/224\n",
      "Batch loss: 0.055566053837537766 batch: 104/224\n",
      "Batch loss: 0.056749556213617325 batch: 105/224\n",
      "Batch loss: 0.08649738878011703 batch: 106/224\n",
      "Batch loss: 0.06201349198818207 batch: 107/224\n",
      "Batch loss: 0.03658045083284378 batch: 108/224\n",
      "Batch loss: 0.042422711849212646 batch: 109/224\n",
      "Batch loss: 0.05842185392975807 batch: 110/224\n",
      "Batch loss: 0.0453217551112175 batch: 111/224\n",
      "Batch loss: 0.05732966959476471 batch: 112/224\n",
      "Batch loss: 0.03883900120854378 batch: 113/224\n",
      "Batch loss: 0.050353072583675385 batch: 114/224\n",
      "Batch loss: 0.053975801914930344 batch: 115/224\n",
      "Batch loss: 0.04152456298470497 batch: 116/224\n",
      "Batch loss: 0.033748142421245575 batch: 117/224\n",
      "Batch loss: 0.033423032611608505 batch: 118/224\n",
      "Batch loss: 0.052455414086580276 batch: 119/224\n",
      "Batch loss: 0.041720420122146606 batch: 120/224\n",
      "Batch loss: 0.043084483593702316 batch: 121/224\n",
      "Batch loss: 0.04138750210404396 batch: 122/224\n",
      "Batch loss: 0.039604898542165756 batch: 123/224\n",
      "Batch loss: 0.03220734745264053 batch: 124/224\n",
      "Batch loss: 0.056140076369047165 batch: 125/224\n",
      "Batch loss: 0.08384410291910172 batch: 126/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.04934341460466385 batch: 127/224\n",
      "Batch loss: 0.04394033923745155 batch: 128/224\n",
      "Batch loss: 0.04339754208922386 batch: 129/224\n",
      "Batch loss: 0.06880565732717514 batch: 130/224\n",
      "Batch loss: 0.047745876014232635 batch: 131/224\n",
      "Batch loss: 0.052546177059412 batch: 132/224\n",
      "Batch loss: 0.07212667912244797 batch: 133/224\n",
      "Batch loss: 0.06326363980770111 batch: 134/224\n",
      "Batch loss: 0.04006703197956085 batch: 135/224\n",
      "Batch loss: 0.05521726235747337 batch: 136/224\n",
      "Batch loss: 0.034062717109918594 batch: 137/224\n",
      "Batch loss: 0.04586609825491905 batch: 138/224\n",
      "Batch loss: 0.044333647936582565 batch: 139/224\n",
      "Batch loss: 0.05560121312737465 batch: 140/224\n",
      "Batch loss: 0.035645991563797 batch: 141/224\n",
      "Batch loss: 0.05942403897643089 batch: 142/224\n",
      "Batch loss: 0.04062683880329132 batch: 143/224\n",
      "Batch loss: 0.03970889002084732 batch: 144/224\n",
      "Batch loss: 0.04035767540335655 batch: 145/224\n",
      "Batch loss: 0.08041439205408096 batch: 146/224\n",
      "Batch loss: 0.05579068139195442 batch: 147/224\n",
      "Batch loss: 0.02551397681236267 batch: 148/224\n",
      "Batch loss: 0.061896730214357376 batch: 149/224\n",
      "Batch loss: 0.05584663152694702 batch: 150/224\n",
      "Batch loss: 0.025175508111715317 batch: 151/224\n",
      "Batch loss: 0.044762928038835526 batch: 152/224\n",
      "Batch loss: 0.08984823524951935 batch: 153/224\n",
      "Batch loss: 0.0919647142291069 batch: 154/224\n",
      "Batch loss: 0.04125390946865082 batch: 155/224\n",
      "Batch loss: 0.05350394546985626 batch: 156/224\n",
      "Batch loss: 0.047855645418167114 batch: 157/224\n",
      "Batch loss: 0.0668172538280487 batch: 158/224\n",
      "Batch loss: 0.04563961178064346 batch: 159/224\n",
      "Batch loss: 0.07342829555273056 batch: 160/224\n",
      "Batch loss: 0.028886498883366585 batch: 161/224\n",
      "Batch loss: 0.04979022219777107 batch: 162/224\n",
      "Batch loss: 0.04796542972326279 batch: 163/224\n",
      "Batch loss: 0.032029300928115845 batch: 164/224\n",
      "Batch loss: 0.04914981126785278 batch: 165/224\n",
      "Batch loss: 0.05683204159140587 batch: 166/224\n",
      "Batch loss: 0.046283822506666183 batch: 167/224\n",
      "Batch loss: 0.05282970145344734 batch: 168/224\n",
      "Batch loss: 0.062158022075891495 batch: 169/224\n",
      "Batch loss: 0.04097095876932144 batch: 170/224\n",
      "Batch loss: 0.04319129139184952 batch: 171/224\n",
      "Batch loss: 0.04254499450325966 batch: 172/224\n",
      "Batch loss: 0.0471615232527256 batch: 173/224\n",
      "Batch loss: 0.03577636182308197 batch: 174/224\n",
      "Batch loss: 0.05367165803909302 batch: 175/224\n",
      "Batch loss: 0.05138865485787392 batch: 176/224\n",
      "Batch loss: 0.04681502655148506 batch: 177/224\n",
      "Batch loss: 0.057758960872888565 batch: 178/224\n",
      "Batch loss: 0.09478642791509628 batch: 179/224\n",
      "Batch loss: 0.05100972205400467 batch: 180/224\n",
      "Batch loss: 0.06809251010417938 batch: 181/224\n",
      "Batch loss: 0.04606950655579567 batch: 182/224\n",
      "Batch loss: 0.057617805898189545 batch: 183/224\n",
      "Batch loss: 0.06461206823587418 batch: 184/224\n",
      "Batch loss: 0.06183295324444771 batch: 185/224\n",
      "Batch loss: 0.03168397769331932 batch: 186/224\n",
      "Batch loss: 0.07106026262044907 batch: 187/224\n",
      "Batch loss: 0.053147051483392715 batch: 188/224\n",
      "Batch loss: 0.04656188562512398 batch: 189/224\n",
      "Batch loss: 0.07148903608322144 batch: 190/224\n",
      "Batch loss: 0.05820212885737419 batch: 191/224\n",
      "Batch loss: 0.049223385751247406 batch: 192/224\n",
      "Batch loss: 0.046152468770742416 batch: 193/224\n",
      "Batch loss: 0.06531207263469696 batch: 194/224\n",
      "Batch loss: 0.04298298805952072 batch: 195/224\n",
      "Batch loss: 0.041980158537626266 batch: 196/224\n",
      "Batch loss: 0.03599778190255165 batch: 197/224\n",
      "Batch loss: 0.03260694444179535 batch: 198/224\n",
      "Batch loss: 0.05125854164361954 batch: 199/224\n",
      "Batch loss: 0.05274107679724693 batch: 200/224\n",
      "Batch loss: 0.03632936626672745 batch: 201/224\n",
      "Batch loss: 0.06451132893562317 batch: 202/224\n",
      "Batch loss: 0.05211711674928665 batch: 203/224\n",
      "Batch loss: 0.05348174273967743 batch: 204/224\n",
      "Batch loss: 0.056589528918266296 batch: 205/224\n",
      "Batch loss: 0.08345098048448563 batch: 206/224\n",
      "Batch loss: 0.06192343682050705 batch: 207/224\n",
      "Batch loss: 0.05051388218998909 batch: 208/224\n",
      "Batch loss: 0.05982864275574684 batch: 209/224\n",
      "Batch loss: 0.04053870216012001 batch: 210/224\n",
      "Batch loss: 0.03211477771401405 batch: 211/224\n",
      "Batch loss: 0.041399989277124405 batch: 212/224\n",
      "Batch loss: 0.05051757022738457 batch: 213/224\n",
      "Batch loss: 0.0644085481762886 batch: 214/224\n",
      "Batch loss: 0.0533362552523613 batch: 215/224\n",
      "Batch loss: 0.07675350457429886 batch: 216/224\n",
      "Batch loss: 0.06710118800401688 batch: 217/224\n",
      "Batch loss: 0.04131632298231125 batch: 218/224\n",
      "Batch loss: 0.051554396748542786 batch: 219/224\n",
      "Batch loss: 0.061073511838912964 batch: 220/224\n",
      "Batch loss: 0.08610936999320984 batch: 221/224\n",
      "Batch loss: 0.057269543409347534 batch: 222/224\n",
      "Batch loss: 0.05614681914448738 batch: 223/224\n",
      "Batch loss: 0.05439312756061554 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 71/75..  Training Loss: 0.00011..  Test Loss: 0.00111..  Test Accuracy: 0.89239\n",
      "Running epoch 72/75\n",
      "Batch loss: 0.0413220040500164 batch: 1/224\n",
      "Batch loss: 0.08881500363349915 batch: 2/224\n",
      "Batch loss: 0.0441104881465435 batch: 3/224\n",
      "Batch loss: 0.08784683793783188 batch: 4/224\n",
      "Batch loss: 0.08731023967266083 batch: 5/224\n",
      "Batch loss: 0.03160924091935158 batch: 6/224\n",
      "Batch loss: 0.03881018981337547 batch: 7/224\n",
      "Batch loss: 0.04746923968195915 batch: 8/224\n",
      "Batch loss: 0.04062436521053314 batch: 9/224\n",
      "Batch loss: 0.05027487128973007 batch: 10/224\n",
      "Batch loss: 0.0655379593372345 batch: 11/224\n",
      "Batch loss: 0.06743107736110687 batch: 12/224\n",
      "Batch loss: 0.036982085555791855 batch: 13/224\n",
      "Batch loss: 0.027575813233852386 batch: 14/224\n",
      "Batch loss: 0.03708820417523384 batch: 15/224\n",
      "Batch loss: 0.06867976486682892 batch: 16/224\n",
      "Batch loss: 0.03266230598092079 batch: 17/224\n",
      "Batch loss: 0.05257750302553177 batch: 18/224\n",
      "Batch loss: 0.04745687544345856 batch: 19/224\n",
      "Batch loss: 0.06047350913286209 batch: 20/224\n",
      "Batch loss: 0.04492662847042084 batch: 21/224\n",
      "Batch loss: 0.048282016068696976 batch: 22/224\n",
      "Batch loss: 0.08750031888484955 batch: 23/224\n",
      "Batch loss: 0.0502607524394989 batch: 24/224\n",
      "Batch loss: 0.03898792341351509 batch: 25/224\n",
      "Batch loss: 0.04768120124936104 batch: 26/224\n",
      "Batch loss: 0.03518150746822357 batch: 27/224\n",
      "Batch loss: 0.055371299386024475 batch: 28/224\n",
      "Batch loss: 0.05970761924982071 batch: 29/224\n",
      "Batch loss: 0.048356812447309494 batch: 30/224\n",
      "Batch loss: 0.045108746737241745 batch: 31/224\n",
      "Batch loss: 0.06221386045217514 batch: 32/224\n",
      "Batch loss: 0.021375199779868126 batch: 33/224\n",
      "Batch loss: 0.0582955963909626 batch: 34/224\n",
      "Batch loss: 0.06331264972686768 batch: 35/224\n",
      "Batch loss: 0.06023973226547241 batch: 36/224\n",
      "Batch loss: 0.05135586857795715 batch: 37/224\n",
      "Batch loss: 0.0720100924372673 batch: 38/224\n",
      "Batch loss: 0.05581469461321831 batch: 39/224\n",
      "Batch loss: 0.03583228588104248 batch: 40/224\n",
      "Batch loss: 0.05322994291782379 batch: 41/224\n",
      "Batch loss: 0.0366244800388813 batch: 42/224\n",
      "Batch loss: 0.07361891120672226 batch: 43/224\n",
      "Batch loss: 0.06448893249034882 batch: 44/224\n",
      "Batch loss: 0.04430869594216347 batch: 45/224\n",
      "Batch loss: 0.11147207021713257 batch: 46/224\n",
      "Batch loss: 0.04691504314541817 batch: 47/224\n",
      "Batch loss: 0.03246460482478142 batch: 48/224\n",
      "Batch loss: 0.05120665207505226 batch: 49/224\n",
      "Batch loss: 0.03642001003026962 batch: 50/224\n",
      "Batch loss: 0.04568338394165039 batch: 51/224\n",
      "Batch loss: 0.044055111706256866 batch: 52/224\n",
      "Batch loss: 0.04895497113466263 batch: 53/224\n",
      "Batch loss: 0.0596923790872097 batch: 54/224\n",
      "Batch loss: 0.03645988553762436 batch: 55/224\n",
      "Batch loss: 0.037765827029943466 batch: 56/224\n",
      "Batch loss: 0.05548981949687004 batch: 57/224\n",
      "Batch loss: 0.05794224143028259 batch: 58/224\n",
      "Batch loss: 0.03838732838630676 batch: 59/224\n",
      "Batch loss: 0.026006516069173813 batch: 60/224\n",
      "Batch loss: 0.040194373577833176 batch: 61/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.023659316822886467 batch: 62/224\n",
      "Batch loss: 0.04489349201321602 batch: 63/224\n",
      "Batch loss: 0.07271944731473923 batch: 64/224\n",
      "Batch loss: 0.03833943232893944 batch: 65/224\n",
      "Batch loss: 0.03803344815969467 batch: 66/224\n",
      "Batch loss: 0.053556885570287704 batch: 67/224\n",
      "Batch loss: 0.03453448414802551 batch: 68/224\n",
      "Batch loss: 0.0601770393550396 batch: 69/224\n",
      "Batch loss: 0.05614768713712692 batch: 70/224\n",
      "Batch loss: 0.06503966450691223 batch: 71/224\n",
      "Batch loss: 0.04057709872722626 batch: 72/224\n",
      "Batch loss: 0.05438993498682976 batch: 73/224\n",
      "Batch loss: 0.054866157472133636 batch: 74/224\n",
      "Batch loss: 0.06245243549346924 batch: 75/224\n",
      "Batch loss: 0.061841536313295364 batch: 76/224\n",
      "Batch loss: 0.05425981804728508 batch: 77/224\n",
      "Batch loss: 0.034708887338638306 batch: 78/224\n",
      "Batch loss: 0.061460599303245544 batch: 79/224\n",
      "Batch loss: 0.04578785598278046 batch: 80/224\n",
      "Batch loss: 0.07047867029905319 batch: 81/224\n",
      "Batch loss: 0.06262639909982681 batch: 82/224\n",
      "Batch loss: 0.05620991438627243 batch: 83/224\n",
      "Batch loss: 0.04474334791302681 batch: 84/224\n",
      "Batch loss: 0.0726764053106308 batch: 85/224\n",
      "Batch loss: 0.055964287370443344 batch: 86/224\n",
      "Batch loss: 0.03699399530887604 batch: 87/224\n",
      "Batch loss: 0.05487598851323128 batch: 88/224\n",
      "Batch loss: 0.06047063693404198 batch: 89/224\n",
      "Batch loss: 0.05110570415854454 batch: 90/224\n",
      "Batch loss: 0.03504865616559982 batch: 91/224\n",
      "Batch loss: 0.04654630273580551 batch: 92/224\n",
      "Batch loss: 0.027621667832136154 batch: 93/224\n",
      "Batch loss: 0.06592724472284317 batch: 94/224\n",
      "Batch loss: 0.03758753836154938 batch: 95/224\n",
      "Batch loss: 0.08467680960893631 batch: 96/224\n",
      "Batch loss: 0.05076585337519646 batch: 97/224\n",
      "Batch loss: 0.03773636743426323 batch: 98/224\n",
      "Batch loss: 0.0491369292140007 batch: 99/224\n",
      "Batch loss: 0.07089297473430634 batch: 100/224\n",
      "Batch loss: 0.06539202481508255 batch: 101/224\n",
      "Batch loss: 0.05172460153698921 batch: 102/224\n",
      "Batch loss: 0.06893809139728546 batch: 103/224\n",
      "Batch loss: 0.0608694888651371 batch: 104/224\n",
      "Batch loss: 0.03794580698013306 batch: 105/224\n",
      "Batch loss: 0.046847712248563766 batch: 106/224\n",
      "Batch loss: 0.06230729818344116 batch: 107/224\n",
      "Batch loss: 0.06318299472332001 batch: 108/224\n",
      "Batch loss: 0.027718469500541687 batch: 109/224\n",
      "Batch loss: 0.04162103682756424 batch: 110/224\n",
      "Batch loss: 0.04022597521543503 batch: 111/224\n",
      "Batch loss: 0.039719440042972565 batch: 112/224\n",
      "Batch loss: 0.05158360302448273 batch: 113/224\n",
      "Batch loss: 0.020499754697084427 batch: 114/224\n",
      "Batch loss: 0.04087221994996071 batch: 115/224\n",
      "Batch loss: 0.041688933968544006 batch: 116/224\n",
      "Batch loss: 0.05480444058775902 batch: 117/224\n",
      "Batch loss: 0.09142767637968063 batch: 118/224\n",
      "Batch loss: 0.04589080438017845 batch: 119/224\n",
      "Batch loss: 0.04490753263235092 batch: 120/224\n",
      "Batch loss: 0.05276162922382355 batch: 121/224\n",
      "Batch loss: 0.03891813009977341 batch: 122/224\n",
      "Batch loss: 0.06236507371068001 batch: 123/224\n",
      "Batch loss: 0.039040371775627136 batch: 124/224\n",
      "Batch loss: 0.0700792446732521 batch: 125/224\n",
      "Batch loss: 0.061637748032808304 batch: 126/224\n",
      "Batch loss: 0.06705368310213089 batch: 127/224\n",
      "Batch loss: 0.05868279188871384 batch: 128/224\n",
      "Batch loss: 0.05447306111454964 batch: 129/224\n",
      "Batch loss: 0.09362028539180756 batch: 130/224\n",
      "Batch loss: 0.027185119688510895 batch: 131/224\n",
      "Batch loss: 0.04904460534453392 batch: 132/224\n",
      "Batch loss: 0.08479153364896774 batch: 133/224\n",
      "Batch loss: 0.06525837630033493 batch: 134/224\n",
      "Batch loss: 0.051546961069107056 batch: 135/224\n",
      "Batch loss: 0.07332401722669601 batch: 136/224\n",
      "Batch loss: 0.05384368821978569 batch: 137/224\n",
      "Batch loss: 0.07461635768413544 batch: 138/224\n",
      "Batch loss: 0.08812981098890305 batch: 139/224\n",
      "Batch loss: 0.0840851292014122 batch: 140/224\n",
      "Batch loss: 0.044661324471235275 batch: 141/224\n",
      "Batch loss: 0.04951269179582596 batch: 142/224\n",
      "Batch loss: 0.04636913537979126 batch: 143/224\n",
      "Batch loss: 0.054145026952028275 batch: 144/224\n",
      "Batch loss: 0.03259779140353203 batch: 145/224\n",
      "Batch loss: 0.06944774836301804 batch: 146/224\n",
      "Batch loss: 0.031588468700647354 batch: 147/224\n",
      "Batch loss: 0.045344628393650055 batch: 148/224\n",
      "Batch loss: 0.09004496037960052 batch: 149/224\n",
      "Batch loss: 0.06492350995540619 batch: 150/224\n",
      "Batch loss: 0.03857235983014107 batch: 151/224\n",
      "Batch loss: 0.051441583782434464 batch: 152/224\n",
      "Batch loss: 0.06884120404720306 batch: 153/224\n",
      "Batch loss: 0.07039003074169159 batch: 154/224\n",
      "Batch loss: 0.05334717407822609 batch: 155/224\n",
      "Batch loss: 0.03323911875486374 batch: 156/224\n",
      "Batch loss: 0.04988676682114601 batch: 157/224\n",
      "Batch loss: 0.08750932663679123 batch: 158/224\n",
      "Batch loss: 0.04784712195396423 batch: 159/224\n",
      "Batch loss: 0.044603537768125534 batch: 160/224\n",
      "Batch loss: 0.02881401591002941 batch: 161/224\n",
      "Batch loss: 0.049657586961984634 batch: 162/224\n",
      "Batch loss: 0.02347799390554428 batch: 163/224\n",
      "Batch loss: 0.051999591290950775 batch: 164/224\n",
      "Batch loss: 0.06932815909385681 batch: 165/224\n",
      "Batch loss: 0.046348936855793 batch: 166/224\n",
      "Batch loss: 0.07281367480754852 batch: 167/224\n",
      "Batch loss: 0.041384417563676834 batch: 168/224\n",
      "Batch loss: 0.06463290005922318 batch: 169/224\n",
      "Batch loss: 0.05347740650177002 batch: 170/224\n",
      "Batch loss: 0.04682690650224686 batch: 171/224\n",
      "Batch loss: 0.04537888616323471 batch: 172/224\n",
      "Batch loss: 0.078565813601017 batch: 173/224\n",
      "Batch loss: 0.0693163275718689 batch: 174/224\n",
      "Batch loss: 0.0538436695933342 batch: 175/224\n",
      "Batch loss: 0.05288226902484894 batch: 176/224\n",
      "Batch loss: 0.062310654670000076 batch: 177/224\n",
      "Batch loss: 0.024683471769094467 batch: 178/224\n",
      "Batch loss: 0.06260116398334503 batch: 179/224\n",
      "Batch loss: 0.06637577712535858 batch: 180/224\n",
      "Batch loss: 0.04621400684118271 batch: 181/224\n",
      "Batch loss: 0.0609937459230423 batch: 182/224\n",
      "Batch loss: 0.04791681468486786 batch: 183/224\n",
      "Batch loss: 0.06478647887706757 batch: 184/224\n",
      "Batch loss: 0.0704905167222023 batch: 185/224\n",
      "Batch loss: 0.04105854406952858 batch: 186/224\n",
      "Batch loss: 0.05888771638274193 batch: 187/224\n",
      "Batch loss: 0.04976505786180496 batch: 188/224\n",
      "Batch loss: 0.08552806079387665 batch: 189/224\n",
      "Batch loss: 0.04447513446211815 batch: 190/224\n",
      "Batch loss: 0.05417902022600174 batch: 191/224\n",
      "Batch loss: 0.09569671005010605 batch: 192/224\n",
      "Batch loss: 0.0500016026198864 batch: 193/224\n",
      "Batch loss: 0.059997424483299255 batch: 194/224\n",
      "Batch loss: 0.06201530992984772 batch: 195/224\n",
      "Batch loss: 0.04861939698457718 batch: 196/224\n",
      "Batch loss: 0.03912108764052391 batch: 197/224\n",
      "Batch loss: 0.03297915309667587 batch: 198/224\n",
      "Batch loss: 0.04850265756249428 batch: 199/224\n",
      "Batch loss: 0.05043213814496994 batch: 200/224\n",
      "Batch loss: 0.03227992355823517 batch: 201/224\n",
      "Batch loss: 0.06739376485347748 batch: 202/224\n",
      "Batch loss: 0.044734712690114975 batch: 203/224\n",
      "Batch loss: 0.04676726460456848 batch: 204/224\n",
      "Batch loss: 0.06989417225122452 batch: 205/224\n",
      "Batch loss: 0.07134287804365158 batch: 206/224\n",
      "Batch loss: 0.041082873940467834 batch: 207/224\n",
      "Batch loss: 0.04570784792304039 batch: 208/224\n",
      "Batch loss: 0.04252038896083832 batch: 209/224\n",
      "Batch loss: 0.04003787785768509 batch: 210/224\n",
      "Batch loss: 0.03292706236243248 batch: 211/224\n",
      "Batch loss: 0.04282012954354286 batch: 212/224\n",
      "Batch loss: 0.06787600368261337 batch: 213/224\n",
      "Batch loss: 0.048150211572647095 batch: 214/224\n",
      "Batch loss: 0.058214057236909866 batch: 215/224\n",
      "Batch loss: 0.040694501250982285 batch: 216/224\n",
      "Batch loss: 0.045076269656419754 batch: 217/224\n",
      "Batch loss: 0.03956591337919235 batch: 218/224\n",
      "Batch loss: 0.030816828832030296 batch: 219/224\n",
      "Batch loss: 0.03965931758284569 batch: 220/224\n",
      "Batch loss: 0.08839443325996399 batch: 221/224\n",
      "Batch loss: 0.10086475312709808 batch: 222/224\n",
      "Batch loss: 0.06501790881156921 batch: 223/224\n",
      "Batch loss: 0.035456184297800064 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 72/75..  Training Loss: 0.00011..  Test Loss: 0.00114..  Test Accuracy: 0.89271\n",
      "Running epoch 73/75\n",
      "Batch loss: 0.06200363487005234 batch: 1/224\n",
      "Batch loss: 0.05584919452667236 batch: 2/224\n",
      "Batch loss: 0.04031209275126457 batch: 3/224\n",
      "Batch loss: 0.09368927776813507 batch: 4/224\n",
      "Batch loss: 0.0650479719042778 batch: 5/224\n",
      "Batch loss: 0.04810101538896561 batch: 6/224\n",
      "Batch loss: 0.029128000140190125 batch: 7/224\n",
      "Batch loss: 0.043369609862565994 batch: 8/224\n",
      "Batch loss: 0.05748569965362549 batch: 9/224\n",
      "Batch loss: 0.04304322972893715 batch: 10/224\n",
      "Batch loss: 0.059259578585624695 batch: 11/224\n",
      "Batch loss: 0.052275437861680984 batch: 12/224\n",
      "Batch loss: 0.04531846195459366 batch: 13/224\n",
      "Batch loss: 0.04460780322551727 batch: 14/224\n",
      "Batch loss: 0.04782853648066521 batch: 15/224\n",
      "Batch loss: 0.08015706390142441 batch: 16/224\n",
      "Batch loss: 0.05986782908439636 batch: 17/224\n",
      "Batch loss: 0.04635091871023178 batch: 18/224\n",
      "Batch loss: 0.05115338787436485 batch: 19/224\n",
      "Batch loss: 0.058927446603775024 batch: 20/224\n",
      "Batch loss: 0.057182375341653824 batch: 21/224\n",
      "Batch loss: 0.037917762994766235 batch: 22/224\n",
      "Batch loss: 0.050687819719314575 batch: 23/224\n",
      "Batch loss: 0.06760205328464508 batch: 24/224\n",
      "Batch loss: 0.06528890877962112 batch: 25/224\n",
      "Batch loss: 0.03788798674941063 batch: 26/224\n",
      "Batch loss: 0.03891916945576668 batch: 27/224\n",
      "Batch loss: 0.05126461014151573 batch: 28/224\n",
      "Batch loss: 0.07671026140451431 batch: 29/224\n",
      "Batch loss: 0.04382345825433731 batch: 30/224\n",
      "Batch loss: 0.02033977210521698 batch: 31/224\n",
      "Batch loss: 0.031192241236567497 batch: 32/224\n",
      "Batch loss: 0.037679724395275116 batch: 33/224\n",
      "Batch loss: 0.08791434019804001 batch: 34/224\n",
      "Batch loss: 0.05759375914931297 batch: 35/224\n",
      "Batch loss: 0.06515397876501083 batch: 36/224\n",
      "Batch loss: 0.05144644156098366 batch: 37/224\n",
      "Batch loss: 0.0432567335665226 batch: 38/224\n",
      "Batch loss: 0.039399560540914536 batch: 39/224\n",
      "Batch loss: 0.026830708608031273 batch: 40/224\n",
      "Batch loss: 0.05263689532876015 batch: 41/224\n",
      "Batch loss: 0.06296069920063019 batch: 42/224\n",
      "Batch loss: 0.09557259827852249 batch: 43/224\n",
      "Batch loss: 0.039227426052093506 batch: 44/224\n",
      "Batch loss: 0.05308181047439575 batch: 45/224\n",
      "Batch loss: 0.07652097940444946 batch: 46/224\n",
      "Batch loss: 0.06116439774632454 batch: 47/224\n",
      "Batch loss: 0.037344902753829956 batch: 48/224\n",
      "Batch loss: 0.05151061341166496 batch: 49/224\n",
      "Batch loss: 0.06955819576978683 batch: 50/224\n",
      "Batch loss: 0.06920143216848373 batch: 51/224\n",
      "Batch loss: 0.06073292717337608 batch: 52/224\n",
      "Batch loss: 0.05163485184311867 batch: 53/224\n",
      "Batch loss: 0.0453459694981575 batch: 54/224\n",
      "Batch loss: 0.04316851869225502 batch: 55/224\n",
      "Batch loss: 0.04406684264540672 batch: 56/224\n",
      "Batch loss: 0.05543346703052521 batch: 57/224\n",
      "Batch loss: 0.061040326952934265 batch: 58/224\n",
      "Batch loss: 0.02213677205145359 batch: 59/224\n",
      "Batch loss: 0.07503911852836609 batch: 60/224\n",
      "Batch loss: 0.028433552011847496 batch: 61/224\n",
      "Batch loss: 0.04556221142411232 batch: 62/224\n",
      "Batch loss: 0.05441168323159218 batch: 63/224\n",
      "Batch loss: 0.10239196568727493 batch: 64/224\n",
      "Batch loss: 0.06424670666456223 batch: 65/224\n",
      "Batch loss: 0.06330680102109909 batch: 66/224\n",
      "Batch loss: 0.052520472556352615 batch: 67/224\n",
      "Batch loss: 0.04551424831151962 batch: 68/224\n",
      "Batch loss: 0.04979824274778366 batch: 69/224\n",
      "Batch loss: 0.0642222911119461 batch: 70/224\n",
      "Batch loss: 0.04406008496880531 batch: 71/224\n",
      "Batch loss: 0.035322319716215134 batch: 72/224\n",
      "Batch loss: 0.046929363161325455 batch: 73/224\n",
      "Batch loss: 0.04873279482126236 batch: 74/224\n",
      "Batch loss: 0.055639784783124924 batch: 75/224\n",
      "Batch loss: 0.08052844554185867 batch: 76/224\n",
      "Batch loss: 0.047853026539087296 batch: 77/224\n",
      "Batch loss: 0.07127892225980759 batch: 78/224\n",
      "Batch loss: 0.04772456735372543 batch: 79/224\n",
      "Batch loss: 0.038498420268297195 batch: 80/224\n",
      "Batch loss: 0.0674721971154213 batch: 81/224\n",
      "Batch loss: 0.07023267447948456 batch: 82/224\n",
      "Batch loss: 0.07082635909318924 batch: 83/224\n",
      "Batch loss: 0.051958244293928146 batch: 84/224\n",
      "Batch loss: 0.04904819279909134 batch: 85/224\n",
      "Batch loss: 0.05380086600780487 batch: 86/224\n",
      "Batch loss: 0.059508372098207474 batch: 87/224\n",
      "Batch loss: 0.06561610847711563 batch: 88/224\n",
      "Batch loss: 0.05220440402626991 batch: 89/224\n",
      "Batch loss: 0.05657925456762314 batch: 90/224\n",
      "Batch loss: 0.021929150447249413 batch: 91/224\n",
      "Batch loss: 0.06104462593793869 batch: 92/224\n",
      "Batch loss: 0.031768158078193665 batch: 93/224\n",
      "Batch loss: 0.06720297038555145 batch: 94/224\n",
      "Batch loss: 0.04527536779642105 batch: 95/224\n",
      "Batch loss: 0.05110465735197067 batch: 96/224\n",
      "Batch loss: 0.04337158799171448 batch: 97/224\n",
      "Batch loss: 0.03219792619347572 batch: 98/224\n",
      "Batch loss: 0.06254945695400238 batch: 99/224\n",
      "Batch loss: 0.04266621172428131 batch: 100/224\n",
      "Batch loss: 0.06943061202764511 batch: 101/224\n",
      "Batch loss: 0.07166535407304764 batch: 102/224\n",
      "Batch loss: 0.07551836967468262 batch: 103/224\n",
      "Batch loss: 0.04607541114091873 batch: 104/224\n",
      "Batch loss: 0.03657594695687294 batch: 105/224\n",
      "Batch loss: 0.05810902640223503 batch: 106/224\n",
      "Batch loss: 0.05213764309883118 batch: 107/224\n",
      "Batch loss: 0.042459264397621155 batch: 108/224\n",
      "Batch loss: 0.02496602013707161 batch: 109/224\n",
      "Batch loss: 0.04039890319108963 batch: 110/224\n",
      "Batch loss: 0.06640389561653137 batch: 111/224\n",
      "Batch loss: 0.02599632926285267 batch: 112/224\n",
      "Batch loss: 0.04726003110408783 batch: 113/224\n",
      "Batch loss: 0.06173713505268097 batch: 114/224\n",
      "Batch loss: 0.039432864636182785 batch: 115/224\n",
      "Batch loss: 0.05165738984942436 batch: 116/224\n",
      "Batch loss: 0.043853431940078735 batch: 117/224\n",
      "Batch loss: 0.05018666759133339 batch: 118/224\n",
      "Batch loss: 0.06862427294254303 batch: 119/224\n",
      "Batch loss: 0.07901883870363235 batch: 120/224\n",
      "Batch loss: 0.03246677294373512 batch: 121/224\n",
      "Batch loss: 0.03579097241163254 batch: 122/224\n",
      "Batch loss: 0.05429576709866524 batch: 123/224\n",
      "Batch loss: 0.07839585095643997 batch: 124/224\n",
      "Batch loss: 0.06469051539897919 batch: 125/224\n",
      "Batch loss: 0.07674504071474075 batch: 126/224\n",
      "Batch loss: 0.05956576019525528 batch: 127/224\n",
      "Batch loss: 0.027509042993187904 batch: 128/224\n",
      "Batch loss: 0.057132184505462646 batch: 129/224\n",
      "Batch loss: 0.08203857392072678 batch: 130/224\n",
      "Batch loss: 0.030195511877536774 batch: 131/224\n",
      "Batch loss: 0.05725196376442909 batch: 132/224\n",
      "Batch loss: 0.05147349461913109 batch: 133/224\n",
      "Batch loss: 0.07697684317827225 batch: 134/224\n",
      "Batch loss: 0.03661012649536133 batch: 135/224\n",
      "Batch loss: 0.04214904457330704 batch: 136/224\n",
      "Batch loss: 0.01810193434357643 batch: 137/224\n",
      "Batch loss: 0.06421298533678055 batch: 138/224\n",
      "Batch loss: 0.07366853207349777 batch: 139/224\n",
      "Batch loss: 0.08980390429496765 batch: 140/224\n",
      "Batch loss: 0.05628500506281853 batch: 141/224\n",
      "Batch loss: 0.06734617799520493 batch: 142/224\n",
      "Batch loss: 0.033815231174230576 batch: 143/224\n",
      "Batch loss: 0.02718307450413704 batch: 144/224\n",
      "Batch loss: 0.04466370865702629 batch: 145/224\n",
      "Batch loss: 0.09121722728013992 batch: 146/224\n",
      "Batch loss: 0.06134491041302681 batch: 147/224\n",
      "Batch loss: 0.043427981436252594 batch: 148/224\n",
      "Batch loss: 0.07419786602258682 batch: 149/224\n",
      "Batch loss: 0.09251725673675537 batch: 150/224\n",
      "Batch loss: 0.06288327276706696 batch: 151/224\n",
      "Batch loss: 0.057605572044849396 batch: 152/224\n",
      "Batch loss: 0.06605764478445053 batch: 153/224\n",
      "Batch loss: 0.044483382254838943 batch: 154/224\n",
      "Batch loss: 0.059151869267225266 batch: 155/224\n",
      "Batch loss: 0.048824962228536606 batch: 156/224\n",
      "Batch loss: 0.05921928957104683 batch: 157/224\n",
      "Batch loss: 0.07809430360794067 batch: 158/224\n",
      "Batch loss: 0.06603498756885529 batch: 159/224\n",
      "Batch loss: 0.04634811729192734 batch: 160/224\n",
      "Batch loss: 0.04665730893611908 batch: 161/224\n",
      "Batch loss: 0.03506865352392197 batch: 162/224\n",
      "Batch loss: 0.041052158921957016 batch: 163/224\n",
      "Batch loss: 0.0429019033908844 batch: 164/224\n",
      "Batch loss: 0.05249052494764328 batch: 165/224\n",
      "Batch loss: 0.06276541203260422 batch: 166/224\n",
      "Batch loss: 0.04883793741464615 batch: 167/224\n",
      "Batch loss: 0.051795922219753265 batch: 168/224\n",
      "Batch loss: 0.05590660125017166 batch: 169/224\n",
      "Batch loss: 0.04203672334551811 batch: 170/224\n",
      "Batch loss: 0.029140694066882133 batch: 171/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.04210337996482849 batch: 172/224\n",
      "Batch loss: 0.05196399614214897 batch: 173/224\n",
      "Batch loss: 0.0576925165951252 batch: 174/224\n",
      "Batch loss: 0.032251354306936264 batch: 175/224\n",
      "Batch loss: 0.057517170906066895 batch: 176/224\n",
      "Batch loss: 0.053596217185258865 batch: 177/224\n",
      "Batch loss: 0.05135011300444603 batch: 178/224\n",
      "Batch loss: 0.06810862571001053 batch: 179/224\n",
      "Batch loss: 0.03998994082212448 batch: 180/224\n",
      "Batch loss: 0.06514796614646912 batch: 181/224\n",
      "Batch loss: 0.07837648689746857 batch: 182/224\n",
      "Batch loss: 0.049694206565618515 batch: 183/224\n",
      "Batch loss: 0.07130581885576248 batch: 184/224\n",
      "Batch loss: 0.051081717014312744 batch: 185/224\n",
      "Batch loss: 0.05774804204702377 batch: 186/224\n",
      "Batch loss: 0.06751081347465515 batch: 187/224\n",
      "Batch loss: 0.05995521321892738 batch: 188/224\n",
      "Batch loss: 0.03565050661563873 batch: 189/224\n",
      "Batch loss: 0.02311892621219158 batch: 190/224\n",
      "Batch loss: 0.03450709953904152 batch: 191/224\n",
      "Batch loss: 0.06825552135705948 batch: 192/224\n",
      "Batch loss: 0.05800173431634903 batch: 193/224\n",
      "Batch loss: 0.06272994726896286 batch: 194/224\n",
      "Batch loss: 0.038848139345645905 batch: 195/224\n",
      "Batch loss: 0.027053818106651306 batch: 196/224\n",
      "Batch loss: 0.04736676067113876 batch: 197/224\n",
      "Batch loss: 0.045521330088377 batch: 198/224\n",
      "Batch loss: 0.05099383369088173 batch: 199/224\n",
      "Batch loss: 0.03823668509721756 batch: 200/224\n",
      "Batch loss: 0.05770004540681839 batch: 201/224\n",
      "Batch loss: 0.0459161102771759 batch: 202/224\n",
      "Batch loss: 0.06339213252067566 batch: 203/224\n",
      "Batch loss: 0.05451338365674019 batch: 204/224\n",
      "Batch loss: 0.0685596913099289 batch: 205/224\n",
      "Batch loss: 0.035121675580739975 batch: 206/224\n",
      "Batch loss: 0.05680633708834648 batch: 207/224\n",
      "Batch loss: 0.06451871991157532 batch: 208/224\n",
      "Batch loss: 0.047458749264478683 batch: 209/224\n",
      "Batch loss: 0.04667429253458977 batch: 210/224\n",
      "Batch loss: 0.03800944611430168 batch: 211/224\n",
      "Batch loss: 0.053578633815050125 batch: 212/224\n",
      "Batch loss: 0.06840337067842484 batch: 213/224\n",
      "Batch loss: 0.06665626168251038 batch: 214/224\n",
      "Batch loss: 0.09228614717721939 batch: 215/224\n",
      "Batch loss: 0.05017262324690819 batch: 216/224\n",
      "Batch loss: 0.0596696212887764 batch: 217/224\n",
      "Batch loss: 0.04607706144452095 batch: 218/224\n",
      "Batch loss: 0.0811205729842186 batch: 219/224\n",
      "Batch loss: 0.06928194314241409 batch: 220/224\n",
      "Batch loss: 0.05076819658279419 batch: 221/224\n",
      "Batch loss: 0.06417570263147354 batch: 222/224\n",
      "Batch loss: 0.025689827278256416 batch: 223/224\n",
      "Batch loss: 0.04461834952235222 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 73/75..  Training Loss: 0.00011..  Test Loss: 0.00112..  Test Accuracy: 0.89200\n",
      "Running epoch 74/75\n",
      "Batch loss: 0.03464460000395775 batch: 1/224\n",
      "Batch loss: 0.04743960499763489 batch: 2/224\n",
      "Batch loss: 0.041223857551813126 batch: 3/224\n",
      "Batch loss: 0.061585694551467896 batch: 4/224\n",
      "Batch loss: 0.06467162817716599 batch: 5/224\n",
      "Batch loss: 0.04447031393647194 batch: 6/224\n",
      "Batch loss: 0.04019836336374283 batch: 7/224\n",
      "Batch loss: 0.049843717366456985 batch: 8/224\n",
      "Batch loss: 0.04030447080731392 batch: 9/224\n",
      "Batch loss: 0.07389021664857864 batch: 10/224\n",
      "Batch loss: 0.06688497960567474 batch: 11/224\n",
      "Batch loss: 0.05152381211519241 batch: 12/224\n",
      "Batch loss: 0.0323001891374588 batch: 13/224\n",
      "Batch loss: 0.04874846339225769 batch: 14/224\n",
      "Batch loss: 0.04328983649611473 batch: 15/224\n",
      "Batch loss: 0.041749272495508194 batch: 16/224\n",
      "Batch loss: 0.04167469963431358 batch: 17/224\n",
      "Batch loss: 0.05349688231945038 batch: 18/224\n",
      "Batch loss: 0.05528128147125244 batch: 19/224\n",
      "Batch loss: 0.03818819671869278 batch: 20/224\n",
      "Batch loss: 0.041671089828014374 batch: 21/224\n",
      "Batch loss: 0.05580667033791542 batch: 22/224\n",
      "Batch loss: 0.06671200692653656 batch: 23/224\n",
      "Batch loss: 0.05794649198651314 batch: 24/224\n",
      "Batch loss: 0.04544106498360634 batch: 25/224\n",
      "Batch loss: 0.021481255069375038 batch: 26/224\n",
      "Batch loss: 0.07048959285020828 batch: 27/224\n",
      "Batch loss: 0.0760376825928688 batch: 28/224\n",
      "Batch loss: 0.09970207512378693 batch: 29/224\n",
      "Batch loss: 0.044510871171951294 batch: 30/224\n",
      "Batch loss: 0.03455779328942299 batch: 31/224\n",
      "Batch loss: 0.04818651080131531 batch: 32/224\n",
      "Batch loss: 0.030222700908780098 batch: 33/224\n",
      "Batch loss: 0.04999788850545883 batch: 34/224\n",
      "Batch loss: 0.05775945633649826 batch: 35/224\n",
      "Batch loss: 0.0498092845082283 batch: 36/224\n",
      "Batch loss: 0.05078137293457985 batch: 37/224\n",
      "Batch loss: 0.06453008949756622 batch: 38/224\n",
      "Batch loss: 0.07060841470956802 batch: 39/224\n",
      "Batch loss: 0.03464648872613907 batch: 40/224\n",
      "Batch loss: 0.05113185942173004 batch: 41/224\n",
      "Batch loss: 0.044056326150894165 batch: 42/224\n",
      "Batch loss: 0.07558636367321014 batch: 43/224\n",
      "Batch loss: 0.03721361234784126 batch: 44/224\n",
      "Batch loss: 0.035417065024375916 batch: 45/224\n",
      "Batch loss: 0.06161319091916084 batch: 46/224\n",
      "Batch loss: 0.05057991296052933 batch: 47/224\n",
      "Batch loss: 0.04167602211236954 batch: 48/224\n",
      "Batch loss: 0.05476855859160423 batch: 49/224\n",
      "Batch loss: 0.041525810956954956 batch: 50/224\n",
      "Batch loss: 0.055974338203668594 batch: 51/224\n",
      "Batch loss: 0.027503106743097305 batch: 52/224\n",
      "Batch loss: 0.0396430566906929 batch: 53/224\n",
      "Batch loss: 0.04305839538574219 batch: 54/224\n",
      "Batch loss: 0.02396237663924694 batch: 55/224\n",
      "Batch loss: 0.05368182063102722 batch: 56/224\n",
      "Batch loss: 0.07446181774139404 batch: 57/224\n",
      "Batch loss: 0.04852566868066788 batch: 58/224\n",
      "Batch loss: 0.048525791615247726 batch: 59/224\n",
      "Batch loss: 0.04951432719826698 batch: 60/224\n",
      "Batch loss: 0.065110944211483 batch: 61/224\n",
      "Batch loss: 0.0365331694483757 batch: 62/224\n",
      "Batch loss: 0.048812370747327805 batch: 63/224\n",
      "Batch loss: 0.06481081992387772 batch: 64/224\n",
      "Batch loss: 0.029728306457400322 batch: 65/224\n",
      "Batch loss: 0.037896573543548584 batch: 66/224\n",
      "Batch loss: 0.05288165062665939 batch: 67/224\n",
      "Batch loss: 0.06116129085421562 batch: 68/224\n",
      "Batch loss: 0.03277445212006569 batch: 69/224\n",
      "Batch loss: 0.04889002814888954 batch: 70/224\n",
      "Batch loss: 0.04724731296300888 batch: 71/224\n",
      "Batch loss: 0.04216482490301132 batch: 72/224\n",
      "Batch loss: 0.06880663335323334 batch: 73/224\n",
      "Batch loss: 0.04432162269949913 batch: 74/224\n",
      "Batch loss: 0.04666128382086754 batch: 75/224\n",
      "Batch loss: 0.05804916471242905 batch: 76/224\n",
      "Batch loss: 0.0706503838300705 batch: 77/224\n",
      "Batch loss: 0.032528601586818695 batch: 78/224\n",
      "Batch loss: 0.07606060802936554 batch: 79/224\n",
      "Batch loss: 0.02715679630637169 batch: 80/224\n",
      "Batch loss: 0.0714724212884903 batch: 81/224\n",
      "Batch loss: 0.0775214359164238 batch: 82/224\n",
      "Batch loss: 0.06356869637966156 batch: 83/224\n",
      "Batch loss: 0.059178438037633896 batch: 84/224\n",
      "Batch loss: 0.03705894947052002 batch: 85/224\n",
      "Batch loss: 0.045253679156303406 batch: 86/224\n",
      "Batch loss: 0.026431979611516 batch: 87/224\n",
      "Batch loss: 0.05951865389943123 batch: 88/224\n",
      "Batch loss: 0.05439803749322891 batch: 89/224\n",
      "Batch loss: 0.0422859825193882 batch: 90/224\n",
      "Batch loss: 0.05302799120545387 batch: 91/224\n",
      "Batch loss: 0.04826777055859566 batch: 92/224\n",
      "Batch loss: 0.022325515747070312 batch: 93/224\n",
      "Batch loss: 0.02924741804599762 batch: 94/224\n",
      "Batch loss: 0.04336642846465111 batch: 95/224\n",
      "Batch loss: 0.03689362108707428 batch: 96/224\n",
      "Batch loss: 0.04144928231835365 batch: 97/224\n",
      "Batch loss: 0.024923300370573997 batch: 98/224\n",
      "Batch loss: 0.06871424615383148 batch: 99/224\n",
      "Batch loss: 0.05128384381532669 batch: 100/224\n",
      "Batch loss: 0.06346391886472702 batch: 101/224\n",
      "Batch loss: 0.04014831408858299 batch: 102/224\n",
      "Batch loss: 0.09086121618747711 batch: 103/224\n",
      "Batch loss: 0.03313251584768295 batch: 104/224\n",
      "Batch loss: 0.04558286815881729 batch: 105/224\n",
      "Batch loss: 0.07221977412700653 batch: 106/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.050059136003255844 batch: 107/224\n",
      "Batch loss: 0.04854460060596466 batch: 108/224\n",
      "Batch loss: 0.04576338827610016 batch: 109/224\n",
      "Batch loss: 0.029850296676158905 batch: 110/224\n",
      "Batch loss: 0.05453507974743843 batch: 111/224\n",
      "Batch loss: 0.05531250685453415 batch: 112/224\n",
      "Batch loss: 0.052627548575401306 batch: 113/224\n",
      "Batch loss: 0.027456870302557945 batch: 114/224\n",
      "Batch loss: 0.038795556873083115 batch: 115/224\n",
      "Batch loss: 0.045147258788347244 batch: 116/224\n",
      "Batch loss: 0.026734475046396255 batch: 117/224\n",
      "Batch loss: 0.04805969074368477 batch: 118/224\n",
      "Batch loss: 0.057176049798727036 batch: 119/224\n",
      "Batch loss: 0.08211410790681839 batch: 120/224\n",
      "Batch loss: 0.042504020035266876 batch: 121/224\n",
      "Batch loss: 0.0564974844455719 batch: 122/224\n",
      "Batch loss: 0.025654826313257217 batch: 123/224\n",
      "Batch loss: 0.04030594602227211 batch: 124/224\n",
      "Batch loss: 0.07654423266649246 batch: 125/224\n",
      "Batch loss: 0.035652682185173035 batch: 126/224\n",
      "Batch loss: 0.06892841309309006 batch: 127/224\n",
      "Batch loss: 0.039745647460222244 batch: 128/224\n",
      "Batch loss: 0.036942530423402786 batch: 129/224\n",
      "Batch loss: 0.05787628889083862 batch: 130/224\n",
      "Batch loss: 0.04152008518576622 batch: 131/224\n",
      "Batch loss: 0.06052113324403763 batch: 132/224\n",
      "Batch loss: 0.047237735241651535 batch: 133/224\n",
      "Batch loss: 0.055717453360557556 batch: 134/224\n",
      "Batch loss: 0.032740358263254166 batch: 135/224\n",
      "Batch loss: 0.05045095831155777 batch: 136/224\n",
      "Batch loss: 0.034063830971717834 batch: 137/224\n",
      "Batch loss: 0.053114473819732666 batch: 138/224\n",
      "Batch loss: 0.07509876042604446 batch: 139/224\n",
      "Batch loss: 0.06488858163356781 batch: 140/224\n",
      "Batch loss: 0.048553790897130966 batch: 141/224\n",
      "Batch loss: 0.042757466435432434 batch: 142/224\n",
      "Batch loss: 0.029734771698713303 batch: 143/224\n",
      "Batch loss: 0.03736196830868721 batch: 144/224\n",
      "Batch loss: 0.0403786301612854 batch: 145/224\n",
      "Batch loss: 0.06966816633939743 batch: 146/224\n",
      "Batch loss: 0.0728318840265274 batch: 147/224\n",
      "Batch loss: 0.02949429117143154 batch: 148/224\n",
      "Batch loss: 0.06290465593338013 batch: 149/224\n",
      "Batch loss: 0.043751634657382965 batch: 150/224\n",
      "Batch loss: 0.02506742626428604 batch: 151/224\n",
      "Batch loss: 0.09223102778196335 batch: 152/224\n",
      "Batch loss: 0.05163393169641495 batch: 153/224\n",
      "Batch loss: 0.04132678732275963 batch: 154/224\n",
      "Batch loss: 0.04364476352930069 batch: 155/224\n",
      "Batch loss: 0.028967127203941345 batch: 156/224\n",
      "Batch loss: 0.04775755852460861 batch: 157/224\n",
      "Batch loss: 0.09922271221876144 batch: 158/224\n",
      "Batch loss: 0.07428938895463943 batch: 159/224\n",
      "Batch loss: 0.05572832375764847 batch: 160/224\n",
      "Batch loss: 0.036297757178545 batch: 161/224\n",
      "Batch loss: 0.03733168914914131 batch: 162/224\n",
      "Batch loss: 0.029616938903927803 batch: 163/224\n",
      "Batch loss: 0.051497213542461395 batch: 164/224\n",
      "Batch loss: 0.05241527780890465 batch: 165/224\n",
      "Batch loss: 0.051168933510780334 batch: 166/224\n",
      "Batch loss: 0.05964890122413635 batch: 167/224\n",
      "Batch loss: 0.027362380176782608 batch: 168/224\n",
      "Batch loss: 0.067929208278656 batch: 169/224\n",
      "Batch loss: 0.05718246102333069 batch: 170/224\n",
      "Batch loss: 0.038875121623277664 batch: 171/224\n",
      "Batch loss: 0.04351085424423218 batch: 172/224\n",
      "Batch loss: 0.06636375933885574 batch: 173/224\n",
      "Batch loss: 0.040747519582509995 batch: 174/224\n",
      "Batch loss: 0.04287153109908104 batch: 175/224\n",
      "Batch loss: 0.027117852121591568 batch: 176/224\n",
      "Batch loss: 0.06757419556379318 batch: 177/224\n",
      "Batch loss: 0.03311615064740181 batch: 178/224\n",
      "Batch loss: 0.07671944797039032 batch: 179/224\n",
      "Batch loss: 0.03481493145227432 batch: 180/224\n",
      "Batch loss: 0.03203406557440758 batch: 181/224\n",
      "Batch loss: 0.05231017246842384 batch: 182/224\n",
      "Batch loss: 0.06818430870771408 batch: 183/224\n",
      "Batch loss: 0.044572580605745316 batch: 184/224\n",
      "Batch loss: 0.07669459283351898 batch: 185/224\n",
      "Batch loss: 0.056057389825582504 batch: 186/224\n",
      "Batch loss: 0.06917029619216919 batch: 187/224\n",
      "Batch loss: 0.04245639964938164 batch: 188/224\n",
      "Batch loss: 0.050104957073926926 batch: 189/224\n",
      "Batch loss: 0.06203271076083183 batch: 190/224\n",
      "Batch loss: 0.04927156865596771 batch: 191/224\n",
      "Batch loss: 0.05299504101276398 batch: 192/224\n",
      "Batch loss: 0.058565832674503326 batch: 193/224\n",
      "Batch loss: 0.05195476859807968 batch: 194/224\n",
      "Batch loss: 0.058747511357069016 batch: 195/224\n",
      "Batch loss: 0.03422768786549568 batch: 196/224\n",
      "Batch loss: 0.028814757242798805 batch: 197/224\n",
      "Batch loss: 0.044769227504730225 batch: 198/224\n",
      "Batch loss: 0.03705630823969841 batch: 199/224\n",
      "Batch loss: 0.032504644244909286 batch: 200/224\n",
      "Batch loss: 0.04654224216938019 batch: 201/224\n",
      "Batch loss: 0.07511989772319794 batch: 202/224\n",
      "Batch loss: 0.054400790482759476 batch: 203/224\n",
      "Batch loss: 0.05305946618318558 batch: 204/224\n",
      "Batch loss: 0.0732472613453865 batch: 205/224\n",
      "Batch loss: 0.06947225332260132 batch: 206/224\n",
      "Batch loss: 0.043371837586164474 batch: 207/224\n",
      "Batch loss: 0.04536518454551697 batch: 208/224\n",
      "Batch loss: 0.06439622491598129 batch: 209/224\n",
      "Batch loss: 0.05592266842722893 batch: 210/224\n",
      "Batch loss: 0.026105470955371857 batch: 211/224\n",
      "Batch loss: 0.048484694212675095 batch: 212/224\n",
      "Batch loss: 0.060522858053445816 batch: 213/224\n",
      "Batch loss: 0.056696999818086624 batch: 214/224\n",
      "Batch loss: 0.07113978266716003 batch: 215/224\n",
      "Batch loss: 0.03575696051120758 batch: 216/224\n",
      "Batch loss: 0.05561889335513115 batch: 217/224\n",
      "Batch loss: 0.031525272876024246 batch: 218/224\n",
      "Batch loss: 0.0491969920694828 batch: 219/224\n",
      "Batch loss: 0.04194047674536705 batch: 220/224\n",
      "Batch loss: 0.05582963302731514 batch: 221/224\n",
      "Batch loss: 0.07137006521224976 batch: 222/224\n",
      "Batch loss: 0.02771834284067154 batch: 223/224\n",
      "Batch loss: 0.05347384512424469 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 74/75..  Training Loss: 0.00010..  Test Loss: 0.00109..  Test Accuracy: 0.89246\n",
      "Running epoch 75/75\n",
      "Batch loss: 0.07073556631803513 batch: 1/224\n",
      "Batch loss: 0.06808004528284073 batch: 2/224\n",
      "Batch loss: 0.0555897057056427 batch: 3/224\n",
      "Batch loss: 0.05942395702004433 batch: 4/224\n",
      "Batch loss: 0.06343574821949005 batch: 5/224\n",
      "Batch loss: 0.04281862825155258 batch: 6/224\n",
      "Batch loss: 0.03896012529730797 batch: 7/224\n",
      "Batch loss: 0.06638113409280777 batch: 8/224\n",
      "Batch loss: 0.02800954319536686 batch: 9/224\n",
      "Batch loss: 0.032356295734643936 batch: 10/224\n",
      "Batch loss: 0.0592343844473362 batch: 11/224\n",
      "Batch loss: 0.05349783971905708 batch: 12/224\n",
      "Batch loss: 0.053236689418554306 batch: 13/224\n",
      "Batch loss: 0.0384867824614048 batch: 14/224\n",
      "Batch loss: 0.04550439491868019 batch: 15/224\n",
      "Batch loss: 0.03659888356924057 batch: 16/224\n",
      "Batch loss: 0.046271488070487976 batch: 17/224\n",
      "Batch loss: 0.040257345885038376 batch: 18/224\n",
      "Batch loss: 0.0514841303229332 batch: 19/224\n",
      "Batch loss: 0.06570679694414139 batch: 20/224\n",
      "Batch loss: 0.06325051188468933 batch: 21/224\n",
      "Batch loss: 0.06971967220306396 batch: 22/224\n",
      "Batch loss: 0.050404999405145645 batch: 23/224\n",
      "Batch loss: 0.049660470336675644 batch: 24/224\n",
      "Batch loss: 0.042894408106803894 batch: 25/224\n",
      "Batch loss: 0.051930565387010574 batch: 26/224\n",
      "Batch loss: 0.047821059823036194 batch: 27/224\n",
      "Batch loss: 0.057162556797266006 batch: 28/224\n",
      "Batch loss: 0.04901127889752388 batch: 29/224\n",
      "Batch loss: 0.04849535971879959 batch: 30/224\n",
      "Batch loss: 0.04239626228809357 batch: 31/224\n",
      "Batch loss: 0.04834363982081413 batch: 32/224\n",
      "Batch loss: 0.042976249009370804 batch: 33/224\n",
      "Batch loss: 0.04228876903653145 batch: 34/224\n",
      "Batch loss: 0.05947335064411163 batch: 35/224\n",
      "Batch loss: 0.046381764113903046 batch: 36/224\n",
      "Batch loss: 0.040347643196582794 batch: 37/224\n",
      "Batch loss: 0.044505611062049866 batch: 38/224\n",
      "Batch loss: 0.049789972603321075 batch: 39/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.03237288445234299 batch: 40/224\n",
      "Batch loss: 0.06333883106708527 batch: 41/224\n",
      "Batch loss: 0.03172579035162926 batch: 42/224\n",
      "Batch loss: 0.05337534099817276 batch: 43/224\n",
      "Batch loss: 0.04885147511959076 batch: 44/224\n",
      "Batch loss: 0.028364328667521477 batch: 45/224\n",
      "Batch loss: 0.06559499353170395 batch: 46/224\n",
      "Batch loss: 0.05238613113760948 batch: 47/224\n",
      "Batch loss: 0.050307516008615494 batch: 48/224\n",
      "Batch loss: 0.04830467700958252 batch: 49/224\n",
      "Batch loss: 0.018696295097470284 batch: 50/224\n",
      "Batch loss: 0.0766160786151886 batch: 51/224\n",
      "Batch loss: 0.04006842523813248 batch: 52/224\n",
      "Batch loss: 0.05645934119820595 batch: 53/224\n",
      "Batch loss: 0.0329350046813488 batch: 54/224\n",
      "Batch loss: 0.035563092678785324 batch: 55/224\n",
      "Batch loss: 0.0515919029712677 batch: 56/224\n",
      "Batch loss: 0.0675818920135498 batch: 57/224\n",
      "Batch loss: 0.0414372980594635 batch: 58/224\n",
      "Batch loss: 0.07163408398628235 batch: 59/224\n",
      "Batch loss: 0.04935839772224426 batch: 60/224\n",
      "Batch loss: 0.08162464201450348 batch: 61/224\n",
      "Batch loss: 0.023300547152757645 batch: 62/224\n",
      "Batch loss: 0.0633481964468956 batch: 63/224\n",
      "Batch loss: 0.05151427909731865 batch: 64/224\n",
      "Batch loss: 0.04534872993826866 batch: 65/224\n",
      "Batch loss: 0.03850402310490608 batch: 66/224\n",
      "Batch loss: 0.04654446616768837 batch: 67/224\n",
      "Batch loss: 0.057610347867012024 batch: 68/224\n",
      "Batch loss: 0.08607044816017151 batch: 69/224\n",
      "Batch loss: 0.05721732974052429 batch: 70/224\n",
      "Batch loss: 0.05710292235016823 batch: 71/224\n",
      "Batch loss: 0.037238456308841705 batch: 72/224\n",
      "Batch loss: 0.06982321292161942 batch: 73/224\n",
      "Batch loss: 0.03637341037392616 batch: 74/224\n",
      "Batch loss: 0.06285961717367172 batch: 75/224\n",
      "Batch loss: 0.05761227011680603 batch: 76/224\n",
      "Batch loss: 0.06658071279525757 batch: 77/224\n",
      "Batch loss: 0.0622422881424427 batch: 78/224\n",
      "Batch loss: 0.07020848244428635 batch: 79/224\n",
      "Batch loss: 0.03778385370969772 batch: 80/224\n",
      "Batch loss: 0.06467045843601227 batch: 81/224\n",
      "Batch loss: 0.08098724484443665 batch: 82/224\n",
      "Batch loss: 0.06099734455347061 batch: 83/224\n",
      "Batch loss: 0.06403923034667969 batch: 84/224\n",
      "Batch loss: 0.05804550647735596 batch: 85/224\n",
      "Batch loss: 0.05643966794013977 batch: 86/224\n",
      "Batch loss: 0.031582850962877274 batch: 87/224\n",
      "Batch loss: 0.06647362560033798 batch: 88/224\n",
      "Batch loss: 0.027005504816770554 batch: 89/224\n",
      "Batch loss: 0.07148860394954681 batch: 90/224\n",
      "Batch loss: 0.039135511964559555 batch: 91/224\n",
      "Batch loss: 0.06037173792719841 batch: 92/224\n",
      "Batch loss: 0.029575111344456673 batch: 93/224\n",
      "Batch loss: 0.059948455542325974 batch: 94/224\n",
      "Batch loss: 0.04745824262499809 batch: 95/224\n",
      "Batch loss: 0.048952411860227585 batch: 96/224\n",
      "Batch loss: 0.04873263090848923 batch: 97/224\n",
      "Batch loss: 0.033125534653663635 batch: 98/224\n",
      "Batch loss: 0.06650074571371078 batch: 99/224\n",
      "Batch loss: 0.0653107687830925 batch: 100/224\n",
      "Batch loss: 0.08097857981920242 batch: 101/224\n",
      "Batch loss: 0.05047650635242462 batch: 102/224\n",
      "Batch loss: 0.07007492333650589 batch: 103/224\n",
      "Batch loss: 0.054719921201467514 batch: 104/224\n",
      "Batch loss: 0.06631024181842804 batch: 105/224\n",
      "Batch loss: 0.07667402923107147 batch: 106/224\n",
      "Batch loss: 0.06935785710811615 batch: 107/224\n",
      "Batch loss: 0.034785252064466476 batch: 108/224\n",
      "Batch loss: 0.037926215678453445 batch: 109/224\n",
      "Batch loss: 0.03541547432541847 batch: 110/224\n",
      "Batch loss: 0.07016492635011673 batch: 111/224\n",
      "Batch loss: 0.023137670010328293 batch: 112/224\n",
      "Batch loss: 0.04238303378224373 batch: 113/224\n",
      "Batch loss: 0.03817871958017349 batch: 114/224\n",
      "Batch loss: 0.04193372651934624 batch: 115/224\n",
      "Batch loss: 0.0571443997323513 batch: 116/224\n",
      "Batch loss: 0.021076476201415062 batch: 117/224\n",
      "Batch loss: 0.0632086917757988 batch: 118/224\n",
      "Batch loss: 0.036522869020700455 batch: 119/224\n",
      "Batch loss: 0.05940500646829605 batch: 120/224\n",
      "Batch loss: 0.028874551877379417 batch: 121/224\n",
      "Batch loss: 0.05398780107498169 batch: 122/224\n",
      "Batch loss: 0.026950394734740257 batch: 123/224\n",
      "Batch loss: 0.028058858588337898 batch: 124/224\n",
      "Batch loss: 0.039804812520742416 batch: 125/224\n",
      "Batch loss: 0.04877813160419464 batch: 126/224\n",
      "Batch loss: 0.07937327772378922 batch: 127/224\n",
      "Batch loss: 0.04021945968270302 batch: 128/224\n",
      "Batch loss: 0.017800966277718544 batch: 129/224\n",
      "Batch loss: 0.049250926822423935 batch: 130/224\n",
      "Batch loss: 0.03827789053320885 batch: 131/224\n",
      "Batch loss: 0.05084800720214844 batch: 132/224\n",
      "Batch loss: 0.05145186558365822 batch: 133/224\n",
      "Batch loss: 0.06863570213317871 batch: 134/224\n",
      "Batch loss: 0.044905658811330795 batch: 135/224\n",
      "Batch loss: 0.04627050831913948 batch: 136/224\n",
      "Batch loss: 0.03851162642240524 batch: 137/224\n",
      "Batch loss: 0.06241542100906372 batch: 138/224\n",
      "Batch loss: 0.06623058021068573 batch: 139/224\n",
      "Batch loss: 0.08276568353176117 batch: 140/224\n",
      "Batch loss: 0.043784186244010925 batch: 141/224\n",
      "Batch loss: 0.04015519469976425 batch: 142/224\n",
      "Batch loss: 0.04147927090525627 batch: 143/224\n",
      "Batch loss: 0.03806937858462334 batch: 144/224\n",
      "Batch loss: 0.057219695299863815 batch: 145/224\n",
      "Batch loss: 0.08913090825080872 batch: 146/224\n",
      "Batch loss: 0.031428951770067215 batch: 147/224\n",
      "Batch loss: 0.032755181193351746 batch: 148/224\n",
      "Batch loss: 0.06743259727954865 batch: 149/224\n",
      "Batch loss: 0.07095698267221451 batch: 150/224\n",
      "Batch loss: 0.03847586363554001 batch: 151/224\n",
      "Batch loss: 0.05696754530072212 batch: 152/224\n",
      "Batch loss: 0.053017787635326385 batch: 153/224\n",
      "Batch loss: 0.0577567033469677 batch: 154/224\n",
      "Batch loss: 0.0192501712590456 batch: 155/224\n",
      "Batch loss: 0.040186475962400436 batch: 156/224\n",
      "Batch loss: 0.04221450537443161 batch: 157/224\n",
      "Batch loss: 0.0954701229929924 batch: 158/224\n",
      "Batch loss: 0.04081517830491066 batch: 159/224\n",
      "Batch loss: 0.06819634884595871 batch: 160/224\n",
      "Batch loss: 0.03289185091853142 batch: 161/224\n",
      "Batch loss: 0.05125580355525017 batch: 162/224\n",
      "Batch loss: 0.03707420825958252 batch: 163/224\n",
      "Batch loss: 0.035547081381082535 batch: 164/224\n",
      "Batch loss: 0.0695989653468132 batch: 165/224\n",
      "Batch loss: 0.059788018465042114 batch: 166/224\n",
      "Batch loss: 0.050230078399181366 batch: 167/224\n",
      "Batch loss: 0.06306196749210358 batch: 168/224\n",
      "Batch loss: 0.05021970346570015 batch: 169/224\n",
      "Batch loss: 0.04895586147904396 batch: 170/224\n",
      "Batch loss: 0.033112309873104095 batch: 171/224\n",
      "Batch loss: 0.04556627199053764 batch: 172/224\n",
      "Batch loss: 0.039764948189258575 batch: 173/224\n",
      "Batch loss: 0.03349832445383072 batch: 174/224\n",
      "Batch loss: 0.0593070313334465 batch: 175/224\n",
      "Batch loss: 0.0757623016834259 batch: 176/224\n",
      "Batch loss: 0.052243366837501526 batch: 177/224\n",
      "Batch loss: 0.04204326122999191 batch: 178/224\n",
      "Batch loss: 0.05671367421746254 batch: 179/224\n",
      "Batch loss: 0.04924725368618965 batch: 180/224\n",
      "Batch loss: 0.024338796734809875 batch: 181/224\n",
      "Batch loss: 0.05257062986493111 batch: 182/224\n",
      "Batch loss: 0.0664450079202652 batch: 183/224\n",
      "Batch loss: 0.04601437225937843 batch: 184/224\n",
      "Batch loss: 0.07352092117071152 batch: 185/224\n",
      "Batch loss: 0.04415450617671013 batch: 186/224\n",
      "Batch loss: 0.064170703291893 batch: 187/224\n",
      "Batch loss: 0.03806215524673462 batch: 188/224\n",
      "Batch loss: 0.05796293169260025 batch: 189/224\n",
      "Batch loss: 0.03507913276553154 batch: 190/224\n",
      "Batch loss: 0.05639990419149399 batch: 191/224\n",
      "Batch loss: 0.058581024408340454 batch: 192/224\n",
      "Batch loss: 0.045858751982450485 batch: 193/224\n",
      "Batch loss: 0.07691594958305359 batch: 194/224\n",
      "Batch loss: 0.038699351251125336 batch: 195/224\n",
      "Batch loss: 0.06394830346107483 batch: 196/224\n",
      "Batch loss: 0.038948141038417816 batch: 197/224\n",
      "Batch loss: 0.045568421483039856 batch: 198/224\n",
      "Batch loss: 0.06146819517016411 batch: 199/224\n",
      "Batch loss: 0.07365375012159348 batch: 200/224\n",
      "Batch loss: 0.07324592024087906 batch: 201/224\n",
      "Batch loss: 0.051223792135715485 batch: 202/224\n",
      "Batch loss: 0.062169913202524185 batch: 203/224\n",
      "Batch loss: 0.05520923435688019 batch: 204/224\n",
      "Batch loss: 0.06579113751649857 batch: 205/224\n",
      "Batch loss: 0.046922117471694946 batch: 206/224\n",
      "Batch loss: 0.044575419276952744 batch: 207/224\n",
      "Batch loss: 0.03978462517261505 batch: 208/224\n",
      "Batch loss: 0.03799125552177429 batch: 209/224\n",
      "Batch loss: 0.04544598609209061 batch: 210/224\n",
      "Batch loss: 0.029554078355431557 batch: 211/224\n",
      "Batch loss: 0.04962018132209778 batch: 212/224\n",
      "Batch loss: 0.08424610644578934 batch: 213/224\n",
      "Batch loss: 0.02811131253838539 batch: 214/224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.0631348118185997 batch: 215/224\n",
      "Batch loss: 0.05659375339746475 batch: 216/224\n",
      "Batch loss: 0.05056965723633766 batch: 217/224\n",
      "Batch loss: 0.042137276381254196 batch: 218/224\n",
      "Batch loss: 0.04461299628019333 batch: 219/224\n",
      "Batch loss: 0.03990843892097473 batch: 220/224\n",
      "Batch loss: 0.06067013368010521 batch: 221/224\n",
      "Batch loss: 0.055344078689813614 batch: 222/224\n",
      "Batch loss: 0.052918631583452225 batch: 223/224\n",
      "Batch loss: 0.025598084554076195 batch: 224/224\n",
      "Running evaluation loop...\n",
      "batch: 1/56\n",
      "batch: 2/56\n",
      "batch: 3/56\n",
      "batch: 4/56\n",
      "batch: 5/56\n",
      "batch: 6/56\n",
      "batch: 7/56\n",
      "batch: 8/56\n",
      "batch: 9/56\n",
      "batch: 10/56\n",
      "batch: 11/56\n",
      "batch: 12/56\n",
      "batch: 13/56\n",
      "batch: 14/56\n",
      "batch: 15/56\n",
      "batch: 16/56\n",
      "batch: 17/56\n",
      "batch: 18/56\n",
      "batch: 19/56\n",
      "batch: 20/56\n",
      "batch: 21/56\n",
      "batch: 22/56\n",
      "batch: 23/56\n",
      "batch: 24/56\n",
      "batch: 25/56\n",
      "batch: 26/56\n",
      "batch: 27/56\n",
      "batch: 28/56\n",
      "batch: 29/56\n",
      "batch: 30/56\n",
      "batch: 31/56\n",
      "batch: 32/56\n",
      "batch: 33/56\n",
      "batch: 34/56\n",
      "batch: 35/56\n",
      "batch: 36/56\n",
      "batch: 37/56\n",
      "batch: 38/56\n",
      "batch: 39/56\n",
      "batch: 40/56\n",
      "batch: 41/56\n",
      "batch: 42/56\n",
      "batch: 43/56\n",
      "batch: 44/56\n",
      "batch: 45/56\n",
      "batch: 46/56\n",
      "batch: 47/56\n",
      "batch: 48/56\n",
      "batch: 49/56\n",
      "batch: 50/56\n",
      "batch: 51/56\n",
      "batch: 52/56\n",
      "batch: 53/56\n",
      "batch: 54/56\n",
      "batch: 55/56\n",
      "batch: 56/56\n",
      "Epoch: 75/75..  Training Loss: 0.00010..  Test Loss: 0.00114..  Test Accuracy: 0.89196\n"
     ]
    }
   ],
   "source": [
    "#Train LIGHT network\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(torchModelLight.TorchModel.parameters(), lr=0.0015)\n",
    "#optimizer = optim.Adam(torchModelLight.TorchModel.parameters(), lr=0.0035)\n",
    "\n",
    "epochs = 75\n",
    "steps = 0\n",
    "#train_losses, test_losses = [], []\n",
    "\n",
    "torchModelLight.to(device)\n",
    "#optimizer.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f'Running epoch {e+1}/{epochs}')\n",
    "    for images, labels in mMiniBatcherTrain.getBatchIterator():\n",
    "        #print('Training batch...')\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        #print(f'labels.shape: {labels.shape}')\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        images = torch.from_numpy(images).to(device)\n",
    "        \n",
    "        #print('Running torch')\n",
    "        output = torchModelLight(images)\n",
    "        \n",
    "        #print(f'output.shape: {output.shape}')\n",
    "        #print('Calculating loss')\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        #print('Back prop.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        print(f'Batch loss: {batch_loss} {mMiniBatcherTrain.getBatchInfo()}')\n",
    "        running_loss += batch_loss\n",
    "        \n",
    "        del labels\n",
    "        del images\n",
    "        del output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "   \n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    print('Running evaluation loop...')\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        torchModelLight.eval()\n",
    "        for images, labels in mMiniBatcherTest.getBatchIterator():\n",
    "            #print('Validation batch...')\n",
    "            #labels = torch.from_numpy(labels.values).type(torch.FloatTensor)\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "            images = torch.from_numpy(images).to(device)\n",
    "            output = torchModelLight(images)\n",
    "            test_loss += criterion(output, labels).to('cpu') #Want the loss on CPU\n",
    "\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "            #print(top_p)\n",
    "            #top_p_target, top_class_target = labels.topk(1, dim=1)\n",
    "            #equals = top_class == top_class_target\n",
    "            equals = top_class == labels.view(top_class.shape)\n",
    "            accuracy += torch.sum(equals.type(torch.FloatTensor)).to('cpu')\n",
    "            print(mMiniBatcherTest.getBatchInfo())\n",
    "            \n",
    "            del labels\n",
    "            del images\n",
    "            del output\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() \n",
    "\n",
    "    torchModelLight.train()\n",
    "\n",
    "    train_losses.append(running_loss/len(mMiniBatcherTrain.X))\n",
    "    test_losses.append(test_loss/len(mMiniBatcherTest.X))\n",
    "\n",
    "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "          \"Training Loss: {:.5f}.. \".format(train_losses[-1]),\n",
    "          \"Test Loss: {:.5f}.. \".format(test_losses[-1]),\n",
    "          \"Test Accuracy: {:.5f}\".format(accuracy/len(mMiniBatcherTest.X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "corrected-montreal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1dd019d9280>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-finder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-smoke",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "false-patrol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchModel.Bert.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "handy-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4626, -1.5387, -1.6581,  5.5220, -1.1519, -1.9339, -0.4945],\n",
       "        [-2.4869, -2.2417, -0.1596,  5.9095, -0.8429, -2.5162, -0.1103],\n",
       "        [-2.2634, -1.6346,  0.2437,  5.0969, -1.4610, -0.9196,  0.4518],\n",
       "        [-1.4876, -2.0991, -1.3574,  5.9189, -0.3710, -3.3775, -0.0616],\n",
       "        [-1.9867, -2.1058, -0.8074,  5.6330, -0.1044, -3.1820, -0.1562],\n",
       "        [-2.2982, -2.8106, -0.7049,  5.6376, -0.9757, -1.1950, -0.0750],\n",
       "        [-1.3132, -0.8251, -0.8038,  4.2634, -0.0722, -2.5963,  0.0976],\n",
       "        [-1.7288, -1.0971, -0.2127,  5.4893, -1.3920, -1.5332,  0.2278],\n",
       "        [-2.0425, -1.3268, -1.0715,  5.6121, -0.3769, -1.5521,  0.0577],\n",
       "        [-1.2958, -1.1475, -0.4645,  3.3163, -0.1179, -1.1458, -0.0809],\n",
       "        [-2.9055, -2.2610, -0.9447,  6.2404, -0.8267, -2.1755,  0.2848],\n",
       "        [-2.0705, -1.8187, -1.2083,  5.5571, -0.0396, -2.7881,  0.2048],\n",
       "        [-1.7872, -2.5268, -1.0414,  4.4269, -0.7507, -0.7875,  0.1495],\n",
       "        [-2.2243, -2.2796, -0.6230,  5.6976, -1.0154, -1.8560,  0.7660],\n",
       "        [-2.0808, -2.6473, -0.5863,  5.9454, -0.7398, -2.9108, -0.1682],\n",
       "        [-1.1619, -0.5904, -0.0986,  3.2129, -1.1888, -0.2766,  0.5494],\n",
       "        [-2.0235, -1.0664, -0.0868,  5.7351, -0.2674, -1.5629,  0.4473],\n",
       "        [-2.3887, -2.2231,  0.0325,  5.7711, -0.7572, -2.3600, -0.0986],\n",
       "        [-1.7973, -2.4545, -1.1000,  5.7570, -0.9475, -1.9836, -0.5926],\n",
       "        [-1.5662, -1.6547, -1.4485,  3.7843, -1.4425, -1.8323,  0.2154],\n",
       "        [-0.4742, -1.3688, -0.9707,  4.1654,  0.0107, -1.2688, -0.6327],\n",
       "        [-3.1214, -3.0270, -0.7252,  6.3862, -1.0545, -1.4159,  0.7737],\n",
       "        [-2.4808, -2.2597, -0.4638,  6.2402, -0.3606, -2.1318,  0.1496],\n",
       "        [-2.0498, -2.1140, -1.6466,  5.2267, -2.0208, -1.0849, -0.2112],\n",
       "        [-2.6305, -2.2619, -0.7004,  6.7615, -0.9292, -2.4601, -0.0890]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchModel(X_test[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "exclusive-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "targets = []\n",
    "for X_batch, labels in mMiniBatcherTest.getBatchIterator():\n",
    "  \n",
    "        X_batch = torch.tensor(X_batch).to(device)\n",
    "        output = torchModelLight.predict(X_batch)\n",
    "        Y_pred.extend(output)\n",
    "        targets.extend(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "restricted-mixer",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(Y_pred)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "complex-consultancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 2, 6, ..., 6, 2, 6], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y_pred = torchModel.predict(X_test[200:300])\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "grand-lloyd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 4, ..., 6, 2, 6], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#targets = Y_test[200:300]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "existing-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "000000. kund ringer in, har råkat dubbelköpa en biljett i appen. xxxx biljett-id: pj0wpwz. gör ett återköp av den ena biljetten. kund får återköpskvitto.\n"
     ]
    }
   ],
   "source": [
    "testId = 21\n",
    "print(Y_test[testId])\n",
    "print(X_test.iloc[testId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "competitive-america",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8919642857142858\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(Y_pred == targets)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adverse-austria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1dd021b0e50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(targets, Y_pred, normalize='true', labels=list(set(targets)))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(targets))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "collaborative-wallace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x26bf4440c10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAEGCAYAAADmLRl+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAySUlEQVR4nO3de3xU9Zn48c8zl1wJCSFcAyKIShFFbATBwlKtt7a/2uuutfXXdu1at9Zqu3aLP+1qpVKttGpFV7Feq0i9FmtVsCqrLIqCIKLcQa4JECCQEEgyM8/vj3MCESEzQ+acMxOe9+t1XuTM5TzfM0OenO/3fC+iqhhjTC4LBV0AY4zpKEtkxpicZ4nMGJPzLJEZY3KeJTJjTM6LBF2AtsIlxRrp3i2Q2PnrGgOJC4AEF5qAb1pLJMD/gkF+7hJM8L2x3TTH93Yo+HmfL9btO+IpvXbB4qaZqnp+R+KlIqsSWaR7N3rfcGUgsU+4dH4gcSHYX2aNxQKLDRAu7xFYbImEA4tNQN/53JppHT7G9h1x3pl5TEqvDfdZWdHhgCnIqkRmjMl+CiRIBF2MT7BEZoxJi6K0aGpVS79YIjPGpM2uyIwxOU1R4lk2tNESmTEmbYmgb3cfxBKZMSYtCsSzLJFZh1hjTNoSaEpbMiLyMxH5UESWiMgTIlIgIgNFZJ6IrBKRv4hIXrLjWCIzxqRFgRbVlLb2iEgl8FOgSlWHAWHgIuBW4HZVHQzsBC5NViZLZMaYtChKPMUtBRGgUEQiQBFQDZwFPO0+/wjw1VQOYowxqVOIp95EViEibYfNTFXVqQCquklEJgPrgb3ALGABUKeqrUNONgKVyYLkbiJLKMfc9BGxbnlsvup4+t2yjNA+p5NeZHeMfQOL2XzlYM+LUTV+N5dP3Ew4pLz0RDlPTunleUyAn932MaPO3kXd9giXn3OSLzHbCuq8AR56cQ57G8PE40IiLlx18SjfYhd3aeGnv/qQAYMbQOGOXw9j2QdlnsetPKaBCTcv3L/fu7KRx6aewIzpAz2PfTCnZ3/KalW16lBPiEg34EJgIFAHPAUc0bhMTxOZiJwP3IlT9/2Tqt6SqWOXvbKF5r6FhPY6yWvjhCH7n+tz9yoaRpRlKtRhhULKFZM2ce1Fg6itjnLXiyt5e2Yp61cWeB77lae687dHenLN7Ws9j3WwIM+71YQffpbddUnbgDPusl8sY8FbFfz2l6cSiSTIL/Cnh/um9V248pKxgPP5P/rCq8yd7d8fj08S4pkZcf8FYK2qbgMQkWeBM4EyEYm4V2X9gE3JDuRZG5mIhIG7gQuAocC3RWRoJo4d2dFMl8W72DX20+NRQ3vjFC2rZ88I72fROHFEI5s/zqNmfT6xlhCzZ5Qx+rxdnscFWPJOCfV1wQx6DvK8g1TUpYVhI3Yy669OTScWC7GnIep7OYafXkv1xiK21RT5HhtaG/slpS2J9cAZIlIkIgKcDXwEvA58033N94AZyQ7k5RXZSGCVqq4BEJHpOJeRH3X0wD2mb2Dbt/rtr0q2VbxwJ42f6Uqi0Ptf8u69W9i2+cBVQW11lCGnBTgdkE+CPm8FfnPvQlThpacrefmZfr7E7d13L7t2RvnZjUsYeHw9q5Z15b7bhtC0z98WmnHnbOZ/ZvX1NWZbTj+yjl+Rqeo8EXkaeA+IAQuBqcDfgeki8hv3sQeSHcvLu5aVwIY2+4dstBORy0RkvojMjzfsSXrQ4vfriJdEaDq2+JDPd523g/qR5UdYZJMLfvH9Kn560Sj+64oRfPlfNjLstJ2+xA2FlcFD6nnx6f789Dtj2Lc3zLd+4G/VPhJJMGrsFua81sfXuAdLqKS0JaOqN6jqEFUdpqqXqGqTqq5R1ZGqOlhVv6WqTcmOE3j3C1WdqqpVqloV7nLo5NRW4aoGit+vY+B/LqbPfWsoWlZP7/vXABCqb6Fg7R72DC/1utgAbK+J0qNv8/79ij4t1Fb7X9XwW9DnvX2r0xa3a0ceb73WgxOG7fYtbu3WfJYvKQPgf//Rm8FD/IndqmrMVlYvL6VuR76vcdtqvSJLZfOLl4lsE9C/zX5KjXbJ1H6jH2snD2ft706h+keDaBxSQs2/DQKgZMFOGoaXoVF/8vPyRUVUDmymV/8mItEE4y+s4+1Z/iTRIAV53vmFcQqLYvt/HjF6B+tWJf8DmAk7t+ezbUsBlQOcmsPwkdtZv6aLL7FbjTs32GolgCLECaW0+cXLyv27wPEiMhAngV0EXOxhPEre2cGOC/y75E7Ehbuvq2TStDWEwjBrejnrVvhz527CXWs4ZXQ9XbvF+PO8xTz2h77M/Isvk3EGet7dypu4/vbFAIQjyuwXe7Ngrj/nDXDf7z7DL36zmEg0Qc2mIu64cZhvsfMLYowYWcuU357sW8zDSaXa6CfxcqVxEfkicAdO94sHVfXm9l6ff2w/tamu/RX4VNc9bKprP82tmcaupi0dykJDTinQ+59P7QbLuIGrFxyuH1kmefppquqLwItexjDG+MvpEBt48/on5G7PfmNMYPxsyE+FJTJjTFpUhbjaFZkxJscl7IrMGJPLFKFZsyt1ZFdpjDFZzxr7jTGdQjzL+pFZIjPGpKW1Z382sURmjElbwu5aGmNymTNo3BLZYeWvawxsqNDMzYsCiQtwwaAzAosd9BAlAowf27YtsNhBDUvTWEvHj4HQogEO7zqErEpkxpjsp0rWdYjNrtIYY3KAkEhxa/coIieKyKI2224RuVpEykXkFRFZ6f6bdN56S2TGmLQozhVZKlu7x1FdrqqnquqpwGeBRuA5YALwqqoeD7zq7rfLEpkxJm0eTKx4NrBaVdfhrO3xiPu4LdBrjMk8JbX5+NN0EfCE+3MvVa12f64Bkq57Z4nMGJMWZzm4lFPHYVcabyUiecBXgGs/FUtVRSTp7K+WyIwxaUprYZHDrjTexgXAe6q6xd3fIiJ9VLVaRPoAW5MFsTYyY0xaFKdnfypbir7NgWolwPM4C/NCFizQa4zppDI1Q6yIFAPnAD9q8/AtwJMicimwDvjnZMexRGaMSYuqZGysparuAbof9Nh2nLuYKbNEZoxJi9PYb0OUMq5q/G4un7iZcEh56YlynpyS9G5thzw7tQcvTStHBAYO2cd/3L6evALnxso911cyc3o5M1Z94GkZKvo0cc3k1XSraEFVeGl6T2Y83NvTmG35/ZkfLBRS7nxyAdu35HHjFaf4Fjeo8/7ZbR8z6uxd1G2PcPk5J/kS8/Cyb85+z0ojIg+KyFYRWeJVDHD+Q18xaRPXf2cg/zb+RD5/YR3HHL/Ps3i11VH++kAFU15awdTXlxNPwOwZzgiKFe8X0rDLn79U8Zhw/6QB/Oi84fzsGyfx5Uu2cMzgRl9i+/2ZH8qFl2xkw5oiX2MGed6vPNWd6//v8b7ESsZp7JeUNr94mVYfBs738PgAnDiikc0f51GzPp9YS4jZM8oYfd4uT2PGY0LTvhDxGDTtDdG9VwvxONw/sS+XXr/Z09itdm7LY/WHxQDs3RNmw6oCuvfu+MwGqQjiM2+re699nD5uOzOf8W9VeQj2vJe8U0J9XfZU5zzo2d8hnkVS1TeAHV4dv1X33i1s25y3f7+2OkpFH+9+oSv6tPDNf9/KJacP5dunDqO4JM5nx9fz/EMVjD53N917+T8tTc/KJo47qZHli4p9ief3Z36wH01YxYO/P45EwreQQPDnnS1ae/YfLVdkKRGRy0RkvojMb6Ep6OIkVV8X5q2ZpTwy7yOmLVzCvsYwrzzVjTf/VsaF/+r//FYFRXGuv2cF900cQGNDp2jybNfIf6qlbkceqz4qCbooR7UEoZQ2vwT+P98drjAVoKuUJx2KcLDtNVF69G3ev1/Rp4Xa6mjmCniQhW92oXf/Zsq6xwE484t1/Hlyb5r3hfjBmKGAU938/pjP8PDcpZ6VAyAcSXD9PSt5/fkK5s4s9zRWW35/5m0NHbGbM8bXcvrY7UTzExQVx7nmlo+YPGGo57GDPO9sogoticCvgT4h8ETWUcsXFVE5sJle/ZvYXhNl/IV13HLFAM/i9axsYel7RexrFPILlUVzSvjGZdu48NLa/a+5cPDJnicxUK6+ZS0bVhfy3AP+thX5/Zm39fAdg3j4jkEAnHz6Tr7x/Q2+JDEI9ryziVO1tESWUYm4cPd1lUyatoZQGGZNL2fdigLP4g05rZGxX9rFFeedSDiiDB62lwu+u92zeIdzUlUDX/h6LWuXFTLlBaerxyOT+/Pu7DLPY/v9mWeLIM97wl1rOGV0PV27xfjzvMU89oe+zPxLhS+xDyVTPfszRVTTrs2ldmCRJ4DxQAWwBbhBVR9o7z1dpVxHSVodejPmaJ2zP7HP324TBwt3Szr5p2fiO3cGFjuoOfvfjs1kd2JHh7JQj6Hd9Rt//mJKr72v6rEFKQwa7zDPPk1V/bZXxzbGBMmqlsaYTiDZfPx+s0RmjEmLc9cyezrngiUyY0yaPJrqukMskRlj0mZVS2NMTmsdNJ5NLJEZY9KWbXcts6s0xpispyrENJTSloyIlInI0yKyTESWishoW2ncGOOLDM5+cSfwsqoOAYYDS7GVxo0xXsvUxIoiUgqMAx4AUNVmVa0j11cal1CIUJdgpmc5r++pgcQFWP9UcDN/HvMtb6fkTiqgoToQ3DAhgHAf/6Ylb0tqMjNbRxqN/e0t0DsQ2AY8JCLDgQXAVdhK48YYr6XZj6y9BXojwGnAlao6T0Tu5KBqZKorjVvV0hiTtgSS0pbERmCjqs5z95/GSWxb3BXGsZXGjTGeUIVYIpTS1v5xtAbYICInug+dDXyErTRujPFDBjvEXgk8LiJ5wBrgBzgXWLbSuDHGO5kca6mqi4BDtaHZSuPGGG+pDVEyxuQ6GzRujMlpqjZo3BiT84S4LQdnjMl11kZmjMlpNh+ZB6J5CW57fDHRvAThMMyZ2Z3H7vJv0dSq8bu5fOJmwiHlpSfKeXJK0mFhHdL3x8tIFIQgJGhY2HLrYMoeraZwQT0aEWK98th+RT+02Ns51f0+77YeenEOexvDxONCIi5cdfEoX+L+7LaPGXX2Luq2R7j8nJN8idmq8pgGJty8cP9+78pGHpt6AjOmD/S1HACo006WTTxLZCLSH3gUZ8Cn4gwWvTPTcVqahQnfO5l9jWHCkQSTpy1m/hvdWPZ+10yH+pRQSLli0iauvWgQtdVR7npxJW/PLGX9Sm8Xbd164yASXQ98dfuGd6HuO70hLJQ9Vk3pc1up+653q48Hdd5tTfjhZ9ldl+dbPIBXnurO3x7pyTW3r/U1LsCm9V248pKxgPP5P/rCq8yd7d8fj4Nl211LL1vsYsB/qOpQ4AzgChHxYG17YV+jc/URiSiRiPpWfz9xRCObP86jZn0+sZYQs2eUMfq8Xb7Ebmvf8BIIO+fcdHwR4e0tnsbLlvP225J3SqivC371oOGn11K9sYhtNUWBxFe3sT+VzS9eLtBbDVS7P9eLyFKgEmcsVUaFQsofn11E32P28sK0Pixf7M9UQN17t7Bt84GrgtrqKENOa/Q8bs/fOFcE9ed0Z8855Z94rsvrO9kzptTT+EGddysFfnPvQlThpacrefmZfr7FzgbjztnM/8zqG2gZjpqqZVsiciwwAph3iOcuAy4DKJDiIzp+IiH85KsjKC6J8au7lzLg+D2sW3lkx8p2WyYeR7x7lNCuGD0nriVWmU/TUOdcuz6zFQ0JjWPLgi2kx37x/Sq2by2gtLyZm+99j41ri1nyXtLZkDuFSCTBqLFbeOSeIYGWI9vuWnp+7SciXYBngKtVdffBz6vqVFWtUtWqPOlYG8ue+giL55VSNXZnh46Tqu01UXr0bd6/X9GnhdrqzExcdzjx7s7xE6UR9o7sSt4q50qo+PWdFC7Yzfar+oN4+58siPP+RPytzv+TXTvyeOu1Hpww7FP/rTqtqjFbWb28lLod+YGVQdVJZKlsfvE0kYlIFCeJPa6qz3oRo7RbC8UlMQDy8uOMGFPHhjX+tB0sX1RE5cBmevVvIhJNMP7COt6e5V21TvYlkL3x/T8XvN9AS/8CChbW03XGNrb98lg03/t2Cb/Pu638wjiFRbH9P48YvYN1qzrn1fehjDs3+GolZHTO/ozw8q6l4MzFvVRV/+BVnG49m7nmlhWEwooIvPlyBe/MLk/+xgxIxIW7r6tk0rQ1hMIwa3o561Z4d+cutCtGj9vWOTtxpfFzZewbUUKfnyxHYkrPiU7bWdMJRey8rNKzcvh93m11K2/i+tsXAxCOKLNf7M2CuRW+xJ5w1xpOGV1P124x/jxvMY/9oS8z/+JPbID8ghgjRtYy5bcn+xbzcLKtjUzUoxKJyOeAN4EPgIT78P9T1RcP957ScIWe0eUrnpQnmUR9fSBxAdY/Fdx/zKDn7A/36BFY7MROf5ogDiWoOfvn1kxjV9OWDl0qFQyu1GN/96OUXrv8GzcsaGeq64zx8q7lHMiyzibGmIzIsguy3O/Zb4zxmWbfXUtLZMaY9GXokkxEPgbqgTgQU9UqESkH/gIcC3wM/LOqttsOkF1zcRhjckKGu198XlVPbdOWlvZK44e9IhORu2gn76rqT1MtpTGm81CcTugeuhAY7/78CDAb+GV7b2ivajm/neeMMUcrBTKz0njr0Wa5i/De5z6XuZXGVfWRtvsiUqSq/g2oM8ZkrTR6bbW30jjA51R1k4j0BF4RkWWfjJOhlcZFZLSIfAQsc/eHi8g9yd5njOnENMUt2WFUN7n/bgWeA0bi0UrjdwDnAdvdgO8D41J4nzGmU0qtoT9ZY7+IFItISevPwLnAErxaaVxVN8gnByLHU3mfMaaTykz3i17Ac25uiQDTVPVlEXkXD1Ya3yAiYwB1B4FfBSw94qK3QxOJQIcKBSXIYUJjF+8LLDbA/44M7vvWWCyw2LENGwOJq5qBSTcVNAN3LVV1DTD8EI9vJ82VxlOpWl4OXIEzKeJm4FR33xhz1JIUN38kvSJT1VrgOz6UxRiTK7JssGUqdy0HicjfRGSbiGwVkRkiMsiPwhljslSG7lpmSipVy2nAk0AfoC/wFPCEl4UyxmSx1g6xqWw+SSWRFanqn1U15m6PAf6t+2WMyTqqqW1+aW+sZes0qy+JyARgOk4u/hfgsJMjGmOOAt6OtUxbe439C3ASV2uJ204JqcC1XhXKGJPdkg8a8ld7Yy0DWIvdGJP1fG7IT0VKPftFZBgwlDZtY6r6qFeFMsZkM38b8lORNJGJyA04cwMNxWkbuwCYA1giM+ZolWVXZKnctfwmznCBGlX9Ac6QAn8WMTTGZKdEiptPUqla7lXVhIjERKQrzpQa/T0uV1qqxu/m8ombCYeUl54o58kpSedhs9hHILYbVtwYpXGVgMAJN7UQyodVE6MkmkHCMPi6FkpO9u7PdUWfJq6ZvJpuFS2oCi9N78mMh/1bWu1o+r4PK72JFX2RSiKbLyJlwP04dzIbgLeSvUlECoA3gHw3ztOqesORF/XQQiHlikmbuPaiQdRWR7nrxZW8PbOU9Su97+p2tMVefWuU8jMTDP1DnEQLJPbC0l9EOebyGOVjE+x4M8Ta26Oc8mCzZ2WIx4T7Jw1g9YfFFBbH+ePzS1g4pyvrV3m/uvzR9n23J9vuWiatWqrqj1W1TlXvBc4BvudWMZNpAs5S1eE4A83PF5EzOlTaQzhxRCObP86jZn0+sZYQs2eUMfq8XZkOc9THjtXDrgVCr687MziFohDpCgjE9xx4TV4Pb/+H79yWx+oPiwHYuyfMhlUFdO+dgRkdUnA0fd9JZdkQpfY6xJ7W3nOq+l57B1ZnCfMGdzfqbhk/te69W9i2OW//fm11lCGn+TMj99EUe98mIVoOK34VZc8KoctnEhz3yxjH/WeMJZfnseb3gMLwR5s8K8PBelY2cdxJjSxfVOxLvKPp+8417VUtf9/OcwqclezgIhLGqY4OBu5W1XmHeM1lwGUABXhfPTBHRuPQsFQ4bkILXU9RVt8SYcODEeINMOgXLVSck2DbzBArb4hy8v3eXyEVFMW5/p4V3DdxAI0Ntjyr37Ktatleh9jPd/TgqhoHTnXb2J4TkWGquuSg10wFpgJ0lfK0P57tNVF69D3QJlPRp4Xa6miHym2xPy2/l5LfC7qe4nxFFefE2fBghN0LQwz6pTNBYcW5CVbe6P35hyMJrr9nJa8/X8HcmeXJ35AhR9P33S4l64Yo+bJAr6rWAa8D52f62MsXFVE5sJle/ZuIRBOMv7COt2f50zvkaIqdV+Eks8a1zn/gunlhigYpeT2UXfND7mMhCo/x+k+1cvUta9mwupDnHujjcaxPOpq+76Qy2EYmImERWSgiL7j7A0VknoisEpG/iEhesmN4dk0uIj2AFlWtE5FCnBsFt2Y6TiIu3H1dJZOmrSEUhlnTy1m3wp87OUdb7OOubWH5tVESLVDYTzl+YgvdPy+suTWKxiGUB4Nv8LZaeVJVA1/4ei1rlxUy5QVnivBHJvfn3dllnsaFo+/7bk+Gq5at0+d3dfdvBW5X1ekici9wKfDf7ZfHo7k2ROQUnFWCwzhXfk+q6k3tvaerlOsoSWuqbtNBwc/ZXxZY7MS+YM89CPP0VXbrjg7VC/P799d+V/8spdeuueY/FrS3rqWI9MPJEzcDPwf+D7AN6K2qMREZDdyoque1FyeVIUqCM9X1IFW9SUSOcYO80977VHUxMCLZ8Y0xOSj1659kK43fAfwnUOLudwfqVLV1ZZiNOOuFtCuVquU9OIMNzgJuAuqBZ4DTU3ivMaaTEU2rannYlcZF5MvAVlVdICLjO1KmVBLZKFU9TUQWAqjqzlQa34wxnVhm7lqeCXxFRL6IM7NOV+BOoExEIu5VWT9gU7IDpXLXssXtD6awvxHfx+Ggxphs03pVlmxrj6peq6r9VPVY4CLgNVX9Dk4Ph2+6L0tppfFUEtkfgeeAniJyM84UPpNSeJ8xprPydojSL4Gfi8gqnDazB5K9IZV1LR8XkQU4U/kI8FVV9WSlcWNMDkivjSy1Q6rOBma7P68BRqbz/lTuWh4DNAJ/a/uYqq5PJ5AxphPJlSFKbfydA4uQFAADgeXASR6WyxiTxSTLWslTqVqe3HbfnRXjx56VyBhj0pT2ECVVfU9ERnlRGGNMjsi1qqWI/LzNbgg4DdjsWYmMMdnNg8b+jkrliqykzc8xnDazZ7wojITDhEu7eXHopHTv3kDiAhANaDoW4M1TAgsNwInz44HFXv21foHFDorUZOj/Wi4lMrcjbImqXuNTeYwxuSBXElnrEAEROdPPAhljspuQW3ct38FpD1skIs8DTwF7Wp9U1Wc9LpsxJhvlaBtZAbAdZ/aL1v5kClgiM+ZolUOJrKd7x3IJBxJYqyw7DWOMr7IsA7SXyMJAFz6ZwFpl2WkYY/yUS1XL6mRTUxtjjlI5lMiya70nY0x20Ny6a2mrgBhjDi1XrshUdYefBTHG5I5caiPLGaGQcueTC9i+JY8br/BvzE1FnyaumbyabhUtqAovTe/JjId7+xI7mpfgtscXE81LEA7DnJndeeyuAb7EBqgav5vLJ24mHFJeeqKcJ6f08jRevF6pmRinebWCQO//CrPnLWXXXxOE3VFtFT8O0+Vz3q05XXlMAxNuXrh/v3dlI49NPYEZ0wd6FjMbYh/S0ZbI3GFO84FNqvplL2JceMlGNqwpoqg4lvzFGRSPCfdPGsDqD4spLI7zx+eXsHBOV9avKvI8dkuzMOF7J7OvMUw4kmDytMXMf6Mby97vmvzNHRQKKVdM2sS1Fw2itjrKXS+u5O2Zpaxf6d2CsVsnxykeE6LydyG0RUnsgz1vKd0uDlF+SdizuG1tWt+FKy8ZCzifwaMvvMrc2d4m8GyI/Skdm8Z6PxEpAN4A8nFy0dOqeoOIDASm40xzvQC4RFWb2zuWd3++DmhdRdgT3Xvt4/Rx25n5TB+vQhzWzm15rP6wGIC9e8JsWFVA997errR9gLCv0fkFjkSUSERR9ef+zIkjGtn8cR416/OJtYSYPaOM0eft8ixevEHZu1ApvdA5P4kK4ZJg70UNP72W6o1FbKvx/o9WNsUGd4hSBhYfAZqAs1R1OHAqcL6InMGBlcYHAztxVhpvl6eJzF1F+EvAn7yK8aMJq3jw98eRCPguSs/KJo47qZHli4p9ixkKKVP+upAn5s5j4dwyli8uSf6mDOjeu4Vtmw+sCFhbHaWij3cJvGUThMuEml/H+fjiFmomxkjsdX5Ldj6ZYO1FLVT/OkZ8t3/1nXHnbOZ/ZvX1LV62xG6VoVWUVFUb3N2ouynOKKKn3ccfAb6arDxeX5HdgbOK8GHTjIhcJiLzRWR+s6a3hP3If6qlbkceqz7y5xf4cAqK4lx/zwrumziAxgb/mh0TCeEnXx3BJf80khNOaWDA8XuSvykXxZV9y5Wyb4Y4dloUKRR2PJyg7JshBv01wrHTIkQqhK23+zMlUCSSYNTYLcx5zf9aQJCxPyH1VZQqWn+/3e2ytocRkbCILAK2Aq8Aq/FopfEjkuoqwu7y6VMBSiM90vqTOnTEbs4YX8vpY7cTzU9QVBznmls+YvKEoR0pelrCkQTX37OS15+vYO7Mct/itrWnPsLieaVUjd3JupXeXxFur4nSo++BJouKPi3UVns3p1qkpxDpCYXDnL+7JWc7iSzS/UD1suxrITZe7U8badWYraxeXkrdjnxf4mVL7E/IwErjAKoaB04VkTKcZSeHHElxvLwia11F+GOchruzROSxTAZ4+I5B/N+zx/CDc0dz6zVDWTyvzNckBsrVt6xlw+pCnnvA37+Qpd1aKC5xfnHz8uOMGFPHhjX+tJksX1RE5cBmevVvIhJNMP7COt6eVepZvEiFEO0lNH/s/PY0vqPkDRJitQd+m+pfT5B/nD/tZuPODbBaGWDs/VKsVqbTRUNV63AW5h2Nu9K4+1RKK417dkWmqtcC1wK4V2TXqOp3vYoXhJOqGvjC12tZu6yQKS98AMAjk/vz7uwyz2N369nMNbesIBRWRODNlyt4Z7Y/V4SJuHD3dZVMmraGUBhmTS9n3Qrv7lgC9PxFmM2/iqMtSl6l0PuGMFtvi7NvhdMdI9pH6H2d93cv8wtijBhZy5Tfnpz8xZ0o9qdk5q5lD6BFVetEpBA4B6ehv3Wl8emkuNK4qHrfQNomkbXb/aI00kNHl37N8/IcytE61XWivj6w2AAnzg/u3Fd/LaDuCwGaWzONXU1bOnTpWtSzv574zZ8nfyGw6L9/vuBwVUsROQWnMT+MUzt8UlVvEpFBOEmsHFgIfFdVm9qL40vLdNtVhI0xuS8TPftVdTEw4hCPZ36lcWOM+YQMdYjNJEtkxpj0WSIzxuSy1p792cQSmTEmbZLIrkxmicwYkx5rIzPGdAZWtTTG5D5LZMaYXGdXZMaY3GeJzBiT03JsFSXfaTxOfOfOoIvhv33pzcPWmaz8vLeDzdtT+Pd2h+95as+4bYHEVe34BJjWj8wY0zn4MNlEOiyRGWPSZldkxpjcZh1ijTGdgTX2G2NyniUyY0xuU7Kusd+PBXqNMZ1MJhYfEZH+IvK6iHwkIh+KyFXu4+Ui8oqIrHT/7ZasPJbIjDHpS31dy/bEgP9Q1aHAGcAVIjIUmAC8qqrHA6+6++2yRGaMSUtrh9gMrDRerarvuT/XA0txFuO9EGdREkhxpXFrIzPGpEc1nYkVK0Rkfpv9qe6i3J8gIsfiLEQyD+ilqtXuUzVA0uWuOkUiqxq/m8snbiYcUl56opwnp/i3zJfF9jd2NC/BbY8vJpqXIByGOTO789hdAzyNqfUJmn5XT2JtHID8CSXotgTND+1B18UpuK+M8BDvl7UL8vv+lAytNA4gIl2AZ4CrVXW3yIHV6lRVRZJ3v/U0kbmrjNcDcSCW7ISORCikXDFpE9deNIja6ih3vbiSt2eWsn6l92P4LLb/sVuahQnfO5l9jWHCkQSTpy1m/hvdWPZ+V89iNv+xgfCoPAomFqItCvsU7SIU/KYrTZMbPIvbVpCf+aFkqme/iERxktjjqvqs+/AWEemjqtUi0gfYmuw4frSRfV5VT/UiiQGcOKKRzR/nUbM+n1hLiNkzyhh93i4vQlnsLIgNwr5GZ0XxSESJRBTVDq032y5tSBB/v4XIl5yEIVFBSkKEjo0QOsa/Ck2wn/lBFEhoals7xLn0egBYqqp/aPPU8zgrjEOKK43nfGN/994tbNuct3+/tjpKRZ+Oj/C32NkZG5yrkyl/XcgTc+excG4ZyxeXeBYrUZ1AykI0/7aevZfupOnWenSv/32ogv7MPyUzdy3PBC4BzhKRRe72ReAW4BwRWQl8wd1vl9d/UhSY5dZx7ztMI99lwGUABRR5XBzTGSQSwk++OoLikhi/unspA47fw7qVxd4EiyuJlTHyru5CeGiUpjsbaHm8kbwfehQvR2RopfE5ODdBD+XsdI7l9RXZ51T1NOACnD4i4w5+gapOVdUqVa2Kkp92gO01UXr0bd6/X9Gnhdpq7xteLXYwsdvaUx9h8bxSqsZ6N4ed9AgjPUKEhzrnFxmfR2JFzLN4h5Mtn3krSWhKm188TWSqusn9dyvwHDAy0zGWLyqicmAzvfo3EYkmGH9hHW/PKs10GIudJbFLu7VQXOIkkrz8OCPG1LFhjXdX8qHuIaRniMR6J2Z8QQuhY8OexTucID/zT0m1WuljDdyzqqWIFAMhVa13fz4XuCnTcRJx4e7rKpk0bQ2hMMyaXs66Ff7cybHY/sfu1rOZa25ZQSisiMCbL1fwzuxyT2PmXVVC08R6tEUJ9Q2Tf20JsTeaaL6zAa1LsO+XuwgPjlDw+zLPyhDkZ34wp0Nsdo21FPWoQCIyCOcqDJyEOU1Vb27vPV2lXEdJWlVjk+NCJd411CdT+PfgptkOaqrrefoqu3VHh27zdu3aT6tO/0lKr339tWsXeNVjoS3PrshUdQ0w3KvjG2OCk21XZJ2iZ78xxkc2Q6wxJvf5e0cyFZbIjDHps6qlMSan2QK9xphOwa7IjDE5L7vymCUyY0z6JJFddUtLZMaY9CiQXXnMEpkxJj2CWodYY0wnYIns8CQvSqR3v0BixzZsDCQuQLhHj8Bix7cFM+avVagsoBkcgL3n1gYWu/rnYwKJ2/LY25k5kCUyY0xOy8I2spyf6toY4z9JJFLakh5H5EER2SoiS9o8ZiuNG2O8pk7VMpUtuYeB8w96zFYaN8Z4TMlYIlPVN4AdBz1sK40bY3yQehtZSiuNH+ToXGncGOOvNPqRJV1pvD2prjRuVUtjTPoy10Z2KFvcFcbJppXGjTGdiSrEE6ltR+boW2ncGBOADF2RicgTwFvAiSKyUUQuJQtXGjfGdEYZ6tmvqt8+zFNpLaeW84ms8pgGJty8cP9+78pGHpt6AjOmD/QlftX43Vw+cTPhkPLSE+U8OSXpDZaMeejFOextDBOPC4m4cNXFo3yLHdR5B/l9V/Rp4prJq+lW0YKq8NL0nsx4uLdn8fLCMR66aAZ54TjhUIJ/rBjEPXNH8tsv/oOTem8llgjxQXUvJr4yjljCx0WDFTia5uwXkTLgT8AwnNP/V1V9K5MxNq3vwpWXjAUgFFIefeFV5s7255cqFFKumLSJay8aRG11lLteXMnbM0tZv9K/9RIn/PCz7K7L8y0eBHveQX7f8Zhw/6QBrP6wmMLiOH98fgkL53Rl/SpvVjpvjof54ZNfYW9LlEgoziPf/itz1h7D35cez7UvOhcst37pH3z95KU8+f4wT8pwaAqaXWOUvG4juxN4WVWH4KxxudTLYMNPr6V6YxHbarz5j3WwE0c0svnjPGrW5xNrCTF7Rhmjz9vlS+wgZct5+/1979yWx+oPiwHYuyfMhlUFdO/d4mFEYW9LFIBIKEEklEBVmLN2AO5633xQ05NeJXs8LMMhKF439qfNsysyESkFxgHfB1DVZqDZq3gA487ZzP/M6utliE/o3ruFbZsPXA3VVkcZclqjb/EV+M29C1GFl56u5OVn/Jk5JOjzbuX3991Wz8omjjupkeWLij2NE5IE0y95mmPKdjF90TA+qDlw9RkJxfk/Q1dw62tnelqGQzqKZr8YCGwDHhKR4cAC4CpV/cSfDxG5DLgMoCBccsTBIpEEo8Zu4ZF7hhx5iXPML75fxfatBZSWN3Pzve+xcW0xS95LOr62Uwjy+y4oinP9PSu4b+IAGhu8bWZOaIh/fvSfKclv4vYLX2ZwxXZW1XYH4LovvMmCjX14b1MAyTzLEpmXVcsIcBrw36o6AtjDIQZ/qupUVa1S1aq8cOERB6sas5XVy0up25F/xMdI1/aaKD36HrjIrOjTQm111L/4W502qV078njrtR6cMGy3P3EDPm8I5vsGCEcSXH/PSl5/voK5M8t9i1vflM+7Gyo589gNAFw++l26Fe7lttcDuBrL7KDxjPAykW0ENqrqPHf/aZzE5olx5/pfzVi+qIjKgc306t9EJJpg/IV1vD3Ln4kC8wvjFBbF9v88YvQO1q3ytprTKsjzbhXE9w3K1besZcPqQp57oI/n0boV7qUkvwmA/EiM0QM2sHZHGV8/+SPGHLuBX/79HBTxvByfokAikdrmE8+ui1W1RkQ2iMiJqrocp1/IR17Eyi+IMWJkLVN+e7IXhz+sRFy4+7pKJk1bQygMs6aXs26FP3csu5U3cf3tiwEIR5TZL/ZmwdwKX2IHed4Q3Pd9UlUDX/h6LWuXFTLlhQ8AeGRyf96dXeZJvIriRn5zwWuEQwlCosxcPpg31hzLez+/l+rdJfz54mcBeHXlIO5764iHMx6ZLKtainpYIBE5Faf7RR6wBviBqu483OtL83vpmN4Xe1ae9thU18GI9A9manOAxLbgprre9GPPKiftWv3YH9hbs6FDl3Gl0R46puwbKb325dr7FnRk0HiqPG2pVNVFgM9/KowxnlLQLOtHlvM9+40xATiaevYbYzqpLGsjs0RmjEmPqq93JFNhicwYkz67IjPG5DZF4/GgC/EJlsiMMek52qbxMcZ0UlnW/cKmujbGpEUBTWhKWzIicr6ILBeRVSKSdCHew7FEZoxJj7oTK6aytUNEwsDdwAXAUODbIjL0SIpkVUtjTNoy1Ng/ElilqmsARGQ6zirjaY/J9nSsZbpEZBuw7gjfXgEENXjOYlvsXIk9QFU7NLhXRF52y5GKAmBfm/39K42LyDeB81X1h+7+JcAoVf1JumXKqiuyjnzAIjLfj8GpFttiH62xW6nq+UHGPxRrIzPGBGUT0L/Nfj/3sbRZIjPGBOVd4HgRGSgiecBFOKuMpy2rqpYdNNViW2yLnTtUNSYiPwFmAmHgQVX98EiOlVWN/cYYcySsammMyXmWyIwxOa9TJLJMDXM4grgPishWEVniV8w2sfuLyOsi8pGIfCgiV/kYu0BE3hGR993Yv/YrdpsyhEVkoYi84HPcj0XkAxFZJCLzfY5dJiJPi8gyEVkqIqP9jJ/Ncr6NzB3msAI4B2cJuneBb6uqJys2HRR7HNAAPKqqw7yOd1DsPkAfVX1PREpwFkD+qk/nLUCxqjaISBSYg7P48ttex25Thp/jrAfRVVW/7GPcj4EqVfW9Q6yIPAK8qap/cu/yFalqnd/lyEad4Yps/zAHVW0GWoc5eE5V3wB2+BHrELGrVfU99+d6YClQ6VNsVdUGdzfqbr79RRSRfsCXcFboOiqISCkwDngAQFWbLYkd0BkSWSWwoc3+Rnz6hc4WInIsMAKYl+SlmYwZFpFFwFbglTYLMfvhDuA/gSDmklFglogsEJHLfIw7ENgGPORWqf8kIv6syJwDOkMiO6qJSBfgGeBqVd3tV1xVjavqqTi9sUeKiC9VaxH5MrBVVRf4Ee8QPqeqp+HM2HCF27zghwhwGvDfqjoC2AP41h6c7TpDIsvYMIdc47ZPPQM8rqrPBlEGt3rzOuDX+Lszga+4bVXTgbNE5DGfYqOqm9x/twLP4TRt+GEjsLHNle/TOInN0DkSWcaGOeQSt8H9AWCpqv7B59g9RKTM/bkQ50bLMj9iq+q1qtpPVY/F+a5fU9Xv+hFbRIrdGyu41bpzAV/uWKtqDbBBRE50HzqbI5juprPK+SFKmRzmkC4ReQIYD1SIyEbgBlV9wI/YOFcmlwAfuG1VAP9PVV/0IXYf4BH3jnEIeFJVfe0GEZBewHPO3xAiwDRVfdnH+FcCj7t/sNcAP/AxdlbL+e4XxhjTGaqWxpijnCUyY0zOs0RmjMl5lsiMMTnPEpkxJudZIsshIhJ3Z11YIiJPiUhRB471sLuKDe5wl8OuJygi40VkzBHE+FhEPrXazuEeP+g1De09f4jX3ygi16RbRtM5WCLLLXtV9VR3po1m4PK2T4rIEfULVNUfJpk1YzyQdiIzxi+WyHLXm8Bg92rpTRF5HvjIHcx9m4i8KyKLReRH4IwEEJEp7rxt/wB6th5IRGaLSJX78/ki8p4719ir7oD0y4GfuVeDY92e/c+4Md4VkTPd93YXkVnuHGV/AiTZSYjIX90B2B8ePAhbRG53H39VRHq4jx0nIi+773lTRIZk5NM0OS3ne/YfjdwrrwuA1l7lpwHDVHWtmwx2qerpIpIP/K+IzMKZHeNEnKXpe+EMb3nwoOP2AO4HxrnHKlfVHSJyL9CgqpPd100DblfVOSJyDM6ois8ANwBzVPUmEfkScGkKp/OvboxC4F0ReUZVtwPFwHxV/ZmI/Jd77J/gLL5xuaquFJFRwD3AWUfwMZpOxBJZbilsMxzpTZyxlmOAd1R1rfv4ucApre1fQClwPM5cVk+oahzYLCKvHeL4ZwBvtB5LVQ8319oXgKHuUB2Aru4sHOOAr7vv/buI7EzhnH4qIl9zf+7vlnU7zhQ9f3Effwx41o0xBniqTez8FGKYTs4SWW7Z606ds5/7C72n7UPAlao686DXfTGD5QgBZ6jqvkOUJWUiMh4nKY5W1UYRmQ0UHObl6satO/gzMMbayDqfmcC/u1P8ICInuDM1vAH8i9uG1gf4/CHe+zYwTkQGuu8tdx+vB0ravG4WzgBm3Ned6v74BnCx+9gFQLckZS0FdrpJbAjOFWGrENB6VXkxTpV1N7BWRL7lxhARGZ4khjkKWCLrfP6E0/71njiLotyHc+X9HLDSfe5R4K2D36iq24DLcKpx73Ogavc34Gutjf3AT4Eq92bCRxy4e/prnET4IU4Vc32Ssr4MRERkKXALTiJttQdnwsYlOG1gN7mPfwe41C3fh/g0rbnJbjb7hTEm59kVmTEm51kiM8bkPEtkxpicZ4nMGJPzLJEZY3KeJTJjTM6zRGaMyXn/Hwys4j1VHFOdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(targets, Y_pred, normalize=None, labels=list(set(Y_test)))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(Y_test))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "mathematical-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torchModelLight, 'torchModel_conv_batch_norm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "alike-creek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight \t torch.Size([256, 768])\n",
      "0.bias \t torch.Size([256])\n",
      "3.weight \t torch.Size([128, 256])\n",
      "3.bias \t torch.Size([128])\n",
      "6.weight \t torch.Size([7, 128])\n",
      "6.bias \t torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in (torchModel.TorchModel.state_dict()):\n",
    "    print(param_tensor, \"\\t\", torchModel.TorchModel.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "artificial-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torchModelLight.TorchModel.state_dict(),'torchModel_conv_batch_norm_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mature-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchModel =  torch.load('torchModel.pth', map_location=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "white-violation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_classes = 7\n",
    "torchModel = TorchNLP(n_classes)\n",
    "torchModel.TorchModel.load_state_dict(torch.load('torchModel_state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "flying-sitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 5, 6, 4, 6, 5, 6, 6, 1, 6, 4, 4, 6, 2], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchModel.eval()\n",
    "torchModel.predict(X[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "swedish-valuation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 5, 4, 2, 6, 5, 4, 6, 4, 2, 6, 6, 6, 6], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "isolated-split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transformer = NLPTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "copyrighted-mentor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/1\n",
      "output shape: (1, 768)\n",
      "transformed.shape: (1, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 9.87884760e-01, -1.32418856e-01,  9.23658788e-01,\n",
       "         9.53841284e-02,  5.00432737e-02, -1.74600735e-01,\n",
       "        -6.13515228e-02, -8.26982021e-01, -8.38522017e-02,\n",
       "        -9.93365169e-01,  6.42717481e-02, -3.05998195e-02,\n",
       "         4.04555025e-03, -3.32312472e-02, -2.55960405e-01,\n",
       "         8.80274400e-02,  2.30186395e-02,  1.60196766e-01,\n",
       "         5.89367636e-02,  9.98809397e-01, -4.29829396e-02,\n",
       "        -9.95216131e-01,  3.62579674e-02, -4.38297138e-04,\n",
       "         2.73966137e-02, -8.72960865e-01,  7.15051591e-02,\n",
       "        -9.36015248e-01,  2.12837279e-01, -9.79796946e-02,\n",
       "        -4.70896997e-02, -7.83243418e-01, -9.90428030e-01,\n",
       "         4.68692094e-01, -2.34113559e-01,  1.40514001e-01,\n",
       "         1.79409549e-01,  3.13502923e-02, -7.68147185e-02,\n",
       "        -5.33504598e-02,  1.49713665e-01,  8.09488297e-01,\n",
       "        -1.73246086e-01, -8.50201309e-01,  9.99719203e-01,\n",
       "        -1.37132853e-01,  9.39538836e-01,  7.69650459e-01,\n",
       "         1.41458228e-01, -1.47941232e-01, -6.37947395e-02,\n",
       "        -8.67902637e-01,  1.08504213e-01, -9.65803623e-01,\n",
       "         1.49407331e-02,  2.07303800e-02, -9.81663585e-01,\n",
       "         1.05088249e-01,  5.12147807e-02,  1.31235763e-01,\n",
       "        -3.44073819e-03, -3.62985343e-01, -9.87949610e-01,\n",
       "         9.26043630e-01,  9.29073393e-01,  1.51128694e-02,\n",
       "         4.33979891e-02,  4.75415289e-02, -8.69932845e-02,\n",
       "         8.07452854e-03, -9.59095001e-01,  1.23830967e-01,\n",
       "         9.88831818e-01,  8.51991355e-01, -1.29586697e-01,\n",
       "         9.24283981e-01,  7.72088289e-01, -1.04758069e-01,\n",
       "        -4.51123789e-02, -1.14460118e-01, -1.77017257e-01,\n",
       "         8.10712799e-02, -1.74016710e-02, -4.57279906e-02,\n",
       "         1.06058113e-01,  2.24296153e-01, -7.44234025e-02,\n",
       "         5.17070033e-02, -6.75211549e-02,  2.49666478e-02,\n",
       "        -1.75171733e-01,  8.97964230e-04,  7.26983547e-02,\n",
       "         9.34231162e-01, -1.63953170e-01, -1.17903529e-02,\n",
       "        -2.53972169e-02,  9.99979377e-01,  2.04567343e-01,\n",
       "         9.15516820e-03,  9.73296762e-01, -2.94533640e-01,\n",
       "        -9.93927777e-01, -5.40984748e-03,  8.26986969e-01,\n",
       "        -4.07696456e-01,  6.66125566e-02, -3.58887650e-02,\n",
       "         1.49177104e-01, -9.94956851e-01,  8.73952433e-02,\n",
       "        -9.55923676e-01, -3.91323268e-01,  7.62638152e-02,\n",
       "         1.46565333e-01, -1.11397719e-02,  9.67091024e-01,\n",
       "        -2.31624201e-01,  9.17749330e-02, -6.80222735e-02,\n",
       "         1.71332166e-01, -4.66349535e-02,  1.32077664e-01,\n",
       "        -1.58976525e-01, -6.62122726e-01, -5.40081486e-02,\n",
       "        -3.32526751e-02,  8.98359418e-01, -1.99286357e-01,\n",
       "         6.03545070e-01,  3.82536054e-02, -8.79447818e-01,\n",
       "        -1.25714794e-01,  9.39347208e-01, -7.25749284e-02,\n",
       "        -9.92515743e-01, -1.49767682e-01, -1.75145760e-01,\n",
       "        -2.24834725e-01,  1.76261105e-02,  4.74077575e-02,\n",
       "         1.09260350e-01, -9.99690831e-01, -9.58487272e-01,\n",
       "        -8.12201947e-03, -7.34324396e-01, -1.74461643e-03,\n",
       "         5.87813675e-01, -1.00879423e-01, -9.10852551e-01,\n",
       "        -6.18233263e-01, -1.10021770e-01, -6.57703161e-01,\n",
       "         3.12299747e-03,  9.86170709e-01, -2.23261818e-01,\n",
       "         1.21166475e-01,  9.59120214e-01, -1.27927616e-01,\n",
       "        -1.73066705e-02,  1.25968590e-01,  1.66406006e-01,\n",
       "         7.24125564e-01,  1.29632264e-01, -9.13709879e-01,\n",
       "        -2.45866533e-02, -3.55299190e-02, -1.57266483e-01,\n",
       "         9.69902277e-01,  1.23990424e-01,  4.69423123e-02,\n",
       "         1.25817033e-02,  1.15582138e-01,  7.38638490e-02,\n",
       "        -4.38574925e-02,  2.26562575e-01,  1.24764994e-01,\n",
       "        -3.75630595e-02, -6.71812773e-01, -6.99741542e-01,\n",
       "         9.99577522e-01, -4.68620569e-01,  3.56503993e-01,\n",
       "         1.25227243e-01,  1.17101289e-01, -1.58169374e-01,\n",
       "        -9.53495324e-01, -1.36480227e-01,  7.44853973e-01,\n",
       "         1.96240842e-02, -9.40314829e-01, -9.71407816e-02,\n",
       "        -8.27994823e-01, -9.95052159e-01, -9.31593001e-01,\n",
       "         1.81438103e-01,  2.96223938e-01, -1.54388070e-01,\n",
       "         4.43649851e-02, -9.62442458e-02, -6.02657758e-02,\n",
       "        -6.61145031e-01, -1.23252958e-01,  1.07687816e-01,\n",
       "        -1.43475458e-01, -9.78900552e-01, -8.12900245e-01,\n",
       "         8.36511433e-01,  9.63216364e-01, -9.27752793e-01,\n",
       "        -9.53642368e-01, -9.12320018e-01,  9.36179042e-01,\n",
       "        -6.44248948e-02, -1.54130697e-01,  8.87327671e-01,\n",
       "        -8.01799074e-02,  9.95756626e-01, -4.76233736e-02,\n",
       "         9.05309498e-01,  9.07611132e-01,  5.94950318e-02,\n",
       "         2.23824784e-01, -7.27538228e-01, -4.66104560e-02,\n",
       "         9.72329736e-01, -1.24100195e-02, -1.96909234e-01,\n",
       "         9.46532562e-02,  3.94853383e-01, -9.97741163e-01,\n",
       "         1.26728833e-01,  6.70959651e-01,  1.77091241e-01,\n",
       "         9.92703676e-01, -9.54746902e-01, -6.82581007e-01,\n",
       "        -1.22183017e-01, -9.16884065e-01,  4.49617743e-01,\n",
       "        -2.22764671e-01,  9.82711434e-01, -6.76237494e-02,\n",
       "         6.72039628e-01, -4.65895325e-01, -1.08713433e-01,\n",
       "        -6.15159631e-01,  5.20374894e-01, -8.21085572e-02,\n",
       "        -9.59127396e-02,  9.29847240e-01, -9.91687417e-01,\n",
       "         4.11454625e-02,  8.38159621e-01,  8.85095656e-01,\n",
       "         5.27625203e-01, -7.80446455e-02, -2.96771377e-01,\n",
       "        -2.05299839e-01, -8.61541271e-01, -6.37079403e-02,\n",
       "        -9.99631509e-02, -9.23573375e-01, -1.05650453e-02,\n",
       "        -1.40563324e-01,  1.22940980e-01, -2.31838539e-01,\n",
       "        -1.47443376e-02,  1.31691128e-01,  9.37365741e-03,\n",
       "         4.96731579e-01,  1.18892841e-01, -1.16127461e-01,\n",
       "         9.58310843e-01,  4.56776649e-01,  9.98762608e-01,\n",
       "        -1.81366280e-02, -4.69992757e-01, -1.05193444e-01,\n",
       "        -2.44153924e-02,  9.99982119e-01, -1.92033365e-01,\n",
       "         2.83748694e-02, -8.87830198e-01, -1.29027665e-01,\n",
       "        -9.98184383e-01, -9.00837719e-01,  5.30095518e-01,\n",
       "        -8.19737971e-01, -9.99273717e-01, -9.99067426e-01,\n",
       "         9.97384548e-01,  8.18728507e-02,  2.16504019e-02,\n",
       "         2.34984066e-02,  1.23277150e-01, -9.84870076e-01,\n",
       "        -1.88233443e-02, -1.00030914e-01,  5.91951609e-01,\n",
       "         2.56650597e-02,  2.73030907e-01,  1.72987878e-01,\n",
       "         4.47571203e-02,  1.06981583e-01, -3.47060105e-03,\n",
       "         9.75282669e-01, -5.91291524e-02, -8.80587220e-01,\n",
       "        -4.23034467e-02, -2.32661609e-02,  8.11151505e-01,\n",
       "        -9.86781061e-01, -7.47970402e-01,  7.86787271e-01,\n",
       "        -6.30483747e-01,  1.53785303e-01, -9.92731214e-01,\n",
       "         1.55308485e-01,  4.79659177e-02,  1.96690023e-01,\n",
       "        -2.05168903e-01, -8.28776479e-01,  1.20640524e-01,\n",
       "        -9.70084548e-01,  9.21621174e-02, -9.44940448e-01,\n",
       "         3.21600325e-02,  6.70268059e-01, -7.64345527e-01,\n",
       "         6.49086386e-02, -1.79652348e-01,  3.45870614e-01,\n",
       "        -8.70767310e-02, -9.99876499e-01,  8.56240511e-01,\n",
       "        -9.83945370e-01, -8.08890611e-02, -5.80687299e-02,\n",
       "        -2.58035541e-01,  1.44258723e-01, -9.97529268e-01,\n",
       "         7.47424185e-01,  9.99826550e-01, -9.43594217e-01,\n",
       "         1.36571929e-01, -9.87605870e-01, -8.20396096e-02,\n",
       "         5.23272865e-02,  9.34578538e-01,  9.91877094e-02,\n",
       "        -1.31645322e-01,  2.18357235e-01, -3.45268548e-02,\n",
       "        -6.00858554e-02, -1.18987635e-04,  4.99359109e-02,\n",
       "        -1.24445572e-01,  1.15300827e-01,  9.35345292e-01,\n",
       "         1.14561506e-02,  1.49533972e-01, -6.57843888e-01,\n",
       "         8.96646559e-01, -1.60401881e-01, -2.62772050e-02,\n",
       "        -2.81583425e-02, -9.98753726e-01, -1.36677697e-01,\n",
       "         1.14464939e-01,  8.96474794e-02,  2.79777870e-02,\n",
       "         1.62195235e-01, -6.79174662e-02,  2.21869513e-01,\n",
       "        -8.37410390e-01, -8.16139802e-02, -2.16459692e-01,\n",
       "         9.76308048e-01,  9.90937889e-01,  5.01280464e-03,\n",
       "         6.57447755e-01,  5.65025099e-02, -7.94954374e-02,\n",
       "        -1.08255818e-01, -1.52632996e-01,  1.66849226e-01,\n",
       "        -1.14833094e-01,  2.10991487e-01,  1.94244370e-01,\n",
       "         9.98736322e-02, -8.57768357e-01, -1.56709448e-01,\n",
       "         4.93155755e-02, -4.66057919e-02,  1.15144225e-02,\n",
       "         7.33102143e-01,  9.39131320e-01, -1.35076255e-01,\n",
       "         9.69730318e-03,  8.27089846e-01, -6.09620959e-02,\n",
       "        -8.50796998e-02,  1.10981710e-01, -9.85597610e-01,\n",
       "         5.56893528e-01, -4.19436879e-02, -5.87671027e-02,\n",
       "        -1.67067125e-01,  4.63584751e-01, -1.67271215e-02,\n",
       "         5.85151136e-01,  2.86403894e-02, -6.94426179e-01,\n",
       "         9.68107581e-01, -1.39854789e-01, -1.25161752e-01,\n",
       "        -9.67220545e-01,  8.38356376e-01,  4.25054505e-02,\n",
       "         3.15170497e-01,  9.96140480e-01, -1.02899991e-01,\n",
       "         2.42264330e-01, -7.01951444e-01,  8.59010637e-01,\n",
       "        -7.26661503e-01, -2.54261941e-02,  9.79368150e-01,\n",
       "         9.68816698e-01,  7.04898894e-01,  9.17769447e-02,\n",
       "        -1.93875059e-01, -9.59845841e-01,  4.99777257e-01,\n",
       "        -7.30110183e-02,  1.15090787e-01, -5.27648270e-01,\n",
       "         3.93262431e-02, -3.73773067e-03, -4.16283235e-02,\n",
       "         1.79496840e-01, -1.49443686e-01,  7.18278289e-01,\n",
       "         9.34013784e-01, -3.42585564e-01, -2.00246051e-02,\n",
       "        -1.05394833e-01, -8.28380790e-03,  9.47183430e-01,\n",
       "        -9.97888446e-01,  2.23299384e-01,  8.67530346e-01,\n",
       "        -1.10120751e-01,  9.20205951e-01, -6.03777170e-02,\n",
       "         5.13461590e-01,  7.68884271e-02,  9.90557969e-02,\n",
       "        -9.65955079e-01,  7.37092018e-01, -3.60416472e-02,\n",
       "        -9.05243278e-01,  7.86836684e-01,  6.98100701e-02,\n",
       "         9.69725966e-01, -7.75136173e-01,  1.36839941e-01,\n",
       "         2.36903548e-01, -1.03423320e-01, -3.41754756e-03,\n",
       "         9.99944448e-01,  6.15122139e-01, -8.35210010e-02,\n",
       "         4.18267734e-02,  7.40192384e-02,  7.62263536e-02,\n",
       "         9.99989748e-01, -1.02766387e-01,  1.10453486e-01,\n",
       "        -1.74168825e-01, -1.83029816e-01, -5.30538112e-02,\n",
       "         5.47422282e-02, -2.40141544e-02,  1.65172324e-01,\n",
       "        -2.92164803e-01, -9.91329968e-01, -9.44916904e-01,\n",
       "        -1.68072596e-01, -1.57724425e-01, -4.06991467e-02,\n",
       "         1.35326728e-01,  1.12689584e-01,  2.06698939e-01,\n",
       "        -1.86117589e-02,  1.14057608e-01,  8.68230283e-01,\n",
       "         1.88659742e-01,  9.30301547e-01, -4.02751006e-02,\n",
       "         3.57201584e-02, -1.74068600e-01, -1.08619533e-01,\n",
       "         9.65277433e-01, -3.08498800e-01, -1.05663892e-02,\n",
       "         7.59291887e-01,  1.27529681e-01,  2.35254765e-01,\n",
       "        -1.26605239e-02, -8.43072295e-01,  2.01105878e-01,\n",
       "         1.06234431e-01,  9.74039018e-01,  9.81043100e-01,\n",
       "         8.94979462e-02,  1.11512430e-01,  1.04971137e-02,\n",
       "         3.09407581e-02,  6.34262040e-02,  1.37421876e-01,\n",
       "         9.51368034e-01,  5.01705930e-02, -7.61705125e-03,\n",
       "        -7.17050731e-02,  1.45482734e-01,  1.53432405e-02,\n",
       "         2.53481299e-01,  1.37645051e-01,  8.87295961e-01,\n",
       "         6.98809028e-01, -8.66174340e-01, -2.82295123e-02,\n",
       "        -1.17916152e-01,  2.30459627e-02, -9.01012123e-02,\n",
       "         8.87196243e-01,  9.81127083e-01, -1.72762737e-01,\n",
       "         4.77049015e-02, -5.45026958e-02,  1.15169518e-01,\n",
       "         9.67023671e-01,  1.34537946e-02,  9.85794127e-01,\n",
       "         9.09284890e-01, -1.33593492e-02,  9.77738440e-01,\n",
       "        -8.58973786e-02,  2.09894732e-01, -1.26838744e-01,\n",
       "         9.88876760e-01, -3.06235086e-02,  5.06016552e-01,\n",
       "        -6.25079544e-03, -7.01278523e-02, -9.36174452e-01,\n",
       "        -5.86285852e-02, -4.03332226e-02,  5.55662066e-03,\n",
       "         5.40328622e-01, -4.26507294e-02,  9.82567132e-01,\n",
       "         1.52601093e-01,  9.34689641e-01, -2.21022993e-01,\n",
       "        -8.88966739e-01,  9.99924362e-01,  7.65197352e-02,\n",
       "        -9.89851415e-01, -1.34128109e-01,  7.49765113e-02,\n",
       "         3.57694104e-02, -3.46463881e-02, -8.57569635e-01,\n",
       "        -6.84047461e-01,  5.19301519e-02, -1.40292078e-01,\n",
       "        -7.06801295e-01,  1.02843247e-01, -5.74272394e-01,\n",
       "        -2.87485495e-02,  7.15316534e-01,  1.03238188e-01,\n",
       "        -1.35705099e-02, -6.46869689e-02, -4.09290986e-03,\n",
       "         2.47846246e-02, -6.79604590e-01,  3.95971872e-02,\n",
       "        -8.91052961e-01,  1.71253696e-01,  9.51308906e-01,\n",
       "         1.37178302e-01,  9.98260558e-01,  7.53134489e-01,\n",
       "        -8.09876204e-01,  1.46848604e-01,  2.74807364e-01,\n",
       "         1.70416102e-01, -8.29093397e-01,  6.36783540e-02,\n",
       "         5.19820116e-02, -3.29090565e-01, -4.41777498e-01,\n",
       "        -3.74789978e-03,  4.84285355e-02, -7.05577508e-02,\n",
       "        -9.87320542e-01,  4.32025455e-02,  9.12765980e-01,\n",
       "        -9.30648446e-02, -9.96712923e-01, -7.13378489e-02,\n",
       "        -9.80336845e-01,  9.38998722e-03,  8.97116512e-02,\n",
       "         4.12291475e-02, -1.78118348e-01, -4.57995348e-02,\n",
       "        -2.13644169e-02,  4.83820364e-02, -4.92476076e-01,\n",
       "         1.01309739e-01, -9.91156816e-01, -7.81566184e-03,\n",
       "        -9.22233090e-02, -6.22565560e-02,  5.46081290e-02,\n",
       "         1.40425608e-01,  1.39402494e-01, -8.72549191e-02,\n",
       "        -2.63568014e-01, -8.72225463e-02,  4.73688915e-02,\n",
       "         9.94099319e-01, -9.17939305e-01, -1.30561724e-01,\n",
       "        -1.03723342e-02, -8.69139433e-01,  1.64526135e-01,\n",
       "         9.78321195e-01, -7.87118729e-03, -1.44165792e-02,\n",
       "         9.96945620e-01,  3.97383943e-02,  4.84721996e-02,\n",
       "        -9.56135988e-01,  1.38724506e-01,  4.55310121e-02,\n",
       "         3.26460376e-02, -9.90412116e-01,  1.90723360e-01,\n",
       "        -2.63121258e-02,  1.21823795e-01, -9.75535572e-01,\n",
       "         9.66803849e-01,  1.08118027e-01, -3.64457406e-02,\n",
       "        -1.76723823e-01, -1.51639953e-01, -1.33415282e-01,\n",
       "         9.82127309e-01, -8.20393264e-01, -9.25611019e-01,\n",
       "         8.18077624e-01,  1.02323398e-01,  1.73582613e-01,\n",
       "         9.66197699e-02, -7.69783616e-01,  9.98489916e-01,\n",
       "         4.58732873e-01, -9.14680839e-01, -1.63749218e-01,\n",
       "        -4.50384580e-02,  9.90804732e-01,  2.10070029e-01,\n",
       "        -5.26313521e-02, -1.34022161e-02, -6.74578100e-02,\n",
       "        -7.86037073e-02, -9.97750640e-01,  9.45417464e-01,\n",
       "        -6.08589612e-02, -6.14745095e-02,  9.72017169e-01,\n",
       "         1.39726520e-01,  8.52394290e-03, -3.23624723e-02,\n",
       "         6.70563877e-01,  8.99979949e-01, -8.05637419e-01,\n",
       "         6.09667785e-02,  7.66995013e-01,  1.54911861e-01,\n",
       "         8.80626023e-01,  9.59716976e-01,  3.46200801e-02,\n",
       "        -4.84198451e-01,  2.28561237e-02, -9.59732413e-01,\n",
       "        -3.25344466e-02, -8.69872451e-01,  3.03483196e-03,\n",
       "         7.66040862e-01, -9.87142086e-01, -3.15766156e-01,\n",
       "        -9.99501705e-01,  1.03958800e-01,  3.66623811e-02,\n",
       "         5.52725457e-02, -6.98110275e-03,  5.20197637e-02,\n",
       "         9.35874641e-01,  1.77974150e-01, -9.33702469e-01,\n",
       "        -9.62518990e-01, -6.22293800e-02, -1.27697185e-01,\n",
       "        -6.05403244e-01,  7.40106702e-01,  8.01661983e-02,\n",
       "        -1.11532025e-01, -9.64974463e-01,  5.82960993e-02,\n",
       "        -9.21778530e-02, -7.66020000e-01,  8.65100265e-01,\n",
       "         1.60856724e-01,  8.68070591e-03,  8.70434642e-01,\n",
       "         2.46635273e-01,  7.83751085e-02,  9.92844641e-01,\n",
       "         6.14387035e-01, -1.75793748e-02, -7.66082630e-02,\n",
       "         1.61964029e-01, -6.91452101e-02,  9.20241177e-01,\n",
       "         8.14272016e-02, -8.68668184e-02, -8.73784065e-01,\n",
       "         6.65964484e-01, -9.70765054e-01, -8.72335657e-02,\n",
       "         5.02584577e-02, -1.94932193e-01,  7.35808117e-03,\n",
       "         9.45980608e-01, -2.73332279e-02,  3.05887312e-01,\n",
       "         1.56341922e-02, -2.66288724e-02,  4.70141843e-02,\n",
       "         9.61634874e-01,  1.28051080e-02,  7.28413016e-02,\n",
       "         2.82213300e-01,  9.99950945e-01, -1.01348929e-01,\n",
       "         1.73830435e-01,  9.69642997e-01,  1.66812807e-01,\n",
       "         1.07169673e-01, -9.81038690e-01, -2.54572295e-02]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "convertible-kelly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(torchModelLight.parameters()).is_cuda#torchModelLight.TorchModel.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "registered-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakePredictions(X, model, transformer, classes = None):\n",
    "    \n",
    "    #Check model device\n",
    "    #modelOnCuda = next(torchModelLight.parameters()).is_cuda\n",
    "    \n",
    "    #Make sure both model and inputs are on the same device\n",
    "    cuda_enabled = torch.cuda.is_available()\n",
    "    if cuda_enabled:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    X = transformer.transform(X) #Output is numpy array\n",
    "    X = torch.tensor(X).to(device)\n",
    "    output = model.predict(X)\n",
    "    \n",
    "    if classes is not None:\n",
    "        output = [classes[i] for i in output]\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "asian-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Avvikelse', 'Beröm', 'Fråga', 'Förseningsersättning', 'Klagomål',\n",
    "       'Skada', 'Synpunkt/Önskemål']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "lesbian-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.Series(data={'Beskrivning_Anonymized':'Ni har världens bästa utvecklare. Måste bara säga det :)'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "international-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/1\n",
      "output shape: (1, 768)\n",
      "transformed.shape: (1, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Beröm']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MakePredictions(testing, torchModelLight, transformer, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cardiovascular-madonna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torchModel.predict(testing)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "several-spokesman",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-7542f42db7f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "classes[output[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "operational-solomon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 6, ..., 1, 0, 4], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "swedish-wireless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Avvikelse', 'Beröm', 'Fråga', 'Förseningsersättning', 'Klagomål',\n",
       "       'Skada', 'Synpunkt/Önskemål'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "modular-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate loaded model on full dataset\n",
    "mMiniBatcherFull = MiniBatcher(X[:50000], Y[:50000], batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/500\n",
      "batch: 2/500\n",
      "batch: 3/500\n",
      "batch: 4/500\n",
      "batch: 5/500\n",
      "batch: 6/500\n",
      "batch: 7/500\n",
      "batch: 8/500\n",
      "batch: 9/500\n",
      "batch: 10/500\n",
      "batch: 11/500\n",
      "batch: 12/500\n",
      "batch: 13/500\n",
      "batch: 14/500\n",
      "batch: 15/500\n",
      "batch: 16/500\n",
      "batch: 17/500\n",
      "batch: 18/500\n",
      "batch: 19/500\n",
      "batch: 20/500\n",
      "batch: 21/500\n",
      "batch: 22/500\n",
      "batch: 23/500\n",
      "batch: 24/500\n",
      "batch: 25/500\n",
      "batch: 26/500\n",
      "batch: 27/500\n",
      "batch: 28/500\n"
     ]
    }
   ],
   "source": [
    "Y_pred = []\n",
    "targets = []\n",
    "for X_batch, labels in mMiniBatcherFull.getBatchIterator():\n",
    "  \n",
    "        output = torchModel.predict(X_batch)\n",
    "        Y_pred.extend(output)\n",
    "        targets.extend(labels)\n",
    "        print(mMiniBatcherFull.getBatchInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(Y_pred == targets)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(targets, Y_pred, normalize='true', labels=list(set(Y_test)))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(Y_test))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(targets, Y_pred, normalize=None, labels=list(set(Y_test)))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(Y_test))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "selected-least",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(range(Y_test.columns.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-report",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
