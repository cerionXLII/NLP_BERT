{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "external-disability",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "composed-comfort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quick-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased', use_fast=False)\n",
    "#model = AutoModel.from_pretrained('KB/bert-base-swedish-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "latter-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs = tok(['hälsa hälsan hälsans'], return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True)\n",
    "#inputs = tok(X[:10].values.tolist(), return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ultimate-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs = model(**inputs)\n",
    "#outputs = outputs['pooler_output'].detach().numpy().reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fundamental-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPTransformer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print('Init called')\n",
    "        self.model_name = 'KB/bert-base-swedish-cased'\n",
    "        self.Bert = AutoModel.from_pretrained(self.model_name)\n",
    "        self.Tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.batch_size = 50\n",
    "        \n",
    "        #cuda_enabled = torch.cuda.is_available()\n",
    "        #if cuda_enabled:\n",
    "        #    device = torch.device('cuda')\n",
    "        #    self.batch_size = 25\n",
    "        #else:\n",
    "        #    device = torch.device('cpu')\n",
    "        #    self.batch_size = 20\n",
    "\n",
    "        #self.Bert.to(device)\n",
    "        #print(f'We are running on device: {device}')\n",
    "        \n",
    "        #Freeze the Bert model layers\n",
    "        for param in self.Bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        print('Fit called')\n",
    "        \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def partial_fit(self, X, y=None):\n",
    "        print('Partial Fit called')\n",
    "\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        print('Transform Called')\n",
    "        \n",
    "        #Check device\n",
    "        device = self.Bert.device\n",
    "        \n",
    "        #Preprocess data\n",
    "        X = X.str.replace('\\n','')\n",
    "        X = X.str.replace('\\r','')\n",
    "        X = X.str.lower()\n",
    "        X = X.values.tolist()\n",
    "       \n",
    "        # Transform input tokens. This is most efficient if done in one batch \n",
    "        inputs = self.Tokenizer(X, return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True)\n",
    "\n",
    "        # Run Bert model, We must mini batch this in order to not overflow the memory of the system\n",
    "        transformed = []\n",
    "        \n",
    "        batches = int(np.ceil(len(X) / self.batch_size))\n",
    "        for batchId in range(batches):\n",
    "            print(f'Running batch {batchId+1}/{batches}')\n",
    "        \n",
    "            inputs_batch = {}\n",
    "            for key in inputs.keys():\n",
    "                inputs_batch[key] = inputs[key][batchId * self.batch_size:(batchId + 1) * self.batch_size].to(device)\n",
    "            \n",
    "            inputs_batch\n",
    "            outputs = self.Bert(**inputs_batch)\n",
    "            #inputs = self.Tokenizer(X[batchId * self.batch_size:(batchId + 1) * self.batch_size].values.tolist(), return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True).to(device)\n",
    "            #outputs = self.Bert(**inputs)\n",
    "            #outputs = outputs['pooler_output'].detach().numpy()\n",
    "            \n",
    "            outputs = outputs['pooler_output'].to('cpu').detach().numpy()\n",
    "            print(f'output shape: {outputs.shape}')\n",
    "            transformed.extend(outputs)\n",
    "        \n",
    "        transformed = np.array(transformed)\n",
    "        print(f'transformed.shape: {transformed.shape}')\n",
    "        \n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "extensive-honor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([    \n",
    "            ('nlpTransformer', NLPTransformer()),\n",
    "            ('clf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-smooth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "terminal-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blessed-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-01876a210620>:3: DtypeWarning: Columns (1,3,5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,39,41,43,45,47,49,51,52,54,56,57,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/CRMIncidents_Anonymized_Complete_Table.csv')\n"
     ]
    }
   ],
   "source": [
    "#Test data anonymized\n",
    "#df = pd.read_csv('../data/CRMIncidents_Anonymized.csv')\n",
    "df = pd.read_csv('../data/CRMIncidents_Anonymized_Complete_Table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acceptable-value",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>CRMIncidentId</th>\n",
       "      <th>IncidentId</th>\n",
       "      <th>LineId</th>\n",
       "      <th>Linje</th>\n",
       "      <th>JourneyId</th>\n",
       "      <th>TurNummer</th>\n",
       "      <th>Trafikslag</th>\n",
       "      <th>Ankomstdag</th>\n",
       "      <th>...</th>\n",
       "      <th>Enhet</th>\n",
       "      <th>Queue_SK</th>\n",
       "      <th>Kö</th>\n",
       "      <th>ModifiedOn</th>\n",
       "      <th>ContactId</th>\n",
       "      <th>Contact_SK</th>\n",
       "      <th>iBID</th>\n",
       "      <th>IsActive</th>\n",
       "      <th>TicketId</th>\n",
       "      <th>Beskrivning_Anonymized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>673326.0</td>\n",
       "      <td>77DE23B5-43D8-E811-80F6-005056B63599</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2018-10-29 11:33:19</td>\n",
       "      <td>586769.0</td>\n",
       "      <td>C0BDFF48-2C0B-E811-80F1-005056B64D75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hej,\\r\\r\\n \\r\\r\\nHar nu fått tag i föraren som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>673354.0</td>\n",
       "      <td>EBD68435-41D8-E811-80F6-005056B63599</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-25</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2018-11-08 08:17:09</td>\n",
       "      <td>331145.0</td>\n",
       "      <td>E429FD9A-5DEC-E411-80D6-0050569071BE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buss 000 00:00\\r\\r\\n\\r\\r\\nKristianstad Hästtor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>673617.0</td>\n",
       "      <td>3ED0AA37-16D9-E811-80F4-005056B62B18</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2021-10-07 08:49:10</td>\n",
       "      <td>726145.0</td>\n",
       "      <td>12F4B35F-16D9-E811-80F4-005056B62B18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Skadeanmälan för påkörning av bil bakifrån vid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>673807.0</td>\n",
       "      <td>AAA91AC9-E0D8-E811-80F5-005056B64D75</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2018-11-08 10:36:12</td>\n",
       "      <td>603794.0</td>\n",
       "      <td>56A30A33-A32D-E811-80F2-005056B62B18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hej, \\r\\r\\nVarför heter en av hållplatserna i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>673850.0</td>\n",
       "      <td>5F1165E6-63D9-E811-80F5-005056B64D75</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>...</td>\n",
       "      <td>Kundtjänst</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Support Synpunkter</td>\n",
       "      <td>2019-11-27 15:06:13</td>\n",
       "      <td>22229.0</td>\n",
       "      <td>383EAEB1-1DEB-E411-80D8-005056903A38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hej!\\r\\r\\nHar en fråga som gäller busskurerna ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1 Unnamed: 0  CRMIncidentId  \\\n",
       "0             0          0       673326.0   \n",
       "1             1          1       673354.0   \n",
       "2             2          2       673617.0   \n",
       "3             3          3       673807.0   \n",
       "4             4          4       673850.0   \n",
       "\n",
       "                             IncidentId  LineId Linje  JourneyId TurNummer  \\\n",
       "0  77DE23B5-43D8-E811-80F6-005056B63599    -1.0   NaN       -1.0       NaN   \n",
       "1  EBD68435-41D8-E811-80F6-005056B63599    -1.0   NaN       -1.0       NaN   \n",
       "2  3ED0AA37-16D9-E811-80F4-005056B62B18    -1.0   NaN       -1.0       NaN   \n",
       "3  AAA91AC9-E0D8-E811-80F5-005056B64D75    -1.0   NaN       -1.0       NaN   \n",
       "4  5F1165E6-63D9-E811-80F5-005056B64D75    -1.0   NaN       -1.0       NaN   \n",
       "\n",
       "  Trafikslag  Ankomstdag  ...       Enhet Queue_SK                  Kö  \\\n",
       "0        NaN  2018-10-25  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "1        NaN  2018-10-25  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "2        NaN  2018-10-26  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "3        NaN  2018-10-26  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "4        NaN  2018-10-26  ...  Kundtjänst      8.0  Support Synpunkter   \n",
       "\n",
       "            ModifiedOn ContactId                            Contact_SK iBID  \\\n",
       "0  2018-10-29 11:33:19  586769.0  C0BDFF48-2C0B-E811-80F1-005056B64D75  NaN   \n",
       "1  2018-11-08 08:17:09  331145.0  E429FD9A-5DEC-E411-80D6-0050569071BE  NaN   \n",
       "2  2021-10-07 08:49:10  726145.0  12F4B35F-16D9-E811-80F4-005056B62B18  NaN   \n",
       "3  2018-11-08 10:36:12  603794.0  56A30A33-A32D-E811-80F2-005056B62B18  NaN   \n",
       "4  2019-11-27 15:06:13   22229.0  383EAEB1-1DEB-E411-80D8-005056903A38  NaN   \n",
       "\n",
       "  IsActive TicketId                             Beskrivning_Anonymized  \n",
       "0     True      NaN  Hej,\\r\\r\\n \\r\\r\\nHar nu fått tag i föraren som...  \n",
       "1     True      NaN  Buss 000 00:00\\r\\r\\n\\r\\r\\nKristianstad Hästtor...  \n",
       "2     True      NaN  Skadeanmälan för påkörning av bil bakifrån vid...  \n",
       "3     True      NaN  Hej, \\r\\r\\nVarför heter en av hållplatserna i ...  \n",
       "4     True      NaN  Hej!\\r\\r\\nHar en fråga som gäller busskurerna ...  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "about-doubt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'CRMIncidentId', 'IncidentId', 'LineId',\n",
       "       'Linje', 'JourneyId', 'TurNummer', 'Trafikslag', 'Ankomstdag',\n",
       "       'Händelsedatum', 'Hanteratdatum', 'Ärendenummer', 'KategoriId11',\n",
       "       'Kategori11', 'KategoriId12', 'Kategori12', 'KategoriId13',\n",
       "       'Kategori13', 'KategoriId21', 'Kategori21', 'KategoriId22',\n",
       "       'Kategori22', 'KategoriId23', 'Kategori23', 'KategoriId31',\n",
       "       'Kategori31', 'KategoriId32', 'Kategori32', 'KategoriId33',\n",
       "       'Kategori33', 'KategoriId41', 'Kategori41', 'KategoriId42',\n",
       "       'Kategori42', 'KategoriId43', 'Kategori43', 'Titel', 'CaseType_SK',\n",
       "       'Ärendetyp', 'CaseOrigin_SK', 'Ursprung', 'Priority_SK', 'Prioritet',\n",
       "       'IncidentStage_SK', 'ÄrendeStatus', 'Owner_SK', 'Handläggare',\n",
       "       'BusinessUnit_SK', 'Enhet', 'Queue_SK', 'Kö', 'ModifiedOn', 'ContactId',\n",
       "       'Contact_SK', 'iBID', 'IsActive', 'TicketId', 'Beskrivning_Anonymized'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "impossible-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove nans, select relevant columns\n",
    "df = df[['Ärendetyp', 'Beskrivning_Anonymized', 'Prioritet', 'Ankomstdag']]\n",
    "df = df[~df['Ärendetyp'].isna()]\n",
    "df = df[~df['Beskrivning_Anonymized'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prescribed-member",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([288232.,  88707.,      0.,   5276.,      0., 160978., 799256.,\n",
       "             0.,   6202.,   2816.]),\n",
       " array([0. , 0.6, 1.2, 1.8, 2.4, 3. , 3.6, 4.2, 4.8, 5.4, 6. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(df['Ärendetyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "differential-situation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Förseningsersättning    799256\n",
       "Klagomål                288232\n",
       "Fråga                   160978\n",
       "Synpunkt/Önskemål        88707\n",
       "Beröm                     6202\n",
       "Skada                     5276\n",
       "Avvikelse                 2816\n",
       "Name: Ärendetyp, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Ärendetyp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "focused-press",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1351467, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spoken-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df['Ärendetyp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "neither-butter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Klagomål', 'Synpunkt/Önskemål', 'Skada', 'Fråga',\n",
       "       'Förseningsersättning', 'Beröm', 'Avvikelse'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quality-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a balanced training set\n",
    "desiredCount = 20000\n",
    "#desiredCount = 1500\n",
    "dfBalanced = None\n",
    "for category in categories:\n",
    "    sample = df[df['Ärendetyp'] == category].sample(n=desiredCount, replace=True, random_state=42)\n",
    "    if dfBalanced is None:\n",
    "        dfBalanced = sample\n",
    "    else:\n",
    "        dfBalanced = pd.concat([dfBalanced, sample], ignore_index=True)\n",
    "\n",
    "\n",
    "#Next random shuffle of all rows\n",
    "df = dfBalanced.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "boolean-birmingham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df[:10000] #Max for this computer\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "measured-reflection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([20000., 20000.,     0., 20000.,     0., 20000., 20000.,     0.,\n",
       "        20000., 20000.]),\n",
       " array([0. , 0.6, 1.2, 1.8, 2.4, 3. , 3.6, 4.2, 4.8, 5.4, 6. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmMAAAJACAYAAADB8GHlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAArF0lEQVR4nO3dfdxlZV0v/s9XUDIVQZk4CChEY4aWKBNSadJBEU0DyxReJmAckSOknh6MspPkw8m0sijDg0nA+Zn4SHIIROKo5APKIMiTGiNiDiGMYpppJHr9/ljX7WzGex7ve+aaGd7v12u/7rWvda21rrX22nt/1rXW2ne11gIAwBj3Gt0AAIB7MmEMAGAgYQwAYCBhDABgIGEMAGCgHUc3YFPttttubZ999hndDACA9bryyiu/3FpbMt+4bTaM7bPPPlm+fPnoZgAArFdVfWFt45ymBAAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGGi9Yayq9q6qD1TVDVV1fVW9pJc/qKouqaob+99de3lV1WlVtaKqrqmqx87M69he/8aqOnam/MCqurZPc1pV1eZYWQCArc2G9IzdleQ3Wmv7Jzk4yUlVtX+SU5Jc2lpbmuTS/jxJnppkaX+ckOT0ZApvSV6R5HFJDkryirkA1+u8YGa6wxe+agAAW7/1hrHW2q2ttU/24X9L8ukkeyY5IsnZvdrZSY7sw0ckOadNLk+yS1XtkeQpSS5prd3RWvtqkkuSHN7H7dxau7y11pKcMzMvAIDt2o4bU7mq9knymCQfT7J7a+3WPupLSXbvw3sm+eLMZCt72brKV85TPt/yT8jU25aHPvShG9P0TbLPKX+/2ZfBxrv5tT8/ugmLYnvav7aX1yTZvl6X7YX9i81pa9i/NvgC/qq6f5J3J3lpa+3rs+N6j1Zb5LZ9n9baGa21Za21ZUuWLNnciwMA2Ow2KIxV1b0zBbG3ttbe04tv66cY0//e3stvSbL3zOR79bJ1le81TzkAwHZvQ+6mrCRvSfLp1tqfzow6P8ncHZHHJnnvTPkx/a7Kg5N8rZ/OvDjJYVW1a79w/7AkF/dxX6+qg/uyjpmZFwDAdm1Drhn7mSTPS3JtVV3dy343yWuTvKOqjk/yhSTP7uMuTPK0JCuSfDPJ85OktXZHVb0qyRW93itba3f04RclOSvJfZNc1B8AANu99Yax1tqHk6ztd78Onad+S3LSWuZ1ZpIz5ylfnuRR62sLAMD2xi/wAwAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAwkjAEADCSMAQAMJIwBAAy03jBWVWdW1e1Vdd1M2dur6ur+uLmqru7l+1TVt2bGvWlmmgOr6tqqWlFVp1VV9fIHVdUlVXVj/7vrZlhPAICt0ob0jJ2V5PDZgtbac1prB7TWDkjy7iTvmRn9ublxrbUTZ8pPT/KCJEv7Y26epyS5tLW2NMml/TkAwD3CesNYa+2yJHfMN673bj07ydvWNY+q2iPJzq21y1trLck5SY7so49IcnYfPnumHABgu7fQa8aekOS21tqNM2X7VtVVVfWhqnpCL9szycqZOit7WZLs3lq7tQ9/Kcnua1tYVZ1QVcuravmqVasW2HQAgPEWGsaOzt17xW5N8tDW2mOS/HqSv62qnTd0Zr3XrK1j/BmttWWttWVLlizZ1DYDAGw1dtzUCatqxyS/mOTAubLW2p1J7uzDV1bV55I8PMktSfaamXyvXpYkt1XVHq21W/vpzNs3tU0AANuahfSMPSnJZ1pr3zv9WFVLqmqHPvzDmS7Uv6mfhvx6VR3crzM7Jsl7+2TnJzm2Dx87Uw4AsN3bkJ+2eFuSjyX50apaWVXH91FH5fsv3P/ZJNf0n7p4V5ITW2tzF/+/KMlfJ1mR5HNJLurlr03y5Kq6MVPAe+2mrw4AwLZlvacpW2tHr6X8uHnK3p3ppy7mq788yaPmKf9KkkPX1w4AgO2RX+AHABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGGi9Yayqzqyq26vqupmyU6vqlqq6uj+eNjPud6pqRVV9tqqeMlN+eC9bUVWnzJTvW1Uf7+Vvr6r7LOYKAgBszTakZ+ysJIfPU/6G1toB/XFhklTV/kmOSvLIPs1fVdUOVbVDkjcmeWqS/ZMc3esmyR/1ef1Ikq8mOX4hKwQAsC1ZbxhrrV2W5I4NnN8RSc5trd3ZWvt8khVJDuqPFa21m1pr/5nk3CRHVFUl+a9J3tWnPzvJkRu3CgAA266FXDN2clVd009j7trL9kzyxZk6K3vZ2sofnORfW2t3rVE+r6o6oaqWV9XyVatWLaDpAABbh00NY6cn2S/JAUluTfIni9WgdWmtndFaW9ZaW7ZkyZItsUgAgM1qx02ZqLV229xwVb05yQX96S1J9p6pulcvy1rKv5Jkl6rasfeOzdYHANjubVLPWFXtMfP0mUnm7rQ8P8lRVbVTVe2bZGmSTyS5IsnSfufkfTJd5H9+a60l+UCSZ/Xpj03y3k1pEwDAtmi9PWNV9bYkhyTZrapWJnlFkkOq6oAkLcnNSV6YJK2166vqHUluSHJXkpNaa9/p8zk5ycVJdkhyZmvt+r6I305yblW9OslVSd6yWCsHALC1W28Ya60dPU/xWgNTa+01SV4zT/mFSS6cp/ymTHdbAgDc4/gFfgCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgdYbxqrqzKq6vaqumyl7fVV9pqquqarzqmqXXr5PVX2rqq7ujzfNTHNgVV1bVSuq6rSqql7+oKq6pKpu7H933QzrCQCwVdqQnrGzkhy+RtklSR7VWvuJJP+U5Hdmxn2utXZAf5w4U356khckWdofc/M8JcmlrbWlSS7tzwEA7hHWG8Zaa5cluWONsve31u7qTy9Pste65lFVeyTZubV2eWutJTknyZF99BFJzu7DZ8+UAwBs9xbjmrFfTXLRzPN9q+qqqvpQVT2hl+2ZZOVMnZW9LEl2b63d2oe/lGT3tS2oqk6oquVVtXzVqlWL0HQAgLEWFMaq6uVJ7kry1l50a5KHttYek+TXk/xtVe28ofPrvWZtHePPaK0ta60tW7JkyQJaDgCwddhxUyesquOSPD3JoT1EpbV2Z5I7+/CVVfW5JA9Pckvufipzr16WJLdV1R6ttVv76czbN7VNAADbmk3qGauqw5O8LMkvtNa+OVO+pKp26MM/nOlC/Zv6acivV9XB/S7KY5K8t092fpJj+/CxM+UAANu99faMVdXbkhySZLeqWpnkFZnuntwpySX9Fyou73dO/mySV1bVt5N8N8mJrbW5i/9flOnOzPtmusZs7jqz1yZ5R1Udn+QLSZ69KGsGALANWG8Ya60dPU/xW9ZS991J3r2WccuTPGqe8q8kOXR97QAA2B75BX4AgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgYQxAICBhDEAgIGEMQCAgTYojFXVmVV1e1VdN1P2oKq6pKpu7H937eVVVadV1YqquqaqHjszzbG9/o1VdexM+YFVdW2f5rSqqsVcSQCArdWG9oydleTwNcpOSXJpa21pkkv78yR5apKl/XFCktOTKbwleUWSxyU5KMkr5gJcr/OCmenWXBYAwHZpg8JYa+2yJHesUXxEkrP78NlJjpwpP6dNLk+yS1XtkeQpSS5prd3RWvtqkkuSHN7H7dxau7y11pKcMzMvAIDt2kKuGdu9tXZrH/5Skt378J5JvjhTb2UvW1f5ynnKv09VnVBVy6tq+apVqxbQdACArcOiXMDfe7TaYsxrPcs5o7W2rLW2bMmSJZt7cQAAm91Cwtht/RRj+t/be/ktSfaeqbdXL1tX+V7zlAMAbPcWEsbOTzJ3R+SxSd47U35Mv6vy4CRf66czL05yWFXt2i/cPyzJxX3c16vq4H4X5TEz8wIA2K7tuCGVquptSQ5JsltVrcx0V+Rrk7yjqo5P8oUkz+7VL0zytCQrknwzyfOTpLV2R1W9KskVvd4rW2tzNwW8KNMdm/dNclF/AABs9zYojLXWjl7LqEPnqduSnLSW+ZyZ5Mx5ypcnedSGtAUAYHviF/gBAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABhLGAAAGEsYAAAYSxgAABtrkMFZVP1pVV888vl5VL62qU6vqlpnyp81M8ztVtaKqPltVT5kpP7yXraiqUxa6UgAA24odN3XC1tpnkxyQJFW1Q5JbkpyX5PlJ3tBa++PZ+lW1f5KjkjwyyUOS/ENVPbyPfmOSJydZmeSKqjq/tXbDprYNAGBbsclhbA2HJvlca+0LVbW2OkckObe1dmeSz1fViiQH9XErWms3JUlVndvrCmMAwHZvsa4ZOyrJ22aen1xV11TVmVW1ay/bM8kXZ+qs7GVrK/8+VXVCVS2vquWrVq1apKYDAIyz4DBWVfdJ8gtJ3tmLTk+yX6ZTmLcm+ZOFLmNOa+2M1tqy1tqyJUuWLNZsAQCGWYzTlE9N8snW2m1JMvc3SarqzUku6E9vSbL3zHR79bKsoxwAYLu2GKcpj87MKcqq2mNm3DOTXNeHz09yVFXtVFX7Jlma5BNJrkiytKr27b1sR/W6AADbvQX1jFXV/TLdBfnCmeLXVdUBSVqSm+fGtdaur6p3ZLow/64kJ7XWvtPnc3KSi5PskOTM1tr1C2kXAMC2YkFhrLX270kevEbZ89ZR/zVJXjNP+YVJLlxIWwAAtkV+gR8AYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYCBhDABgIGEMAGAgYQwAYKAFh7Gqurmqrq2qq6tqeS97UFVdUlU39r+79vKqqtOqakVVXVNVj52Zz7G9/o1VdexC2wUAsC1YrJ6xn2utHdBaW9afn5Lk0tba0iSX9udJ8tQkS/vjhCSnJ1N4S/KKJI9LclCSV8wFOACA7dnmOk15RJKz+/DZSY6cKT+nTS5PsktV7ZHkKUkuaa3d0Vr7apJLkhy+mdoGALDVWIww1pK8v6qurKoTetnurbVb+/CXkuzeh/dM8sWZaVf2srWV301VnVBVy6tq+apVqxah6QAAY+24CPN4fGvtlqr6oSSXVNVnZke21lpVtUVYTlprZyQ5I0mWLVu2KPMEABhpwT1jrbVb+t/bk5yX6Zqv2/rpx/S/t/fqtyTZe2byvXrZ2soBALZrCwpjVXW/qnrA3HCSw5Jcl+T8JHN3RB6b5L19+Pwkx/S7Kg9O8rV+OvPiJIdV1a79wv3DehkAwHZtoacpd09yXlXNzetvW2vvq6orkryjqo5P8oUkz+71L0zytCQrknwzyfOTpLV2R1W9KskVvd4rW2t3LLBtAABbvQWFsdbaTUkePU/5V5IcOk95S3LSWuZ1ZpIzF9IeAIBtjV/gBwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhIGAMAGEgYAwAYSBgDABhok8NYVe1dVR+oqhuq6vqqekkvP7Wqbqmqq/vjaTPT/E5Vraiqz1bVU2bKD+9lK6rqlIWtEgDAtmPHBUx7V5LfaK19sqoekOTKqrqkj3tDa+2PZytX1f5JjkryyCQPSfIPVfXwPvqNSZ6cZGWSK6rq/NbaDQtoGwDANmGTw1hr7dYkt/bhf6uqTyfZcx2THJHk3NbanUk+X1UrkhzUx61ord2UJFV1bq8rjAEA271FuWasqvZJ8pgkH+9FJ1fVNVV1ZlXt2sv2TPLFmclW9rK1lc+3nBOqanlVLV+1atViNB0AYKgFh7Gqun+Sdyd5aWvt60lOT7JfkgMy9Zz9yUKXMae1dkZrbVlrbdmSJUsWa7YAAMMs5JqxVNW9MwWxt7bW3pMkrbXbZsa/OckF/ektSfaemXyvXpZ1lAMAbNcWcjdlJXlLkk+31v50pnyPmWrPTHJdHz4/yVFVtVNV7ZtkaZJPJLkiydKq2req7pPpIv/zN7VdAADbkoX0jP1Mkuclubaqru5lv5vk6Ko6IElLcnOSFyZJa+36qnpHpgvz70pyUmvtO0lSVScnuTjJDknObK1dv4B2AQBsMxZyN+WHk9Q8oy5cxzSvSfKaecovXNd0AADbK7/ADwAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADCQMAYAMJAwBgAwkDAGADDQVhPGqurwqvpsVa2oqlNGtwcAYEvYKsJYVe2Q5I1Jnppk/yRHV9X+Y1sFALD5bRVhLMlBSVa01m5qrf1nknOTHDG4TQAAm1211ka3IVX1rCSHt9b+W3/+vCSPa62dvEa9E5Kc0J/+aJLPbuam7Zbky5t5Gfc0tunisj0Xn226uGzPxWebLq4ttT0f1lpbMt+IHbfAwhdNa+2MJGdsqeVV1fLW2rIttbx7Att0cdmei882XVy25+KzTRfX1rA9t5bTlLck2Xvm+V69DABgu7a1hLErkiytqn2r6j5Jjkpy/uA2AQBsdlvFacrW2l1VdXKSi5PskOTM1tr1g5uVbMFTovcgtunisj0Xn226uGzPxWebLq7h23OruIAfAOCeams5TQkAcI8kjAEADCSMraGq/qaq3ltV9x7dliSpqu9U1dUzj32qapequrCqHlJV79oCbfjrre0/IvT136WqPjpTdkBVPW3m+SFV9dMbMK8tsh3Xsfy51/hTVfXJDWnzPPM4pap+oapeWVVP2hztvCfZ0u+xjVVVO1XV+VX1li283JdX1fVVdU3fZx9XVTdX1W6bOL9Tq+o3F7udrFtVPbOqrqyqx41uy8aqqiOrqlXVIzZx+o/2v4dU1QUbMd1Z/TdRN4ut4gL+rUVVPSrJF5N8JMlhSf5+bIuSJN9qrR0wT/lc6Jh356iqHVtrdy1GA+Z+jHdLq6odWmvfmW9ca21u/WeDywFJliW5sD8/JMk3knw069Ba+5esZTtuId97javqKUn+MMkTN2TCqqpM136+the5C3kRtNb+Net5jw325CR/l+SHq+qRW+KGp6r6qSRPT/LY1tqdPYDdZ3Mvl83iuUkOTfIXST4+uC0b6+gkH+5/X7GxE7fWNvpgd0u4x/SMzdfDNF+1/rhX/7tV6j1Al/ej0/Oqatde/sGq+rOqWp7kJVX1y1V1Xe9xuazX2aGqXl9VV/TpX9jLD+nTv6uqPlNVb+1f9HPzXdaHv1FVr+nzvLyqdu/l+/Xn11bVq6vqG718j6q6rG/z66rqCb38sKr6WO8JemdV3b+X31xVf1RVn0zyy1X14qq6obf13F7n4D7tVVX10ar60Zp+EuWVSZ7Tl/XbSU5M8j/68yf0I5vT+jQ3zR3l1NTbeF0fPq6q3lNV76uqG6vqdTPb/fiq+qeq+kRVvbmq/nIzvLw7J/nqzDJ/a+a1+oOZ9n62qs5Jcl2Svftrel3f/s+ZeU0/VFNP701V9dqqem5v/7VVtd9iN77m6TlZ7GVsZHvmPZqtqpdW1Q+uUXZKVT23D5/Q3wef6dvr8Zu4/I06+t6YWWeez6paQC/VBtgjyZdba3cmSWvty/1AZm7Z962qi6rqBVV1/6q6tL+/r62qI2bqvby/jz6c6T+pzJW/oO/rn6qqd6/5+myL5j4H+/DT+no/rMb3CM7tM9/LAP2zb3N8pi2a/j3x+CTHJzmqqg6vqnfOjD+kqi6oqhOr6vUz5d9bt9nXZGb8T/bvk/2q6sD+uXllVV1cVXvMU/+1M99Lf9zLlvT99or++JmNWrnW2j3ikeQb6xhXSe7Vh89O8n+T3Gd0m3t7vpPk6v44r5ddk+SJffiVSf6sD38wyV/NTHttkj378C797wlJfq8P75RkeZJ9M/UifS3TD+7eK8nHkjx+Zr7L+nBL8ow+/LqZeV2Q5Og+fOLc9k7yG0le3od3SPKATP964rIk9+vlv53k9/vwzUleNrMO/5JkpzXW4YFJduzDT0ry7j58XJK/nJn21CS/OfP8rCTv7Ou3f6b/h5ok+yS5bmYeN/Vl/ECSL2T6QeKH9LY9KMm9k/zj7LIW6TX+TH8NDuzlh2W65XruS/eCJD/b2/vdJAf3er+U5JK+fXdP8s+ZvjgPSfKvfXinTD+k/Ad9mpek7zeLuK/+VN9v5l6v3ZI8ZPD756wkz5qn/OYku61R9oEkSzL1/lw5Nz7JY/s2/S+bsPxDklywGdZrp74//M361msRl3n/vp/+U5K/yurPoJv7PvkPSY7pZTsm2XlmP1jR9+MDM30u/WCmA48Vc+/RJA+eWdark/zayH1nkbbZ3OfgoX1d9+vPT83MZ9OAdv1Skk8m+amZsuMW6zNtM7b7uUne0oc/muRx/b05911yepJf6e/jFTPTXZTV32dzr8kh/T300/39/tBMn+0fTbKk13lOpp/a+t5nSZIHZ/pXjHO/RrFL//u3M8t4aJJPb8y63WN6xta0lt6F05M8MskPJ3n5TN2n9SPkK3vPygW9/KBao4dmMzT1W621A/rjmVX1wEwv/of6+LMzfUHPefvM8EeSnFVVL8j0RZ1MX/DHVNXVmbqnH5xkaR/3idbaytbadzN96O4zT3v+M9MOnEw78Fydn8oUdJJpp5xzRZLnV9WpSX68tfZvSQ7OFIY+0ttxbJKHrWUdrkny1qr6lSRzp113TvLO3pv1hkyv2Yb6u9bad1trN2QKLvO5tLX2tdbafyS5obftoCQfaq3d0Vr79sy6Loa51/gRSQ5Pck5VVabX6rAkV2X64HxEVr9WX2itXd6HH5/kba2177TWbkvyoSQ/2cdd0Vq7tU29GZ9L8v5efm3mf30X4vt6TpI8oqr+bq5CVT25qs7rw2vrZT2rqt5UVct7T8LTe/ndjtz7EfAh65rXrKp6VZ/3SzKF6w9U1Qf6uJ0zHYCtynRw8Fu9/WmtfTLT++ykXvfmqvqDmV6fR/TyJ9bqnverquoBayx/vUffNfVCv6Gv+6f7NO+pqZf21TOze3vf3gfV9D97N7vW2jcyhakTkqxK8vaqOq6Pfm+mYHjO3Oom+V9VdU2mkLZnpvfbEzIdVH6ztfb13P20+qOq6h+r6tpMX7ob877ealXVzyZ5c5Knt9Y+N8/4eXsEa+1nG6oW0BNeVc9I8rJMr9Gr5nuvbMWOTnJuHz43yS8neV+SZ1TVjkl+Psl7+/v4pprOojw402fnR+aZ349lOuB9RmvtnzP11D4qySX9u+n3MnVQzPpakv9I8paq+sUk3+zlT0ryl32685PsXP2Mz4a4J4Wx+858UJ7Xy5Zm6kl6ZGvtC5l6cJYl+YkkT6yqn6iqH0jyv5M8tbV2YKbEPeczSZ7QWntMkt9P8r+23Oqs1b/PDbTWTsy0M+2d5Mq+U1amI865gLdva23uC/rOmfl8J/NfU/jt1qP/Oup8T2vtskxh8ZZMwfCY3oZLZtqwf2vt+PnWIdOb642Zeieu6G+4VyX5QGvtUUmekakHa0PNruPaTkVvyHbYLFprH8vUk7AkU/v+cGY7/Uhrbe6C7X9f60zubnZdvjvz/LtZ/PV6f6aDmn+qqr+qqidm6m16RFXNvW+en+TMPny/JJe31h6dqaf0BTPz2idTAP75JG/q78N1Wde8UtMpiyVJnt9a+/NMPa4/11r7uV7lSUku7cOPzHSgMWt57h4Ovtxae2ymI/G5002/meSkNl3/94Qk35pZ/k8neVOSIzIdyf9Fph67A/v2eM3MvP+zfw69KVPIOSnTF8Rx/T2cJL/ap12W5MUz5ZtVD/wfbK29IsnJmXpYkumL7vB+EJFMYWpJpl7eA5LclvW/T89KcnJr7ceT/MEG1N8W7JTp2r4jW2ufWUud97TWfrLvu5/OdAouSf48yZ/37bFypv4vZro+9tGZ9tvX1+pTaY/OdGbix5I8L8nDW2sHJfnrJL/W63w4U6/6YzIFmpctdCW3hKp6UJL/muSvq+rmJL+V5NmZDkye3cct7wf8ybRuz860j543870169ZMweoxc4tJcv3MZ+6Pt9YOm52gTddiH5TkXZl60d/XR90r03adm3bPfgCzQe5JYexuPUy9bLZ3IUmeXdO1Sldl+uDdP1Oivqm19vle520z9R+YTe+h2SStta8l+Wr1a68yveE+NF/dqtqvtfbx1trvZzqS3TvTfzn479XvFq2qh1fV/RahaZdn9QfzUTNteFiS21prb870gfDYXvdnqupHep37VdXD52n/vZLs3Vr7QKbeigdmOlXywKz+36XHzUzyb5lOg67t+UJckSmg79oD4S+tb4JN0XtZdkjylUyv1a/W6uvp9qyqH5pnsn/MdK3cDj30/GyST2yO9q3LfD0nmXo9/0+SX6mqXTL1oF7UJ1lbL2uSvKP3YN6Y6bTx+u6cWte8/meSB7bWTlzLB3Iy9UhetJZx83nPPMv6SJI/raoXZ+q9nuvJ3dij77neomszfTHM9WzelNX/w/fFVfWpTO+lvbO6x3SzqenazNnlHJDpNH4yHYx+NdOBUzK9R29vrX27qn4uq3u+L0tyZE3Xlz0g08HUnAckubV/Nj13M63GlvbtTKe9jl9HnbX1CK7tbMNCe8L3SnJxX95vZdvpgXxWkv/TWntYa22f1treST6f6YzJYzMdgJ07U/+8TAc/R69RPutfMx3w/WFNveyfTbKkpptVUlX3rqq7bZ/+efzA1tqFSf5HpgCcTNv612bqHbAxK3dPCmPz+V7vQlXtm+nI9tDW2k9kupNyfUdmC+mhWYhjMx0NXZPpA/GVa6n3+t49fV2mD4RPZQpENyT5ZC//31mcHpKXJvn13qYfydSVm0zn5T9VVVdlOv/+570L+bgkb+v1P5b5v2x3SPL/9Q+Nq5Kc1qa73F6X6c1z1Rpt/0CS/Xvv53MyXfv3zP78CVmA1totmXo+P5HpS/fmmXVcqO/12qYHmP5B+/5MH8If69vgXZk/XJ6X6XTup5L8v0zX3H1pkdq2UdbSc/I3ma7jODrJO2dCyrp6WdcMTS3Th+7sZ9bs+21d87oiyYH9yHptDsrqAHtDplA568Aks3cszvUwfm9Zbbqj9b8luW+mU/Bz+/TGHn3P9l6u2bO5Y//SeFKm630enem9sSU+e+6f5OzqFy5nOlg9dWb8SzLty69L8tYky/p+e0ymswhzp3zfnmlfvSjTazPnf2a6dOIjc/W3A9/N1DtzUFX97lrqnJXF6xHckJ7wv8h0bdiPJ3nhApe3JR2d6bNu1rszHfxfkOSpWX1AltbaVzP1ND6stbbWg9MeaJ+e6UDiMZlC3x/1g52rc/c79pPpM/iC/h74cJJf7+UvzrTPX1NVN2TqodxwbSu4KG9LPLLGBfyZuWi7P390pg+Ie2W6tuG2TIHhvpl+7mKfXu+t6RfkZtoxfqkPn5rk5tHrOXD7/mBWX9B4VKbz9sPbtcjreP/+d8f0oDe6TVvTI1OPz9KZ569OvyC4b69bkvzYzPhvzAw/K8lZffisTD9Pcq8k+2U6RfMDmXoEPtrL907y9SSHbMC8npXp2pKPJXlAL782yb59+JFJzp2Z/hcyhYQH9+cHpN8U0Z/fnNUX9y9L8sE+vN/MPN6V5Misvkh490yB+ZBMPwexIv3i6UwXDT+yD38wq2+WOSQzF//Pjct0tP9/e9kjMgW9Q9Zsm8f4R1ZfLP6gTGH++P781Ky+ceHLSX6o7weXzOy7f5/kOX34hJl5/WJW/x/nJZl6J//L2vaXNfelTOF97iahv5nZf4/LVn4B//b88DtjXWttrvfmM1n9W2NprX2rql6U5H1V9e+5+5Hc6zIdKf5eto7fJBvpwEwXL1amrt9fHduczeLUmn5Q9QcydUn/3djmbHXun+Qv+unIuzIFjrmLy9+a6Q6lT2/gvP45U0/VzklObK39R1V9JNNpiRsyHfF+ckMb1lp7Zz8tdn5NPwx8Rqb39L9keu++b6bu+VW1Z5KPVlXLdLr7V1prt65nMS/tp+S+m+mL96JMp5rSWrutphsRLsr03nhWktNquiFnxyR/lrv3vK3L+5KcWFWfznRa5fL11Gew1todVXV4ksuqatUao+d6BFf1v3O93y/NdGbg5Zle87me+PMy7VefytRj/LLW2pdqw38E9dRMl9d8NVNP+r6btFIsKv8ofANU1f1ba9/oQeONSW5srb1hdLtgW1HTXZBXtdU3IKyr7lmZjuK3yC/fV9UlmX6SYX1hC7aYmu6q/FZrrVXVUZl+OuiI0e1i89AztmFeUFXHZjq9cFWm66yADVBVV2a6PvM3RrdlPq21J49uA8zjnnC2gU7PGADAQPf0uykBAIYSxgAABhLGAAAGEsYAAAYSxgAABvr/AZBTOL7hB3YJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(df['Ärendetyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "discrete-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Beskrivning_Anonymized'].str.lower() #Lower case to slim the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "educational-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(df['Ärendetyp'], columns=['Ärendetyp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "annoying-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = Y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wicked-acoustic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Avvikelse', 'Beröm', 'Fråga', 'Förseningsersättning', 'Klagomål',\n",
       "       'Skada', 'Synpunkt/Önskemål'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "logical-attendance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "neutral-field",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init called\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "transformer = NLPTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "protecting-defensive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "desirable-benefit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "sublime-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.to_pickle('targets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 2/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 3/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 4/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 5/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 6/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 7/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 8/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 9/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 10/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 11/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 12/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 13/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 14/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 15/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 16/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 17/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 18/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 19/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 20/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 21/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 22/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 23/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 24/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 25/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 26/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 27/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 28/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 29/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 30/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 31/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 32/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 33/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 34/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 35/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 36/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 37/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 38/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 39/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 40/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 41/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 42/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 43/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 44/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 45/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 46/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 47/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 48/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 49/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 50/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 51/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 52/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 53/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 54/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 55/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 56/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 57/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 58/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 59/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 60/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 61/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 62/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 63/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 64/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 65/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 66/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 67/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 68/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 69/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 70/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 71/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 72/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 73/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 74/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 75/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 76/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 77/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 78/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 79/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 80/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 81/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 82/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 83/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 84/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 85/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 86/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 87/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 88/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 89/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 90/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 91/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 92/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 93/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 94/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 95/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 96/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 97/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 98/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 99/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 100/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 101/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 102/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 103/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 104/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 105/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 106/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 107/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 108/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 109/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 110/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 111/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 112/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 113/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 114/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 115/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 116/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 117/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 118/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 119/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 120/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 121/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 122/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 123/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 124/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 125/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 126/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 127/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 128/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 129/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 130/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 131/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 132/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 133/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 134/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 135/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 136/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 137/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 138/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 139/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 140/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 141/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 142/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 143/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 144/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 145/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 146/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 147/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 148/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 149/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 150/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 151/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 152/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 153/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 154/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 155/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 156/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 157/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 158/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 159/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 160/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 161/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 162/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 163/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 164/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 165/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 166/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 167/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 168/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 169/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 170/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 171/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 172/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 173/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 174/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 175/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 176/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 177/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 178/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 179/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 180/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 181/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 182/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 183/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 184/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 185/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 186/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 187/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 188/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 189/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 190/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 191/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 192/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 193/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 194/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 195/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 196/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 197/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 198/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 199/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 200/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 201/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 202/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 203/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 204/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 205/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 206/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 207/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 208/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 209/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 210/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 211/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 212/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 213/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 214/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 215/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 216/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 217/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 218/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 219/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 220/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 221/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 222/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 223/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 224/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 225/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 226/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 227/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 228/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 229/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 230/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 231/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 232/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 233/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 234/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 235/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 236/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 237/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 238/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 239/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 240/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 241/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 242/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 243/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 244/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 245/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 246/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 247/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 248/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 249/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 250/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 251/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 252/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 253/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 254/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 255/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 256/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 257/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 258/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 259/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 260/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 261/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 262/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 263/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 264/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 265/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 266/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 267/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 268/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 269/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 270/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 271/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 272/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 273/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 274/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 275/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 276/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 277/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 278/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 279/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 280/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 281/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 282/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 283/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 284/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 285/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 286/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 287/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 288/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 289/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 290/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 291/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 292/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 293/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 294/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 295/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 296/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 297/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 298/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 299/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 300/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 301/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 302/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 303/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 304/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 305/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 306/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 307/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 308/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 309/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 310/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 311/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 312/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 313/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 314/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 315/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 316/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 317/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 318/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 319/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 320/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 321/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 322/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 323/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 324/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 325/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 326/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 327/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 328/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 329/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 330/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 331/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 332/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 333/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 334/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 335/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 336/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 337/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 338/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 339/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 340/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 341/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 342/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 343/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 344/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 345/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 346/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 347/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 348/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 349/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 350/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 351/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 352/2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape: (50, 768)\n",
      "Running batch 353/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 354/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 355/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 356/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 357/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 358/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 359/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 360/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 361/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 362/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 363/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 364/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 365/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 366/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 367/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 368/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 369/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 370/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 371/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 372/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 373/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 374/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 375/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 376/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 377/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 378/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 379/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 380/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 381/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 382/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 383/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 384/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 385/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 386/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 387/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 388/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 389/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 390/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 391/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 392/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 393/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 394/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 395/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 396/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 397/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 398/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 399/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 400/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 401/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 402/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 403/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 404/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 405/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 406/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 407/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 408/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 409/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 410/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 411/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 412/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 413/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 414/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 415/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 416/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 417/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 418/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 419/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 420/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 421/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 422/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 423/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 424/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 425/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 426/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 427/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 428/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 429/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 430/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 431/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 432/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 433/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 434/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 435/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 436/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 437/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 438/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 439/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 440/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 441/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 442/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 443/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 444/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 445/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 446/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 447/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 448/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 449/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 450/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 451/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 452/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 453/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 454/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 455/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 456/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 457/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 458/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 459/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 460/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 461/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 462/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 463/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 464/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 465/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 466/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 467/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 468/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 469/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 470/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 471/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 472/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 473/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 474/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 475/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 476/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 477/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 478/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 479/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 480/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 481/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 482/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 483/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 484/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 485/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 486/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 487/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 488/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 489/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 490/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 491/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 492/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 493/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 494/2800\n",
      "output shape: (50, 768)\n",
      "Running batch 495/2800\n"
     ]
    }
   ],
   "source": [
    "X_transformed = transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('transformed_data.pkl', X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "narrative-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note in case of pytorch CrossEntropyLoss, the targets should be the class index, not one hot encoded\n",
    "Y = Y.values.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "accessible-compact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "28           0      0      0                     0         1      0   \n",
       "286          0      0      0                     0         0      0   \n",
       "853          0      1      0                     0         0      0   \n",
       "278          0      0      0                     0         0      0   \n",
       "35           0      0      0                     0         1      0   \n",
       "..         ...    ...    ...                   ...       ...    ...   \n",
       "480          0      0      1                     0         0      0   \n",
       "49           0      0      0                     0         1      0   \n",
       "499          0      0      1                     0         0      0   \n",
       "74           0      0      0                     0         1      0   \n",
       "490          0      0      1                     0         0      0   \n",
       "\n",
       "     Synpunkt/Önskemål  \n",
       "28                   0  \n",
       "286                  1  \n",
       "853                  0  \n",
       "278                  1  \n",
       "35                   0  \n",
       "..                 ...  \n",
       "480                  0  \n",
       "49                   0  \n",
       "499                  0  \n",
       "74                   0  \n",
       "490                  0  \n",
       "\n",
       "[1050 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mexican-portrait",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "flexible-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This could also be a target\n",
    "Y2 = pd.get_dummies(df['Prioritet'], columns=['Prioritet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-pricing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "marked-little",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hög</th>\n",
       "      <th>Normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hög  Normal\n",
       "0    0       1\n",
       "1    0       1\n",
       "2    1       0\n",
       "3    0       1\n",
       "4    0       1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "greater-turning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    hej,\\r\\n \\r\\nhar nu fått tag i föraren som är ...\n",
       "1    buss 000 00:00\\r\\n\\r\\nkristianstad hästtorget ...\n",
       "2    skadeanmälan för påkörning av bil bakifrån vid...\n",
       "3    hej, \\r\\nvarför heter en av hållplatserna i lu...\n",
       "4    hej!\\r\\nhar en fråga som gäller busskurerna i ...\n",
       "Name: Beskrivning_Anonymized, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "instrumental-mercury",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hej, \\r\\nvarför heter en av hållplatserna i ludvigsborg kvarndamms gatan i hörby kommun, skåne? dels så finns det ingen väg som heter så där, den heter kvarndamsvägen. dessutom är hållplatsen på ludvigsborgsvägen. xxxx ni vidarebefordra detta till rätt avdelning så det blir ändrat för att inte göra det förvirrat för resenärerna?\\r\\nhälsningar fredrik\\r\\r\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "latter-external",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "0          0      0      0                     0         1      0   \n",
       "1          0      0      0                     0         0      0   \n",
       "2          0      0      0                     0         0      1   \n",
       "3          0      0      0                     0         1      0   \n",
       "4          0      0      1                     0         0      0   \n",
       "\n",
       "   Synpunkt/Önskemål  \n",
       "0                  0  \n",
       "1                  1  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "reverse-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tok('hej, Har nu fått tag i föraren som är', return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "pressing-hayes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  8819,    19,  1177,   346,   902,  1326,    31, 15367,    67,\n",
       "            54,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "civilian-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "standard-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "chicken-profile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8400,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fuzzy-design",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "935           1      0      0                     0         0      0   \n",
       "1009          1      0      0                     0         0      0   \n",
       "262           0      0      0                     0         0      0   \n",
       "931           1      0      0                     0         0      0   \n",
       "814           0      1      0                     0         0      0   \n",
       "\n",
       "      Synpunkt/Önskemål  \n",
       "935                   0  \n",
       "1009                  0  \n",
       "262                   1  \n",
       "931                   0  \n",
       "814                   0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mineral-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatcher():\n",
    "    def __init__(self, X, Y, batch_size=100, max_epochs=1):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batchId = 0\n",
    "        self.epochId = 0\n",
    "        \n",
    "    def getBatchIterator(self):\n",
    "        self.batchId = 0\n",
    "        self.epochId = 0\n",
    "        \n",
    "        while True:\n",
    "            X_mini = self.X[self.batchId * self.batch_size:(self.batchId + 1) * self.batch_size]\n",
    "            Y_mini = self.Y[self.batchId * self.batch_size:(self.batchId + 1) * self.batch_size]\n",
    "            self.batchId += 1\n",
    "            #print(self.batchId, self.epochId)\n",
    "            #print(len(X_mini))\n",
    "            if len(X_mini) < self.batch_size:\n",
    "                self.epochId += 1\n",
    "                self.batchId = 0\n",
    "                if self.epochId <= self.max_epochs:\n",
    "                    #print('will break')\n",
    "                    break\n",
    "            \n",
    "            yield X_mini, Y_mini\n",
    "            \n",
    "    def getBatchInfo(self):\n",
    "        return f'batch: {self.batchId}/{int(np.ceil(self.X.shape[0] / self.batch_size))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dietary-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mMiniBatcher = MiniBatcher(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "comparable-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchIterator = mMiniBatcher.getBatchIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "voluntary-extension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit called\n",
      "Transform Called\n",
      "Running batch 1/168\n",
      "output shape: (50, 768)\n",
      "Running batch 2/168\n",
      "output shape: (50, 768)\n",
      "Running batch 3/168\n",
      "output shape: (50, 768)\n",
      "Running batch 4/168\n",
      "output shape: (50, 768)\n",
      "Running batch 5/168\n",
      "output shape: (50, 768)\n",
      "Running batch 6/168\n",
      "output shape: (50, 768)\n",
      "Running batch 7/168\n",
      "output shape: (50, 768)\n",
      "Running batch 8/168\n",
      "output shape: (50, 768)\n",
      "Running batch 9/168\n",
      "output shape: (50, 768)\n",
      "Running batch 10/168\n",
      "output shape: (50, 768)\n",
      "Running batch 11/168\n",
      "output shape: (50, 768)\n",
      "Running batch 12/168\n",
      "output shape: (50, 768)\n",
      "Running batch 13/168\n",
      "output shape: (50, 768)\n",
      "Running batch 14/168\n",
      "output shape: (50, 768)\n",
      "Running batch 15/168\n",
      "output shape: (50, 768)\n",
      "Running batch 16/168\n",
      "output shape: (50, 768)\n",
      "Running batch 17/168\n",
      "output shape: (50, 768)\n",
      "Running batch 18/168\n",
      "output shape: (50, 768)\n",
      "Running batch 19/168\n",
      "output shape: (50, 768)\n",
      "Running batch 20/168\n",
      "output shape: (50, 768)\n",
      "Running batch 21/168\n",
      "output shape: (50, 768)\n",
      "Running batch 22/168\n",
      "output shape: (50, 768)\n",
      "Running batch 23/168\n",
      "output shape: (50, 768)\n",
      "Running batch 24/168\n",
      "output shape: (50, 768)\n",
      "Running batch 25/168\n",
      "output shape: (50, 768)\n",
      "Running batch 26/168\n",
      "output shape: (50, 768)\n",
      "Running batch 27/168\n",
      "output shape: (50, 768)\n",
      "Running batch 28/168\n",
      "output shape: (50, 768)\n",
      "Running batch 29/168\n",
      "output shape: (50, 768)\n",
      "Running batch 30/168\n",
      "output shape: (50, 768)\n",
      "Running batch 31/168\n",
      "output shape: (50, 768)\n",
      "Running batch 32/168\n",
      "output shape: (50, 768)\n",
      "Running batch 33/168\n",
      "output shape: (50, 768)\n",
      "Running batch 34/168\n",
      "output shape: (50, 768)\n",
      "Running batch 35/168\n",
      "output shape: (50, 768)\n",
      "Running batch 36/168\n",
      "output shape: (50, 768)\n",
      "Running batch 37/168\n",
      "output shape: (50, 768)\n",
      "Running batch 38/168\n",
      "output shape: (50, 768)\n",
      "Running batch 39/168\n",
      "output shape: (50, 768)\n",
      "Running batch 40/168\n",
      "output shape: (50, 768)\n",
      "Running batch 41/168\n",
      "output shape: (50, 768)\n",
      "Running batch 42/168\n",
      "output shape: (50, 768)\n",
      "Running batch 43/168\n",
      "output shape: (50, 768)\n",
      "Running batch 44/168\n",
      "output shape: (50, 768)\n",
      "Running batch 45/168\n",
      "output shape: (50, 768)\n",
      "Running batch 46/168\n",
      "output shape: (50, 768)\n",
      "Running batch 47/168\n",
      "output shape: (50, 768)\n",
      "Running batch 48/168\n",
      "output shape: (50, 768)\n",
      "Running batch 49/168\n",
      "output shape: (50, 768)\n",
      "Running batch 50/168\n",
      "output shape: (50, 768)\n",
      "Running batch 51/168\n",
      "output shape: (50, 768)\n",
      "Running batch 52/168\n",
      "output shape: (50, 768)\n",
      "Running batch 53/168\n",
      "output shape: (50, 768)\n",
      "Running batch 54/168\n",
      "output shape: (50, 768)\n",
      "Running batch 55/168\n",
      "output shape: (50, 768)\n",
      "Running batch 56/168\n",
      "output shape: (50, 768)\n",
      "Running batch 57/168\n",
      "output shape: (50, 768)\n",
      "Running batch 58/168\n",
      "output shape: (50, 768)\n",
      "Running batch 59/168\n",
      "output shape: (50, 768)\n",
      "Running batch 60/168\n",
      "output shape: (50, 768)\n",
      "Running batch 61/168\n",
      "output shape: (50, 768)\n",
      "Running batch 62/168\n",
      "output shape: (50, 768)\n",
      "Running batch 63/168\n",
      "output shape: (50, 768)\n",
      "Running batch 64/168\n",
      "output shape: (50, 768)\n",
      "Running batch 65/168\n",
      "output shape: (50, 768)\n",
      "Running batch 66/168\n",
      "output shape: (50, 768)\n",
      "Running batch 67/168\n",
      "output shape: (50, 768)\n",
      "Running batch 68/168\n",
      "output shape: (50, 768)\n",
      "Running batch 69/168\n",
      "output shape: (50, 768)\n",
      "Running batch 70/168\n",
      "output shape: (50, 768)\n",
      "Running batch 71/168\n",
      "output shape: (50, 768)\n",
      "Running batch 72/168\n",
      "output shape: (50, 768)\n",
      "Running batch 73/168\n",
      "output shape: (50, 768)\n",
      "Running batch 74/168\n",
      "output shape: (50, 768)\n",
      "Running batch 75/168\n",
      "output shape: (50, 768)\n",
      "Running batch 76/168\n",
      "output shape: (50, 768)\n",
      "Running batch 77/168\n",
      "output shape: (50, 768)\n",
      "Running batch 78/168\n",
      "output shape: (50, 768)\n",
      "Running batch 79/168\n",
      "output shape: (50, 768)\n",
      "Running batch 80/168\n",
      "output shape: (50, 768)\n",
      "Running batch 81/168\n",
      "output shape: (50, 768)\n",
      "Running batch 82/168\n",
      "output shape: (50, 768)\n",
      "Running batch 83/168\n",
      "output shape: (50, 768)\n",
      "Running batch 84/168\n",
      "output shape: (50, 768)\n",
      "Running batch 85/168\n",
      "output shape: (50, 768)\n",
      "Running batch 86/168\n",
      "output shape: (50, 768)\n",
      "Running batch 87/168\n",
      "output shape: (50, 768)\n",
      "Running batch 88/168\n",
      "output shape: (50, 768)\n",
      "Running batch 89/168\n",
      "output shape: (50, 768)\n",
      "Running batch 90/168\n",
      "output shape: (50, 768)\n",
      "Running batch 91/168\n",
      "output shape: (50, 768)\n",
      "Running batch 92/168\n",
      "output shape: (50, 768)\n",
      "Running batch 93/168\n",
      "output shape: (50, 768)\n",
      "Running batch 94/168\n",
      "output shape: (50, 768)\n",
      "Running batch 95/168\n",
      "output shape: (50, 768)\n",
      "Running batch 96/168\n",
      "output shape: (50, 768)\n",
      "Running batch 97/168\n",
      "output shape: (50, 768)\n",
      "Running batch 98/168\n",
      "output shape: (50, 768)\n",
      "Running batch 99/168\n",
      "output shape: (50, 768)\n",
      "Running batch 100/168\n",
      "output shape: (50, 768)\n",
      "Running batch 101/168\n",
      "output shape: (50, 768)\n",
      "Running batch 102/168\n",
      "output shape: (50, 768)\n",
      "Running batch 103/168\n",
      "output shape: (50, 768)\n",
      "Running batch 104/168\n",
      "output shape: (50, 768)\n",
      "Running batch 105/168\n",
      "output shape: (50, 768)\n",
      "Running batch 106/168\n",
      "output shape: (50, 768)\n",
      "Running batch 107/168\n",
      "output shape: (50, 768)\n",
      "Running batch 108/168\n",
      "output shape: (50, 768)\n",
      "Running batch 109/168\n",
      "output shape: (50, 768)\n",
      "Running batch 110/168\n",
      "output shape: (50, 768)\n",
      "Running batch 111/168\n",
      "output shape: (50, 768)\n",
      "Running batch 112/168\n",
      "output shape: (50, 768)\n",
      "Running batch 113/168\n",
      "output shape: (50, 768)\n",
      "Running batch 114/168\n",
      "output shape: (50, 768)\n",
      "Running batch 115/168\n",
      "output shape: (50, 768)\n",
      "Running batch 116/168\n",
      "output shape: (50, 768)\n",
      "Running batch 117/168\n",
      "output shape: (50, 768)\n",
      "Running batch 118/168\n",
      "output shape: (50, 768)\n",
      "Running batch 119/168\n",
      "output shape: (50, 768)\n",
      "Running batch 120/168\n",
      "output shape: (50, 768)\n",
      "Running batch 121/168\n",
      "output shape: (50, 768)\n",
      "Running batch 122/168\n",
      "output shape: (50, 768)\n",
      "Running batch 123/168\n",
      "output shape: (50, 768)\n",
      "Running batch 124/168\n",
      "output shape: (50, 768)\n",
      "Running batch 125/168\n",
      "output shape: (50, 768)\n",
      "Running batch 126/168\n",
      "output shape: (50, 768)\n",
      "Running batch 127/168\n",
      "output shape: (50, 768)\n",
      "Running batch 128/168\n",
      "output shape: (50, 768)\n",
      "Running batch 129/168\n",
      "output shape: (50, 768)\n",
      "Running batch 130/168\n",
      "output shape: (50, 768)\n",
      "Running batch 131/168\n",
      "output shape: (50, 768)\n",
      "Running batch 132/168\n",
      "output shape: (50, 768)\n",
      "Running batch 133/168\n",
      "output shape: (50, 768)\n",
      "Running batch 134/168\n",
      "output shape: (50, 768)\n",
      "Running batch 135/168\n",
      "output shape: (50, 768)\n",
      "Running batch 136/168\n",
      "output shape: (50, 768)\n",
      "Running batch 137/168\n",
      "output shape: (50, 768)\n",
      "Running batch 138/168\n",
      "output shape: (50, 768)\n",
      "Running batch 139/168\n",
      "output shape: (50, 768)\n",
      "Running batch 140/168\n",
      "output shape: (50, 768)\n",
      "Running batch 141/168\n",
      "output shape: (50, 768)\n",
      "Running batch 142/168\n",
      "output shape: (50, 768)\n",
      "Running batch 143/168\n",
      "output shape: (50, 768)\n",
      "Running batch 144/168\n",
      "output shape: (50, 768)\n",
      "Running batch 145/168\n",
      "output shape: (50, 768)\n",
      "Running batch 146/168\n",
      "output shape: (50, 768)\n",
      "Running batch 147/168\n",
      "output shape: (50, 768)\n",
      "Running batch 148/168\n",
      "output shape: (50, 768)\n",
      "Running batch 149/168\n",
      "output shape: (50, 768)\n",
      "Running batch 150/168\n",
      "output shape: (50, 768)\n",
      "Running batch 151/168\n",
      "output shape: (50, 768)\n",
      "Running batch 152/168\n",
      "output shape: (50, 768)\n",
      "Running batch 153/168\n",
      "output shape: (50, 768)\n",
      "Running batch 154/168\n",
      "output shape: (50, 768)\n",
      "Running batch 155/168\n",
      "output shape: (50, 768)\n",
      "Running batch 156/168\n",
      "output shape: (50, 768)\n",
      "Running batch 157/168\n",
      "output shape: (50, 768)\n",
      "Running batch 158/168\n",
      "output shape: (50, 768)\n",
      "Running batch 159/168\n",
      "output shape: (50, 768)\n",
      "Running batch 160/168\n",
      "output shape: (50, 768)\n",
      "Running batch 161/168\n",
      "output shape: (50, 768)\n",
      "Running batch 162/168\n",
      "output shape: (50, 768)\n",
      "Running batch 163/168\n",
      "output shape: (50, 768)\n",
      "Running batch 164/168\n",
      "output shape: (50, 768)\n",
      "Running batch 165/168\n",
      "output shape: (50, 768)\n",
      "Running batch 166/168\n",
      "output shape: (50, 768)\n",
      "Running batch 167/168\n",
      "output shape: (50, 768)\n",
      "Running batch 168/168\n",
      "output shape: (50, 768)\n",
      "transformed.shape: (8400, 768)\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "#output = pipeline.fit(X_train[:25], Y_train[:25].values)\n",
    "#output = pipeline.fit(X_train[:1500], Y_train[:1500].values)\n",
    "output = pipeline.fit(X_train, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.predict(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "outdoor-service",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/2\n",
      "output shape: (10, 768)\n",
      "Running batch 2/2\n",
      "output shape: (5, 768)\n",
      "transformed.shape: (15, 768)\n"
     ]
    }
   ],
   "source": [
    "probs = output.predict_proba(X_test[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "biological-portal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "first-nightmare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "521          0      0      1                     0         0      0   \n",
       "941          0      0      0                     0         0      0   \n",
       "741          0      0      1                     0         0      0   \n",
       "980          0      0      0                     0         0      0   \n",
       "411          0      0      0                     0         0      0   \n",
       "679          0      0      1                     0         0      0   \n",
       "673          0      0      0                     0         0      0   \n",
       "513          0      0      0                     0         1      0   \n",
       "773          0      0      0                     0         1      0   \n",
       "136          0      0      0                     0         0      0   \n",
       "889          0      0      1                     0         0      0   \n",
       "76           0      0      0                     1         0      0   \n",
       "739          0      0      0                     0         0      0   \n",
       "806          0      0      0                     0         0      0   \n",
       "939          0      0      0                     0         0      0   \n",
       "\n",
       "     Synpunkt/Önskemål  \n",
       "521                  0  \n",
       "941                  1  \n",
       "741                  0  \n",
       "980                  1  \n",
       "411                  1  \n",
       "679                  0  \n",
       "673                  1  \n",
       "513                  0  \n",
       "773                  0  \n",
       "136                  1  \n",
       "889                  0  \n",
       "76                   0  \n",
       "739                  1  \n",
       "806                  1  \n",
       "939                  1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "desirable-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will predict the classes for each row. The class with the highest probability is selected\n",
    "def PredictClasses(model, X):\n",
    "    probs = model.predict_proba(X)\n",
    "    probs = np.array(probs) #List to (N, num_classes, 2)\n",
    "    predictedClasses = np.argmax(probs[:,:,1].T, axis=1) #First index classifies it as 0, second as 1, Then get the max index for each row\n",
    "    \n",
    "    return predictedClasses\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "deadly-oxide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hej, \\r\\nvarför heter en av hållplatserna i ludvigsborg kvarndamms gatan i hörby kommun, skåne? dels så finns det ingen väg som heter så där, den heter kvarndamsvägen. dessutom är hållplatsen på ludvigsborgsvägen. xxxx ni vidarebefordra detta till rätt avdelning så det blir ändrat för att inte göra det förvirrat för resenärerna?\\r\\nhälsningar fredrik\\r\\r\\n'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "soviet-tourist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/1\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (10, 768)\n"
     ]
    }
   ],
   "source": [
    "result = PredictClasses(pipeline, X_test[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "local-september",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 6, 3, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "powerful-tenant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7487</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6033</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9930</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7051</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8158</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "2750          0      0      0                     1         0      0   \n",
       "7487          0      0      0                     1         0      0   \n",
       "5272          0      0      0                     1         0      0   \n",
       "5653          0      0      0                     1         0      0   \n",
       "3999          0      0      0                     1         0      0   \n",
       "6033          0      0      0                     1         0      0   \n",
       "582           0      0      0                     0         1      0   \n",
       "9930          0      0      0                     1         0      0   \n",
       "7051          0      0      0                     1         0      0   \n",
       "8158          0      0      0                     1         0      0   \n",
       "\n",
       "      Synpunkt/Önskemål  \n",
       "2750                  0  \n",
       "7487                  0  \n",
       "5272                  0  \n",
       "5653                  0  \n",
       "3999                  0  \n",
       "6033                  0  \n",
       "582                   0  \n",
       "9930                  0  \n",
       "7051                  0  \n",
       "8158                  0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "biblical-boulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 6, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bearing-circus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 4, 3, 3, 3], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ost = Y_test[10:20].to_numpy().argmax(axis=1)\n",
    "ost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "annoying-victorian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "accomplished-cabinet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(ost, result,normalize='true')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "informative-personal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1ae8f022d00>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix=cm,).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "brown-transfer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ost, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "spatial-tract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-d119748ae7af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#results = PredictClasses(pipeline, X_test[:15])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPredictClasses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-d220fc1fb3d3>\u001b[0m in \u001b[0;36mPredictClasses\u001b[1;34m(model, X)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#This will predict the classes for each row. The class with the highest probability is selected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mPredictClasses\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#List to (N, num_classes, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpredictedClasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#First index classifies it as 0, second as 1, Then get the max index for each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, **predict_proba_params)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_proba_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b00c47e656f5>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0minputs_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatchId\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchId\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pooler_output'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'output shape: {outputs.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1020\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         )\n\u001b[1;32m-> 1022\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m   1023\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    609\u001b[0m                 )\n\u001b[0;32m    610\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    612\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[1;32m--> 427\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    428\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0mattention_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1832\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1834\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1835\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#results = PredictClasses(pipeline, X_test[:15])\n",
    "results = PredictClasses(pipeline, X_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "accompanied-resource",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 4, 6, 6, 6, 4, 4, 6, 4, 6, 4, 6, 6, 6, 6, 6, 2, 4, 6, 6, 2,\n",
       "       2, 4, 4, 4, 4, 6, 2, 4, 4, 6, 6, 6, 4, 4, 6, 2, 6, 4, 4, 4, 3, 6,\n",
       "       4, 6, 2, 6, 2, 6, 6, 2, 6, 6, 4, 6, 4, 4, 4, 6, 4, 4, 4, 6, 6, 6,\n",
       "       6, 2, 4, 6, 6, 6, 6, 6, 4, 4, 2, 4, 6, 4, 6, 4, 2, 6, 4, 2, 6, 6,\n",
       "       2, 2, 4, 4, 4, 4, 6, 4, 4, 4, 4, 6, 6, 4, 4, 2, 2, 6, 4, 4, 3, 6,\n",
       "       6, 4, 6, 6, 4, 4, 6, 6, 6, 4, 6, 6, 6, 2, 4, 6, 6, 4, 6, 6, 6, 6,\n",
       "       6, 6, 6, 4, 4, 4, 6, 4, 4, 6, 6, 4, 6, 2, 6, 4, 2, 4, 4, 2, 6, 2,\n",
       "       6, 6, 2, 6, 4, 6, 6, 4, 6, 6, 2, 6, 4, 6, 3, 4, 4, 4, 2, 6, 4, 4,\n",
       "       6, 2, 6, 2, 4, 6, 4, 2, 6, 2, 4, 6, 6, 6, 4, 6, 6, 4, 2, 6, 6, 2,\n",
       "       6, 2, 6], dtype=int64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cross-spray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 5, ..., 4, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = np.argmax(np.array(Y), axis=1)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "greenhouse-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Avvikelse', 'Beröm', 'Fråga', 'Förseningsersättning', 'Klagomål',\n",
       "       'Skada', 'Synpunkt/Önskemål'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "polyphonic-administration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "corrected-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateModelUsingProbs(model, X_test, Y_test):\n",
    "    Y_pred = PredictClasses(model, X_test) #1D-array with classes\n",
    "    targets = np.argmax(np.array(Y_test), axis=1) #1D-array with classes\n",
    "    \n",
    "    total_hits = np.sum(np.sum(Y_pred == targets))\n",
    "    total_misses = np.sum(np.sum(Y_pred != targets))\n",
    "    total_accuracy = total_hits/(total_hits + total_misses)\n",
    "    print(f'Total Accuracy: {total_accuracy}')\n",
    "    #cm = confusion_matrix(targets, Y_pred, normalize='true')\n",
    "    #ConfusionMatrixDisplay(confusion_matrix=cm,).plot()\n",
    "    cm = confusion_matrix(targets, Y_pred, normalize='true', labels=list(range(Y_test.columns.shape[0])))\n",
    "    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(range(Y_test.columns.shape[0]))).plot()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "mathematical-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model and print the accuracy\n",
    "\n",
    "def EvaluateModel(model, X_test, Y_test):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    \n",
    "    total_hits = np.sum(np.sum(Y_pred == Y_test))\n",
    "    total_misses = np.sum(np.sum(Y_pred != Y_test))\n",
    "    total_accuracy = total_hits/(total_hits + total_misses)\n",
    "       \n",
    "    target_names = [name for name in Y.columns]\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1scores = []\n",
    "    for (name, col) in zip(target_names, range(len(target_names))):\n",
    "        y_test = Y_test[name].values\n",
    "        y_pred = Y_pred[:, col]\n",
    "        \n",
    "        if(np.max(y_test) <= 1):\n",
    "            #Only one category\n",
    "            precisions.append(precision_score(y_test, y_pred))\n",
    "            recalls.append(recall_score(y_test, y_pred))\n",
    "            f1scores.append(f1_score(y_test, y_pred))\n",
    "        print(f'Category: {name}')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print('-'*42)\n",
    "     \n",
    "    \n",
    "    print(f'Total Accuracy: {total_accuracy}')\n",
    "    print(f'Average Precission: {np.average(precisions)}')\n",
    "    print(f'Average Recall: {np.average(recalls)}')\n",
    "    print(f'Average F1 Score: {np.average(f1scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "directed-essex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/10\n",
      "output shape: (10, 768)\n",
      "Running batch 2/10\n",
      "output shape: (10, 768)\n",
      "Running batch 3/10\n",
      "output shape: (10, 768)\n",
      "Running batch 4/10\n",
      "output shape: (10, 768)\n",
      "Running batch 5/10\n",
      "output shape: (10, 768)\n",
      "Running batch 6/10\n",
      "output shape: (10, 768)\n",
      "Running batch 7/10\n",
      "output shape: (10, 768)\n",
      "Running batch 8/10\n",
      "output shape: (10, 768)\n",
      "Running batch 9/10\n",
      "output shape: (10, 768)\n",
      "Running batch 10/10\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (100, 768)\n",
      "Category: Avvikelse\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Beröm\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        99\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.49      0.50      0.50       100\n",
      "weighted avg       0.98      0.99      0.99       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Fråga\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86        76\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.76       100\n",
      "   macro avg       0.38      0.50      0.43       100\n",
      "weighted avg       0.58      0.76      0.66       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Förseningsersättning\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        94\n",
      "           1       1.00      0.17      0.29         6\n",
      "\n",
      "    accuracy                           0.95       100\n",
      "   macro avg       0.97      0.58      0.63       100\n",
      "weighted avg       0.95      0.95      0.93       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Klagomål\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.94      0.78        67\n",
      "           1       0.33      0.06      0.10        33\n",
      "\n",
      "    accuracy                           0.65       100\n",
      "   macro avg       0.50      0.50      0.44       100\n",
      "weighted avg       0.56      0.65      0.56       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Skada\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        99\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       100\n",
      "   macro avg       0.49      0.50      0.50       100\n",
      "weighted avg       0.98      0.99      0.99       100\n",
      "\n",
      "------------------------------------------\n",
      "Category: Synpunkt/Önskemål\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82        65\n",
      "           1       0.79      0.31      0.45        35\n",
      "\n",
      "    accuracy                           0.73       100\n",
      "   macro avg       0.75      0.63      0.64       100\n",
      "weighted avg       0.74      0.73      0.69       100\n",
      "\n",
      "------------------------------------------\n",
      "Total Accuracy: 0.8671428571428571\n",
      "Average Precission: 0.30272108843537415\n",
      "Average Recall: 0.07736549165120594\n",
      "Average F1 Score: 0.11960828287358898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\dataScience\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "EvaluateModel(pipeline, X_test[:100], Y_test[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "animal-washer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/42\n",
      "output shape: (50, 768)\n",
      "Running batch 2/42\n",
      "output shape: (50, 768)\n",
      "Running batch 3/42\n",
      "output shape: (50, 768)\n",
      "Running batch 4/42\n",
      "output shape: (50, 768)\n",
      "Running batch 5/42\n",
      "output shape: (50, 768)\n",
      "Running batch 6/42\n",
      "output shape: (50, 768)\n",
      "Running batch 7/42\n",
      "output shape: (50, 768)\n",
      "Running batch 8/42\n",
      "output shape: (50, 768)\n",
      "Running batch 9/42\n",
      "output shape: (50, 768)\n",
      "Running batch 10/42\n",
      "output shape: (50, 768)\n",
      "Running batch 11/42\n",
      "output shape: (50, 768)\n",
      "Running batch 12/42\n",
      "output shape: (50, 768)\n",
      "Running batch 13/42\n",
      "output shape: (50, 768)\n",
      "Running batch 14/42\n",
      "output shape: (50, 768)\n",
      "Running batch 15/42\n",
      "output shape: (50, 768)\n",
      "Running batch 16/42\n",
      "output shape: (50, 768)\n",
      "Running batch 17/42\n",
      "output shape: (50, 768)\n",
      "Running batch 18/42\n",
      "output shape: (50, 768)\n",
      "Running batch 19/42\n",
      "output shape: (50, 768)\n",
      "Running batch 20/42\n",
      "output shape: (50, 768)\n",
      "Running batch 21/42\n",
      "output shape: (50, 768)\n",
      "Running batch 22/42\n",
      "output shape: (50, 768)\n",
      "Running batch 23/42\n",
      "output shape: (50, 768)\n",
      "Running batch 24/42\n",
      "output shape: (50, 768)\n",
      "Running batch 25/42\n",
      "output shape: (50, 768)\n",
      "Running batch 26/42\n",
      "output shape: (50, 768)\n",
      "Running batch 27/42\n",
      "output shape: (50, 768)\n",
      "Running batch 28/42\n",
      "output shape: (50, 768)\n",
      "Running batch 29/42\n",
      "output shape: (50, 768)\n",
      "Running batch 30/42\n",
      "output shape: (50, 768)\n",
      "Running batch 31/42\n",
      "output shape: (50, 768)\n",
      "Running batch 32/42\n",
      "output shape: (50, 768)\n",
      "Running batch 33/42\n",
      "output shape: (50, 768)\n",
      "Running batch 34/42\n",
      "output shape: (50, 768)\n",
      "Running batch 35/42\n",
      "output shape: (50, 768)\n",
      "Running batch 36/42\n",
      "output shape: (50, 768)\n",
      "Running batch 37/42\n",
      "output shape: (50, 768)\n",
      "Running batch 38/42\n",
      "output shape: (50, 768)\n",
      "Running batch 39/42\n",
      "output shape: (50, 768)\n",
      "Running batch 40/42\n",
      "output shape: (50, 768)\n",
      "Running batch 41/42\n",
      "output shape: (50, 768)\n",
      "Running batch 42/42\n",
      "output shape: (50, 768)\n",
      "transformed.shape: (2100, 768)\n",
      "Total Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EvaluateModelUsingProbs(pipeline, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "massive-revision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/80\n",
      "output shape: (10, 768)\n",
      "Running batch 2/80\n",
      "output shape: (10, 768)\n",
      "Running batch 3/80\n",
      "output shape: (10, 768)\n",
      "Running batch 4/80\n",
      "output shape: (10, 768)\n",
      "Running batch 5/80\n",
      "output shape: (10, 768)\n",
      "Running batch 6/80\n",
      "output shape: (10, 768)\n",
      "Running batch 7/80\n",
      "output shape: (10, 768)\n",
      "Running batch 8/80\n",
      "output shape: (10, 768)\n",
      "Running batch 9/80\n",
      "output shape: (10, 768)\n",
      "Running batch 10/80\n",
      "output shape: (10, 768)\n",
      "Running batch 11/80\n",
      "output shape: (10, 768)\n",
      "Running batch 12/80\n",
      "output shape: (10, 768)\n",
      "Running batch 13/80\n",
      "output shape: (10, 768)\n",
      "Running batch 14/80\n",
      "output shape: (10, 768)\n",
      "Running batch 15/80\n",
      "output shape: (10, 768)\n",
      "Running batch 16/80\n",
      "output shape: (10, 768)\n",
      "Running batch 17/80\n",
      "output shape: (10, 768)\n",
      "Running batch 18/80\n",
      "output shape: (10, 768)\n",
      "Running batch 19/80\n",
      "output shape: (10, 768)\n",
      "Running batch 20/80\n",
      "output shape: (10, 768)\n",
      "Running batch 21/80\n",
      "output shape: (10, 768)\n",
      "Running batch 22/80\n",
      "output shape: (10, 768)\n",
      "Running batch 23/80\n",
      "output shape: (10, 768)\n",
      "Running batch 24/80\n",
      "output shape: (10, 768)\n",
      "Running batch 25/80\n",
      "output shape: (10, 768)\n",
      "Running batch 26/80\n",
      "output shape: (10, 768)\n",
      "Running batch 27/80\n",
      "output shape: (10, 768)\n",
      "Running batch 28/80\n",
      "output shape: (10, 768)\n",
      "Running batch 29/80\n",
      "output shape: (10, 768)\n",
      "Running batch 30/80\n",
      "output shape: (10, 768)\n",
      "Running batch 31/80\n",
      "output shape: (10, 768)\n",
      "Running batch 32/80\n",
      "output shape: (10, 768)\n",
      "Running batch 33/80\n",
      "output shape: (10, 768)\n",
      "Running batch 34/80\n",
      "output shape: (10, 768)\n",
      "Running batch 35/80\n",
      "output shape: (10, 768)\n",
      "Running batch 36/80\n",
      "output shape: (10, 768)\n",
      "Running batch 37/80\n",
      "output shape: (10, 768)\n",
      "Running batch 38/80\n",
      "output shape: (10, 768)\n",
      "Running batch 39/80\n",
      "output shape: (10, 768)\n",
      "Running batch 40/80\n",
      "output shape: (10, 768)\n",
      "Running batch 41/80\n",
      "output shape: (10, 768)\n",
      "Running batch 42/80\n",
      "output shape: (10, 768)\n",
      "Running batch 43/80\n",
      "output shape: (10, 768)\n",
      "Running batch 44/80\n",
      "output shape: (10, 768)\n",
      "Running batch 45/80\n",
      "output shape: (10, 768)\n",
      "Running batch 46/80\n",
      "output shape: (10, 768)\n",
      "Running batch 47/80\n",
      "output shape: (10, 768)\n",
      "Running batch 48/80\n",
      "output shape: (10, 768)\n",
      "Running batch 49/80\n",
      "output shape: (10, 768)\n",
      "Running batch 50/80\n",
      "output shape: (10, 768)\n",
      "Running batch 51/80\n",
      "output shape: (10, 768)\n",
      "Running batch 52/80\n",
      "output shape: (10, 768)\n",
      "Running batch 53/80\n",
      "output shape: (10, 768)\n",
      "Running batch 54/80\n",
      "output shape: (10, 768)\n",
      "Running batch 55/80\n",
      "output shape: (10, 768)\n",
      "Running batch 56/80\n",
      "output shape: (10, 768)\n",
      "Running batch 57/80\n",
      "output shape: (10, 768)\n",
      "Running batch 58/80\n",
      "output shape: (10, 768)\n",
      "Running batch 59/80\n",
      "output shape: (10, 768)\n",
      "Running batch 60/80\n",
      "output shape: (10, 768)\n",
      "Running batch 61/80\n",
      "output shape: (10, 768)\n",
      "Running batch 62/80\n",
      "output shape: (10, 768)\n",
      "Running batch 63/80\n",
      "output shape: (10, 768)\n",
      "Running batch 64/80\n",
      "output shape: (10, 768)\n",
      "Running batch 65/80\n",
      "output shape: (10, 768)\n",
      "Running batch 66/80\n",
      "output shape: (10, 768)\n",
      "Running batch 67/80\n",
      "output shape: (10, 768)\n",
      "Running batch 68/80\n",
      "output shape: (10, 768)\n",
      "Running batch 69/80\n",
      "output shape: (10, 768)\n",
      "Running batch 70/80\n",
      "output shape: (10, 768)\n",
      "Running batch 71/80\n",
      "output shape: (10, 768)\n",
      "Running batch 72/80\n",
      "output shape: (10, 768)\n",
      "Running batch 73/80\n",
      "output shape: (10, 768)\n",
      "Running batch 74/80\n",
      "output shape: (10, 768)\n",
      "Running batch 75/80\n",
      "output shape: (10, 768)\n",
      "Running batch 76/80\n",
      "output shape: (10, 768)\n",
      "Running batch 77/80\n",
      "output shape: (10, 768)\n",
      "Running batch 78/80\n",
      "output shape: (10, 768)\n",
      "Running batch 79/80\n",
      "output shape: (10, 768)\n",
      "Running batch 80/80\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (800, 768)\n",
      "Total Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "EvaluateModelUsingProbs(pipeline, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "worldwide-wednesday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hej,\\r\\r\\n \\r\\r\\nhar nu fått tag i föraren som är i grekland på semester.\\r\\r\\n \\r\\r\\nden unga killen som frågade fick svaret av föraren att det är läge xxxx som gäller på knutpunkten.\\r\\r\\n\\r\\r\\ndetta gav föraren resenären efter att ha själv letat upp det på skånetrafikens app.\\r\\r\\nmotparten lämnar tyvärr en del information till sin egen fördel.\\r\\r\\nföraren försökte att hjälpa så gott han kunde.\\r\\r\\n \\r\\r\\n\\r\\r\\nmed vänlig hälsning,\\r\\r\\n  xxxx prata\\r\\r\\n\\r\\r\\ngruppchef\\r\\r\\n \\r\\r\\nnobina sverige ab\\r\\r\\n\\r\\r\\nadress: streetname_replaced 00, 000 00 helsingborg\\r\\r\\nmobil:  +00 00 000 00 00\\r\\r\\ndirekt: +00 0 000 000 00\\r\\r\\n\\r\\r\\nväxel:  +00 0 000 000 00\\r\\r\\n\\r\\r\\nhemsida:\\r\\r\\nwww.nobina.com\\r\\r\\n \\r\\r\\nnobina sverige ab, org.nr 000000-0000 | styrelsens säte - stockholm\\r\\r\\n\\r\\r\\np please consider the environment before printing this e-mail.\\r\\r\\n\\r\\r\\n \\r\\r\\n\\r\\r\\nfrån: xxxx prata \\r\\r\\n\\r\\r\\nskickat: den 00 oktober 0000 00:00\\r\\r\\n\\r\\r\\ntill: email@address.replaced email@address.replaced\\r\\r\\n\\r\\r\\nkopia: xxxx xxxx email@address.replaced\\r\\r\\n\\r\\r\\nämne: vb: för kännedom ärendenummer: st-000000-w0k0 epostnr:00000000\\r\\r\\n\\r\\r\\n \\r\\r\\nhej,\\r\\r\\n \\r\\r\\nföraren är på semester och är åter 00/00.\\r\\r\\n\\r\\r\\nhar försökt att nå föraren som inte svarar.\\r\\r\\nmen stämmer händelsen så är det verkligen inte ett bra beteende ifrån vår förare.\\r\\r\\njag kommer att ta upp detta ärendet när han är tillbaka.\\r\\r\\n \\r\\r\\n\\r\\r\\nmed vänlig hälsning,\\r\\r\\n  xxxx prata\\r\\r\\n\\r\\r\\ngruppchef\\r\\r\\n \\r\\r\\nnobina sverige ab\\r\\r\\n\\r\\r\\nadress: streetname_replaced 00, 000 00 helsingborg\\r\\r\\nmobil:  +00 00 000 00 00\\r\\r\\ndirekt: +00 0 000 000 00\\r\\r\\n\\r\\r\\nväxel:  +00 0 000 000 00\\r\\r\\n\\r\\r\\nhemsida:\\r\\r\\nwww.nobina.com\\r\\r\\n \\r\\r\\nnobina sverige ab, org.nr 000000-0000 | styrelsens säte - stockholm\\r\\r\\n\\r\\r\\np please consider the environment before printing this e-mail.\\r\\r\\n\\r\\r\\n \\r\\r\\n\\r\\r\\nfrån: xxxx xxxx \\r\\r\\n\\r\\r\\nskickat: den 00 oktober 0000 00:00\\r\\r\\n\\r\\r\\ntill: xxxx prata email@address.replaced\\r\\r\\n\\r\\r\\nämne: vb: för kännedom ärendenummer: st-000000-w0k0 epostnr:00000000\\r\\r\\n\\r\\r\\n \\r\\r\\nhej xxxx xxxx r har fått ett klagomål som jag lagt in i oms\\r\\r\\n \\r\\r\\nmed vänlig hälsning,\\r\\r\\n  xxxx  xxxx \\r\\r\\nkams-chef\\r\\r\\n(kvalitet, arbetsmiljö, miljö, säkerhet)\\r\\r\\n \\r\\r\\nnobina sverige ab\\r\\r\\n\\r\\r\\nadress: streetname_replaced 00, 000 00 helsingborg\\r\\r\\nmobil:  +00 00 000 00 00\\r\\r\\ndirekt: +00 00 000 00 00\\r\\r\\n\\r\\r\\nväxel:  +00 0 000 000 00\\r\\r\\n\\r\\r\\nhemsida:\\r\\r\\nwww.nobina.com\\r\\r\\n \\r\\r\\nnobina sverige ab, org.nr 000000-0000 | styrelsens säte - stockholm\\r\\r\\n\\r\\r\\np please consider the environment before printing this e-mail.\\r\\r\\n \\r\\r\\nfrån: xxxx xxxx email@address.replaced\\r\\r\\n\\r\\r\\nskickat: den 00 oktober 0000 00:00\\r\\r\\n\\r\\r\\ntill: xxxx xxxx email@address.replaced\\r\\r\\n\\r\\r\\nämne: för kännedom ärendenummer: st-000000-w0k0 epostnr:00000000\\r\\r\\n \\r\\r\\nhej! \\r\\r\\ndetta är ett ärende från kund som vi tagit emot och som vi känner att även du behöver få kännedom om.\\r\\r\\nhälsningar\\r\\r\\nskånetrafikens kundtjänst\\r\\r\\n \\r\\r\\ninformation om ärendet \\r\\r\\närenderubrik: bemötande\\u2009\\r\\r\\närendetyp: \\u2009klagomål\\r\\r\\n\\u2009händelsedatum: 0000-00-00 00:00\\u2009\\r\\r\\nkundens beskrivning: xxxx borde nog ge era chaufförer i helsingborg en lektion i service!!! här frågar man en busschaufför om hans buss går till streetname_replaced och han snäser av: var vill du? xxxx  svarar: vi vill till streetname_replaced!!\\r\\r\\n\\r\\r\\nhan: jag är ingen taxi så kan inte svara dig???? va fan är detta?? \\r\\r\\n\\r\\r\\nchauffören det gäller körde följande buss från centralen ca 00.00 idag söndag! bussen reg nr:\\r\\r\\n\\r\\r\\nhoppas ni tar tag i detta!!😡😡\\r\\r\\n\\r\\r\\nvet inte vilken avgång det rör sig om men reg.nr är xbc 000\\u2009\\r\\r\\ntrafikslag: \\u2009\\r\\r\\nlinje: \\u2009\\r\\r\\ntur: \\u2009\\r\\r\\navgångstid planerad: \\u2009\\r\\r\\navgångstid verklig: \\u2009\\r\\r\\nlinjetext:',\n",
       "       'buss 000 00:00\\r\\r\\n\\r\\r\\nkristianstad hästtorget  - xxxx streetname_replaced \\r\\r\\n\\r\\r\\ninget ljud på automatiska utropen utanför bussen, som berättar vilket nummer på bussen och vilken slutstation linjen har. \\r\\r\\n\\r\\r\\nbra volym på automatiska hållplats utropen inne i bussen.\\r\\r\\n\\r\\r\\nbad föraren om hjälp av med rullatorn, jag var tvungen att säga till två gånger innan föraren förstod.\\r\\r\\n\\r\\r\\nnär föraren förstod fick jag utmärkt hjälp ut genom främre dörren. \\r\\r\\n xxxx 000 00:00\\r\\r\\n xxxx streetname_replaced - kristianstad streetname_replaced \\r\\r\\n\\r\\r\\ninget ljud på automatiska utropen utanför bussen, som berättar vilket nummer på bussen\\r\\r\\n och vilken slutstation linjen har. \\r\\r\\n\\r\\r\\ninget ljud på automatiska hållplats utropen inne i bussen.\\r\\r\\n\\r\\r\\nbad föraren om hjälp av med rullatorn, jag var tvungen att säga till två gånger innan\\r\\r\\n föraren förstod.\\r\\r\\n\\r\\r\\nnär föraren förstod fick jag utmärkt hjälp ut genom främre dörren. \\r\\r\\n\\r\\r\\nmed vänlig hälsningar.\\r\\r\\n xxxx johnson'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "closing-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "civil-electron",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jag vill ha ersättning för min biljett, då tåg...\n",
       "1                      bussen hade dålig air condition\n",
       "2    beskrivning i avic:\\r\\r\\n xxxx sjukresebeställ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "noticed-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/1\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (10, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6, 1, 4, 4, 6, 6, 4, 2, 5, 0], dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictClasses(pipeline, X[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "final-baker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avvikelse</th>\n",
       "      <th>Beröm</th>\n",
       "      <th>Fråga</th>\n",
       "      <th>Förseningsersättning</th>\n",
       "      <th>Klagomål</th>\n",
       "      <th>Skada</th>\n",
       "      <th>Synpunkt/Önskemål</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Avvikelse  Beröm  Fråga  Förseningsersättning  Klagomål  Skada  \\\n",
       "10          0      0      1                     0         0      0   \n",
       "11          0      0      0                     0         0      0   \n",
       "12          0      0      0                     0         0      0   \n",
       "13          0      0      0                     0         0      0   \n",
       "14          0      0      0                     0         0      0   \n",
       "15          0      0      0                     0         1      0   \n",
       "16          0      0      0                     0         1      0   \n",
       "17          0      0      1                     0         0      0   \n",
       "18          0      0      0                     0         1      0   \n",
       "19          0      0      0                     0         1      0   \n",
       "\n",
       "    Synpunkt/Önskemål  \n",
       "10                  0  \n",
       "11                  1  \n",
       "12                  1  \n",
       "13                  1  \n",
       "14                  1  \n",
       "15                  0  \n",
       "16                  0  \n",
       "17                  0  \n",
       "18                  0  \n",
       "19                  0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "statutory-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save, Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "israeli-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "tough-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'BertModel.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "tight-english",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertModel.joblib']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dump(pipeline, modelName ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "reserved-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedModel = load(modelName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "early-reference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform Called\n",
      "Running batch 1/2\n",
      "output shape: (10, 768)\n",
      "Running batch 2/2\n",
      "output shape: (10, 768)\n",
      "transformed.shape: (20, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel.predict(X_test[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "premium-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "driven-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TorchNLP(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        #print('Init called')\n",
    "        self.model_name = 'KB/bert-base-swedish-cased'\n",
    "        self.Bert = AutoModel.from_pretrained(self.model_name)\n",
    "        self.Tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.n_classes = n_classes\n",
    "        self.TorchModel = nn.Sequential(nn.Linear(768, 256),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.42),\n",
    "                          nn.Linear(256, 128),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Dropout(p=0.42),              \n",
    "                          nn.Linear(128, n_classes))\n",
    "                          #nn.Softmax(dim=1)) Cannot use softmax here since nn.CrossEntropyLoss expects scores!\n",
    "        \n",
    "        #Freeze the Bert model layers\n",
    "        for param in self.Bert.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def forward(self, X):\n",
    "        #print('Forward Called')\n",
    "       \n",
    "        #Check device\n",
    "        device = self.Bert.device\n",
    "        \n",
    "        # Transform input tokens. This is most efficient if done in one batch \n",
    "        X = self.Tokenizer(X.values.tolist(), return_tensors=\"pt\", padding='max_length', max_length = 512, truncation=True).to(device)\n",
    "        \n",
    "        X = self.Bert(**X)\n",
    "        X = X['pooler_output']\n",
    "        X = self.TorchModel(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "            output = self.forward(X)\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "            top_class = top_class.to('cpu').numpy().reshape(-1,)\n",
    "            self.train()\n",
    "            return top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "artistic-sewing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "married-steel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#n_classes = Y.shape[1] #In case of one hot encoded, which we don't have anymore\n",
    "n_classes = Y.max() + 1\n",
    "torchModel = TorchNLP(n_classes = n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "sustainable-venue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Called\n"
     ]
    }
   ],
   "source": [
    "output = torchModel(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "rotary-clinton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1394, 0.1599, 0.1548, 0.1324, 0.1248, 0.1408, 0.1479],\n",
       "        [0.1428, 0.1565, 0.1440, 0.1306, 0.1277, 0.1471, 0.1513],\n",
       "        [0.1410, 0.1569, 0.1521, 0.1396, 0.1235, 0.1379, 0.1489],\n",
       "        [0.1318, 0.1499, 0.1585, 0.1312, 0.1373, 0.1510, 0.1404],\n",
       "        [0.1423, 0.1517, 0.1463, 0.1242, 0.1348, 0.1532, 0.1476]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "tired-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mMiniBatcherTrain = MiniBatcher(X_train[:1000], Y_train[:1000], batch_size=15)\n",
    "#mMiniBatcherTest = MiniBatcher(X_test[:100], Y_test[:100], batch_size=15)\n",
    "mMiniBatcherTrain = MiniBatcher(X_train, Y_train, batch_size=100)\n",
    "mMiniBatcherTest = MiniBatcher(X_test[:500], Y_test[:500], batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "brown-conservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([50325, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([512, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([2, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) False\n",
      "<class 'torch.Tensor'> torch.Size([768]) False\n",
      "<class 'torch.Tensor'> torch.Size([128, 768]) True\n",
      "<class 'torch.Tensor'> torch.Size([128]) True\n",
      "<class 'torch.Tensor'> torch.Size([64, 128]) True\n",
      "<class 'torch.Tensor'> torch.Size([64]) True\n",
      "<class 'torch.Tensor'> torch.Size([7, 64]) True\n",
      "<class 'torch.Tensor'> torch.Size([7]) True\n"
     ]
    }
   ],
   "source": [
    "for param in torchModel.parameters():\n",
    "    print(type(param.data), param.size(), param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "modular-vertical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "cuda_enabled = torch.cuda.is_available()\n",
    "if cuda_enabled:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f'We are running on device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "personal-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "sized-conditions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.56.2-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Requirement already satisfied: importlib-metadata in c:\\programdata\\anaconda3\\envs\\tf38\\lib\\site-packages (from numba) (2.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.18 in c:\\programdata\\anaconda3\\envs\\tf38\\lib\\site-packages (from numba) (1.23.2)\n",
      "Requirement already satisfied: setuptools<60 in c:\\programdata\\anaconda3\\envs\\tf38\\lib\\site-packages (from numba) (51.3.3.post20210118)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-win_amd64.whl (23.2 MB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\envs\\tf38\\lib\\site-packages (from importlib-metadata->numba) (3.4.0)\n",
      "Installing collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.39.1 numba-0.56.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "solid-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install GPUtil\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "unexpected-idaho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM  |\n",
      "-------------------\n",
      "|  0 | 19% | 100% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 25% |  8% |\n"
     ]
    }
   ],
   "source": [
    "#free_gpu_cache()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "incredible-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 1/15\n",
      "Batch loss: 1.954135537147522 batch: 1/840\n",
      "Batch loss: 1.9362761974334717 batch: 2/840\n",
      "Batch loss: 1.8906426429748535 batch: 3/840\n",
      "Batch loss: 1.8690556287765503 batch: 4/840\n",
      "Batch loss: 1.8202908039093018 batch: 5/840\n",
      "Batch loss: 1.835722804069519 batch: 6/840\n",
      "Batch loss: 1.8324123620986938 batch: 7/840\n",
      "Batch loss: 1.763242244720459 batch: 8/840\n",
      "Batch loss: 1.6997215747833252 batch: 9/840\n",
      "Batch loss: 1.6596179008483887 batch: 10/840\n",
      "Batch loss: 1.5897629261016846 batch: 11/840\n",
      "Batch loss: 1.539951205253601 batch: 12/840\n",
      "Batch loss: 1.4772002696990967 batch: 13/840\n",
      "Batch loss: 1.5468162298202515 batch: 14/840\n",
      "Batch loss: 1.5089606046676636 batch: 15/840\n",
      "Batch loss: 1.3836619853973389 batch: 16/840\n",
      "Batch loss: 1.4789026975631714 batch: 17/840\n",
      "Batch loss: 1.3889561891555786 batch: 18/840\n",
      "Batch loss: 1.5200637578964233 batch: 19/840\n",
      "Batch loss: 1.5299617052078247 batch: 20/840\n",
      "Batch loss: 1.3485318422317505 batch: 21/840\n",
      "Batch loss: 1.2940984964370728 batch: 22/840\n",
      "Batch loss: 1.5352778434753418 batch: 23/840\n",
      "Batch loss: 1.235742449760437 batch: 24/840\n",
      "Batch loss: 1.2986042499542236 batch: 25/840\n",
      "Batch loss: 1.2476935386657715 batch: 26/840\n",
      "Batch loss: 1.3339478969573975 batch: 27/840\n",
      "Batch loss: 1.2867741584777832 batch: 28/840\n",
      "Batch loss: 1.2653846740722656 batch: 29/840\n",
      "Batch loss: 1.0862339735031128 batch: 30/840\n",
      "Batch loss: 1.220850944519043 batch: 31/840\n",
      "Batch loss: 1.1114062070846558 batch: 32/840\n",
      "Batch loss: 1.1543275117874146 batch: 33/840\n",
      "Batch loss: 1.1870888471603394 batch: 34/840\n",
      "Batch loss: 1.1671419143676758 batch: 35/840\n",
      "Batch loss: 1.1610159873962402 batch: 36/840\n",
      "Batch loss: 1.3352938890457153 batch: 37/840\n",
      "Batch loss: 1.2488096952438354 batch: 38/840\n",
      "Batch loss: 1.264317274093628 batch: 39/840\n",
      "Batch loss: 1.0311836004257202 batch: 40/840\n",
      "Batch loss: 1.1945686340332031 batch: 41/840\n",
      "Batch loss: 1.0834273099899292 batch: 42/840\n",
      "Batch loss: 1.1145806312561035 batch: 43/840\n",
      "Batch loss: 1.2047219276428223 batch: 44/840\n",
      "Batch loss: 1.0403906106948853 batch: 45/840\n",
      "Batch loss: 1.0007107257843018 batch: 46/840\n",
      "Batch loss: 0.9474323391914368 batch: 47/840\n",
      "Batch loss: 1.1333937644958496 batch: 48/840\n",
      "Batch loss: 1.1486001014709473 batch: 49/840\n",
      "Batch loss: 1.1545413732528687 batch: 50/840\n",
      "Batch loss: 1.1436004638671875 batch: 51/840\n",
      "Batch loss: 1.1851648092269897 batch: 52/840\n",
      "Batch loss: 1.0394036769866943 batch: 53/840\n",
      "Batch loss: 1.0036412477493286 batch: 54/840\n",
      "Batch loss: 1.0303726196289062 batch: 55/840\n",
      "Batch loss: 1.1121147871017456 batch: 56/840\n",
      "Batch loss: 1.028930902481079 batch: 57/840\n",
      "Batch loss: 0.8809975981712341 batch: 58/840\n",
      "Batch loss: 1.0596389770507812 batch: 59/840\n",
      "Batch loss: 0.9840267300605774 batch: 60/840\n",
      "Batch loss: 1.080477237701416 batch: 61/840\n",
      "Batch loss: 0.9393677711486816 batch: 62/840\n",
      "Batch loss: 0.9547160863876343 batch: 63/840\n",
      "Batch loss: 1.0325440168380737 batch: 64/840\n",
      "Batch loss: 1.0178077220916748 batch: 65/840\n",
      "Batch loss: 1.0337761640548706 batch: 66/840\n",
      "Batch loss: 1.0157135725021362 batch: 67/840\n",
      "Batch loss: 0.893993079662323 batch: 68/840\n",
      "Batch loss: 1.0283994674682617 batch: 69/840\n",
      "Batch loss: 0.9264284372329712 batch: 70/840\n",
      "Batch loss: 1.1055896282196045 batch: 71/840\n",
      "Batch loss: 0.9738218784332275 batch: 72/840\n",
      "Batch loss: 0.9335829019546509 batch: 73/840\n",
      "Batch loss: 0.8083973526954651 batch: 74/840\n",
      "Batch loss: 1.0539231300354004 batch: 75/840\n",
      "Batch loss: 0.7630273699760437 batch: 76/840\n",
      "Batch loss: 0.9329349994659424 batch: 77/840\n",
      "Batch loss: 1.1043530702590942 batch: 78/840\n",
      "Batch loss: 0.992200493812561 batch: 79/840\n",
      "Batch loss: 0.9793035387992859 batch: 80/840\n",
      "Batch loss: 0.8959264159202576 batch: 81/840\n",
      "Batch loss: 1.0197243690490723 batch: 82/840\n",
      "Batch loss: 0.766976535320282 batch: 83/840\n",
      "Batch loss: 0.9879166483879089 batch: 84/840\n",
      "Batch loss: 0.9384203553199768 batch: 85/840\n",
      "Batch loss: 1.0815166234970093 batch: 86/840\n",
      "Batch loss: 0.8335462808609009 batch: 87/840\n",
      "Batch loss: 0.8019119501113892 batch: 88/840\n",
      "Batch loss: 0.7948311567306519 batch: 89/840\n",
      "Batch loss: 0.8508981466293335 batch: 90/840\n",
      "Batch loss: 0.8373168706893921 batch: 91/840\n",
      "Batch loss: 0.8426118493080139 batch: 92/840\n",
      "Batch loss: 0.9080469012260437 batch: 93/840\n",
      "Batch loss: 0.8164279460906982 batch: 94/840\n",
      "Batch loss: 0.8902287483215332 batch: 95/840\n",
      "Batch loss: 0.8466624617576599 batch: 96/840\n",
      "Batch loss: 0.8288511633872986 batch: 97/840\n",
      "Batch loss: 0.9889401197433472 batch: 98/840\n",
      "Batch loss: 0.8436118364334106 batch: 99/840\n",
      "Batch loss: 0.8645439743995667 batch: 100/840\n",
      "Batch loss: 0.7811578512191772 batch: 101/840\n",
      "Batch loss: 0.8405274152755737 batch: 102/840\n",
      "Batch loss: 0.9149134755134583 batch: 103/840\n",
      "Batch loss: 0.6584300994873047 batch: 104/840\n",
      "Batch loss: 0.8460894227027893 batch: 105/840\n",
      "Batch loss: 0.8819904327392578 batch: 106/840\n",
      "Batch loss: 0.782034158706665 batch: 107/840\n",
      "Batch loss: 1.0001685619354248 batch: 108/840\n",
      "Batch loss: 0.902615487575531 batch: 109/840\n",
      "Batch loss: 0.8200502991676331 batch: 110/840\n",
      "Batch loss: 0.752482533454895 batch: 111/840\n",
      "Batch loss: 0.9851238131523132 batch: 112/840\n",
      "Batch loss: 0.837787926197052 batch: 113/840\n",
      "Batch loss: 0.8182244896888733 batch: 114/840\n",
      "Batch loss: 0.7908470034599304 batch: 115/840\n",
      "Batch loss: 0.7464261651039124 batch: 116/840\n",
      "Batch loss: 0.7903379201889038 batch: 117/840\n",
      "Batch loss: 0.6690411567687988 batch: 118/840\n",
      "Batch loss: 0.7446272969245911 batch: 119/840\n",
      "Batch loss: 0.6441056132316589 batch: 120/840\n",
      "Batch loss: 0.9393509030342102 batch: 121/840\n",
      "Batch loss: 1.0170327425003052 batch: 122/840\n",
      "Batch loss: 0.6680334210395813 batch: 123/840\n",
      "Batch loss: 0.8095518350601196 batch: 124/840\n",
      "Batch loss: 0.8093446493148804 batch: 125/840\n",
      "Batch loss: 0.9660314917564392 batch: 126/840\n",
      "Batch loss: 0.9760226607322693 batch: 127/840\n",
      "Batch loss: 0.8753406405448914 batch: 128/840\n",
      "Batch loss: 0.9433822631835938 batch: 129/840\n",
      "Batch loss: 0.8210005760192871 batch: 130/840\n",
      "Batch loss: 0.8992106914520264 batch: 131/840\n",
      "Batch loss: 1.0911098718643188 batch: 132/840\n",
      "Batch loss: 0.8546934723854065 batch: 133/840\n",
      "Batch loss: 0.8074359893798828 batch: 134/840\n",
      "Batch loss: 0.7497779726982117 batch: 135/840\n",
      "Batch loss: 0.9733660817146301 batch: 136/840\n",
      "Batch loss: 0.7884703278541565 batch: 137/840\n",
      "Batch loss: 0.7221632599830627 batch: 138/840\n",
      "Batch loss: 0.7682856917381287 batch: 139/840\n",
      "Batch loss: 0.8646344542503357 batch: 140/840\n",
      "Batch loss: 0.6181581020355225 batch: 141/840\n",
      "Batch loss: 0.8521243929862976 batch: 142/840\n",
      "Batch loss: 0.7345171570777893 batch: 143/840\n",
      "Batch loss: 0.7914539575576782 batch: 144/840\n",
      "Batch loss: 0.9151392579078674 batch: 145/840\n",
      "Batch loss: 0.7904053330421448 batch: 146/840\n",
      "Batch loss: 0.6873084902763367 batch: 147/840\n",
      "Batch loss: 0.9534533023834229 batch: 148/840\n",
      "Batch loss: 0.7679104804992676 batch: 149/840\n",
      "Batch loss: 0.948434054851532 batch: 150/840\n",
      "Batch loss: 0.7833020687103271 batch: 151/840\n",
      "Batch loss: 0.7823042869567871 batch: 152/840\n",
      "Batch loss: 0.7167266607284546 batch: 153/840\n",
      "Batch loss: 0.835968017578125 batch: 154/840\n",
      "Batch loss: 0.7055265307426453 batch: 155/840\n",
      "Batch loss: 0.9171289801597595 batch: 156/840\n",
      "Batch loss: 0.8241891264915466 batch: 157/840\n",
      "Batch loss: 0.7217200994491577 batch: 158/840\n",
      "Batch loss: 0.7859199643135071 batch: 159/840\n",
      "Batch loss: 0.7243488430976868 batch: 160/840\n",
      "Batch loss: 0.8287220597267151 batch: 161/840\n",
      "Batch loss: 0.7503248453140259 batch: 162/840\n",
      "Batch loss: 0.9720410704612732 batch: 163/840\n",
      "Batch loss: 0.648097813129425 batch: 164/840\n",
      "Batch loss: 0.9456954002380371 batch: 165/840\n",
      "Batch loss: 0.6855098605155945 batch: 166/840\n",
      "Batch loss: 0.7917210459709167 batch: 167/840\n",
      "Batch loss: 0.8354200124740601 batch: 168/840\n",
      "Batch loss: 0.6926817893981934 batch: 169/840\n",
      "Batch loss: 0.9126407504081726 batch: 170/840\n",
      "Batch loss: 0.8226249814033508 batch: 171/840\n",
      "Batch loss: 1.0305819511413574 batch: 172/840\n",
      "Batch loss: 0.8312104940414429 batch: 173/840\n",
      "Batch loss: 0.7417733669281006 batch: 174/840\n",
      "Batch loss: 0.6876199841499329 batch: 175/840\n",
      "Batch loss: 0.869575560092926 batch: 176/840\n",
      "Batch loss: 0.8097705245018005 batch: 177/840\n",
      "Batch loss: 0.8241686820983887 batch: 178/840\n",
      "Batch loss: 0.9661252498626709 batch: 179/840\n",
      "Batch loss: 0.7413063049316406 batch: 180/840\n",
      "Batch loss: 0.794253408908844 batch: 181/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7951816320419312 batch: 182/840\n",
      "Batch loss: 0.8690511584281921 batch: 183/840\n",
      "Batch loss: 0.7038179039955139 batch: 184/840\n",
      "Batch loss: 0.6490274667739868 batch: 185/840\n",
      "Batch loss: 0.6118298172950745 batch: 186/840\n",
      "Batch loss: 0.899085283279419 batch: 187/840\n",
      "Batch loss: 0.6294940710067749 batch: 188/840\n",
      "Batch loss: 0.7716383934020996 batch: 189/840\n",
      "Batch loss: 0.8323737978935242 batch: 190/840\n",
      "Batch loss: 0.9039003252983093 batch: 191/840\n",
      "Batch loss: 0.6373999118804932 batch: 192/840\n",
      "Batch loss: 0.7699265480041504 batch: 193/840\n",
      "Batch loss: 0.6271510720252991 batch: 194/840\n",
      "Batch loss: 0.8850708603858948 batch: 195/840\n",
      "Batch loss: 0.8654938340187073 batch: 196/840\n",
      "Batch loss: 0.9021247029304504 batch: 197/840\n",
      "Batch loss: 0.6622282266616821 batch: 198/840\n",
      "Batch loss: 0.6796248555183411 batch: 199/840\n",
      "Batch loss: 1.0166761875152588 batch: 200/840\n",
      "Batch loss: 0.7078800797462463 batch: 201/840\n",
      "Batch loss: 0.7737282514572144 batch: 202/840\n",
      "Batch loss: 0.730421781539917 batch: 203/840\n",
      "Batch loss: 0.8351210951805115 batch: 204/840\n",
      "Batch loss: 0.8579612970352173 batch: 205/840\n",
      "Batch loss: 0.7078004479408264 batch: 206/840\n",
      "Batch loss: 0.7552841901779175 batch: 207/840\n",
      "Batch loss: 0.7476751804351807 batch: 208/840\n",
      "Batch loss: 0.7401540875434875 batch: 209/840\n",
      "Batch loss: 0.6846746802330017 batch: 210/840\n",
      "Batch loss: 0.6300572752952576 batch: 211/840\n",
      "Batch loss: 0.7512983679771423 batch: 212/840\n",
      "Batch loss: 0.9291923642158508 batch: 213/840\n",
      "Batch loss: 0.8425344824790955 batch: 214/840\n",
      "Batch loss: 0.7447025179862976 batch: 215/840\n",
      "Batch loss: 0.7951079607009888 batch: 216/840\n",
      "Batch loss: 0.7788694500923157 batch: 217/840\n",
      "Batch loss: 0.8157122135162354 batch: 218/840\n",
      "Batch loss: 0.8199535608291626 batch: 219/840\n",
      "Batch loss: 0.928852379322052 batch: 220/840\n",
      "Batch loss: 0.6830756664276123 batch: 221/840\n",
      "Batch loss: 0.9202058911323547 batch: 222/840\n",
      "Batch loss: 0.6869634985923767 batch: 223/840\n",
      "Batch loss: 0.9134249091148376 batch: 224/840\n",
      "Batch loss: 0.8379405736923218 batch: 225/840\n",
      "Batch loss: 0.9067552089691162 batch: 226/840\n",
      "Batch loss: 0.9043168425559998 batch: 227/840\n",
      "Batch loss: 0.6310155391693115 batch: 228/840\n",
      "Batch loss: 0.6600959300994873 batch: 229/840\n",
      "Batch loss: 0.7562967538833618 batch: 230/840\n",
      "Batch loss: 0.7302035689353943 batch: 231/840\n",
      "Batch loss: 0.7495886087417603 batch: 232/840\n",
      "Batch loss: 0.9023680090904236 batch: 233/840\n",
      "Batch loss: 0.769793689250946 batch: 234/840\n",
      "Batch loss: 0.7677983045578003 batch: 235/840\n",
      "Batch loss: 0.7672528028488159 batch: 236/840\n",
      "Batch loss: 0.6819242835044861 batch: 237/840\n",
      "Batch loss: 0.8334500193595886 batch: 238/840\n",
      "Batch loss: 0.8023523092269897 batch: 239/840\n",
      "Batch loss: 0.8257597088813782 batch: 240/840\n",
      "Batch loss: 0.8359930515289307 batch: 241/840\n",
      "Batch loss: 0.7063871622085571 batch: 242/840\n",
      "Batch loss: 0.7344509959220886 batch: 243/840\n",
      "Batch loss: 0.8470115065574646 batch: 244/840\n",
      "Batch loss: 0.6776733994483948 batch: 245/840\n",
      "Batch loss: 0.8247623443603516 batch: 246/840\n",
      "Batch loss: 0.9095033407211304 batch: 247/840\n",
      "Batch loss: 0.8381250500679016 batch: 248/840\n",
      "Batch loss: 1.0098897218704224 batch: 249/840\n",
      "Batch loss: 0.6897046566009521 batch: 250/840\n",
      "Batch loss: 0.7548316717147827 batch: 251/840\n",
      "Batch loss: 0.6726234555244446 batch: 252/840\n",
      "Batch loss: 0.822722315788269 batch: 253/840\n",
      "Batch loss: 0.8867841362953186 batch: 254/840\n",
      "Batch loss: 0.7637205719947815 batch: 255/840\n",
      "Batch loss: 0.7656963467597961 batch: 256/840\n",
      "Batch loss: 0.7107309699058533 batch: 257/840\n",
      "Batch loss: 0.7711632251739502 batch: 258/840\n",
      "Batch loss: 0.7474132776260376 batch: 259/840\n",
      "Batch loss: 0.6703073978424072 batch: 260/840\n",
      "Batch loss: 0.7098355889320374 batch: 261/840\n",
      "Batch loss: 0.5455536246299744 batch: 262/840\n",
      "Batch loss: 0.7786839008331299 batch: 263/840\n",
      "Batch loss: 0.8378664255142212 batch: 264/840\n",
      "Batch loss: 0.8840914368629456 batch: 265/840\n",
      "Batch loss: 0.7316944003105164 batch: 266/840\n",
      "Batch loss: 0.8067615628242493 batch: 267/840\n",
      "Batch loss: 0.6892997622489929 batch: 268/840\n",
      "Batch loss: 0.6266966462135315 batch: 269/840\n",
      "Batch loss: 0.6827536821365356 batch: 270/840\n",
      "Batch loss: 0.7031095027923584 batch: 271/840\n",
      "Batch loss: 0.8934045433998108 batch: 272/840\n",
      "Batch loss: 0.8773916363716125 batch: 273/840\n",
      "Batch loss: 0.7823954820632935 batch: 274/840\n",
      "Batch loss: 0.8576266765594482 batch: 275/840\n",
      "Batch loss: 0.6876860857009888 batch: 276/840\n",
      "Batch loss: 0.7551166415214539 batch: 277/840\n",
      "Batch loss: 0.9064822196960449 batch: 278/840\n",
      "Batch loss: 0.9102285504341125 batch: 279/840\n",
      "Batch loss: 0.8898351788520813 batch: 280/840\n",
      "Batch loss: 0.6829480528831482 batch: 281/840\n",
      "Batch loss: 0.6331601738929749 batch: 282/840\n",
      "Batch loss: 0.7179121375083923 batch: 283/840\n",
      "Batch loss: 0.5876362323760986 batch: 284/840\n",
      "Batch loss: 0.6184975504875183 batch: 285/840\n",
      "Batch loss: 0.7929881811141968 batch: 286/840\n",
      "Batch loss: 0.5581722259521484 batch: 287/840\n",
      "Batch loss: 0.6828656792640686 batch: 288/840\n",
      "Batch loss: 0.91942298412323 batch: 289/840\n",
      "Batch loss: 0.9191624522209167 batch: 290/840\n",
      "Batch loss: 0.8821653723716736 batch: 291/840\n",
      "Batch loss: 0.6980569958686829 batch: 292/840\n",
      "Batch loss: 0.91484534740448 batch: 293/840\n",
      "Batch loss: 0.7933852672576904 batch: 294/840\n",
      "Batch loss: 0.6103489995002747 batch: 295/840\n",
      "Batch loss: 0.7421553134918213 batch: 296/840\n",
      "Batch loss: 0.7656187415122986 batch: 297/840\n",
      "Batch loss: 0.7752965688705444 batch: 298/840\n",
      "Batch loss: 0.5974770784378052 batch: 299/840\n",
      "Batch loss: 0.8652617931365967 batch: 300/840\n",
      "Batch loss: 0.8167278170585632 batch: 301/840\n",
      "Batch loss: 0.8088144659996033 batch: 302/840\n",
      "Batch loss: 0.8551755547523499 batch: 303/840\n",
      "Batch loss: 0.6784457564353943 batch: 304/840\n",
      "Batch loss: 0.6364142894744873 batch: 305/840\n",
      "Batch loss: 0.7957452535629272 batch: 306/840\n",
      "Batch loss: 0.6444069147109985 batch: 307/840\n",
      "Batch loss: 0.8154283761978149 batch: 308/840\n",
      "Batch loss: 0.8087526559829712 batch: 309/840\n",
      "Batch loss: 0.9636982679367065 batch: 310/840\n",
      "Batch loss: 0.8403249382972717 batch: 311/840\n",
      "Batch loss: 0.8233022093772888 batch: 312/840\n",
      "Batch loss: 0.7931273579597473 batch: 313/840\n",
      "Batch loss: 0.7337809801101685 batch: 314/840\n",
      "Batch loss: 0.743028998374939 batch: 315/840\n",
      "Batch loss: 0.5364568829536438 batch: 316/840\n",
      "Batch loss: 0.7915827035903931 batch: 317/840\n",
      "Batch loss: 0.8115564584732056 batch: 318/840\n",
      "Batch loss: 0.7115915417671204 batch: 319/840\n",
      "Batch loss: 0.692980170249939 batch: 320/840\n",
      "Batch loss: 0.7651735544204712 batch: 321/840\n",
      "Batch loss: 0.8839263319969177 batch: 322/840\n",
      "Batch loss: 0.9303051829338074 batch: 323/840\n",
      "Batch loss: 0.7766202688217163 batch: 324/840\n",
      "Batch loss: 0.6430789828300476 batch: 325/840\n",
      "Batch loss: 0.7403461337089539 batch: 326/840\n",
      "Batch loss: 0.7041435241699219 batch: 327/840\n",
      "Batch loss: 0.894165575504303 batch: 328/840\n",
      "Batch loss: 0.8581542372703552 batch: 329/840\n",
      "Batch loss: 0.8530000448226929 batch: 330/840\n",
      "Batch loss: 0.8398312926292419 batch: 331/840\n",
      "Batch loss: 0.7468405961990356 batch: 332/840\n",
      "Batch loss: 0.7843902111053467 batch: 333/840\n",
      "Batch loss: 0.8454254865646362 batch: 334/840\n",
      "Batch loss: 0.6000325679779053 batch: 335/840\n",
      "Batch loss: 0.8304339647293091 batch: 336/840\n",
      "Batch loss: 0.9696974158287048 batch: 337/840\n",
      "Batch loss: 0.818204402923584 batch: 338/840\n",
      "Batch loss: 0.6511482000350952 batch: 339/840\n",
      "Batch loss: 0.9295607209205627 batch: 340/840\n",
      "Batch loss: 0.6462159156799316 batch: 341/840\n",
      "Batch loss: 0.6058388352394104 batch: 342/840\n",
      "Batch loss: 1.006306767463684 batch: 343/840\n",
      "Batch loss: 0.7100584506988525 batch: 344/840\n",
      "Batch loss: 0.5219612121582031 batch: 345/840\n",
      "Batch loss: 0.8315108418464661 batch: 346/840\n",
      "Batch loss: 0.7806875109672546 batch: 347/840\n",
      "Batch loss: 0.7622427344322205 batch: 348/840\n",
      "Batch loss: 0.8065559267997742 batch: 349/840\n",
      "Batch loss: 0.6146376729011536 batch: 350/840\n",
      "Batch loss: 0.7561618685722351 batch: 351/840\n",
      "Batch loss: 0.7577092051506042 batch: 352/840\n",
      "Batch loss: 0.7791333198547363 batch: 353/840\n",
      "Batch loss: 0.6618932485580444 batch: 354/840\n",
      "Batch loss: 0.725425660610199 batch: 355/840\n",
      "Batch loss: 0.7679660320281982 batch: 356/840\n",
      "Batch loss: 0.6617759466171265 batch: 357/840\n",
      "Batch loss: 0.8504395484924316 batch: 358/840\n",
      "Batch loss: 0.6721765995025635 batch: 359/840\n",
      "Batch loss: 0.9025173783302307 batch: 360/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8427690863609314 batch: 361/840\n",
      "Batch loss: 0.7094565033912659 batch: 362/840\n",
      "Batch loss: 0.6792579889297485 batch: 363/840\n",
      "Batch loss: 0.8090555667877197 batch: 364/840\n",
      "Batch loss: 0.6996357440948486 batch: 365/840\n",
      "Batch loss: 0.7451508045196533 batch: 366/840\n",
      "Batch loss: 0.6123248934745789 batch: 367/840\n",
      "Batch loss: 0.8089743256568909 batch: 368/840\n",
      "Batch loss: 0.7570825815200806 batch: 369/840\n",
      "Batch loss: 0.844102680683136 batch: 370/840\n",
      "Batch loss: 0.7829903364181519 batch: 371/840\n",
      "Batch loss: 0.5819993615150452 batch: 372/840\n",
      "Batch loss: 0.7638490200042725 batch: 373/840\n",
      "Batch loss: 0.7978590130805969 batch: 374/840\n",
      "Batch loss: 0.5898246169090271 batch: 375/840\n",
      "Batch loss: 0.6014736890792847 batch: 376/840\n",
      "Batch loss: 0.8016733527183533 batch: 377/840\n",
      "Batch loss: 0.6976556181907654 batch: 378/840\n",
      "Batch loss: 0.6751377582550049 batch: 379/840\n",
      "Batch loss: 0.8054524064064026 batch: 380/840\n",
      "Batch loss: 0.9623992443084717 batch: 381/840\n",
      "Batch loss: 0.9835478067398071 batch: 382/840\n",
      "Batch loss: 0.8008708953857422 batch: 383/840\n",
      "Batch loss: 0.802745521068573 batch: 384/840\n",
      "Batch loss: 0.8789119720458984 batch: 385/840\n",
      "Batch loss: 0.8579327464103699 batch: 386/840\n",
      "Batch loss: 0.6696092486381531 batch: 387/840\n",
      "Batch loss: 0.8905937075614929 batch: 388/840\n",
      "Batch loss: 0.6100744009017944 batch: 389/840\n",
      "Batch loss: 0.9699981808662415 batch: 390/840\n",
      "Batch loss: 0.7573015689849854 batch: 391/840\n",
      "Batch loss: 0.6381871700286865 batch: 392/840\n",
      "Batch loss: 0.5888119339942932 batch: 393/840\n",
      "Batch loss: 0.8065388202667236 batch: 394/840\n",
      "Batch loss: 0.6580434441566467 batch: 395/840\n",
      "Batch loss: 0.8163819909095764 batch: 396/840\n",
      "Batch loss: 0.6996429562568665 batch: 397/840\n",
      "Batch loss: 0.8862993121147156 batch: 398/840\n",
      "Batch loss: 0.5646684169769287 batch: 399/840\n",
      "Batch loss: 0.7099617719650269 batch: 400/840\n",
      "Batch loss: 0.7644769549369812 batch: 401/840\n",
      "Batch loss: 0.7106757164001465 batch: 402/840\n",
      "Batch loss: 0.6661720275878906 batch: 403/840\n",
      "Batch loss: 0.7129896283149719 batch: 404/840\n",
      "Batch loss: 0.7167194485664368 batch: 405/840\n",
      "Batch loss: 0.7419559955596924 batch: 406/840\n",
      "Batch loss: 0.6000840663909912 batch: 407/840\n",
      "Batch loss: 0.9253272414207458 batch: 408/840\n",
      "Batch loss: 0.9274099469184875 batch: 409/840\n",
      "Batch loss: 0.8172112107276917 batch: 410/840\n",
      "Batch loss: 0.7899494767189026 batch: 411/840\n",
      "Batch loss: 0.8234570026397705 batch: 412/840\n",
      "Batch loss: 0.7303366661071777 batch: 413/840\n",
      "Batch loss: 0.7387024760246277 batch: 414/840\n",
      "Batch loss: 1.0476547479629517 batch: 415/840\n",
      "Batch loss: 0.5351693630218506 batch: 416/840\n",
      "Batch loss: 0.6464765667915344 batch: 417/840\n",
      "Batch loss: 0.8594509959220886 batch: 418/840\n",
      "Batch loss: 0.7679800391197205 batch: 419/840\n",
      "Batch loss: 0.7371411323547363 batch: 420/840\n",
      "Batch loss: 0.6351109743118286 batch: 421/840\n",
      "Batch loss: 0.6767467260360718 batch: 422/840\n",
      "Batch loss: 0.7796214818954468 batch: 423/840\n",
      "Batch loss: 0.7081206440925598 batch: 424/840\n",
      "Batch loss: 0.9018512964248657 batch: 425/840\n",
      "Batch loss: 0.8348789215087891 batch: 426/840\n",
      "Batch loss: 0.6847171187400818 batch: 427/840\n",
      "Batch loss: 0.8917285799980164 batch: 428/840\n",
      "Batch loss: 0.6321458220481873 batch: 429/840\n",
      "Batch loss: 0.9598494172096252 batch: 430/840\n",
      "Batch loss: 0.7032824158668518 batch: 431/840\n",
      "Batch loss: 0.7439090013504028 batch: 432/840\n",
      "Batch loss: 0.5937683582305908 batch: 433/840\n",
      "Batch loss: 0.6212596297264099 batch: 434/840\n",
      "Batch loss: 0.8266329169273376 batch: 435/840\n",
      "Batch loss: 0.7301924228668213 batch: 436/840\n",
      "Batch loss: 0.7026227712631226 batch: 437/840\n",
      "Batch loss: 0.5260910987854004 batch: 438/840\n",
      "Batch loss: 0.711230993270874 batch: 439/840\n",
      "Batch loss: 0.8737883567810059 batch: 440/840\n",
      "Batch loss: 0.7415962815284729 batch: 441/840\n",
      "Batch loss: 0.7912445068359375 batch: 442/840\n",
      "Batch loss: 0.6550309658050537 batch: 443/840\n",
      "Batch loss: 0.7800386548042297 batch: 444/840\n",
      "Batch loss: 0.674582839012146 batch: 445/840\n",
      "Batch loss: 0.8590657711029053 batch: 446/840\n",
      "Batch loss: 0.8225042223930359 batch: 447/840\n",
      "Batch loss: 0.7985542416572571 batch: 448/840\n",
      "Batch loss: 0.5870850086212158 batch: 449/840\n",
      "Batch loss: 0.7464544773101807 batch: 450/840\n",
      "Batch loss: 0.863778293132782 batch: 451/840\n",
      "Batch loss: 0.8805585503578186 batch: 452/840\n",
      "Batch loss: 0.6894951462745667 batch: 453/840\n",
      "Batch loss: 0.7189857959747314 batch: 454/840\n",
      "Batch loss: 0.7771183848381042 batch: 455/840\n",
      "Batch loss: 0.6516904234886169 batch: 456/840\n",
      "Batch loss: 0.7003863453865051 batch: 457/840\n",
      "Batch loss: 0.7130957245826721 batch: 458/840\n",
      "Batch loss: 0.6404416561126709 batch: 459/840\n",
      "Batch loss: 0.6304706931114197 batch: 460/840\n",
      "Batch loss: 0.8277229070663452 batch: 461/840\n",
      "Batch loss: 0.6793665885925293 batch: 462/840\n",
      "Batch loss: 0.853218138217926 batch: 463/840\n",
      "Batch loss: 0.6483994126319885 batch: 464/840\n",
      "Batch loss: 0.9993390440940857 batch: 465/840\n",
      "Batch loss: 0.6589207649230957 batch: 466/840\n",
      "Batch loss: 0.9731259942054749 batch: 467/840\n",
      "Batch loss: 0.6768772602081299 batch: 468/840\n",
      "Batch loss: 0.6731615662574768 batch: 469/840\n",
      "Batch loss: 0.778638482093811 batch: 470/840\n",
      "Batch loss: 0.8209729790687561 batch: 471/840\n",
      "Batch loss: 0.7281450629234314 batch: 472/840\n",
      "Batch loss: 0.840819239616394 batch: 473/840\n",
      "Batch loss: 0.684548020362854 batch: 474/840\n",
      "Batch loss: 0.7686811089515686 batch: 475/840\n",
      "Batch loss: 0.7592004537582397 batch: 476/840\n",
      "Batch loss: 0.6373613476753235 batch: 477/840\n",
      "Batch loss: 0.6850400567054749 batch: 478/840\n",
      "Batch loss: 0.5830897092819214 batch: 479/840\n",
      "Batch loss: 0.6926044225692749 batch: 480/840\n",
      "Batch loss: 0.7804431319236755 batch: 481/840\n",
      "Batch loss: 0.9289750456809998 batch: 482/840\n",
      "Batch loss: 0.7155503034591675 batch: 483/840\n",
      "Batch loss: 0.6276050806045532 batch: 484/840\n",
      "Batch loss: 0.7821799516677856 batch: 485/840\n",
      "Batch loss: 0.7658100724220276 batch: 486/840\n",
      "Batch loss: 0.5502262711524963 batch: 487/840\n",
      "Batch loss: 0.7421863079071045 batch: 488/840\n",
      "Batch loss: 0.687233030796051 batch: 489/840\n",
      "Batch loss: 0.7978622913360596 batch: 490/840\n",
      "Batch loss: 0.4579855799674988 batch: 491/840\n",
      "Batch loss: 0.7931911945343018 batch: 492/840\n",
      "Batch loss: 0.8103206753730774 batch: 493/840\n",
      "Batch loss: 0.4989774227142334 batch: 494/840\n",
      "Batch loss: 0.8121504783630371 batch: 495/840\n",
      "Batch loss: 0.6312475800514221 batch: 496/840\n",
      "Batch loss: 0.8116934895515442 batch: 497/840\n",
      "Batch loss: 0.6197177171707153 batch: 498/840\n",
      "Batch loss: 0.667772114276886 batch: 499/840\n",
      "Batch loss: 0.7086105346679688 batch: 500/840\n",
      "Batch loss: 0.593315601348877 batch: 501/840\n",
      "Batch loss: 0.761813759803772 batch: 502/840\n",
      "Batch loss: 0.5688883066177368 batch: 503/840\n",
      "Batch loss: 0.8101077079772949 batch: 504/840\n",
      "Batch loss: 0.9146877527236938 batch: 505/840\n",
      "Batch loss: 0.6906892657279968 batch: 506/840\n",
      "Batch loss: 0.9223622679710388 batch: 507/840\n",
      "Batch loss: 0.6861299276351929 batch: 508/840\n",
      "Batch loss: 0.8068592548370361 batch: 509/840\n",
      "Batch loss: 0.726612389087677 batch: 510/840\n",
      "Batch loss: 0.638775110244751 batch: 511/840\n",
      "Batch loss: 0.6498658061027527 batch: 512/840\n",
      "Batch loss: 0.6222566962242126 batch: 513/840\n",
      "Batch loss: 0.8294315934181213 batch: 514/840\n",
      "Batch loss: 0.5554300546646118 batch: 515/840\n",
      "Batch loss: 0.839521050453186 batch: 516/840\n",
      "Batch loss: 0.7247058749198914 batch: 517/840\n",
      "Batch loss: 0.7002192735671997 batch: 518/840\n",
      "Batch loss: 0.8602188229560852 batch: 519/840\n",
      "Batch loss: 0.7972637414932251 batch: 520/840\n",
      "Batch loss: 0.823851466178894 batch: 521/840\n",
      "Batch loss: 0.5886939167976379 batch: 522/840\n",
      "Batch loss: 0.477186918258667 batch: 523/840\n",
      "Batch loss: 0.6180340647697449 batch: 524/840\n",
      "Batch loss: 0.756319522857666 batch: 525/840\n",
      "Batch loss: 0.8393875956535339 batch: 526/840\n",
      "Batch loss: 0.7814403772354126 batch: 527/840\n",
      "Batch loss: 0.6719967722892761 batch: 528/840\n",
      "Batch loss: 0.5802749395370483 batch: 529/840\n",
      "Batch loss: 0.6889181733131409 batch: 530/840\n",
      "Batch loss: 0.5997861623764038 batch: 531/840\n",
      "Batch loss: 0.5248462557792664 batch: 532/840\n",
      "Batch loss: 0.782046377658844 batch: 533/840\n",
      "Batch loss: 0.67730313539505 batch: 534/840\n",
      "Batch loss: 0.6123578548431396 batch: 535/840\n",
      "Batch loss: 0.7296411991119385 batch: 536/840\n",
      "Batch loss: 0.6950559020042419 batch: 537/840\n",
      "Batch loss: 0.559359610080719 batch: 538/840\n",
      "Batch loss: 0.6815558671951294 batch: 539/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.9284355044364929 batch: 540/840\n",
      "Batch loss: 0.6596453189849854 batch: 541/840\n",
      "Batch loss: 0.7162241339683533 batch: 542/840\n",
      "Batch loss: 0.4929408133029938 batch: 543/840\n",
      "Batch loss: 0.7537846565246582 batch: 544/840\n",
      "Batch loss: 0.6873822212219238 batch: 545/840\n",
      "Batch loss: 0.6991969347000122 batch: 546/840\n",
      "Batch loss: 0.6705578565597534 batch: 547/840\n",
      "Batch loss: 0.5631430149078369 batch: 548/840\n",
      "Batch loss: 0.6281132102012634 batch: 549/840\n",
      "Batch loss: 0.5886301398277283 batch: 550/840\n",
      "Batch loss: 0.7284570932388306 batch: 551/840\n",
      "Batch loss: 0.5658173561096191 batch: 552/840\n",
      "Batch loss: 0.7600362300872803 batch: 553/840\n",
      "Batch loss: 0.7598035335540771 batch: 554/840\n",
      "Batch loss: 0.757946789264679 batch: 555/840\n",
      "Batch loss: 0.7470917701721191 batch: 556/840\n",
      "Batch loss: 0.7330151200294495 batch: 557/840\n",
      "Batch loss: 0.6981569528579712 batch: 558/840\n",
      "Batch loss: 0.7330963611602783 batch: 559/840\n",
      "Batch loss: 0.7011208534240723 batch: 560/840\n",
      "Batch loss: 0.7255317568778992 batch: 561/840\n",
      "Batch loss: 0.6361351609230042 batch: 562/840\n",
      "Batch loss: 0.5959472060203552 batch: 563/840\n",
      "Batch loss: 0.7681716084480286 batch: 564/840\n",
      "Batch loss: 0.7729013562202454 batch: 565/840\n",
      "Batch loss: 0.6923785209655762 batch: 566/840\n",
      "Batch loss: 0.7072561383247375 batch: 567/840\n",
      "Batch loss: 0.7201734185218811 batch: 568/840\n",
      "Batch loss: 0.7848525047302246 batch: 569/840\n",
      "Batch loss: 0.48861420154571533 batch: 570/840\n",
      "Batch loss: 0.7693206667900085 batch: 571/840\n",
      "Batch loss: 0.750326931476593 batch: 572/840\n",
      "Batch loss: 0.6381860971450806 batch: 573/840\n",
      "Batch loss: 0.8343979716300964 batch: 574/840\n",
      "Batch loss: 0.661516547203064 batch: 575/840\n",
      "Batch loss: 0.6679654717445374 batch: 576/840\n",
      "Batch loss: 0.5427640676498413 batch: 577/840\n",
      "Batch loss: 0.829922616481781 batch: 578/840\n",
      "Batch loss: 0.7183812856674194 batch: 579/840\n",
      "Batch loss: 0.8241249918937683 batch: 580/840\n",
      "Batch loss: 0.7535507678985596 batch: 581/840\n",
      "Batch loss: 0.8674861192703247 batch: 582/840\n",
      "Batch loss: 0.7260913252830505 batch: 583/840\n",
      "Batch loss: 0.7922950983047485 batch: 584/840\n",
      "Batch loss: 0.7772717475891113 batch: 585/840\n",
      "Batch loss: 0.6777333617210388 batch: 586/840\n",
      "Batch loss: 0.7266984581947327 batch: 587/840\n",
      "Batch loss: 0.6502945423126221 batch: 588/840\n",
      "Batch loss: 0.8332443237304688 batch: 589/840\n",
      "Batch loss: 0.6223947405815125 batch: 590/840\n",
      "Batch loss: 0.519939124584198 batch: 591/840\n",
      "Batch loss: 0.5768404603004456 batch: 592/840\n",
      "Batch loss: 0.7978602647781372 batch: 593/840\n",
      "Batch loss: 0.7698492407798767 batch: 594/840\n",
      "Batch loss: 0.5294814109802246 batch: 595/840\n",
      "Batch loss: 0.6418055891990662 batch: 596/840\n",
      "Batch loss: 0.6147196292877197 batch: 597/840\n",
      "Batch loss: 0.5311290621757507 batch: 598/840\n",
      "Batch loss: 0.6504144072532654 batch: 599/840\n",
      "Batch loss: 0.8397696018218994 batch: 600/840\n",
      "Batch loss: 0.7854253649711609 batch: 601/840\n",
      "Batch loss: 0.5710785388946533 batch: 602/840\n",
      "Batch loss: 0.722515881061554 batch: 603/840\n",
      "Batch loss: 0.7211179137229919 batch: 604/840\n",
      "Batch loss: 0.9216301441192627 batch: 605/840\n",
      "Batch loss: 0.9172370433807373 batch: 606/840\n",
      "Batch loss: 0.6550869941711426 batch: 607/840\n",
      "Batch loss: 0.7144277095794678 batch: 608/840\n",
      "Batch loss: 0.5174921154975891 batch: 609/840\n",
      "Batch loss: 0.7457237243652344 batch: 610/840\n",
      "Batch loss: 0.7449977993965149 batch: 611/840\n",
      "Batch loss: 0.7843619585037231 batch: 612/840\n",
      "Batch loss: 0.7403607964515686 batch: 613/840\n",
      "Batch loss: 0.6714370846748352 batch: 614/840\n",
      "Batch loss: 0.6681466102600098 batch: 615/840\n",
      "Batch loss: 0.6252648830413818 batch: 616/840\n",
      "Batch loss: 0.6509347558021545 batch: 617/840\n",
      "Batch loss: 0.64743971824646 batch: 618/840\n",
      "Batch loss: 0.8931577205657959 batch: 619/840\n",
      "Batch loss: 0.6656956672668457 batch: 620/840\n",
      "Batch loss: 0.6361619830131531 batch: 621/840\n",
      "Batch loss: 0.694710373878479 batch: 622/840\n",
      "Batch loss: 0.6911092400550842 batch: 623/840\n",
      "Batch loss: 0.9067542552947998 batch: 624/840\n",
      "Batch loss: 0.7145543694496155 batch: 625/840\n",
      "Batch loss: 0.6132739782333374 batch: 626/840\n",
      "Batch loss: 0.5757741332054138 batch: 627/840\n",
      "Batch loss: 0.6453720331192017 batch: 628/840\n",
      "Batch loss: 0.5866825580596924 batch: 629/840\n",
      "Batch loss: 0.665867030620575 batch: 630/840\n",
      "Batch loss: 0.8237708210945129 batch: 631/840\n",
      "Batch loss: 0.7086240649223328 batch: 632/840\n",
      "Batch loss: 0.7136219143867493 batch: 633/840\n",
      "Batch loss: 0.8075804114341736 batch: 634/840\n",
      "Batch loss: 0.719127893447876 batch: 635/840\n",
      "Batch loss: 0.6280066967010498 batch: 636/840\n",
      "Batch loss: 0.49677956104278564 batch: 637/840\n",
      "Batch loss: 0.7294143438339233 batch: 638/840\n",
      "Batch loss: 0.7164978981018066 batch: 639/840\n",
      "Batch loss: 0.696796715259552 batch: 640/840\n",
      "Batch loss: 0.8510734438896179 batch: 641/840\n",
      "Batch loss: 0.630959153175354 batch: 642/840\n",
      "Batch loss: 1.0776050090789795 batch: 643/840\n",
      "Batch loss: 0.6909457445144653 batch: 644/840\n",
      "Batch loss: 0.6833533644676208 batch: 645/840\n",
      "Batch loss: 0.6508137583732605 batch: 646/840\n",
      "Batch loss: 0.8548347353935242 batch: 647/840\n",
      "Batch loss: 0.7209470272064209 batch: 648/840\n",
      "Batch loss: 0.6951816082000732 batch: 649/840\n",
      "Batch loss: 0.6777332425117493 batch: 650/840\n",
      "Batch loss: 0.6248751878738403 batch: 651/840\n",
      "Batch loss: 0.7959551215171814 batch: 652/840\n",
      "Batch loss: 0.5687386393547058 batch: 653/840\n",
      "Batch loss: 0.9362084269523621 batch: 654/840\n",
      "Batch loss: 0.6316233277320862 batch: 655/840\n",
      "Batch loss: 0.8072594404220581 batch: 656/840\n",
      "Batch loss: 0.6669940948486328 batch: 657/840\n",
      "Batch loss: 0.8012976050376892 batch: 658/840\n",
      "Batch loss: 0.7411496043205261 batch: 659/840\n",
      "Batch loss: 0.6514655351638794 batch: 660/840\n",
      "Batch loss: 0.7828424572944641 batch: 661/840\n",
      "Batch loss: 0.7602586150169373 batch: 662/840\n",
      "Batch loss: 0.6395235657691956 batch: 663/840\n",
      "Batch loss: 0.7779421210289001 batch: 664/840\n",
      "Batch loss: 0.808180570602417 batch: 665/840\n",
      "Batch loss: 0.7102540731430054 batch: 666/840\n",
      "Batch loss: 0.7743659019470215 batch: 667/840\n",
      "Batch loss: 0.7035906314849854 batch: 668/840\n",
      "Batch loss: 0.5863332152366638 batch: 669/840\n",
      "Batch loss: 0.7667574286460876 batch: 670/840\n",
      "Batch loss: 0.7014737725257874 batch: 671/840\n",
      "Batch loss: 0.687055230140686 batch: 672/840\n",
      "Batch loss: 0.8978049755096436 batch: 673/840\n",
      "Batch loss: 1.0405408143997192 batch: 674/840\n",
      "Batch loss: 0.641416072845459 batch: 675/840\n",
      "Batch loss: 0.5318828821182251 batch: 676/840\n",
      "Batch loss: 0.6577731370925903 batch: 677/840\n",
      "Batch loss: 0.7063036561012268 batch: 678/840\n",
      "Batch loss: 0.9688512682914734 batch: 679/840\n",
      "Batch loss: 0.7732287049293518 batch: 680/840\n",
      "Batch loss: 0.6734228730201721 batch: 681/840\n",
      "Batch loss: 0.7301920056343079 batch: 682/840\n",
      "Batch loss: 0.6689362525939941 batch: 683/840\n",
      "Batch loss: 0.9422650337219238 batch: 684/840\n",
      "Batch loss: 0.7557915449142456 batch: 685/840\n",
      "Batch loss: 0.8198146820068359 batch: 686/840\n",
      "Batch loss: 0.73878014087677 batch: 687/840\n",
      "Batch loss: 0.721604585647583 batch: 688/840\n",
      "Batch loss: 0.6695974469184875 batch: 689/840\n",
      "Batch loss: 0.6030853390693665 batch: 690/840\n",
      "Batch loss: 0.7373685240745544 batch: 691/840\n",
      "Batch loss: 0.7168192267417908 batch: 692/840\n",
      "Batch loss: 0.637640118598938 batch: 693/840\n",
      "Batch loss: 0.8585317730903625 batch: 694/840\n",
      "Batch loss: 0.7786118984222412 batch: 695/840\n",
      "Batch loss: 0.6331905126571655 batch: 696/840\n",
      "Batch loss: 0.6118327379226685 batch: 697/840\n",
      "Batch loss: 0.6612290740013123 batch: 698/840\n",
      "Batch loss: 0.8492124080657959 batch: 699/840\n",
      "Batch loss: 0.7310619354248047 batch: 700/840\n",
      "Batch loss: 0.8446578979492188 batch: 701/840\n",
      "Batch loss: 0.5923353433609009 batch: 702/840\n",
      "Batch loss: 0.7820711731910706 batch: 703/840\n",
      "Batch loss: 0.7974397540092468 batch: 704/840\n",
      "Batch loss: 0.7234027981758118 batch: 705/840\n",
      "Batch loss: 0.8072585463523865 batch: 706/840\n",
      "Batch loss: 0.7493538856506348 batch: 707/840\n",
      "Batch loss: 0.8111811876296997 batch: 708/840\n",
      "Batch loss: 0.8123468160629272 batch: 709/840\n",
      "Batch loss: 0.8226314783096313 batch: 710/840\n",
      "Batch loss: 0.5668047070503235 batch: 711/840\n",
      "Batch loss: 0.8185155987739563 batch: 712/840\n",
      "Batch loss: 0.7445831894874573 batch: 713/840\n",
      "Batch loss: 0.6860826015472412 batch: 714/840\n",
      "Batch loss: 0.8126358985900879 batch: 715/840\n",
      "Batch loss: 0.6833783984184265 batch: 716/840\n",
      "Batch loss: 0.7494419813156128 batch: 717/840\n",
      "Batch loss: 0.6846067905426025 batch: 718/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4618542790412903 batch: 719/840\n",
      "Batch loss: 0.6074709892272949 batch: 720/840\n",
      "Batch loss: 0.7665508985519409 batch: 721/840\n",
      "Batch loss: 0.8022251129150391 batch: 722/840\n",
      "Batch loss: 0.6108312010765076 batch: 723/840\n",
      "Batch loss: 0.8637728691101074 batch: 724/840\n",
      "Batch loss: 0.6542158722877502 batch: 725/840\n",
      "Batch loss: 0.6007346510887146 batch: 726/840\n",
      "Batch loss: 0.9786267876625061 batch: 727/840\n",
      "Batch loss: 0.7840850353240967 batch: 728/840\n",
      "Batch loss: 0.7231331467628479 batch: 729/840\n",
      "Batch loss: 0.6894960999488831 batch: 730/840\n",
      "Batch loss: 0.5432916879653931 batch: 731/840\n",
      "Batch loss: 0.8383905291557312 batch: 732/840\n",
      "Batch loss: 0.6988280415534973 batch: 733/840\n",
      "Batch loss: 0.6126189231872559 batch: 734/840\n",
      "Batch loss: 0.6437366604804993 batch: 735/840\n",
      "Batch loss: 0.7362656593322754 batch: 736/840\n",
      "Batch loss: 0.6232479810714722 batch: 737/840\n",
      "Batch loss: 0.5787289142608643 batch: 738/840\n",
      "Batch loss: 0.8120728135108948 batch: 739/840\n",
      "Batch loss: 0.7618032693862915 batch: 740/840\n",
      "Batch loss: 0.6149795651435852 batch: 741/840\n",
      "Batch loss: 0.6201191544532776 batch: 742/840\n",
      "Batch loss: 0.4743601679801941 batch: 743/840\n",
      "Batch loss: 0.6006105542182922 batch: 744/840\n",
      "Batch loss: 0.6872966289520264 batch: 745/840\n",
      "Batch loss: 0.6298424005508423 batch: 746/840\n",
      "Batch loss: 0.7725096940994263 batch: 747/840\n",
      "Batch loss: 0.7065796852111816 batch: 748/840\n",
      "Batch loss: 0.6082316040992737 batch: 749/840\n",
      "Batch loss: 0.5610285997390747 batch: 750/840\n",
      "Batch loss: 0.7396194338798523 batch: 751/840\n",
      "Batch loss: 0.9280748963356018 batch: 752/840\n",
      "Batch loss: 0.6060839295387268 batch: 753/840\n",
      "Batch loss: 0.697370707988739 batch: 754/840\n",
      "Batch loss: 0.6604989767074585 batch: 755/840\n",
      "Batch loss: 0.6075887680053711 batch: 756/840\n",
      "Batch loss: 0.82044517993927 batch: 757/840\n",
      "Batch loss: 0.6253809928894043 batch: 758/840\n",
      "Batch loss: 0.5744667053222656 batch: 759/840\n",
      "Batch loss: 0.6119848489761353 batch: 760/840\n",
      "Batch loss: 0.6053149700164795 batch: 761/840\n",
      "Batch loss: 0.8653160333633423 batch: 762/840\n",
      "Batch loss: 0.4610530138015747 batch: 763/840\n",
      "Batch loss: 0.7331504821777344 batch: 764/840\n",
      "Batch loss: 0.5596319437026978 batch: 765/840\n",
      "Batch loss: 0.5695728063583374 batch: 766/840\n",
      "Batch loss: 0.7556436061859131 batch: 767/840\n",
      "Batch loss: 0.8188984394073486 batch: 768/840\n",
      "Batch loss: 0.7871633768081665 batch: 769/840\n",
      "Batch loss: 0.6331767439842224 batch: 770/840\n",
      "Batch loss: 0.6877277493476868 batch: 771/840\n",
      "Batch loss: 0.6012520790100098 batch: 772/840\n",
      "Batch loss: 0.5667913556098938 batch: 773/840\n",
      "Batch loss: 0.5376211404800415 batch: 774/840\n",
      "Batch loss: 0.5628949999809265 batch: 775/840\n",
      "Batch loss: 0.5561257004737854 batch: 776/840\n",
      "Batch loss: 0.6621267795562744 batch: 777/840\n",
      "Batch loss: 0.5186286568641663 batch: 778/840\n",
      "Batch loss: 0.7493164539337158 batch: 779/840\n",
      "Batch loss: 0.6406619548797607 batch: 780/840\n",
      "Batch loss: 0.6348430514335632 batch: 781/840\n",
      "Batch loss: 0.7348397970199585 batch: 782/840\n",
      "Batch loss: 0.5613973140716553 batch: 783/840\n",
      "Batch loss: 0.7605627179145813 batch: 784/840\n",
      "Batch loss: 0.582872748374939 batch: 785/840\n",
      "Batch loss: 0.6921576857566833 batch: 786/840\n",
      "Batch loss: 0.634382426738739 batch: 787/840\n",
      "Batch loss: 0.7584924101829529 batch: 788/840\n",
      "Batch loss: 0.8135921359062195 batch: 789/840\n",
      "Batch loss: 0.7582466006278992 batch: 790/840\n",
      "Batch loss: 0.6496978998184204 batch: 791/840\n",
      "Batch loss: 0.47757527232170105 batch: 792/840\n",
      "Batch loss: 0.6901763081550598 batch: 793/840\n",
      "Batch loss: 0.7512154579162598 batch: 794/840\n",
      "Batch loss: 0.6287511587142944 batch: 795/840\n",
      "Batch loss: 0.7911645770072937 batch: 796/840\n",
      "Batch loss: 0.6129522919654846 batch: 797/840\n",
      "Batch loss: 0.6238332986831665 batch: 798/840\n",
      "Batch loss: 0.7167971134185791 batch: 799/840\n",
      "Batch loss: 0.5591557621955872 batch: 800/840\n",
      "Batch loss: 0.7191274762153625 batch: 801/840\n",
      "Batch loss: 0.5413810014724731 batch: 802/840\n",
      "Batch loss: 0.5869043469429016 batch: 803/840\n",
      "Batch loss: 0.8639692664146423 batch: 804/840\n",
      "Batch loss: 0.6430721879005432 batch: 805/840\n",
      "Batch loss: 0.718207597732544 batch: 806/840\n",
      "Batch loss: 0.6268273591995239 batch: 807/840\n",
      "Batch loss: 0.5928024053573608 batch: 808/840\n",
      "Batch loss: 0.6750867962837219 batch: 809/840\n",
      "Batch loss: 0.5447700619697571 batch: 810/840\n",
      "Batch loss: 0.5911977887153625 batch: 811/840\n",
      "Batch loss: 0.6828923225402832 batch: 812/840\n",
      "Batch loss: 0.5606848001480103 batch: 813/840\n",
      "Batch loss: 0.6604775190353394 batch: 814/840\n",
      "Batch loss: 0.7471532225608826 batch: 815/840\n",
      "Batch loss: 0.7211652398109436 batch: 816/840\n",
      "Batch loss: 0.7206898331642151 batch: 817/840\n",
      "Batch loss: 0.6672260761260986 batch: 818/840\n",
      "Batch loss: 0.5931358337402344 batch: 819/840\n",
      "Batch loss: 0.7356935143470764 batch: 820/840\n",
      "Batch loss: 0.6522672176361084 batch: 821/840\n",
      "Batch loss: 0.6370822787284851 batch: 822/840\n",
      "Batch loss: 0.7792375683784485 batch: 823/840\n",
      "Batch loss: 0.8695340752601624 batch: 824/840\n",
      "Batch loss: 0.770775318145752 batch: 825/840\n",
      "Batch loss: 0.5831409096717834 batch: 826/840\n",
      "Batch loss: 0.6043570041656494 batch: 827/840\n",
      "Batch loss: 0.7234512567520142 batch: 828/840\n",
      "Batch loss: 0.6360976696014404 batch: 829/840\n",
      "Batch loss: 0.772331953048706 batch: 830/840\n",
      "Batch loss: 0.5887317061424255 batch: 831/840\n",
      "Batch loss: 0.6855537295341492 batch: 832/840\n",
      "Batch loss: 0.7427964210510254 batch: 833/840\n",
      "Batch loss: 0.6813523173332214 batch: 834/840\n",
      "Batch loss: 0.5802333950996399 batch: 835/840\n",
      "Batch loss: 0.6799648404121399 batch: 836/840\n",
      "Batch loss: 0.7583465576171875 batch: 837/840\n",
      "Batch loss: 0.8439517021179199 batch: 838/840\n",
      "Batch loss: 0.5850529074668884 batch: 839/840\n",
      "Batch loss: 0.6961898803710938 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 1/15..  Training Loss: 0.008..  Test Loss: 0.006..  Test Accuracy: 0.790\n",
      "Running epoch 2/15\n",
      "Batch loss: 0.6029226183891296 batch: 1/840\n",
      "Batch loss: 0.9192140698432922 batch: 2/840\n",
      "Batch loss: 0.7789579033851624 batch: 3/840\n",
      "Batch loss: 0.804303765296936 batch: 4/840\n",
      "Batch loss: 0.599369466304779 batch: 5/840\n",
      "Batch loss: 0.5705364346504211 batch: 6/840\n",
      "Batch loss: 0.6164393424987793 batch: 7/840\n",
      "Batch loss: 0.7701637148857117 batch: 8/840\n",
      "Batch loss: 0.7463527917861938 batch: 9/840\n",
      "Batch loss: 0.7714696526527405 batch: 10/840\n",
      "Batch loss: 0.6816656589508057 batch: 11/840\n",
      "Batch loss: 0.7669410109519958 batch: 12/840\n",
      "Batch loss: 0.5824150443077087 batch: 13/840\n",
      "Batch loss: 0.6656674146652222 batch: 14/840\n",
      "Batch loss: 0.6997206807136536 batch: 15/840\n",
      "Batch loss: 0.6327930688858032 batch: 16/840\n",
      "Batch loss: 0.6221206784248352 batch: 17/840\n",
      "Batch loss: 0.7667720317840576 batch: 18/840\n",
      "Batch loss: 0.7559778094291687 batch: 19/840\n",
      "Batch loss: 0.8402844071388245 batch: 20/840\n",
      "Batch loss: 0.7649118304252625 batch: 21/840\n",
      "Batch loss: 0.6300752758979797 batch: 22/840\n",
      "Batch loss: 0.7544831037521362 batch: 23/840\n",
      "Batch loss: 0.5960456132888794 batch: 24/840\n",
      "Batch loss: 0.7264237999916077 batch: 25/840\n",
      "Batch loss: 0.7244921922683716 batch: 26/840\n",
      "Batch loss: 0.7266708612442017 batch: 27/840\n",
      "Batch loss: 0.7363619208335876 batch: 28/840\n",
      "Batch loss: 0.7444431185722351 batch: 29/840\n",
      "Batch loss: 0.6311648488044739 batch: 30/840\n",
      "Batch loss: 0.6457972526550293 batch: 31/840\n",
      "Batch loss: 0.5797836184501648 batch: 32/840\n",
      "Batch loss: 0.818697988986969 batch: 33/840\n",
      "Batch loss: 0.6421350240707397 batch: 34/840\n",
      "Batch loss: 0.7099639177322388 batch: 35/840\n",
      "Batch loss: 0.6022860407829285 batch: 36/840\n",
      "Batch loss: 0.7591323852539062 batch: 37/840\n",
      "Batch loss: 0.7299278378486633 batch: 38/840\n",
      "Batch loss: 0.8624811768531799 batch: 39/840\n",
      "Batch loss: 0.6424375176429749 batch: 40/840\n",
      "Batch loss: 0.809791624546051 batch: 41/840\n",
      "Batch loss: 0.7507405281066895 batch: 42/840\n",
      "Batch loss: 0.6741464734077454 batch: 43/840\n",
      "Batch loss: 0.8090240359306335 batch: 44/840\n",
      "Batch loss: 0.8429519534111023 batch: 45/840\n",
      "Batch loss: 0.5996167659759521 batch: 46/840\n",
      "Batch loss: 0.5588281154632568 batch: 47/840\n",
      "Batch loss: 0.7374796271324158 batch: 48/840\n",
      "Batch loss: 0.6612227559089661 batch: 49/840\n",
      "Batch loss: 0.6631848216056824 batch: 50/840\n",
      "Batch loss: 0.7401119470596313 batch: 51/840\n",
      "Batch loss: 0.877453625202179 batch: 52/840\n",
      "Batch loss: 0.7513034343719482 batch: 53/840\n",
      "Batch loss: 0.8251457214355469 batch: 54/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7629560232162476 batch: 55/840\n",
      "Batch loss: 0.7648380994796753 batch: 56/840\n",
      "Batch loss: 0.6573198437690735 batch: 57/840\n",
      "Batch loss: 0.6300498843193054 batch: 58/840\n",
      "Batch loss: 0.7975641489028931 batch: 59/840\n",
      "Batch loss: 0.6121431589126587 batch: 60/840\n",
      "Batch loss: 0.9078845977783203 batch: 61/840\n",
      "Batch loss: 0.8052216172218323 batch: 62/840\n",
      "Batch loss: 0.7341633439064026 batch: 63/840\n",
      "Batch loss: 0.8013801574707031 batch: 64/840\n",
      "Batch loss: 0.6917120218276978 batch: 65/840\n",
      "Batch loss: 0.7406668066978455 batch: 66/840\n",
      "Batch loss: 0.8201132416725159 batch: 67/840\n",
      "Batch loss: 0.6787261962890625 batch: 68/840\n",
      "Batch loss: 0.7875643968582153 batch: 69/840\n",
      "Batch loss: 0.7249207496643066 batch: 70/840\n",
      "Batch loss: 0.8109495639801025 batch: 71/840\n",
      "Batch loss: 0.7322428822517395 batch: 72/840\n",
      "Batch loss: 0.8322797417640686 batch: 73/840\n",
      "Batch loss: 0.7041040658950806 batch: 74/840\n",
      "Batch loss: 0.7945618629455566 batch: 75/840\n",
      "Batch loss: 0.4904438853263855 batch: 76/840\n",
      "Batch loss: 0.7310658097267151 batch: 77/840\n",
      "Batch loss: 0.830313503742218 batch: 78/840\n",
      "Batch loss: 0.6753180027008057 batch: 79/840\n",
      "Batch loss: 0.8005918264389038 batch: 80/840\n",
      "Batch loss: 0.7195922136306763 batch: 81/840\n",
      "Batch loss: 0.7205642461776733 batch: 82/840\n",
      "Batch loss: 0.580193281173706 batch: 83/840\n",
      "Batch loss: 0.8434167504310608 batch: 84/840\n",
      "Batch loss: 0.6783515810966492 batch: 85/840\n",
      "Batch loss: 1.0394231081008911 batch: 86/840\n",
      "Batch loss: 0.6402897834777832 batch: 87/840\n",
      "Batch loss: 0.5573580265045166 batch: 88/840\n",
      "Batch loss: 0.6441317200660706 batch: 89/840\n",
      "Batch loss: 0.5881546139717102 batch: 90/840\n",
      "Batch loss: 0.6825526356697083 batch: 91/840\n",
      "Batch loss: 0.6856787800788879 batch: 92/840\n",
      "Batch loss: 0.7719298005104065 batch: 93/840\n",
      "Batch loss: 0.6092115044593811 batch: 94/840\n",
      "Batch loss: 0.7052109241485596 batch: 95/840\n",
      "Batch loss: 0.7824404835700989 batch: 96/840\n",
      "Batch loss: 0.7564022541046143 batch: 97/840\n",
      "Batch loss: 0.9358994960784912 batch: 98/840\n",
      "Batch loss: 0.6666938662528992 batch: 99/840\n",
      "Batch loss: 0.8300975561141968 batch: 100/840\n",
      "Batch loss: 0.6213992834091187 batch: 101/840\n",
      "Batch loss: 0.5890413522720337 batch: 102/840\n",
      "Batch loss: 0.7354185581207275 batch: 103/840\n",
      "Batch loss: 0.5489211082458496 batch: 104/840\n",
      "Batch loss: 0.6681442260742188 batch: 105/840\n",
      "Batch loss: 0.7022404670715332 batch: 106/840\n",
      "Batch loss: 0.6490452289581299 batch: 107/840\n",
      "Batch loss: 0.8160800933837891 batch: 108/840\n",
      "Batch loss: 0.7219924926757812 batch: 109/840\n",
      "Batch loss: 0.6270152926445007 batch: 110/840\n",
      "Batch loss: 0.657133936882019 batch: 111/840\n",
      "Batch loss: 0.8271347880363464 batch: 112/840\n",
      "Batch loss: 0.8789941668510437 batch: 113/840\n",
      "Batch loss: 0.763576328754425 batch: 114/840\n",
      "Batch loss: 0.6846645474433899 batch: 115/840\n",
      "Batch loss: 0.5830299854278564 batch: 116/840\n",
      "Batch loss: 0.7453193068504333 batch: 117/840\n",
      "Batch loss: 0.5787047743797302 batch: 118/840\n",
      "Batch loss: 0.7903535962104797 batch: 119/840\n",
      "Batch loss: 0.6778914928436279 batch: 120/840\n",
      "Batch loss: 0.838039755821228 batch: 121/840\n",
      "Batch loss: 0.8939587473869324 batch: 122/840\n",
      "Batch loss: 0.5641469955444336 batch: 123/840\n",
      "Batch loss: 0.6405159831047058 batch: 124/840\n",
      "Batch loss: 0.651262104511261 batch: 125/840\n",
      "Batch loss: 0.7719004154205322 batch: 126/840\n",
      "Batch loss: 0.8706905841827393 batch: 127/840\n",
      "Batch loss: 0.798916757106781 batch: 128/840\n",
      "Batch loss: 0.8064119815826416 batch: 129/840\n",
      "Batch loss: 0.6094524264335632 batch: 130/840\n",
      "Batch loss: 0.8187436461448669 batch: 131/840\n",
      "Batch loss: 0.9905645847320557 batch: 132/840\n",
      "Batch loss: 0.7358172535896301 batch: 133/840\n",
      "Batch loss: 0.6356213092803955 batch: 134/840\n",
      "Batch loss: 0.6180508136749268 batch: 135/840\n",
      "Batch loss: 0.8259282112121582 batch: 136/840\n",
      "Batch loss: 0.7212246656417847 batch: 137/840\n",
      "Batch loss: 0.5429712533950806 batch: 138/840\n",
      "Batch loss: 0.6425367593765259 batch: 139/840\n",
      "Batch loss: 0.7118792533874512 batch: 140/840\n",
      "Batch loss: 0.5204927921295166 batch: 141/840\n",
      "Batch loss: 0.7049000263214111 batch: 142/840\n",
      "Batch loss: 0.673240065574646 batch: 143/840\n",
      "Batch loss: 0.7121874094009399 batch: 144/840\n",
      "Batch loss: 0.7931289672851562 batch: 145/840\n",
      "Batch loss: 0.9279465675354004 batch: 146/840\n",
      "Batch loss: 0.6206538081169128 batch: 147/840\n",
      "Batch loss: 0.8887471556663513 batch: 148/840\n",
      "Batch loss: 0.8246793150901794 batch: 149/840\n",
      "Batch loss: 0.8405137658119202 batch: 150/840\n",
      "Batch loss: 0.8087702393531799 batch: 151/840\n",
      "Batch loss: 0.6374130845069885 batch: 152/840\n",
      "Batch loss: 0.5996913313865662 batch: 153/840\n",
      "Batch loss: 0.7812432050704956 batch: 154/840\n",
      "Batch loss: 0.5832639932632446 batch: 155/840\n",
      "Batch loss: 0.7412902116775513 batch: 156/840\n",
      "Batch loss: 0.7963218688964844 batch: 157/840\n",
      "Batch loss: 0.580900251865387 batch: 158/840\n",
      "Batch loss: 0.5919039249420166 batch: 159/840\n",
      "Batch loss: 0.6645504236221313 batch: 160/840\n",
      "Batch loss: 0.7380496859550476 batch: 161/840\n",
      "Batch loss: 0.757265567779541 batch: 162/840\n",
      "Batch loss: 0.807160496711731 batch: 163/840\n",
      "Batch loss: 0.5377044081687927 batch: 164/840\n",
      "Batch loss: 0.8108199238777161 batch: 165/840\n",
      "Batch loss: 0.5470985174179077 batch: 166/840\n",
      "Batch loss: 0.859331488609314 batch: 167/840\n",
      "Batch loss: 0.6515962481498718 batch: 168/840\n",
      "Batch loss: 0.6391414403915405 batch: 169/840\n",
      "Batch loss: 0.8413419127464294 batch: 170/840\n",
      "Batch loss: 0.7821393013000488 batch: 171/840\n",
      "Batch loss: 0.8729594349861145 batch: 172/840\n",
      "Batch loss: 0.6686167120933533 batch: 173/840\n",
      "Batch loss: 0.6517744660377502 batch: 174/840\n",
      "Batch loss: 0.6491020917892456 batch: 175/840\n",
      "Batch loss: 0.7444759607315063 batch: 176/840\n",
      "Batch loss: 0.7562717199325562 batch: 177/840\n",
      "Batch loss: 0.6965910196304321 batch: 178/840\n",
      "Batch loss: 0.8233889937400818 batch: 179/840\n",
      "Batch loss: 0.6363637447357178 batch: 180/840\n",
      "Batch loss: 0.6734601855278015 batch: 181/840\n",
      "Batch loss: 0.6929802894592285 batch: 182/840\n",
      "Batch loss: 0.7423838973045349 batch: 183/840\n",
      "Batch loss: 0.6549036502838135 batch: 184/840\n",
      "Batch loss: 0.43263113498687744 batch: 185/840\n",
      "Batch loss: 0.5752149820327759 batch: 186/840\n",
      "Batch loss: 0.6807397603988647 batch: 187/840\n",
      "Batch loss: 0.6762242317199707 batch: 188/840\n",
      "Batch loss: 0.7153347730636597 batch: 189/840\n",
      "Batch loss: 0.7935307025909424 batch: 190/840\n",
      "Batch loss: 0.9103025794029236 batch: 191/840\n",
      "Batch loss: 0.5822007656097412 batch: 192/840\n",
      "Batch loss: 0.6245846748352051 batch: 193/840\n",
      "Batch loss: 0.5361424088478088 batch: 194/840\n",
      "Batch loss: 0.6748999953269958 batch: 195/840\n",
      "Batch loss: 0.9769794940948486 batch: 196/840\n",
      "Batch loss: 0.7660973072052002 batch: 197/840\n",
      "Batch loss: 0.5627879500389099 batch: 198/840\n",
      "Batch loss: 0.6892405152320862 batch: 199/840\n",
      "Batch loss: 0.8509958386421204 batch: 200/840\n",
      "Batch loss: 0.6453380584716797 batch: 201/840\n",
      "Batch loss: 0.7079281806945801 batch: 202/840\n",
      "Batch loss: 0.6765396595001221 batch: 203/840\n",
      "Batch loss: 0.8591858148574829 batch: 204/840\n",
      "Batch loss: 0.8065179586410522 batch: 205/840\n",
      "Batch loss: 0.6959232091903687 batch: 206/840\n",
      "Batch loss: 0.7453796863555908 batch: 207/840\n",
      "Batch loss: 0.7266271114349365 batch: 208/840\n",
      "Batch loss: 0.6658352017402649 batch: 209/840\n",
      "Batch loss: 0.6143521666526794 batch: 210/840\n",
      "Batch loss: 0.6245222687721252 batch: 211/840\n",
      "Batch loss: 0.7604873180389404 batch: 212/840\n",
      "Batch loss: 0.7345529794692993 batch: 213/840\n",
      "Batch loss: 0.8508728742599487 batch: 214/840\n",
      "Batch loss: 0.6949092745780945 batch: 215/840\n",
      "Batch loss: 0.7405847311019897 batch: 216/840\n",
      "Batch loss: 0.6803479194641113 batch: 217/840\n",
      "Batch loss: 0.7164949774742126 batch: 218/840\n",
      "Batch loss: 0.7295795679092407 batch: 219/840\n",
      "Batch loss: 1.065812587738037 batch: 220/840\n",
      "Batch loss: 0.7256700992584229 batch: 221/840\n",
      "Batch loss: 0.8232293725013733 batch: 222/840\n",
      "Batch loss: 0.6850855946540833 batch: 223/840\n",
      "Batch loss: 0.7672192454338074 batch: 224/840\n",
      "Batch loss: 0.7816148996353149 batch: 225/840\n",
      "Batch loss: 0.7311661243438721 batch: 226/840\n",
      "Batch loss: 0.7864633798599243 batch: 227/840\n",
      "Batch loss: 0.6147741079330444 batch: 228/840\n",
      "Batch loss: 0.6168493032455444 batch: 229/840\n",
      "Batch loss: 0.6906480193138123 batch: 230/840\n",
      "Batch loss: 0.7477226853370667 batch: 231/840\n",
      "Batch loss: 0.6750122308731079 batch: 232/840\n",
      "Batch loss: 0.7713662981987 batch: 233/840\n",
      "Batch loss: 0.6300598382949829 batch: 234/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7729421257972717 batch: 235/840\n",
      "Batch loss: 0.7577117681503296 batch: 236/840\n",
      "Batch loss: 0.5641129612922668 batch: 237/840\n",
      "Batch loss: 0.7865288257598877 batch: 238/840\n",
      "Batch loss: 0.7301586866378784 batch: 239/840\n",
      "Batch loss: 0.7609365582466125 batch: 240/840\n",
      "Batch loss: 0.8113994598388672 batch: 241/840\n",
      "Batch loss: 0.6313235759735107 batch: 242/840\n",
      "Batch loss: 0.5920937657356262 batch: 243/840\n",
      "Batch loss: 0.785841703414917 batch: 244/840\n",
      "Batch loss: 0.5799567699432373 batch: 245/840\n",
      "Batch loss: 0.7028455138206482 batch: 246/840\n",
      "Batch loss: 0.6982203722000122 batch: 247/840\n",
      "Batch loss: 0.7640137672424316 batch: 248/840\n",
      "Batch loss: 0.9223235249519348 batch: 249/840\n",
      "Batch loss: 0.6225365996360779 batch: 250/840\n",
      "Batch loss: 0.694206714630127 batch: 251/840\n",
      "Batch loss: 0.6365183591842651 batch: 252/840\n",
      "Batch loss: 0.7373077273368835 batch: 253/840\n",
      "Batch loss: 0.9442362785339355 batch: 254/840\n",
      "Batch loss: 0.7323944568634033 batch: 255/840\n",
      "Batch loss: 0.7443397641181946 batch: 256/840\n",
      "Batch loss: 0.744129478931427 batch: 257/840\n",
      "Batch loss: 0.7245039939880371 batch: 258/840\n",
      "Batch loss: 0.6682178378105164 batch: 259/840\n",
      "Batch loss: 0.5716337561607361 batch: 260/840\n",
      "Batch loss: 0.6033448576927185 batch: 261/840\n",
      "Batch loss: 0.519263744354248 batch: 262/840\n",
      "Batch loss: 0.674630880355835 batch: 263/840\n",
      "Batch loss: 0.728682279586792 batch: 264/840\n",
      "Batch loss: 0.7776702642440796 batch: 265/840\n",
      "Batch loss: 0.6382128000259399 batch: 266/840\n",
      "Batch loss: 0.6667643189430237 batch: 267/840\n",
      "Batch loss: 0.6626213192939758 batch: 268/840\n",
      "Batch loss: 0.5740553140640259 batch: 269/840\n",
      "Batch loss: 0.6218188405036926 batch: 270/840\n",
      "Batch loss: 0.6278319358825684 batch: 271/840\n",
      "Batch loss: 0.8201756477355957 batch: 272/840\n",
      "Batch loss: 0.8617464303970337 batch: 273/840\n",
      "Batch loss: 0.7268837690353394 batch: 274/840\n",
      "Batch loss: 0.7644572257995605 batch: 275/840\n",
      "Batch loss: 0.5813027024269104 batch: 276/840\n",
      "Batch loss: 0.6651880145072937 batch: 277/840\n",
      "Batch loss: 0.8115487098693848 batch: 278/840\n",
      "Batch loss: 0.8564385771751404 batch: 279/840\n",
      "Batch loss: 0.8319611549377441 batch: 280/840\n",
      "Batch loss: 0.676337480545044 batch: 281/840\n",
      "Batch loss: 0.6052840948104858 batch: 282/840\n",
      "Batch loss: 0.752261757850647 batch: 283/840\n",
      "Batch loss: 0.5331385135650635 batch: 284/840\n",
      "Batch loss: 0.6384496092796326 batch: 285/840\n",
      "Batch loss: 0.7907183170318604 batch: 286/840\n",
      "Batch loss: 0.5074187517166138 batch: 287/840\n",
      "Batch loss: 0.6603693962097168 batch: 288/840\n",
      "Batch loss: 0.9948161244392395 batch: 289/840\n",
      "Batch loss: 0.8213522434234619 batch: 290/840\n",
      "Batch loss: 0.7873676419258118 batch: 291/840\n",
      "Batch loss: 0.6730175614356995 batch: 292/840\n",
      "Batch loss: 0.9069156050682068 batch: 293/840\n",
      "Batch loss: 0.6687906384468079 batch: 294/840\n",
      "Batch loss: 0.6102657914161682 batch: 295/840\n",
      "Batch loss: 0.7852917313575745 batch: 296/840\n",
      "Batch loss: 0.7760459780693054 batch: 297/840\n",
      "Batch loss: 0.7043584585189819 batch: 298/840\n",
      "Batch loss: 0.6896561980247498 batch: 299/840\n",
      "Batch loss: 0.7736295461654663 batch: 300/840\n",
      "Batch loss: 0.7498190402984619 batch: 301/840\n",
      "Batch loss: 0.7053563594818115 batch: 302/840\n",
      "Batch loss: 0.8338521718978882 batch: 303/840\n",
      "Batch loss: 0.5910747647285461 batch: 304/840\n",
      "Batch loss: 0.6295678019523621 batch: 305/840\n",
      "Batch loss: 0.7029553055763245 batch: 306/840\n",
      "Batch loss: 0.5665872097015381 batch: 307/840\n",
      "Batch loss: 0.705403208732605 batch: 308/840\n",
      "Batch loss: 0.7400661706924438 batch: 309/840\n",
      "Batch loss: 0.9345749616622925 batch: 310/840\n",
      "Batch loss: 0.7216198444366455 batch: 311/840\n",
      "Batch loss: 0.7472123503684998 batch: 312/840\n",
      "Batch loss: 0.8462965488433838 batch: 313/840\n",
      "Batch loss: 0.6738066673278809 batch: 314/840\n",
      "Batch loss: 0.6638588905334473 batch: 315/840\n",
      "Batch loss: 0.556470513343811 batch: 316/840\n",
      "Batch loss: 0.7716571092605591 batch: 317/840\n",
      "Batch loss: 0.7675206065177917 batch: 318/840\n",
      "Batch loss: 0.7552623748779297 batch: 319/840\n",
      "Batch loss: 0.6480640172958374 batch: 320/840\n",
      "Batch loss: 0.6604530215263367 batch: 321/840\n",
      "Batch loss: 0.831468939781189 batch: 322/840\n",
      "Batch loss: 0.7876414656639099 batch: 323/840\n",
      "Batch loss: 0.8131046295166016 batch: 324/840\n",
      "Batch loss: 0.541635274887085 batch: 325/840\n",
      "Batch loss: 0.6895543932914734 batch: 326/840\n",
      "Batch loss: 0.5582233667373657 batch: 327/840\n",
      "Batch loss: 0.8961377143859863 batch: 328/840\n",
      "Batch loss: 0.7389905452728271 batch: 329/840\n",
      "Batch loss: 0.7811323404312134 batch: 330/840\n",
      "Batch loss: 0.750231146812439 batch: 331/840\n",
      "Batch loss: 0.7296022772789001 batch: 332/840\n",
      "Batch loss: 0.6579037308692932 batch: 333/840\n",
      "Batch loss: 0.6732548475265503 batch: 334/840\n",
      "Batch loss: 0.5951087474822998 batch: 335/840\n",
      "Batch loss: 0.8601725697517395 batch: 336/840\n",
      "Batch loss: 0.9185715317726135 batch: 337/840\n",
      "Batch loss: 0.7805890440940857 batch: 338/840\n",
      "Batch loss: 0.7165205478668213 batch: 339/840\n",
      "Batch loss: 0.9173948168754578 batch: 340/840\n",
      "Batch loss: 0.6037760376930237 batch: 341/840\n",
      "Batch loss: 0.6064789295196533 batch: 342/840\n",
      "Batch loss: 0.9014034867286682 batch: 343/840\n",
      "Batch loss: 0.7242260575294495 batch: 344/840\n",
      "Batch loss: 0.5219289660453796 batch: 345/840\n",
      "Batch loss: 0.6502332091331482 batch: 346/840\n",
      "Batch loss: 0.8185712695121765 batch: 347/840\n",
      "Batch loss: 0.7344782948493958 batch: 348/840\n",
      "Batch loss: 0.7459110021591187 batch: 349/840\n",
      "Batch loss: 0.611625075340271 batch: 350/840\n",
      "Batch loss: 0.7121859788894653 batch: 351/840\n",
      "Batch loss: 0.7245633602142334 batch: 352/840\n",
      "Batch loss: 0.787109911441803 batch: 353/840\n",
      "Batch loss: 0.6635225415229797 batch: 354/840\n",
      "Batch loss: 0.6364280581474304 batch: 355/840\n",
      "Batch loss: 0.7109991312026978 batch: 356/840\n",
      "Batch loss: 0.6298506259918213 batch: 357/840\n",
      "Batch loss: 0.7829416394233704 batch: 358/840\n",
      "Batch loss: 0.7169057726860046 batch: 359/840\n",
      "Batch loss: 0.9260469675064087 batch: 360/840\n",
      "Batch loss: 0.6842662692070007 batch: 361/840\n",
      "Batch loss: 0.6509982347488403 batch: 362/840\n",
      "Batch loss: 0.6454347372055054 batch: 363/840\n",
      "Batch loss: 0.7240166664123535 batch: 364/840\n",
      "Batch loss: 0.5729831457138062 batch: 365/840\n",
      "Batch loss: 0.6742371320724487 batch: 366/840\n",
      "Batch loss: 0.5696076154708862 batch: 367/840\n",
      "Batch loss: 0.8355214595794678 batch: 368/840\n",
      "Batch loss: 0.731550931930542 batch: 369/840\n",
      "Batch loss: 0.849934995174408 batch: 370/840\n",
      "Batch loss: 0.868886411190033 batch: 371/840\n",
      "Batch loss: 0.5202680230140686 batch: 372/840\n",
      "Batch loss: 0.8417753577232361 batch: 373/840\n",
      "Batch loss: 0.7975879907608032 batch: 374/840\n",
      "Batch loss: 0.5797039270401001 batch: 375/840\n",
      "Batch loss: 0.5599792003631592 batch: 376/840\n",
      "Batch loss: 0.7951077222824097 batch: 377/840\n",
      "Batch loss: 0.7017595767974854 batch: 378/840\n",
      "Batch loss: 0.5545739531517029 batch: 379/840\n",
      "Batch loss: 0.8953619599342346 batch: 380/840\n",
      "Batch loss: 0.9078771471977234 batch: 381/840\n",
      "Batch loss: 0.7835339903831482 batch: 382/840\n",
      "Batch loss: 0.725562334060669 batch: 383/840\n",
      "Batch loss: 0.6476001143455505 batch: 384/840\n",
      "Batch loss: 0.7560939788818359 batch: 385/840\n",
      "Batch loss: 0.7022886872291565 batch: 386/840\n",
      "Batch loss: 0.5722173452377319 batch: 387/840\n",
      "Batch loss: 0.7587332129478455 batch: 388/840\n",
      "Batch loss: 0.5638351440429688 batch: 389/840\n",
      "Batch loss: 0.8709649443626404 batch: 390/840\n",
      "Batch loss: 0.7538694143295288 batch: 391/840\n",
      "Batch loss: 0.5313372611999512 batch: 392/840\n",
      "Batch loss: 0.4656153917312622 batch: 393/840\n",
      "Batch loss: 0.8651426434516907 batch: 394/840\n",
      "Batch loss: 0.6792442798614502 batch: 395/840\n",
      "Batch loss: 0.7301221489906311 batch: 396/840\n",
      "Batch loss: 0.6166637539863586 batch: 397/840\n",
      "Batch loss: 0.78658127784729 batch: 398/840\n",
      "Batch loss: 0.54317307472229 batch: 399/840\n",
      "Batch loss: 0.6586026549339294 batch: 400/840\n",
      "Batch loss: 0.7143203020095825 batch: 401/840\n",
      "Batch loss: 0.6930955648422241 batch: 402/840\n",
      "Batch loss: 0.6663808226585388 batch: 403/840\n",
      "Batch loss: 0.8073700070381165 batch: 404/840\n",
      "Batch loss: 0.6213076114654541 batch: 405/840\n",
      "Batch loss: 0.6912751197814941 batch: 406/840\n",
      "Batch loss: 0.743553102016449 batch: 407/840\n",
      "Batch loss: 0.7400141954421997 batch: 408/840\n",
      "Batch loss: 0.7533331513404846 batch: 409/840\n",
      "Batch loss: 0.9567080736160278 batch: 410/840\n",
      "Batch loss: 0.8244326114654541 batch: 411/840\n",
      "Batch loss: 0.8815210461616516 batch: 412/840\n",
      "Batch loss: 0.7173139452934265 batch: 413/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.702406644821167 batch: 414/840\n",
      "Batch loss: 0.9646587371826172 batch: 415/840\n",
      "Batch loss: 0.5132680535316467 batch: 416/840\n",
      "Batch loss: 0.7814856767654419 batch: 417/840\n",
      "Batch loss: 0.8084311485290527 batch: 418/840\n",
      "Batch loss: 0.8190696239471436 batch: 419/840\n",
      "Batch loss: 0.710356593132019 batch: 420/840\n",
      "Batch loss: 0.6220932602882385 batch: 421/840\n",
      "Batch loss: 0.6323842406272888 batch: 422/840\n",
      "Batch loss: 0.8008817434310913 batch: 423/840\n",
      "Batch loss: 0.7588991522789001 batch: 424/840\n",
      "Batch loss: 0.7628356218338013 batch: 425/840\n",
      "Batch loss: 0.7265452742576599 batch: 426/840\n",
      "Batch loss: 0.717620313167572 batch: 427/840\n",
      "Batch loss: 0.8238585591316223 batch: 428/840\n",
      "Batch loss: 0.6396276354789734 batch: 429/840\n",
      "Batch loss: 0.9030716419219971 batch: 430/840\n",
      "Batch loss: 0.6654850244522095 batch: 431/840\n",
      "Batch loss: 0.7328319549560547 batch: 432/840\n",
      "Batch loss: 0.6005955338478088 batch: 433/840\n",
      "Batch loss: 0.6661014556884766 batch: 434/840\n",
      "Batch loss: 0.8167585134506226 batch: 435/840\n",
      "Batch loss: 0.5977407097816467 batch: 436/840\n",
      "Batch loss: 0.7420798540115356 batch: 437/840\n",
      "Batch loss: 0.4375597834587097 batch: 438/840\n",
      "Batch loss: 0.7357406616210938 batch: 439/840\n",
      "Batch loss: 0.8167614936828613 batch: 440/840\n",
      "Batch loss: 0.6695716977119446 batch: 441/840\n",
      "Batch loss: 0.7343645691871643 batch: 442/840\n",
      "Batch loss: 0.7100843191146851 batch: 443/840\n",
      "Batch loss: 0.8099563121795654 batch: 444/840\n",
      "Batch loss: 0.6240289211273193 batch: 445/840\n",
      "Batch loss: 0.7636380791664124 batch: 446/840\n",
      "Batch loss: 0.7400844097137451 batch: 447/840\n",
      "Batch loss: 0.742420494556427 batch: 448/840\n",
      "Batch loss: 0.6646629571914673 batch: 449/840\n",
      "Batch loss: 0.6889075636863708 batch: 450/840\n",
      "Batch loss: 0.7646487951278687 batch: 451/840\n",
      "Batch loss: 0.724306046962738 batch: 452/840\n",
      "Batch loss: 0.6956504583358765 batch: 453/840\n",
      "Batch loss: 0.5825599431991577 batch: 454/840\n",
      "Batch loss: 0.6960315108299255 batch: 455/840\n",
      "Batch loss: 0.6669864654541016 batch: 456/840\n",
      "Batch loss: 0.6236057877540588 batch: 457/840\n",
      "Batch loss: 0.7831437587738037 batch: 458/840\n",
      "Batch loss: 0.5717434883117676 batch: 459/840\n",
      "Batch loss: 0.5561047196388245 batch: 460/840\n",
      "Batch loss: 0.763244092464447 batch: 461/840\n",
      "Batch loss: 0.6604855060577393 batch: 462/840\n",
      "Batch loss: 0.8631288409233093 batch: 463/840\n",
      "Batch loss: 0.5500525832176208 batch: 464/840\n",
      "Batch loss: 0.9536370635032654 batch: 465/840\n",
      "Batch loss: 0.7422077655792236 batch: 466/840\n",
      "Batch loss: 0.9448556303977966 batch: 467/840\n",
      "Batch loss: 0.6937987804412842 batch: 468/840\n",
      "Batch loss: 0.6310485005378723 batch: 469/840\n",
      "Batch loss: 0.7560955882072449 batch: 470/840\n",
      "Batch loss: 0.6851664781570435 batch: 471/840\n",
      "Batch loss: 0.7878330945968628 batch: 472/840\n",
      "Batch loss: 0.7975071668624878 batch: 473/840\n",
      "Batch loss: 0.6574990749359131 batch: 474/840\n",
      "Batch loss: 0.6415067315101624 batch: 475/840\n",
      "Batch loss: 0.7289808392524719 batch: 476/840\n",
      "Batch loss: 0.6408835053443909 batch: 477/840\n",
      "Batch loss: 0.5883412957191467 batch: 478/840\n",
      "Batch loss: 0.5735530257225037 batch: 479/840\n",
      "Batch loss: 0.8292704224586487 batch: 480/840\n",
      "Batch loss: 0.7096595764160156 batch: 481/840\n",
      "Batch loss: 0.7645530104637146 batch: 482/840\n",
      "Batch loss: 0.8307143449783325 batch: 483/840\n",
      "Batch loss: 0.6542266011238098 batch: 484/840\n",
      "Batch loss: 0.7300626635551453 batch: 485/840\n",
      "Batch loss: 0.703372061252594 batch: 486/840\n",
      "Batch loss: 0.5590330362319946 batch: 487/840\n",
      "Batch loss: 0.7426040172576904 batch: 488/840\n",
      "Batch loss: 0.7632491588592529 batch: 489/840\n",
      "Batch loss: 0.7688781023025513 batch: 490/840\n",
      "Batch loss: 0.45476898550987244 batch: 491/840\n",
      "Batch loss: 0.7385368347167969 batch: 492/840\n",
      "Batch loss: 0.8375856280326843 batch: 493/840\n",
      "Batch loss: 0.4861849844455719 batch: 494/840\n",
      "Batch loss: 0.8048438429832458 batch: 495/840\n",
      "Batch loss: 0.7734599113464355 batch: 496/840\n",
      "Batch loss: 0.8219894170761108 batch: 497/840\n",
      "Batch loss: 0.5082494616508484 batch: 498/840\n",
      "Batch loss: 0.7810426354408264 batch: 499/840\n",
      "Batch loss: 0.6848259568214417 batch: 500/840\n",
      "Batch loss: 0.5889864563941956 batch: 501/840\n",
      "Batch loss: 0.7154238224029541 batch: 502/840\n",
      "Batch loss: 0.5614111423492432 batch: 503/840\n",
      "Batch loss: 0.6946288347244263 batch: 504/840\n",
      "Batch loss: 0.7235785722732544 batch: 505/840\n",
      "Batch loss: 0.6911023855209351 batch: 506/840\n",
      "Batch loss: 0.7246540784835815 batch: 507/840\n",
      "Batch loss: 0.6085751056671143 batch: 508/840\n",
      "Batch loss: 0.8077313899993896 batch: 509/840\n",
      "Batch loss: 0.6768929958343506 batch: 510/840\n",
      "Batch loss: 0.5703304409980774 batch: 511/840\n",
      "Batch loss: 0.6899781227111816 batch: 512/840\n",
      "Batch loss: 0.6116536259651184 batch: 513/840\n",
      "Batch loss: 0.7350046038627625 batch: 514/840\n",
      "Batch loss: 0.5861701965332031 batch: 515/840\n",
      "Batch loss: 0.7680050134658813 batch: 516/840\n",
      "Batch loss: 0.7302577495574951 batch: 517/840\n",
      "Batch loss: 0.6421898007392883 batch: 518/840\n",
      "Batch loss: 0.9873073101043701 batch: 519/840\n",
      "Batch loss: 0.710685133934021 batch: 520/840\n",
      "Batch loss: 0.8811385631561279 batch: 521/840\n",
      "Batch loss: 0.7017942070960999 batch: 522/840\n",
      "Batch loss: 0.4838366210460663 batch: 523/840\n",
      "Batch loss: 0.7790950536727905 batch: 524/840\n",
      "Batch loss: 0.7197664380073547 batch: 525/840\n",
      "Batch loss: 0.8641851544380188 batch: 526/840\n",
      "Batch loss: 0.8023478984832764 batch: 527/840\n",
      "Batch loss: 0.683576226234436 batch: 528/840\n",
      "Batch loss: 0.6271611452102661 batch: 529/840\n",
      "Batch loss: 0.7498647570610046 batch: 530/840\n",
      "Batch loss: 0.6117919087409973 batch: 531/840\n",
      "Batch loss: 0.5091969966888428 batch: 532/840\n",
      "Batch loss: 0.759019672870636 batch: 533/840\n",
      "Batch loss: 0.7812132239341736 batch: 534/840\n",
      "Batch loss: 0.6470082998275757 batch: 535/840\n",
      "Batch loss: 0.6707894206047058 batch: 536/840\n",
      "Batch loss: 0.8117685914039612 batch: 537/840\n",
      "Batch loss: 0.6344968676567078 batch: 538/840\n",
      "Batch loss: 0.6435467004776001 batch: 539/840\n",
      "Batch loss: 0.7912895083427429 batch: 540/840\n",
      "Batch loss: 0.5765401721000671 batch: 541/840\n",
      "Batch loss: 0.7578522562980652 batch: 542/840\n",
      "Batch loss: 0.5429040789604187 batch: 543/840\n",
      "Batch loss: 0.7135900855064392 batch: 544/840\n",
      "Batch loss: 0.7039528489112854 batch: 545/840\n",
      "Batch loss: 0.6729803681373596 batch: 546/840\n",
      "Batch loss: 0.7488980889320374 batch: 547/840\n",
      "Batch loss: 0.5348513126373291 batch: 548/840\n",
      "Batch loss: 0.6399645805358887 batch: 549/840\n",
      "Batch loss: 0.558493435382843 batch: 550/840\n",
      "Batch loss: 0.7193220257759094 batch: 551/840\n",
      "Batch loss: 0.6177694201469421 batch: 552/840\n",
      "Batch loss: 0.7414395809173584 batch: 553/840\n",
      "Batch loss: 0.7645357251167297 batch: 554/840\n",
      "Batch loss: 0.710432767868042 batch: 555/840\n",
      "Batch loss: 0.717771053314209 batch: 556/840\n",
      "Batch loss: 0.7014948129653931 batch: 557/840\n",
      "Batch loss: 0.6156557202339172 batch: 558/840\n",
      "Batch loss: 0.6645970940589905 batch: 559/840\n",
      "Batch loss: 0.7446514964103699 batch: 560/840\n",
      "Batch loss: 0.654458224773407 batch: 561/840\n",
      "Batch loss: 0.6876296401023865 batch: 562/840\n",
      "Batch loss: 0.5878861546516418 batch: 563/840\n",
      "Batch loss: 0.6492574214935303 batch: 564/840\n",
      "Batch loss: 0.890495777130127 batch: 565/840\n",
      "Batch loss: 0.6965442895889282 batch: 566/840\n",
      "Batch loss: 0.8126800060272217 batch: 567/840\n",
      "Batch loss: 0.7233667969703674 batch: 568/840\n",
      "Batch loss: 0.7012945413589478 batch: 569/840\n",
      "Batch loss: 0.5210492610931396 batch: 570/840\n",
      "Batch loss: 0.6847760677337646 batch: 571/840\n",
      "Batch loss: 0.777985155582428 batch: 572/840\n",
      "Batch loss: 0.6971357464790344 batch: 573/840\n",
      "Batch loss: 0.7101508975028992 batch: 574/840\n",
      "Batch loss: 0.6349354386329651 batch: 575/840\n",
      "Batch loss: 0.707944393157959 batch: 576/840\n",
      "Batch loss: 0.6460769176483154 batch: 577/840\n",
      "Batch loss: 0.6910712718963623 batch: 578/840\n",
      "Batch loss: 0.7053505778312683 batch: 579/840\n",
      "Batch loss: 0.8714719414710999 batch: 580/840\n",
      "Batch loss: 0.7353156805038452 batch: 581/840\n",
      "Batch loss: 0.9038500785827637 batch: 582/840\n",
      "Batch loss: 0.6592394113540649 batch: 583/840\n",
      "Batch loss: 0.893883228302002 batch: 584/840\n",
      "Batch loss: 0.6618192791938782 batch: 585/840\n",
      "Batch loss: 0.7400809526443481 batch: 586/840\n",
      "Batch loss: 0.678882360458374 batch: 587/840\n",
      "Batch loss: 0.6283236145973206 batch: 588/840\n",
      "Batch loss: 0.816448986530304 batch: 589/840\n",
      "Batch loss: 0.7303951382637024 batch: 590/840\n",
      "Batch loss: 0.5080763101577759 batch: 591/840\n",
      "Batch loss: 0.6122757196426392 batch: 592/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7742642760276794 batch: 593/840\n",
      "Batch loss: 0.6991603374481201 batch: 594/840\n",
      "Batch loss: 0.45113399624824524 batch: 595/840\n",
      "Batch loss: 0.591407835483551 batch: 596/840\n",
      "Batch loss: 0.7118915319442749 batch: 597/840\n",
      "Batch loss: 0.5601863265037537 batch: 598/840\n",
      "Batch loss: 0.5582578182220459 batch: 599/840\n",
      "Batch loss: 0.8223369717597961 batch: 600/840\n",
      "Batch loss: 0.7205594778060913 batch: 601/840\n",
      "Batch loss: 0.6125538349151611 batch: 602/840\n",
      "Batch loss: 0.7006803750991821 batch: 603/840\n",
      "Batch loss: 0.7223252058029175 batch: 604/840\n",
      "Batch loss: 0.9176857471466064 batch: 605/840\n",
      "Batch loss: 0.8329961895942688 batch: 606/840\n",
      "Batch loss: 0.7359417080879211 batch: 607/840\n",
      "Batch loss: 0.8659982085227966 batch: 608/840\n",
      "Batch loss: 0.6168786883354187 batch: 609/840\n",
      "Batch loss: 0.7695086002349854 batch: 610/840\n",
      "Batch loss: 0.7614015936851501 batch: 611/840\n",
      "Batch loss: 0.8142792582511902 batch: 612/840\n",
      "Batch loss: 0.7714126706123352 batch: 613/840\n",
      "Batch loss: 0.7432668805122375 batch: 614/840\n",
      "Batch loss: 0.6640731692314148 batch: 615/840\n",
      "Batch loss: 0.5826902985572815 batch: 616/840\n",
      "Batch loss: 0.6125352382659912 batch: 617/840\n",
      "Batch loss: 0.7527706027030945 batch: 618/840\n",
      "Batch loss: 0.895979106426239 batch: 619/840\n",
      "Batch loss: 0.6663525104522705 batch: 620/840\n",
      "Batch loss: 0.7341112494468689 batch: 621/840\n",
      "Batch loss: 0.6819728016853333 batch: 622/840\n",
      "Batch loss: 0.6287203431129456 batch: 623/840\n",
      "Batch loss: 0.8659553527832031 batch: 624/840\n",
      "Batch loss: 0.7590544819831848 batch: 625/840\n",
      "Batch loss: 0.6061800718307495 batch: 626/840\n",
      "Batch loss: 0.6616079807281494 batch: 627/840\n",
      "Batch loss: 0.6946694254875183 batch: 628/840\n",
      "Batch loss: 0.7375814318656921 batch: 629/840\n",
      "Batch loss: 0.7755905985832214 batch: 630/840\n",
      "Batch loss: 0.7827664017677307 batch: 631/840\n",
      "Batch loss: 0.628215491771698 batch: 632/840\n",
      "Batch loss: 0.5978583097457886 batch: 633/840\n",
      "Batch loss: 0.8589648604393005 batch: 634/840\n",
      "Batch loss: 0.5830287933349609 batch: 635/840\n",
      "Batch loss: 0.7007493376731873 batch: 636/840\n",
      "Batch loss: 0.5907618403434753 batch: 637/840\n",
      "Batch loss: 0.6956007480621338 batch: 638/840\n",
      "Batch loss: 0.6997547745704651 batch: 639/840\n",
      "Batch loss: 0.6059902310371399 batch: 640/840\n",
      "Batch loss: 0.9028146266937256 batch: 641/840\n",
      "Batch loss: 0.663917064666748 batch: 642/840\n",
      "Batch loss: 1.092801570892334 batch: 643/840\n",
      "Batch loss: 0.6191686391830444 batch: 644/840\n",
      "Batch loss: 0.5793887972831726 batch: 645/840\n",
      "Batch loss: 0.5713828802108765 batch: 646/840\n",
      "Batch loss: 0.846193253993988 batch: 647/840\n",
      "Batch loss: 0.7412552833557129 batch: 648/840\n",
      "Batch loss: 0.6929494738578796 batch: 649/840\n",
      "Batch loss: 0.6866304278373718 batch: 650/840\n",
      "Batch loss: 0.6820539832115173 batch: 651/840\n",
      "Batch loss: 0.7325344681739807 batch: 652/840\n",
      "Batch loss: 0.619225025177002 batch: 653/840\n",
      "Batch loss: 0.978009819984436 batch: 654/840\n",
      "Batch loss: 0.5857833027839661 batch: 655/840\n",
      "Batch loss: 0.7157042622566223 batch: 656/840\n",
      "Batch loss: 0.6587038636207581 batch: 657/840\n",
      "Batch loss: 0.8059656620025635 batch: 658/840\n",
      "Batch loss: 0.6832592487335205 batch: 659/840\n",
      "Batch loss: 0.5625000596046448 batch: 660/840\n",
      "Batch loss: 0.7763313055038452 batch: 661/840\n",
      "Batch loss: 0.7792249917984009 batch: 662/840\n",
      "Batch loss: 0.5703359842300415 batch: 663/840\n",
      "Batch loss: 0.696309506893158 batch: 664/840\n",
      "Batch loss: 0.895750105381012 batch: 665/840\n",
      "Batch loss: 0.6039090156555176 batch: 666/840\n",
      "Batch loss: 0.8476486206054688 batch: 667/840\n",
      "Batch loss: 0.7145172953605652 batch: 668/840\n",
      "Batch loss: 0.6364412307739258 batch: 669/840\n",
      "Batch loss: 0.8017176985740662 batch: 670/840\n",
      "Batch loss: 0.672763466835022 batch: 671/840\n",
      "Batch loss: 0.5514310002326965 batch: 672/840\n",
      "Batch loss: 0.886818528175354 batch: 673/840\n",
      "Batch loss: 0.8606014847755432 batch: 674/840\n",
      "Batch loss: 0.7637040019035339 batch: 675/840\n",
      "Batch loss: 0.5476066470146179 batch: 676/840\n",
      "Batch loss: 0.6354988217353821 batch: 677/840\n",
      "Batch loss: 0.7453188896179199 batch: 678/840\n",
      "Batch loss: 0.8154376745223999 batch: 679/840\n",
      "Batch loss: 0.7458692789077759 batch: 680/840\n",
      "Batch loss: 0.7246568202972412 batch: 681/840\n",
      "Batch loss: 0.6892200708389282 batch: 682/840\n",
      "Batch loss: 0.574097216129303 batch: 683/840\n",
      "Batch loss: 1.001095175743103 batch: 684/840\n",
      "Batch loss: 0.7412264943122864 batch: 685/840\n",
      "Batch loss: 0.7862299084663391 batch: 686/840\n",
      "Batch loss: 0.7922530174255371 batch: 687/840\n",
      "Batch loss: 0.6434167623519897 batch: 688/840\n",
      "Batch loss: 0.7113308906555176 batch: 689/840\n",
      "Batch loss: 0.6377032995223999 batch: 690/840\n",
      "Batch loss: 0.7484589219093323 batch: 691/840\n",
      "Batch loss: 0.6618409752845764 batch: 692/840\n",
      "Batch loss: 0.8190010190010071 batch: 693/840\n",
      "Batch loss: 0.9171802997589111 batch: 694/840\n",
      "Batch loss: 0.6786880493164062 batch: 695/840\n",
      "Batch loss: 0.683376669883728 batch: 696/840\n",
      "Batch loss: 0.576196014881134 batch: 697/840\n",
      "Batch loss: 0.7586234211921692 batch: 698/840\n",
      "Batch loss: 0.7628630995750427 batch: 699/840\n",
      "Batch loss: 0.8105953931808472 batch: 700/840\n",
      "Batch loss: 0.8580771088600159 batch: 701/840\n",
      "Batch loss: 0.7867946028709412 batch: 702/840\n",
      "Batch loss: 0.639206051826477 batch: 703/840\n",
      "Batch loss: 0.7644959092140198 batch: 704/840\n",
      "Batch loss: 0.6790940165519714 batch: 705/840\n",
      "Batch loss: 0.7552591562271118 batch: 706/840\n",
      "Batch loss: 0.7112879753112793 batch: 707/840\n",
      "Batch loss: 0.8037510514259338 batch: 708/840\n",
      "Batch loss: 0.7327543497085571 batch: 709/840\n",
      "Batch loss: 0.8296389579772949 batch: 710/840\n",
      "Batch loss: 0.5481941103935242 batch: 711/840\n",
      "Batch loss: 0.795703649520874 batch: 712/840\n",
      "Batch loss: 0.6069822311401367 batch: 713/840\n",
      "Batch loss: 0.7235825061798096 batch: 714/840\n",
      "Batch loss: 0.8470842242240906 batch: 715/840\n",
      "Batch loss: 0.7552571296691895 batch: 716/840\n",
      "Batch loss: 0.7734373211860657 batch: 717/840\n",
      "Batch loss: 0.6380036473274231 batch: 718/840\n",
      "Batch loss: 0.5867109894752502 batch: 719/840\n",
      "Batch loss: 0.6532554030418396 batch: 720/840\n",
      "Batch loss: 0.7343044877052307 batch: 721/840\n",
      "Batch loss: 0.8242468237876892 batch: 722/840\n",
      "Batch loss: 0.5664815902709961 batch: 723/840\n",
      "Batch loss: 0.8527726531028748 batch: 724/840\n",
      "Batch loss: 0.7298452258110046 batch: 725/840\n",
      "Batch loss: 0.640082597732544 batch: 726/840\n",
      "Batch loss: 0.8818922638893127 batch: 727/840\n",
      "Batch loss: 0.8203645944595337 batch: 728/840\n",
      "Batch loss: 0.8387424349784851 batch: 729/840\n",
      "Batch loss: 0.7050416469573975 batch: 730/840\n",
      "Batch loss: 0.5482653975486755 batch: 731/840\n",
      "Batch loss: 0.9335175156593323 batch: 732/840\n",
      "Batch loss: 0.7129432559013367 batch: 733/840\n",
      "Batch loss: 0.6376798152923584 batch: 734/840\n",
      "Batch loss: 0.6214609742164612 batch: 735/840\n",
      "Batch loss: 0.6912397742271423 batch: 736/840\n",
      "Batch loss: 0.7135916948318481 batch: 737/840\n",
      "Batch loss: 0.6082621216773987 batch: 738/840\n",
      "Batch loss: 0.7658849358558655 batch: 739/840\n",
      "Batch loss: 0.8786351680755615 batch: 740/840\n",
      "Batch loss: 0.5474098920822144 batch: 741/840\n",
      "Batch loss: 0.6506959795951843 batch: 742/840\n",
      "Batch loss: 0.48150426149368286 batch: 743/840\n",
      "Batch loss: 0.6574898362159729 batch: 744/840\n",
      "Batch loss: 0.5812066793441772 batch: 745/840\n",
      "Batch loss: 0.6173197627067566 batch: 746/840\n",
      "Batch loss: 0.7829426527023315 batch: 747/840\n",
      "Batch loss: 0.7999216318130493 batch: 748/840\n",
      "Batch loss: 0.5464867949485779 batch: 749/840\n",
      "Batch loss: 0.6149702668190002 batch: 750/840\n",
      "Batch loss: 0.7370191216468811 batch: 751/840\n",
      "Batch loss: 0.9299324750900269 batch: 752/840\n",
      "Batch loss: 0.6259210705757141 batch: 753/840\n",
      "Batch loss: 0.6891704797744751 batch: 754/840\n",
      "Batch loss: 0.7143238186836243 batch: 755/840\n",
      "Batch loss: 0.6613421440124512 batch: 756/840\n",
      "Batch loss: 0.8142955303192139 batch: 757/840\n",
      "Batch loss: 0.7244391441345215 batch: 758/840\n",
      "Batch loss: 0.6434761881828308 batch: 759/840\n",
      "Batch loss: 0.6497369408607483 batch: 760/840\n",
      "Batch loss: 0.6163647174835205 batch: 761/840\n",
      "Batch loss: 0.8074173927307129 batch: 762/840\n",
      "Batch loss: 0.4875809848308563 batch: 763/840\n",
      "Batch loss: 0.6783390641212463 batch: 764/840\n",
      "Batch loss: 0.5722348690032959 batch: 765/840\n",
      "Batch loss: 0.6222872138023376 batch: 766/840\n",
      "Batch loss: 0.8086626529693604 batch: 767/840\n",
      "Batch loss: 0.7660396099090576 batch: 768/840\n",
      "Batch loss: 0.683281660079956 batch: 769/840\n",
      "Batch loss: 0.6326858401298523 batch: 770/840\n",
      "Batch loss: 0.6503607034683228 batch: 771/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7136338949203491 batch: 772/840\n",
      "Batch loss: 0.5603766441345215 batch: 773/840\n",
      "Batch loss: 0.520460307598114 batch: 774/840\n",
      "Batch loss: 0.54917311668396 batch: 775/840\n",
      "Batch loss: 0.5805213451385498 batch: 776/840\n",
      "Batch loss: 0.6006617546081543 batch: 777/840\n",
      "Batch loss: 0.5212373733520508 batch: 778/840\n",
      "Batch loss: 0.8456566333770752 batch: 779/840\n",
      "Batch loss: 0.726939857006073 batch: 780/840\n",
      "Batch loss: 0.7831668257713318 batch: 781/840\n",
      "Batch loss: 0.6716859340667725 batch: 782/840\n",
      "Batch loss: 0.5151993632316589 batch: 783/840\n",
      "Batch loss: 0.7599252462387085 batch: 784/840\n",
      "Batch loss: 0.7033916711807251 batch: 785/840\n",
      "Batch loss: 0.7887646555900574 batch: 786/840\n",
      "Batch loss: 0.6017487049102783 batch: 787/840\n",
      "Batch loss: 0.7289230227470398 batch: 788/840\n",
      "Batch loss: 0.8354370594024658 batch: 789/840\n",
      "Batch loss: 0.7312440276145935 batch: 790/840\n",
      "Batch loss: 0.6571652889251709 batch: 791/840\n",
      "Batch loss: 0.5245264172554016 batch: 792/840\n",
      "Batch loss: 0.6717785000801086 batch: 793/840\n",
      "Batch loss: 0.6919209361076355 batch: 794/840\n",
      "Batch loss: 0.7017455101013184 batch: 795/840\n",
      "Batch loss: 0.7670915722846985 batch: 796/840\n",
      "Batch loss: 0.8048036694526672 batch: 797/840\n",
      "Batch loss: 0.8219286203384399 batch: 798/840\n",
      "Batch loss: 0.7080124020576477 batch: 799/840\n",
      "Batch loss: 0.5959576964378357 batch: 800/840\n",
      "Batch loss: 0.7900893688201904 batch: 801/840\n",
      "Batch loss: 0.6071262955665588 batch: 802/840\n",
      "Batch loss: 0.6315797567367554 batch: 803/840\n",
      "Batch loss: 0.8457129001617432 batch: 804/840\n",
      "Batch loss: 0.6365774273872375 batch: 805/840\n",
      "Batch loss: 0.6789788007736206 batch: 806/840\n",
      "Batch loss: 0.6178592443466187 batch: 807/840\n",
      "Batch loss: 0.6649655699729919 batch: 808/840\n",
      "Batch loss: 0.6788281202316284 batch: 809/840\n",
      "Batch loss: 0.5758408904075623 batch: 810/840\n",
      "Batch loss: 0.5763948559761047 batch: 811/840\n",
      "Batch loss: 0.6571162939071655 batch: 812/840\n",
      "Batch loss: 0.6320579051971436 batch: 813/840\n",
      "Batch loss: 0.6317616701126099 batch: 814/840\n",
      "Batch loss: 0.824118435382843 batch: 815/840\n",
      "Batch loss: 0.7167108058929443 batch: 816/840\n",
      "Batch loss: 0.7546290755271912 batch: 817/840\n",
      "Batch loss: 0.8387067914009094 batch: 818/840\n",
      "Batch loss: 0.6156520247459412 batch: 819/840\n",
      "Batch loss: 0.6365466117858887 batch: 820/840\n",
      "Batch loss: 0.669282853603363 batch: 821/840\n",
      "Batch loss: 0.714084804058075 batch: 822/840\n",
      "Batch loss: 0.7635595202445984 batch: 823/840\n",
      "Batch loss: 0.7860910296440125 batch: 824/840\n",
      "Batch loss: 0.7800630927085876 batch: 825/840\n",
      "Batch loss: 0.671176552772522 batch: 826/840\n",
      "Batch loss: 0.529365599155426 batch: 827/840\n",
      "Batch loss: 0.7464150786399841 batch: 828/840\n",
      "Batch loss: 0.6289170980453491 batch: 829/840\n",
      "Batch loss: 0.6961411237716675 batch: 830/840\n",
      "Batch loss: 0.6037839651107788 batch: 831/840\n",
      "Batch loss: 0.754848837852478 batch: 832/840\n",
      "Batch loss: 0.8561864495277405 batch: 833/840\n",
      "Batch loss: 0.6042765974998474 batch: 834/840\n",
      "Batch loss: 0.6488367319107056 batch: 835/840\n",
      "Batch loss: 0.7277008295059204 batch: 836/840\n",
      "Batch loss: 0.6650522351264954 batch: 837/840\n",
      "Batch loss: 0.8746609687805176 batch: 838/840\n",
      "Batch loss: 0.6077144742012024 batch: 839/840\n",
      "Batch loss: 0.6680744886398315 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 2/15..  Training Loss: 0.007..  Test Loss: 0.005..  Test Accuracy: 0.810\n",
      "Running epoch 3/15\n",
      "Batch loss: 0.5292207598686218 batch: 1/840\n",
      "Batch loss: 1.1986984014511108 batch: 2/840\n",
      "Batch loss: 0.6761066317558289 batch: 3/840\n",
      "Batch loss: 0.7186218500137329 batch: 4/840\n",
      "Batch loss: 0.6352089047431946 batch: 5/840\n",
      "Batch loss: 0.5574839115142822 batch: 6/840\n",
      "Batch loss: 0.6954131126403809 batch: 7/840\n",
      "Batch loss: 0.7303635478019714 batch: 8/840\n",
      "Batch loss: 0.6508526802062988 batch: 9/840\n",
      "Batch loss: 0.7037162780761719 batch: 10/840\n",
      "Batch loss: 0.6162607669830322 batch: 11/840\n",
      "Batch loss: 0.5849102139472961 batch: 12/840\n",
      "Batch loss: 0.574469804763794 batch: 13/840\n",
      "Batch loss: 0.7351755499839783 batch: 14/840\n",
      "Batch loss: 0.6822904944419861 batch: 15/840\n",
      "Batch loss: 0.5665834546089172 batch: 16/840\n",
      "Batch loss: 0.5626401305198669 batch: 17/840\n",
      "Batch loss: 0.7241134643554688 batch: 18/840\n",
      "Batch loss: 0.8139712810516357 batch: 19/840\n",
      "Batch loss: 0.8222740888595581 batch: 20/840\n",
      "Batch loss: 0.8378984928131104 batch: 21/840\n",
      "Batch loss: 0.6387631893157959 batch: 22/840\n",
      "Batch loss: 0.8066418170928955 batch: 23/840\n",
      "Batch loss: 0.6241397261619568 batch: 24/840\n",
      "Batch loss: 0.6128789782524109 batch: 25/840\n",
      "Batch loss: 0.7747552990913391 batch: 26/840\n",
      "Batch loss: 0.7204374670982361 batch: 27/840\n",
      "Batch loss: 0.738588273525238 batch: 28/840\n",
      "Batch loss: 0.6567575931549072 batch: 29/840\n",
      "Batch loss: 0.5358717441558838 batch: 30/840\n",
      "Batch loss: 0.6334027051925659 batch: 31/840\n",
      "Batch loss: 0.5238910913467407 batch: 32/840\n",
      "Batch loss: 0.683676540851593 batch: 33/840\n",
      "Batch loss: 0.5652094483375549 batch: 34/840\n",
      "Batch loss: 0.5891364812850952 batch: 35/840\n",
      "Batch loss: 0.554172158241272 batch: 36/840\n",
      "Batch loss: 0.742882251739502 batch: 37/840\n",
      "Batch loss: 0.6906801462173462 batch: 38/840\n",
      "Batch loss: 0.6924279928207397 batch: 39/840\n",
      "Batch loss: 0.6503846049308777 batch: 40/840\n",
      "Batch loss: 0.8234766125679016 batch: 41/840\n",
      "Batch loss: 0.7231494188308716 batch: 42/840\n",
      "Batch loss: 0.7425180077552795 batch: 43/840\n",
      "Batch loss: 0.7232996225357056 batch: 44/840\n",
      "Batch loss: 0.7576478719711304 batch: 45/840\n",
      "Batch loss: 0.5954294204711914 batch: 46/840\n",
      "Batch loss: 0.5560197234153748 batch: 47/840\n",
      "Batch loss: 0.7273024916648865 batch: 48/840\n",
      "Batch loss: 0.6476754546165466 batch: 49/840\n",
      "Batch loss: 0.6449614763259888 batch: 50/840\n",
      "Batch loss: 0.7566247582435608 batch: 51/840\n",
      "Batch loss: 0.8039968609809875 batch: 52/840\n",
      "Batch loss: 0.6154347062110901 batch: 53/840\n",
      "Batch loss: 0.713840126991272 batch: 54/840\n",
      "Batch loss: 0.642334520816803 batch: 55/840\n",
      "Batch loss: 0.6962252855300903 batch: 56/840\n",
      "Batch loss: 0.7950189709663391 batch: 57/840\n",
      "Batch loss: 0.5700063109397888 batch: 58/840\n",
      "Batch loss: 0.6815273761749268 batch: 59/840\n",
      "Batch loss: 0.5854414105415344 batch: 60/840\n",
      "Batch loss: 0.7995607256889343 batch: 61/840\n",
      "Batch loss: 0.7397872805595398 batch: 62/840\n",
      "Batch loss: 0.7534514665603638 batch: 63/840\n",
      "Batch loss: 0.6259179711341858 batch: 64/840\n",
      "Batch loss: 0.5985757112503052 batch: 65/840\n",
      "Batch loss: 0.6679425239562988 batch: 66/840\n",
      "Batch loss: 0.7322959899902344 batch: 67/840\n",
      "Batch loss: 0.7146134972572327 batch: 68/840\n",
      "Batch loss: 0.7339687943458557 batch: 69/840\n",
      "Batch loss: 0.6987648606300354 batch: 70/840\n",
      "Batch loss: 0.7616095542907715 batch: 71/840\n",
      "Batch loss: 0.8117280006408691 batch: 72/840\n",
      "Batch loss: 0.7067753076553345 batch: 73/840\n",
      "Batch loss: 0.5244948863983154 batch: 74/840\n",
      "Batch loss: 0.7970660924911499 batch: 75/840\n",
      "Batch loss: 0.4835788309574127 batch: 76/840\n",
      "Batch loss: 0.5858955383300781 batch: 77/840\n",
      "Batch loss: 0.874768853187561 batch: 78/840\n",
      "Batch loss: 0.7373043894767761 batch: 79/840\n",
      "Batch loss: 0.7142813801765442 batch: 80/840\n",
      "Batch loss: 0.6474249958992004 batch: 81/840\n",
      "Batch loss: 0.8030839562416077 batch: 82/840\n",
      "Batch loss: 0.6213977932929993 batch: 83/840\n",
      "Batch loss: 0.7868121862411499 batch: 84/840\n",
      "Batch loss: 0.6138266324996948 batch: 85/840\n",
      "Batch loss: 0.932350754737854 batch: 86/840\n",
      "Batch loss: 0.5684287548065186 batch: 87/840\n",
      "Batch loss: 0.48073700070381165 batch: 88/840\n",
      "Batch loss: 0.4704946279525757 batch: 89/840\n",
      "Batch loss: 0.657000720500946 batch: 90/840\n",
      "Batch loss: 0.5853283405303955 batch: 91/840\n",
      "Batch loss: 0.6301538348197937 batch: 92/840\n",
      "Batch loss: 0.78029465675354 batch: 93/840\n",
      "Batch loss: 0.6902642846107483 batch: 94/840\n",
      "Batch loss: 0.6314200758934021 batch: 95/840\n",
      "Batch loss: 0.7050585746765137 batch: 96/840\n",
      "Batch loss: 0.7228636741638184 batch: 97/840\n",
      "Batch loss: 0.7826513051986694 batch: 98/840\n",
      "Batch loss: 0.6391090154647827 batch: 99/840\n",
      "Batch loss: 0.6953184604644775 batch: 100/840\n",
      "Batch loss: 0.6104817390441895 batch: 101/840\n",
      "Batch loss: 0.5838564038276672 batch: 102/840\n",
      "Batch loss: 0.7240455746650696 batch: 103/840\n",
      "Batch loss: 0.5170993208885193 batch: 104/840\n",
      "Batch loss: 0.6524917483329773 batch: 105/840\n",
      "Batch loss: 0.7329418063163757 batch: 106/840\n",
      "Batch loss: 0.6061649918556213 batch: 107/840\n",
      "Batch loss: 0.8365018367767334 batch: 108/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6759828925132751 batch: 109/840\n",
      "Batch loss: 0.5473222136497498 batch: 110/840\n",
      "Batch loss: 0.5582352876663208 batch: 111/840\n",
      "Batch loss: 0.7974791526794434 batch: 112/840\n",
      "Batch loss: 0.7353047132492065 batch: 113/840\n",
      "Batch loss: 0.6366559863090515 batch: 114/840\n",
      "Batch loss: 0.6948081851005554 batch: 115/840\n",
      "Batch loss: 0.6145722270011902 batch: 116/840\n",
      "Batch loss: 0.6543692946434021 batch: 117/840\n",
      "Batch loss: 0.5518554449081421 batch: 118/840\n",
      "Batch loss: 0.6531992554664612 batch: 119/840\n",
      "Batch loss: 0.6035178899765015 batch: 120/840\n",
      "Batch loss: 0.7630844712257385 batch: 121/840\n",
      "Batch loss: 1.0080513954162598 batch: 122/840\n",
      "Batch loss: 0.5163282155990601 batch: 123/840\n",
      "Batch loss: 0.6124362349510193 batch: 124/840\n",
      "Batch loss: 0.7135162353515625 batch: 125/840\n",
      "Batch loss: 0.7433531880378723 batch: 126/840\n",
      "Batch loss: 0.879401683807373 batch: 127/840\n",
      "Batch loss: 0.7361404895782471 batch: 128/840\n",
      "Batch loss: 0.777320921421051 batch: 129/840\n",
      "Batch loss: 0.7002102732658386 batch: 130/840\n",
      "Batch loss: 0.7680094838142395 batch: 131/840\n",
      "Batch loss: 0.9450829029083252 batch: 132/840\n",
      "Batch loss: 0.8155010938644409 batch: 133/840\n",
      "Batch loss: 0.6288754940032959 batch: 134/840\n",
      "Batch loss: 0.5664722323417664 batch: 135/840\n",
      "Batch loss: 0.778412938117981 batch: 136/840\n",
      "Batch loss: 0.6148585677146912 batch: 137/840\n",
      "Batch loss: 0.4889746606349945 batch: 138/840\n",
      "Batch loss: 0.5972638726234436 batch: 139/840\n",
      "Batch loss: 0.7391817569732666 batch: 140/840\n",
      "Batch loss: 0.5256446599960327 batch: 141/840\n",
      "Batch loss: 0.6561333537101746 batch: 142/840\n",
      "Batch loss: 0.5814676880836487 batch: 143/840\n",
      "Batch loss: 0.6083439588546753 batch: 144/840\n",
      "Batch loss: 0.7821031808853149 batch: 145/840\n",
      "Batch loss: 0.8158199191093445 batch: 146/840\n",
      "Batch loss: 0.5236732959747314 batch: 147/840\n",
      "Batch loss: 0.7334246635437012 batch: 148/840\n",
      "Batch loss: 0.6162160634994507 batch: 149/840\n",
      "Batch loss: 0.820746898651123 batch: 150/840\n",
      "Batch loss: 0.6805431842803955 batch: 151/840\n",
      "Batch loss: 0.7007073163986206 batch: 152/840\n",
      "Batch loss: 0.5896347165107727 batch: 153/840\n",
      "Batch loss: 0.7722908854484558 batch: 154/840\n",
      "Batch loss: 0.5831841230392456 batch: 155/840\n",
      "Batch loss: 0.5941354036331177 batch: 156/840\n",
      "Batch loss: 0.7083683013916016 batch: 157/840\n",
      "Batch loss: 0.6186941862106323 batch: 158/840\n",
      "Batch loss: 0.5682351589202881 batch: 159/840\n",
      "Batch loss: 0.7470566034317017 batch: 160/840\n",
      "Batch loss: 0.6944715976715088 batch: 161/840\n",
      "Batch loss: 0.7580193877220154 batch: 162/840\n",
      "Batch loss: 0.7906889915466309 batch: 163/840\n",
      "Batch loss: 0.47355353832244873 batch: 164/840\n",
      "Batch loss: 0.7566112279891968 batch: 165/840\n",
      "Batch loss: 0.5876017212867737 batch: 166/840\n",
      "Batch loss: 0.7391262650489807 batch: 167/840\n",
      "Batch loss: 0.6823269724845886 batch: 168/840\n",
      "Batch loss: 0.6459876894950867 batch: 169/840\n",
      "Batch loss: 0.7545575499534607 batch: 170/840\n",
      "Batch loss: 0.7272630333900452 batch: 171/840\n",
      "Batch loss: 0.8801185488700867 batch: 172/840\n",
      "Batch loss: 0.6814892292022705 batch: 173/840\n",
      "Batch loss: 0.6487762331962585 batch: 174/840\n",
      "Batch loss: 0.5761613249778748 batch: 175/840\n",
      "Batch loss: 0.6622571349143982 batch: 176/840\n",
      "Batch loss: 0.6817266345024109 batch: 177/840\n",
      "Batch loss: 0.6972366571426392 batch: 178/840\n",
      "Batch loss: 0.8675450682640076 batch: 179/840\n",
      "Batch loss: 0.5873242616653442 batch: 180/840\n",
      "Batch loss: 0.6515727043151855 batch: 181/840\n",
      "Batch loss: 0.6569843292236328 batch: 182/840\n",
      "Batch loss: 0.6407313346862793 batch: 183/840\n",
      "Batch loss: 0.614936113357544 batch: 184/840\n",
      "Batch loss: 0.5850903987884521 batch: 185/840\n",
      "Batch loss: 0.5880479216575623 batch: 186/840\n",
      "Batch loss: 0.6788926124572754 batch: 187/840\n",
      "Batch loss: 0.5891046524047852 batch: 188/840\n",
      "Batch loss: 0.6280491352081299 batch: 189/840\n",
      "Batch loss: 0.7070273756980896 batch: 190/840\n",
      "Batch loss: 0.8302708268165588 batch: 191/840\n",
      "Batch loss: 0.46264979243278503 batch: 192/840\n",
      "Batch loss: 0.6020762920379639 batch: 193/840\n",
      "Batch loss: 0.5048969388008118 batch: 194/840\n",
      "Batch loss: 0.5906074643135071 batch: 195/840\n",
      "Batch loss: 0.7448968291282654 batch: 196/840\n",
      "Batch loss: 0.6873935461044312 batch: 197/840\n",
      "Batch loss: 0.597726047039032 batch: 198/840\n",
      "Batch loss: 0.6262585520744324 batch: 199/840\n",
      "Batch loss: 0.8702123761177063 batch: 200/840\n",
      "Batch loss: 0.6169610619544983 batch: 201/840\n",
      "Batch loss: 0.6554545760154724 batch: 202/840\n",
      "Batch loss: 0.6052875518798828 batch: 203/840\n",
      "Batch loss: 0.7332398891448975 batch: 204/840\n",
      "Batch loss: 0.7620304226875305 batch: 205/840\n",
      "Batch loss: 0.6752526164054871 batch: 206/840\n",
      "Batch loss: 0.7029845714569092 batch: 207/840\n",
      "Batch loss: 0.5991977453231812 batch: 208/840\n",
      "Batch loss: 0.7156296372413635 batch: 209/840\n",
      "Batch loss: 0.5676149725914001 batch: 210/840\n",
      "Batch loss: 0.5432453751564026 batch: 211/840\n",
      "Batch loss: 0.5930386781692505 batch: 212/840\n",
      "Batch loss: 0.8196274042129517 batch: 213/840\n",
      "Batch loss: 0.8203933238983154 batch: 214/840\n",
      "Batch loss: 0.7130584716796875 batch: 215/840\n",
      "Batch loss: 0.661228358745575 batch: 216/840\n",
      "Batch loss: 0.6874755024909973 batch: 217/840\n",
      "Batch loss: 0.7344672679901123 batch: 218/840\n",
      "Batch loss: 0.6410859823226929 batch: 219/840\n",
      "Batch loss: 0.9800155162811279 batch: 220/840\n",
      "Batch loss: 0.6587973833084106 batch: 221/840\n",
      "Batch loss: 0.8279618620872498 batch: 222/840\n",
      "Batch loss: 0.67839515209198 batch: 223/840\n",
      "Batch loss: 0.8442971110343933 batch: 224/840\n",
      "Batch loss: 0.8393027782440186 batch: 225/840\n",
      "Batch loss: 0.7502449154853821 batch: 226/840\n",
      "Batch loss: 0.7148061990737915 batch: 227/840\n",
      "Batch loss: 0.5006052851676941 batch: 228/840\n",
      "Batch loss: 0.547107994556427 batch: 229/840\n",
      "Batch loss: 0.6287885308265686 batch: 230/840\n",
      "Batch loss: 0.5333742499351501 batch: 231/840\n",
      "Batch loss: 0.6670415997505188 batch: 232/840\n",
      "Batch loss: 0.8235865235328674 batch: 233/840\n",
      "Batch loss: 0.5976186394691467 batch: 234/840\n",
      "Batch loss: 0.6674396991729736 batch: 235/840\n",
      "Batch loss: 0.6411418318748474 batch: 236/840\n",
      "Batch loss: 0.5893219709396362 batch: 237/840\n",
      "Batch loss: 0.7425962090492249 batch: 238/840\n",
      "Batch loss: 0.6890612840652466 batch: 239/840\n",
      "Batch loss: 0.7552255988121033 batch: 240/840\n",
      "Batch loss: 0.6784260272979736 batch: 241/840\n",
      "Batch loss: 0.6638046503067017 batch: 242/840\n",
      "Batch loss: 0.6215224862098694 batch: 243/840\n",
      "Batch loss: 0.7837616801261902 batch: 244/840\n",
      "Batch loss: 0.5373261570930481 batch: 245/840\n",
      "Batch loss: 0.6769661903381348 batch: 246/840\n",
      "Batch loss: 0.7817390561103821 batch: 247/840\n",
      "Batch loss: 0.7233042120933533 batch: 248/840\n",
      "Batch loss: 0.9018840193748474 batch: 249/840\n",
      "Batch loss: 0.5284417867660522 batch: 250/840\n",
      "Batch loss: 0.612044095993042 batch: 251/840\n",
      "Batch loss: 0.6324005722999573 batch: 252/840\n",
      "Batch loss: 0.7046444416046143 batch: 253/840\n",
      "Batch loss: 0.9489901065826416 batch: 254/840\n",
      "Batch loss: 0.6412734389305115 batch: 255/840\n",
      "Batch loss: 0.6348953247070312 batch: 256/840\n",
      "Batch loss: 0.6809638738632202 batch: 257/840\n",
      "Batch loss: 0.8491652011871338 batch: 258/840\n",
      "Batch loss: 0.5439004302024841 batch: 259/840\n",
      "Batch loss: 0.6350505948066711 batch: 260/840\n",
      "Batch loss: 0.5666463971138 batch: 261/840\n",
      "Batch loss: 0.44795623421669006 batch: 262/840\n",
      "Batch loss: 0.6520458459854126 batch: 263/840\n",
      "Batch loss: 0.6282167434692383 batch: 264/840\n",
      "Batch loss: 0.6813735365867615 batch: 265/840\n",
      "Batch loss: 0.6381561756134033 batch: 266/840\n",
      "Batch loss: 0.79027259349823 batch: 267/840\n",
      "Batch loss: 0.6198767423629761 batch: 268/840\n",
      "Batch loss: 0.5281496047973633 batch: 269/840\n",
      "Batch loss: 0.7161064147949219 batch: 270/840\n",
      "Batch loss: 0.5564363598823547 batch: 271/840\n",
      "Batch loss: 0.8222915530204773 batch: 272/840\n",
      "Batch loss: 0.8429687023162842 batch: 273/840\n",
      "Batch loss: 0.7167608737945557 batch: 274/840\n",
      "Batch loss: 0.8018202781677246 batch: 275/840\n",
      "Batch loss: 0.6126463413238525 batch: 276/840\n",
      "Batch loss: 0.6621963977813721 batch: 277/840\n",
      "Batch loss: 0.8906367421150208 batch: 278/840\n",
      "Batch loss: 0.7562804222106934 batch: 279/840\n",
      "Batch loss: 0.7988795638084412 batch: 280/840\n",
      "Batch loss: 0.6189867258071899 batch: 281/840\n",
      "Batch loss: 0.543018102645874 batch: 282/840\n",
      "Batch loss: 0.7002343535423279 batch: 283/840\n",
      "Batch loss: 0.5056979656219482 batch: 284/840\n",
      "Batch loss: 0.6551500558853149 batch: 285/840\n",
      "Batch loss: 0.7698352932929993 batch: 286/840\n",
      "Batch loss: 0.4890199899673462 batch: 287/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5904445052146912 batch: 288/840\n",
      "Batch loss: 0.8545376062393188 batch: 289/840\n",
      "Batch loss: 0.7372949719429016 batch: 290/840\n",
      "Batch loss: 0.7338487505912781 batch: 291/840\n",
      "Batch loss: 0.587815523147583 batch: 292/840\n",
      "Batch loss: 0.8504446148872375 batch: 293/840\n",
      "Batch loss: 0.644611120223999 batch: 294/840\n",
      "Batch loss: 0.4997488856315613 batch: 295/840\n",
      "Batch loss: 0.6419566869735718 batch: 296/840\n",
      "Batch loss: 0.7463201284408569 batch: 297/840\n",
      "Batch loss: 0.6318334341049194 batch: 298/840\n",
      "Batch loss: 0.6352393627166748 batch: 299/840\n",
      "Batch loss: 0.8721566200256348 batch: 300/840\n",
      "Batch loss: 0.7264401316642761 batch: 301/840\n",
      "Batch loss: 0.6743545532226562 batch: 302/840\n",
      "Batch loss: 0.7715914845466614 batch: 303/840\n",
      "Batch loss: 0.6067880988121033 batch: 304/840\n",
      "Batch loss: 0.6194544434547424 batch: 305/840\n",
      "Batch loss: 0.7523966431617737 batch: 306/840\n",
      "Batch loss: 0.599102258682251 batch: 307/840\n",
      "Batch loss: 0.8265693187713623 batch: 308/840\n",
      "Batch loss: 0.660824716091156 batch: 309/840\n",
      "Batch loss: 0.9150470495223999 batch: 310/840\n",
      "Batch loss: 0.8629376292228699 batch: 311/840\n",
      "Batch loss: 0.8302644491195679 batch: 312/840\n",
      "Batch loss: 0.7749472260475159 batch: 313/840\n",
      "Batch loss: 0.5419723391532898 batch: 314/840\n",
      "Batch loss: 0.7515854835510254 batch: 315/840\n",
      "Batch loss: 0.5131895542144775 batch: 316/840\n",
      "Batch loss: 0.7461003065109253 batch: 317/840\n",
      "Batch loss: 0.7572046518325806 batch: 318/840\n",
      "Batch loss: 0.7196505069732666 batch: 319/840\n",
      "Batch loss: 0.639992356300354 batch: 320/840\n",
      "Batch loss: 0.6267759799957275 batch: 321/840\n",
      "Batch loss: 0.7776464223861694 batch: 322/840\n",
      "Batch loss: 0.7618372440338135 batch: 323/840\n",
      "Batch loss: 0.6961045861244202 batch: 324/840\n",
      "Batch loss: 0.6090784668922424 batch: 325/840\n",
      "Batch loss: 0.643649697303772 batch: 326/840\n",
      "Batch loss: 0.5503444671630859 batch: 327/840\n",
      "Batch loss: 0.8593096137046814 batch: 328/840\n",
      "Batch loss: 0.6676755547523499 batch: 329/840\n",
      "Batch loss: 0.7035089135169983 batch: 330/840\n",
      "Batch loss: 0.7207019925117493 batch: 331/840\n",
      "Batch loss: 0.6973122358322144 batch: 332/840\n",
      "Batch loss: 0.5729824900627136 batch: 333/840\n",
      "Batch loss: 0.7677855491638184 batch: 334/840\n",
      "Batch loss: 0.6890727877616882 batch: 335/840\n",
      "Batch loss: 0.7020449042320251 batch: 336/840\n",
      "Batch loss: 0.8913402557373047 batch: 337/840\n",
      "Batch loss: 0.7355833649635315 batch: 338/840\n",
      "Batch loss: 0.6620103716850281 batch: 339/840\n",
      "Batch loss: 0.9778664112091064 batch: 340/840\n",
      "Batch loss: 0.5598682165145874 batch: 341/840\n",
      "Batch loss: 0.5662665963172913 batch: 342/840\n",
      "Batch loss: 0.9832368493080139 batch: 343/840\n",
      "Batch loss: 0.7752060890197754 batch: 344/840\n",
      "Batch loss: 0.47979941964149475 batch: 345/840\n",
      "Batch loss: 0.6048124432563782 batch: 346/840\n",
      "Batch loss: 0.6332291960716248 batch: 347/840\n",
      "Batch loss: 0.6737316250801086 batch: 348/840\n",
      "Batch loss: 0.7276186347007751 batch: 349/840\n",
      "Batch loss: 0.6286462545394897 batch: 350/840\n",
      "Batch loss: 0.7098740339279175 batch: 351/840\n",
      "Batch loss: 0.6303096413612366 batch: 352/840\n",
      "Batch loss: 0.6900155544281006 batch: 353/840\n",
      "Batch loss: 0.6036521792411804 batch: 354/840\n",
      "Batch loss: 0.5811572074890137 batch: 355/840\n",
      "Batch loss: 0.702908992767334 batch: 356/840\n",
      "Batch loss: 0.5464809536933899 batch: 357/840\n",
      "Batch loss: 0.9108471870422363 batch: 358/840\n",
      "Batch loss: 0.582111120223999 batch: 359/840\n",
      "Batch loss: 0.8233972787857056 batch: 360/840\n",
      "Batch loss: 0.8094844222068787 batch: 361/840\n",
      "Batch loss: 0.6437841057777405 batch: 362/840\n",
      "Batch loss: 0.5875324010848999 batch: 363/840\n",
      "Batch loss: 0.7418036460876465 batch: 364/840\n",
      "Batch loss: 0.6796518564224243 batch: 365/840\n",
      "Batch loss: 0.6995059847831726 batch: 366/840\n",
      "Batch loss: 0.4745640158653259 batch: 367/840\n",
      "Batch loss: 0.8555423021316528 batch: 368/840\n",
      "Batch loss: 0.7084084153175354 batch: 369/840\n",
      "Batch loss: 0.8184750080108643 batch: 370/840\n",
      "Batch loss: 0.6607599854469299 batch: 371/840\n",
      "Batch loss: 0.5264657735824585 batch: 372/840\n",
      "Batch loss: 0.7089617252349854 batch: 373/840\n",
      "Batch loss: 0.7412328124046326 batch: 374/840\n",
      "Batch loss: 0.625572919845581 batch: 375/840\n",
      "Batch loss: 0.541475236415863 batch: 376/840\n",
      "Batch loss: 0.690131664276123 batch: 377/840\n",
      "Batch loss: 0.6620076894760132 batch: 378/840\n",
      "Batch loss: 0.5737888813018799 batch: 379/840\n",
      "Batch loss: 0.9482927918434143 batch: 380/840\n",
      "Batch loss: 1.0053647756576538 batch: 381/840\n",
      "Batch loss: 0.7931101322174072 batch: 382/840\n",
      "Batch loss: 0.7719290256500244 batch: 383/840\n",
      "Batch loss: 0.6393359303474426 batch: 384/840\n",
      "Batch loss: 0.7291098237037659 batch: 385/840\n",
      "Batch loss: 0.6941267251968384 batch: 386/840\n",
      "Batch loss: 0.5857675075531006 batch: 387/840\n",
      "Batch loss: 0.7245252728462219 batch: 388/840\n",
      "Batch loss: 0.5602688789367676 batch: 389/840\n",
      "Batch loss: 0.8016747236251831 batch: 390/840\n",
      "Batch loss: 0.6444945335388184 batch: 391/840\n",
      "Batch loss: 0.6086071133613586 batch: 392/840\n",
      "Batch loss: 0.6348661780357361 batch: 393/840\n",
      "Batch loss: 0.7428893446922302 batch: 394/840\n",
      "Batch loss: 0.6571449041366577 batch: 395/840\n",
      "Batch loss: 0.672706663608551 batch: 396/840\n",
      "Batch loss: 0.5485537052154541 batch: 397/840\n",
      "Batch loss: 0.6721324920654297 batch: 398/840\n",
      "Batch loss: 0.4717581272125244 batch: 399/840\n",
      "Batch loss: 0.6154483556747437 batch: 400/840\n",
      "Batch loss: 0.6863588094711304 batch: 401/840\n",
      "Batch loss: 0.5632344484329224 batch: 402/840\n",
      "Batch loss: 0.6025034785270691 batch: 403/840\n",
      "Batch loss: 0.5916458368301392 batch: 404/840\n",
      "Batch loss: 0.6441299915313721 batch: 405/840\n",
      "Batch loss: 0.705671489238739 batch: 406/840\n",
      "Batch loss: 0.6266862750053406 batch: 407/840\n",
      "Batch loss: 0.8234997391700745 batch: 408/840\n",
      "Batch loss: 0.8286640644073486 batch: 409/840\n",
      "Batch loss: 0.8027642965316772 batch: 410/840\n",
      "Batch loss: 0.7851920127868652 batch: 411/840\n",
      "Batch loss: 0.7649288773536682 batch: 412/840\n",
      "Batch loss: 0.7627025842666626 batch: 413/840\n",
      "Batch loss: 0.641562283039093 batch: 414/840\n",
      "Batch loss: 0.8656739592552185 batch: 415/840\n",
      "Batch loss: 0.5269205570220947 batch: 416/840\n",
      "Batch loss: 0.7758761048316956 batch: 417/840\n",
      "Batch loss: 0.830413281917572 batch: 418/840\n",
      "Batch loss: 0.6438229084014893 batch: 419/840\n",
      "Batch loss: 0.7373525500297546 batch: 420/840\n",
      "Batch loss: 0.5737909078598022 batch: 421/840\n",
      "Batch loss: 0.6246374845504761 batch: 422/840\n",
      "Batch loss: 0.7294979691505432 batch: 423/840\n",
      "Batch loss: 0.7270993590354919 batch: 424/840\n",
      "Batch loss: 0.7620376348495483 batch: 425/840\n",
      "Batch loss: 0.6804660558700562 batch: 426/840\n",
      "Batch loss: 0.6379976868629456 batch: 427/840\n",
      "Batch loss: 0.7744386196136475 batch: 428/840\n",
      "Batch loss: 0.6585944294929504 batch: 429/840\n",
      "Batch loss: 0.8793081045150757 batch: 430/840\n",
      "Batch loss: 0.5939826369285583 batch: 431/840\n",
      "Batch loss: 0.6840217113494873 batch: 432/840\n",
      "Batch loss: 0.46089237928390503 batch: 433/840\n",
      "Batch loss: 0.6013216972351074 batch: 434/840\n",
      "Batch loss: 0.791527271270752 batch: 435/840\n",
      "Batch loss: 0.6470206379890442 batch: 436/840\n",
      "Batch loss: 0.8112750053405762 batch: 437/840\n",
      "Batch loss: 0.45820748805999756 batch: 438/840\n",
      "Batch loss: 0.6521183252334595 batch: 439/840\n",
      "Batch loss: 0.7629374861717224 batch: 440/840\n",
      "Batch loss: 0.7105911374092102 batch: 441/840\n",
      "Batch loss: 0.7835143804550171 batch: 442/840\n",
      "Batch loss: 0.6698861718177795 batch: 443/840\n",
      "Batch loss: 0.795269250869751 batch: 444/840\n",
      "Batch loss: 0.7070181965827942 batch: 445/840\n",
      "Batch loss: 0.7964733242988586 batch: 446/840\n",
      "Batch loss: 0.666452944278717 batch: 447/840\n",
      "Batch loss: 0.6693607568740845 batch: 448/840\n",
      "Batch loss: 0.5406451225280762 batch: 449/840\n",
      "Batch loss: 0.6825119853019714 batch: 450/840\n",
      "Batch loss: 0.6978782415390015 batch: 451/840\n",
      "Batch loss: 0.7754018902778625 batch: 452/840\n",
      "Batch loss: 0.7421532273292542 batch: 453/840\n",
      "Batch loss: 0.5887036323547363 batch: 454/840\n",
      "Batch loss: 0.8021400570869446 batch: 455/840\n",
      "Batch loss: 0.5683062076568604 batch: 456/840\n",
      "Batch loss: 0.6911169290542603 batch: 457/840\n",
      "Batch loss: 0.676493227481842 batch: 458/840\n",
      "Batch loss: 0.5187962651252747 batch: 459/840\n",
      "Batch loss: 0.5082948803901672 batch: 460/840\n",
      "Batch loss: 0.7981167435646057 batch: 461/840\n",
      "Batch loss: 0.6494214534759521 batch: 462/840\n",
      "Batch loss: 0.8252214193344116 batch: 463/840\n",
      "Batch loss: 0.5893120169639587 batch: 464/840\n",
      "Batch loss: 0.9655330181121826 batch: 465/840\n",
      "Batch loss: 0.7415111064910889 batch: 466/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.9584640264511108 batch: 467/840\n",
      "Batch loss: 0.6751030683517456 batch: 468/840\n",
      "Batch loss: 0.5632038116455078 batch: 469/840\n",
      "Batch loss: 0.6779122948646545 batch: 470/840\n",
      "Batch loss: 0.6806524395942688 batch: 471/840\n",
      "Batch loss: 0.7061455249786377 batch: 472/840\n",
      "Batch loss: 0.8148102760314941 batch: 473/840\n",
      "Batch loss: 0.6753948330879211 batch: 474/840\n",
      "Batch loss: 0.7666313648223877 batch: 475/840\n",
      "Batch loss: 0.6829323768615723 batch: 476/840\n",
      "Batch loss: 0.5460050702095032 batch: 477/840\n",
      "Batch loss: 0.5037806034088135 batch: 478/840\n",
      "Batch loss: 0.5709492564201355 batch: 479/840\n",
      "Batch loss: 0.6975857615470886 batch: 480/840\n",
      "Batch loss: 0.6135264039039612 batch: 481/840\n",
      "Batch loss: 0.7064226269721985 batch: 482/840\n",
      "Batch loss: 0.8125025033950806 batch: 483/840\n",
      "Batch loss: 0.6295955181121826 batch: 484/840\n",
      "Batch loss: 0.7056680917739868 batch: 485/840\n",
      "Batch loss: 0.65228271484375 batch: 486/840\n",
      "Batch loss: 0.5058897137641907 batch: 487/840\n",
      "Batch loss: 0.6829477548599243 batch: 488/840\n",
      "Batch loss: 0.6367775201797485 batch: 489/840\n",
      "Batch loss: 0.75360107421875 batch: 490/840\n",
      "Batch loss: 0.4096617102622986 batch: 491/840\n",
      "Batch loss: 0.6956496238708496 batch: 492/840\n",
      "Batch loss: 0.8177298903465271 batch: 493/840\n",
      "Batch loss: 0.44891321659088135 batch: 494/840\n",
      "Batch loss: 0.752946138381958 batch: 495/840\n",
      "Batch loss: 0.7009569406509399 batch: 496/840\n",
      "Batch loss: 0.7487439513206482 batch: 497/840\n",
      "Batch loss: 0.5231101512908936 batch: 498/840\n",
      "Batch loss: 0.6768803596496582 batch: 499/840\n",
      "Batch loss: 0.6220065951347351 batch: 500/840\n",
      "Batch loss: 0.5847339034080505 batch: 501/840\n",
      "Batch loss: 0.7798440456390381 batch: 502/840\n",
      "Batch loss: 0.5213668942451477 batch: 503/840\n",
      "Batch loss: 0.6811915040016174 batch: 504/840\n",
      "Batch loss: 0.7675828337669373 batch: 505/840\n",
      "Batch loss: 0.6482070088386536 batch: 506/840\n",
      "Batch loss: 0.7779091596603394 batch: 507/840\n",
      "Batch loss: 0.5793027877807617 batch: 508/840\n",
      "Batch loss: 0.6584389209747314 batch: 509/840\n",
      "Batch loss: 0.6792506575584412 batch: 510/840\n",
      "Batch loss: 0.5684774518013 batch: 511/840\n",
      "Batch loss: 0.660030722618103 batch: 512/840\n",
      "Batch loss: 0.5678296685218811 batch: 513/840\n",
      "Batch loss: 0.7858923077583313 batch: 514/840\n",
      "Batch loss: 0.45554423332214355 batch: 515/840\n",
      "Batch loss: 0.8053213357925415 batch: 516/840\n",
      "Batch loss: 0.6632695198059082 batch: 517/840\n",
      "Batch loss: 0.6903179883956909 batch: 518/840\n",
      "Batch loss: 0.8299355506896973 batch: 519/840\n",
      "Batch loss: 0.7512891888618469 batch: 520/840\n",
      "Batch loss: 0.8784910440444946 batch: 521/840\n",
      "Batch loss: 0.6754671335220337 batch: 522/840\n",
      "Batch loss: 0.5320752859115601 batch: 523/840\n",
      "Batch loss: 0.7175873517990112 batch: 524/840\n",
      "Batch loss: 0.6512175798416138 batch: 525/840\n",
      "Batch loss: 0.7313984036445618 batch: 526/840\n",
      "Batch loss: 0.6676557064056396 batch: 527/840\n",
      "Batch loss: 0.6464194655418396 batch: 528/840\n",
      "Batch loss: 0.6980313062667847 batch: 529/840\n",
      "Batch loss: 0.7326853275299072 batch: 530/840\n",
      "Batch loss: 0.6853725910186768 batch: 531/840\n",
      "Batch loss: 0.4578430652618408 batch: 532/840\n",
      "Batch loss: 0.645962119102478 batch: 533/840\n",
      "Batch loss: 0.6407473087310791 batch: 534/840\n",
      "Batch loss: 0.6277467012405396 batch: 535/840\n",
      "Batch loss: 0.6562480926513672 batch: 536/840\n",
      "Batch loss: 0.6141652464866638 batch: 537/840\n",
      "Batch loss: 0.6008147597312927 batch: 538/840\n",
      "Batch loss: 0.6748085021972656 batch: 539/840\n",
      "Batch loss: 0.825217068195343 batch: 540/840\n",
      "Batch loss: 0.6378690004348755 batch: 541/840\n",
      "Batch loss: 0.7282984256744385 batch: 542/840\n",
      "Batch loss: 0.4513382613658905 batch: 543/840\n",
      "Batch loss: 0.6461004018783569 batch: 544/840\n",
      "Batch loss: 0.7862672209739685 batch: 545/840\n",
      "Batch loss: 0.6193813681602478 batch: 546/840\n",
      "Batch loss: 0.7189417481422424 batch: 547/840\n",
      "Batch loss: 0.5139961838722229 batch: 548/840\n",
      "Batch loss: 0.6298730969429016 batch: 549/840\n",
      "Batch loss: 0.5331751704216003 batch: 550/840\n",
      "Batch loss: 0.8083446621894836 batch: 551/840\n",
      "Batch loss: 0.560539960861206 batch: 552/840\n",
      "Batch loss: 0.7165554761886597 batch: 553/840\n",
      "Batch loss: 0.7062100768089294 batch: 554/840\n",
      "Batch loss: 0.5826693177223206 batch: 555/840\n",
      "Batch loss: 0.6997038125991821 batch: 556/840\n",
      "Batch loss: 0.6986414194107056 batch: 557/840\n",
      "Batch loss: 0.7054933905601501 batch: 558/840\n",
      "Batch loss: 0.702338695526123 batch: 559/840\n",
      "Batch loss: 0.7031010389328003 batch: 560/840\n",
      "Batch loss: 0.6747907996177673 batch: 561/840\n",
      "Batch loss: 0.6387931108474731 batch: 562/840\n",
      "Batch loss: 0.5175350904464722 batch: 563/840\n",
      "Batch loss: 0.7304028272628784 batch: 564/840\n",
      "Batch loss: 0.7472683191299438 batch: 565/840\n",
      "Batch loss: 0.7583571672439575 batch: 566/840\n",
      "Batch loss: 0.674254834651947 batch: 567/840\n",
      "Batch loss: 0.642535924911499 batch: 568/840\n",
      "Batch loss: 0.6230875253677368 batch: 569/840\n",
      "Batch loss: 0.4600975811481476 batch: 570/840\n",
      "Batch loss: 0.7954020500183105 batch: 571/840\n",
      "Batch loss: 0.7777830362319946 batch: 572/840\n",
      "Batch loss: 0.7194681763648987 batch: 573/840\n",
      "Batch loss: 0.7835903763771057 batch: 574/840\n",
      "Batch loss: 0.4929087162017822 batch: 575/840\n",
      "Batch loss: 0.6718025803565979 batch: 576/840\n",
      "Batch loss: 0.5351783633232117 batch: 577/840\n",
      "Batch loss: 0.9020559787750244 batch: 578/840\n",
      "Batch loss: 0.719678521156311 batch: 579/840\n",
      "Batch loss: 0.8700887560844421 batch: 580/840\n",
      "Batch loss: 0.7624561190605164 batch: 581/840\n",
      "Batch loss: 0.921083927154541 batch: 582/840\n",
      "Batch loss: 0.6436145305633545 batch: 583/840\n",
      "Batch loss: 0.8267760276794434 batch: 584/840\n",
      "Batch loss: 0.5727982521057129 batch: 585/840\n",
      "Batch loss: 0.8010988831520081 batch: 586/840\n",
      "Batch loss: 0.6353273987770081 batch: 587/840\n",
      "Batch loss: 0.6832774877548218 batch: 588/840\n",
      "Batch loss: 0.8563888669013977 batch: 589/840\n",
      "Batch loss: 0.6391000747680664 batch: 590/840\n",
      "Batch loss: 0.5246423482894897 batch: 591/840\n",
      "Batch loss: 0.5344951748847961 batch: 592/840\n",
      "Batch loss: 0.8433985114097595 batch: 593/840\n",
      "Batch loss: 0.6938533782958984 batch: 594/840\n",
      "Batch loss: 0.4371657073497772 batch: 595/840\n",
      "Batch loss: 0.525175929069519 batch: 596/840\n",
      "Batch loss: 0.7126699686050415 batch: 597/840\n",
      "Batch loss: 0.5696234107017517 batch: 598/840\n",
      "Batch loss: 0.5849857926368713 batch: 599/840\n",
      "Batch loss: 0.7818862795829773 batch: 600/840\n",
      "Batch loss: 0.8254173398017883 batch: 601/840\n",
      "Batch loss: 0.6824988722801208 batch: 602/840\n",
      "Batch loss: 0.7506749033927917 batch: 603/840\n",
      "Batch loss: 0.6945915222167969 batch: 604/840\n",
      "Batch loss: 0.8049057722091675 batch: 605/840\n",
      "Batch loss: 0.8220254778862 batch: 606/840\n",
      "Batch loss: 0.7058334946632385 batch: 607/840\n",
      "Batch loss: 0.6531522274017334 batch: 608/840\n",
      "Batch loss: 0.554485559463501 batch: 609/840\n",
      "Batch loss: 0.795515239238739 batch: 610/840\n",
      "Batch loss: 0.7011276483535767 batch: 611/840\n",
      "Batch loss: 0.783488392829895 batch: 612/840\n",
      "Batch loss: 0.7498406767845154 batch: 613/840\n",
      "Batch loss: 0.6739577651023865 batch: 614/840\n",
      "Batch loss: 0.6436756253242493 batch: 615/840\n",
      "Batch loss: 0.5616018772125244 batch: 616/840\n",
      "Batch loss: 0.5462725162506104 batch: 617/840\n",
      "Batch loss: 0.7685183882713318 batch: 618/840\n",
      "Batch loss: 0.8348039388656616 batch: 619/840\n",
      "Batch loss: 0.6269638538360596 batch: 620/840\n",
      "Batch loss: 0.7014845013618469 batch: 621/840\n",
      "Batch loss: 0.7400104999542236 batch: 622/840\n",
      "Batch loss: 0.625187337398529 batch: 623/840\n",
      "Batch loss: 0.8074169158935547 batch: 624/840\n",
      "Batch loss: 0.7010225057601929 batch: 625/840\n",
      "Batch loss: 0.7309373617172241 batch: 626/840\n",
      "Batch loss: 0.6489055156707764 batch: 627/840\n",
      "Batch loss: 0.7446125745773315 batch: 628/840\n",
      "Batch loss: 0.6928624510765076 batch: 629/840\n",
      "Batch loss: 0.6657309532165527 batch: 630/840\n",
      "Batch loss: 0.7815794348716736 batch: 631/840\n",
      "Batch loss: 0.6903691291809082 batch: 632/840\n",
      "Batch loss: 0.5951265096664429 batch: 633/840\n",
      "Batch loss: 0.7602951526641846 batch: 634/840\n",
      "Batch loss: 0.605158805847168 batch: 635/840\n",
      "Batch loss: 0.7629391551017761 batch: 636/840\n",
      "Batch loss: 0.5684154033660889 batch: 637/840\n",
      "Batch loss: 0.7616546750068665 batch: 638/840\n",
      "Batch loss: 0.6860383749008179 batch: 639/840\n",
      "Batch loss: 0.6143381595611572 batch: 640/840\n",
      "Batch loss: 0.8384448289871216 batch: 641/840\n",
      "Batch loss: 0.6158287525177002 batch: 642/840\n",
      "Batch loss: 1.1253068447113037 batch: 643/840\n",
      "Batch loss: 0.7089605927467346 batch: 644/840\n",
      "Batch loss: 0.5701092481613159 batch: 645/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6532209515571594 batch: 646/840\n",
      "Batch loss: 0.789581298828125 batch: 647/840\n",
      "Batch loss: 0.6360241770744324 batch: 648/840\n",
      "Batch loss: 0.646595299243927 batch: 649/840\n",
      "Batch loss: 0.5973663926124573 batch: 650/840\n",
      "Batch loss: 0.712001621723175 batch: 651/840\n",
      "Batch loss: 0.7650576233863831 batch: 652/840\n",
      "Batch loss: 0.6351963877677917 batch: 653/840\n",
      "Batch loss: 0.878908634185791 batch: 654/840\n",
      "Batch loss: 0.6146227717399597 batch: 655/840\n",
      "Batch loss: 0.633087158203125 batch: 656/840\n",
      "Batch loss: 0.6378815174102783 batch: 657/840\n",
      "Batch loss: 0.7844903469085693 batch: 658/840\n",
      "Batch loss: 0.8119395971298218 batch: 659/840\n",
      "Batch loss: 0.49511516094207764 batch: 660/840\n",
      "Batch loss: 0.7709668874740601 batch: 661/840\n",
      "Batch loss: 0.7787835001945496 batch: 662/840\n",
      "Batch loss: 0.618738055229187 batch: 663/840\n",
      "Batch loss: 0.7101131677627563 batch: 664/840\n",
      "Batch loss: 0.8232844471931458 batch: 665/840\n",
      "Batch loss: 0.7267553210258484 batch: 666/840\n",
      "Batch loss: 0.7803328633308411 batch: 667/840\n",
      "Batch loss: 0.6141418218612671 batch: 668/840\n",
      "Batch loss: 0.7186586260795593 batch: 669/840\n",
      "Batch loss: 0.7901802659034729 batch: 670/840\n",
      "Batch loss: 0.7055351734161377 batch: 671/840\n",
      "Batch loss: 0.5935698747634888 batch: 672/840\n",
      "Batch loss: 0.7925467491149902 batch: 673/840\n",
      "Batch loss: 0.8145984411239624 batch: 674/840\n",
      "Batch loss: 0.7195038795471191 batch: 675/840\n",
      "Batch loss: 0.6430642604827881 batch: 676/840\n",
      "Batch loss: 0.669167697429657 batch: 677/840\n",
      "Batch loss: 0.7183033227920532 batch: 678/840\n",
      "Batch loss: 0.7923975586891174 batch: 679/840\n",
      "Batch loss: 0.6826640367507935 batch: 680/840\n",
      "Batch loss: 0.7219721078872681 batch: 681/840\n",
      "Batch loss: 0.6071663498878479 batch: 682/840\n",
      "Batch loss: 0.5542818307876587 batch: 683/840\n",
      "Batch loss: 0.8805963397026062 batch: 684/840\n",
      "Batch loss: 0.7343411445617676 batch: 685/840\n",
      "Batch loss: 0.7805081009864807 batch: 686/840\n",
      "Batch loss: 0.8436467051506042 batch: 687/840\n",
      "Batch loss: 0.6721456050872803 batch: 688/840\n",
      "Batch loss: 0.545055091381073 batch: 689/840\n",
      "Batch loss: 0.5779061913490295 batch: 690/840\n",
      "Batch loss: 0.7198444604873657 batch: 691/840\n",
      "Batch loss: 0.6548673510551453 batch: 692/840\n",
      "Batch loss: 0.6150044798851013 batch: 693/840\n",
      "Batch loss: 0.8867434859275818 batch: 694/840\n",
      "Batch loss: 0.766560971736908 batch: 695/840\n",
      "Batch loss: 0.6803909540176392 batch: 696/840\n",
      "Batch loss: 0.617820680141449 batch: 697/840\n",
      "Batch loss: 0.6619570255279541 batch: 698/840\n",
      "Batch loss: 0.6902887225151062 batch: 699/840\n",
      "Batch loss: 0.767862856388092 batch: 700/840\n",
      "Batch loss: 0.8173931241035461 batch: 701/840\n",
      "Batch loss: 0.6454843878746033 batch: 702/840\n",
      "Batch loss: 0.6969179511070251 batch: 703/840\n",
      "Batch loss: 0.7868345379829407 batch: 704/840\n",
      "Batch loss: 0.6581979393959045 batch: 705/840\n",
      "Batch loss: 0.6888158321380615 batch: 706/840\n",
      "Batch loss: 0.7417828440666199 batch: 707/840\n",
      "Batch loss: 0.7146010398864746 batch: 708/840\n",
      "Batch loss: 0.8363426327705383 batch: 709/840\n",
      "Batch loss: 0.8125569820404053 batch: 710/840\n",
      "Batch loss: 0.4634772539138794 batch: 711/840\n",
      "Batch loss: 0.7446786761283875 batch: 712/840\n",
      "Batch loss: 0.6335785984992981 batch: 713/840\n",
      "Batch loss: 0.6388753056526184 batch: 714/840\n",
      "Batch loss: 0.7643426656723022 batch: 715/840\n",
      "Batch loss: 0.7191261053085327 batch: 716/840\n",
      "Batch loss: 0.7020911574363708 batch: 717/840\n",
      "Batch loss: 0.6115820407867432 batch: 718/840\n",
      "Batch loss: 0.5581796765327454 batch: 719/840\n",
      "Batch loss: 0.5977510213851929 batch: 720/840\n",
      "Batch loss: 0.8213948011398315 batch: 721/840\n",
      "Batch loss: 0.7955420017242432 batch: 722/840\n",
      "Batch loss: 0.589901864528656 batch: 723/840\n",
      "Batch loss: 0.7304213047027588 batch: 724/840\n",
      "Batch loss: 0.6878179907798767 batch: 725/840\n",
      "Batch loss: 0.7057885527610779 batch: 726/840\n",
      "Batch loss: 0.78778475522995 batch: 727/840\n",
      "Batch loss: 0.8471944332122803 batch: 728/840\n",
      "Batch loss: 0.663964033126831 batch: 729/840\n",
      "Batch loss: 0.6210848093032837 batch: 730/840\n",
      "Batch loss: 0.6142894625663757 batch: 731/840\n",
      "Batch loss: 0.8397148251533508 batch: 732/840\n",
      "Batch loss: 0.6541348099708557 batch: 733/840\n",
      "Batch loss: 0.6300913691520691 batch: 734/840\n",
      "Batch loss: 0.7098459005355835 batch: 735/840\n",
      "Batch loss: 0.7155582904815674 batch: 736/840\n",
      "Batch loss: 0.7054893374443054 batch: 737/840\n",
      "Batch loss: 0.5427650213241577 batch: 738/840\n",
      "Batch loss: 0.7526225447654724 batch: 739/840\n",
      "Batch loss: 0.6948500871658325 batch: 740/840\n",
      "Batch loss: 0.5597883462905884 batch: 741/840\n",
      "Batch loss: 0.5738098621368408 batch: 742/840\n",
      "Batch loss: 0.48201456665992737 batch: 743/840\n",
      "Batch loss: 0.5195892453193665 batch: 744/840\n",
      "Batch loss: 0.5957646369934082 batch: 745/840\n",
      "Batch loss: 0.5575375556945801 batch: 746/840\n",
      "Batch loss: 0.7456523180007935 batch: 747/840\n",
      "Batch loss: 0.7320849895477295 batch: 748/840\n",
      "Batch loss: 0.533130407333374 batch: 749/840\n",
      "Batch loss: 0.5691654086112976 batch: 750/840\n",
      "Batch loss: 0.8970606327056885 batch: 751/840\n",
      "Batch loss: 0.8239104747772217 batch: 752/840\n",
      "Batch loss: 0.6469030976295471 batch: 753/840\n",
      "Batch loss: 0.7272319197654724 batch: 754/840\n",
      "Batch loss: 0.7103757262229919 batch: 755/840\n",
      "Batch loss: 0.6257691383361816 batch: 756/840\n",
      "Batch loss: 0.7446935772895813 batch: 757/840\n",
      "Batch loss: 0.6622567176818848 batch: 758/840\n",
      "Batch loss: 0.5843033194541931 batch: 759/840\n",
      "Batch loss: 0.5984998941421509 batch: 760/840\n",
      "Batch loss: 0.5821872353553772 batch: 761/840\n",
      "Batch loss: 0.7869553565979004 batch: 762/840\n",
      "Batch loss: 0.4388342797756195 batch: 763/840\n",
      "Batch loss: 0.6530168056488037 batch: 764/840\n",
      "Batch loss: 0.5021260976791382 batch: 765/840\n",
      "Batch loss: 0.6228669881820679 batch: 766/840\n",
      "Batch loss: 0.7333948612213135 batch: 767/840\n",
      "Batch loss: 0.8017477989196777 batch: 768/840\n",
      "Batch loss: 0.6686837673187256 batch: 769/840\n",
      "Batch loss: 0.6044396162033081 batch: 770/840\n",
      "Batch loss: 0.6780687570571899 batch: 771/840\n",
      "Batch loss: 0.6125106811523438 batch: 772/840\n",
      "Batch loss: 0.5096156597137451 batch: 773/840\n",
      "Batch loss: 0.4799736440181732 batch: 774/840\n",
      "Batch loss: 0.5806646943092346 batch: 775/840\n",
      "Batch loss: 0.5639837384223938 batch: 776/840\n",
      "Batch loss: 0.6042894124984741 batch: 777/840\n",
      "Batch loss: 0.46402865648269653 batch: 778/840\n",
      "Batch loss: 0.7100027203559875 batch: 779/840\n",
      "Batch loss: 0.5452606081962585 batch: 780/840\n",
      "Batch loss: 0.6777303218841553 batch: 781/840\n",
      "Batch loss: 0.6783962845802307 batch: 782/840\n",
      "Batch loss: 0.5760607719421387 batch: 783/840\n",
      "Batch loss: 0.7369829416275024 batch: 784/840\n",
      "Batch loss: 0.6006405353546143 batch: 785/840\n",
      "Batch loss: 0.71966552734375 batch: 786/840\n",
      "Batch loss: 0.5446161031723022 batch: 787/840\n",
      "Batch loss: 0.8335806131362915 batch: 788/840\n",
      "Batch loss: 0.7812325954437256 batch: 789/840\n",
      "Batch loss: 0.8055216073989868 batch: 790/840\n",
      "Batch loss: 0.6355195045471191 batch: 791/840\n",
      "Batch loss: 0.41982653737068176 batch: 792/840\n",
      "Batch loss: 0.6673406958580017 batch: 793/840\n",
      "Batch loss: 0.6365672945976257 batch: 794/840\n",
      "Batch loss: 0.6972249746322632 batch: 795/840\n",
      "Batch loss: 0.8153803944587708 batch: 796/840\n",
      "Batch loss: 0.8093570470809937 batch: 797/840\n",
      "Batch loss: 0.7239332795143127 batch: 798/840\n",
      "Batch loss: 0.6576796174049377 batch: 799/840\n",
      "Batch loss: 0.5656439065933228 batch: 800/840\n",
      "Batch loss: 0.8273488879203796 batch: 801/840\n",
      "Batch loss: 0.49882829189300537 batch: 802/840\n",
      "Batch loss: 0.546786904335022 batch: 803/840\n",
      "Batch loss: 0.7449720501899719 batch: 804/840\n",
      "Batch loss: 0.5465046763420105 batch: 805/840\n",
      "Batch loss: 0.6408942937850952 batch: 806/840\n",
      "Batch loss: 0.6785414814949036 batch: 807/840\n",
      "Batch loss: 0.6913799047470093 batch: 808/840\n",
      "Batch loss: 0.6287401914596558 batch: 809/840\n",
      "Batch loss: 0.6139675974845886 batch: 810/840\n",
      "Batch loss: 0.5884485244750977 batch: 811/840\n",
      "Batch loss: 0.6705777645111084 batch: 812/840\n",
      "Batch loss: 0.5483236312866211 batch: 813/840\n",
      "Batch loss: 0.5836105942726135 batch: 814/840\n",
      "Batch loss: 0.7182996869087219 batch: 815/840\n",
      "Batch loss: 0.7181693911552429 batch: 816/840\n",
      "Batch loss: 0.813944399356842 batch: 817/840\n",
      "Batch loss: 0.6490886211395264 batch: 818/840\n",
      "Batch loss: 0.6792826056480408 batch: 819/840\n",
      "Batch loss: 0.7148258686065674 batch: 820/840\n",
      "Batch loss: 0.5971429944038391 batch: 821/840\n",
      "Batch loss: 0.7128568887710571 batch: 822/840\n",
      "Batch loss: 0.7320404052734375 batch: 823/840\n",
      "Batch loss: 0.7934302687644958 batch: 824/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7179213166236877 batch: 825/840\n",
      "Batch loss: 0.6002063751220703 batch: 826/840\n",
      "Batch loss: 0.6432604789733887 batch: 827/840\n",
      "Batch loss: 0.8179863095283508 batch: 828/840\n",
      "Batch loss: 0.6113344430923462 batch: 829/840\n",
      "Batch loss: 0.7261989712715149 batch: 830/840\n",
      "Batch loss: 0.5813494324684143 batch: 831/840\n",
      "Batch loss: 0.7723399996757507 batch: 832/840\n",
      "Batch loss: 0.7425729632377625 batch: 833/840\n",
      "Batch loss: 0.7208622694015503 batch: 834/840\n",
      "Batch loss: 0.5306658744812012 batch: 835/840\n",
      "Batch loss: 0.6025689840316772 batch: 836/840\n",
      "Batch loss: 0.7683371901512146 batch: 837/840\n",
      "Batch loss: 0.7025558948516846 batch: 838/840\n",
      "Batch loss: 0.6042853593826294 batch: 839/840\n",
      "Batch loss: 0.7173480987548828 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 3/15..  Training Loss: 0.007..  Test Loss: 0.005..  Test Accuracy: 0.814\n",
      "Running epoch 4/15\n",
      "Batch loss: 0.5206408500671387 batch: 1/840\n",
      "Batch loss: 1.11375892162323 batch: 2/840\n",
      "Batch loss: 0.7229720950126648 batch: 3/840\n",
      "Batch loss: 0.6693472266197205 batch: 4/840\n",
      "Batch loss: 0.6335413455963135 batch: 5/840\n",
      "Batch loss: 0.4929981529712677 batch: 6/840\n",
      "Batch loss: 0.7075619697570801 batch: 7/840\n",
      "Batch loss: 0.6048402190208435 batch: 8/840\n",
      "Batch loss: 0.5736889839172363 batch: 9/840\n",
      "Batch loss: 0.6623403429985046 batch: 10/840\n",
      "Batch loss: 0.6530306935310364 batch: 11/840\n",
      "Batch loss: 0.6672430634498596 batch: 12/840\n",
      "Batch loss: 0.7083922624588013 batch: 13/840\n",
      "Batch loss: 0.7160241603851318 batch: 14/840\n",
      "Batch loss: 0.6263395547866821 batch: 15/840\n",
      "Batch loss: 0.5253153443336487 batch: 16/840\n",
      "Batch loss: 0.5486098527908325 batch: 17/840\n",
      "Batch loss: 0.7259644865989685 batch: 18/840\n",
      "Batch loss: 0.8387373089790344 batch: 19/840\n",
      "Batch loss: 0.8206789493560791 batch: 20/840\n",
      "Batch loss: 0.7721521854400635 batch: 21/840\n",
      "Batch loss: 0.6385712623596191 batch: 22/840\n",
      "Batch loss: 0.6569602489471436 batch: 23/840\n",
      "Batch loss: 0.568686842918396 batch: 24/840\n",
      "Batch loss: 0.6213878393173218 batch: 25/840\n",
      "Batch loss: 0.7740215063095093 batch: 26/840\n",
      "Batch loss: 0.6938268542289734 batch: 27/840\n",
      "Batch loss: 0.7019743323326111 batch: 28/840\n",
      "Batch loss: 0.5875966548919678 batch: 29/840\n",
      "Batch loss: 0.5387950539588928 batch: 30/840\n",
      "Batch loss: 0.6023010015487671 batch: 31/840\n",
      "Batch loss: 0.5847218632698059 batch: 32/840\n",
      "Batch loss: 0.6635276675224304 batch: 33/840\n",
      "Batch loss: 0.567918598651886 batch: 34/840\n",
      "Batch loss: 0.6072606444358826 batch: 35/840\n",
      "Batch loss: 0.5847804546356201 batch: 36/840\n",
      "Batch loss: 0.8173204064369202 batch: 37/840\n",
      "Batch loss: 0.7274540662765503 batch: 38/840\n",
      "Batch loss: 0.6967854499816895 batch: 39/840\n",
      "Batch loss: 0.5763232707977295 batch: 40/840\n",
      "Batch loss: 0.7586778998374939 batch: 41/840\n",
      "Batch loss: 0.7768921852111816 batch: 42/840\n",
      "Batch loss: 0.7197971343994141 batch: 43/840\n",
      "Batch loss: 0.6889934539794922 batch: 44/840\n",
      "Batch loss: 0.820767343044281 batch: 45/840\n",
      "Batch loss: 0.6296344995498657 batch: 46/840\n",
      "Batch loss: 0.5614907145500183 batch: 47/840\n",
      "Batch loss: 0.7069549560546875 batch: 48/840\n",
      "Batch loss: 0.6513507962226868 batch: 49/840\n",
      "Batch loss: 0.6906310319900513 batch: 50/840\n",
      "Batch loss: 0.7349098920822144 batch: 51/840\n",
      "Batch loss: 0.7978430390357971 batch: 52/840\n",
      "Batch loss: 0.6091529726982117 batch: 53/840\n",
      "Batch loss: 0.6641615033149719 batch: 54/840\n",
      "Batch loss: 0.6934677362442017 batch: 55/840\n",
      "Batch loss: 0.6823491454124451 batch: 56/840\n",
      "Batch loss: 0.7066705226898193 batch: 57/840\n",
      "Batch loss: 0.5718010067939758 batch: 58/840\n",
      "Batch loss: 0.5725207328796387 batch: 59/840\n",
      "Batch loss: 0.5746898651123047 batch: 60/840\n",
      "Batch loss: 0.7981194853782654 batch: 61/840\n",
      "Batch loss: 0.7348024845123291 batch: 62/840\n",
      "Batch loss: 0.6527225971221924 batch: 63/840\n",
      "Batch loss: 0.6650065779685974 batch: 64/840\n",
      "Batch loss: 0.6457895636558533 batch: 65/840\n",
      "Batch loss: 0.6629142165184021 batch: 66/840\n",
      "Batch loss: 0.7476174831390381 batch: 67/840\n",
      "Batch loss: 0.7167757153511047 batch: 68/840\n",
      "Batch loss: 0.7155864834785461 batch: 69/840\n",
      "Batch loss: 0.6099184155464172 batch: 70/840\n",
      "Batch loss: 0.7511225938796997 batch: 71/840\n",
      "Batch loss: 0.7569541931152344 batch: 72/840\n",
      "Batch loss: 0.6737692356109619 batch: 73/840\n",
      "Batch loss: 0.6281631588935852 batch: 74/840\n",
      "Batch loss: 0.7198997735977173 batch: 75/840\n",
      "Batch loss: 0.4590536952018738 batch: 76/840\n",
      "Batch loss: 0.720604658126831 batch: 77/840\n",
      "Batch loss: 0.8442906141281128 batch: 78/840\n",
      "Batch loss: 0.6806983351707458 batch: 79/840\n",
      "Batch loss: 0.6521221995353699 batch: 80/840\n",
      "Batch loss: 0.5743758082389832 batch: 81/840\n",
      "Batch loss: 0.7165048122406006 batch: 82/840\n",
      "Batch loss: 0.6693659424781799 batch: 83/840\n",
      "Batch loss: 0.7471780180931091 batch: 84/840\n",
      "Batch loss: 0.5937157273292542 batch: 85/840\n",
      "Batch loss: 0.9011015295982361 batch: 86/840\n",
      "Batch loss: 0.5617426633834839 batch: 87/840\n",
      "Batch loss: 0.542584240436554 batch: 88/840\n",
      "Batch loss: 0.4893237054347992 batch: 89/840\n",
      "Batch loss: 0.5591933727264404 batch: 90/840\n",
      "Batch loss: 0.6090479493141174 batch: 91/840\n",
      "Batch loss: 0.6979056596755981 batch: 92/840\n",
      "Batch loss: 0.786608099937439 batch: 93/840\n",
      "Batch loss: 0.6056075096130371 batch: 94/840\n",
      "Batch loss: 0.7184961438179016 batch: 95/840\n",
      "Batch loss: 0.6897013783454895 batch: 96/840\n",
      "Batch loss: 0.6598894596099854 batch: 97/840\n",
      "Batch loss: 0.7872051000595093 batch: 98/840\n",
      "Batch loss: 0.5638586282730103 batch: 99/840\n",
      "Batch loss: 0.7395327687263489 batch: 100/840\n",
      "Batch loss: 0.5894121527671814 batch: 101/840\n",
      "Batch loss: 0.5259002447128296 batch: 102/840\n",
      "Batch loss: 0.6663440465927124 batch: 103/840\n",
      "Batch loss: 0.577675998210907 batch: 104/840\n",
      "Batch loss: 0.6512499451637268 batch: 105/840\n",
      "Batch loss: 0.6330211162567139 batch: 106/840\n",
      "Batch loss: 0.5561537742614746 batch: 107/840\n",
      "Batch loss: 0.7778205871582031 batch: 108/840\n",
      "Batch loss: 0.6392990946769714 batch: 109/840\n",
      "Batch loss: 0.6056361198425293 batch: 110/840\n",
      "Batch loss: 0.5853554010391235 batch: 111/840\n",
      "Batch loss: 0.7150206565856934 batch: 112/840\n",
      "Batch loss: 0.7913064360618591 batch: 113/840\n",
      "Batch loss: 0.5776790380477905 batch: 114/840\n",
      "Batch loss: 0.6288166642189026 batch: 115/840\n",
      "Batch loss: 0.5881909728050232 batch: 116/840\n",
      "Batch loss: 0.6269080638885498 batch: 117/840\n",
      "Batch loss: 0.4825553596019745 batch: 118/840\n",
      "Batch loss: 0.8170546889305115 batch: 119/840\n",
      "Batch loss: 0.6240985989570618 batch: 120/840\n",
      "Batch loss: 0.772996723651886 batch: 121/840\n",
      "Batch loss: 0.7474091053009033 batch: 122/840\n",
      "Batch loss: 0.49376288056373596 batch: 123/840\n",
      "Batch loss: 0.522334635257721 batch: 124/840\n",
      "Batch loss: 0.7062118649482727 batch: 125/840\n",
      "Batch loss: 0.6191122531890869 batch: 126/840\n",
      "Batch loss: 0.7062781453132629 batch: 127/840\n",
      "Batch loss: 0.676433265209198 batch: 128/840\n",
      "Batch loss: 0.6958855986595154 batch: 129/840\n",
      "Batch loss: 0.5636293888092041 batch: 130/840\n",
      "Batch loss: 0.8024196028709412 batch: 131/840\n",
      "Batch loss: 1.0775421857833862 batch: 132/840\n",
      "Batch loss: 0.7838882207870483 batch: 133/840\n",
      "Batch loss: 0.5297893285751343 batch: 134/840\n",
      "Batch loss: 0.5680281519889832 batch: 135/840\n",
      "Batch loss: 0.7067002654075623 batch: 136/840\n",
      "Batch loss: 0.7289568185806274 batch: 137/840\n",
      "Batch loss: 0.5718027353286743 batch: 138/840\n",
      "Batch loss: 0.5871459245681763 batch: 139/840\n",
      "Batch loss: 0.7056137919425964 batch: 140/840\n",
      "Batch loss: 0.5638985633850098 batch: 141/840\n",
      "Batch loss: 0.7039416432380676 batch: 142/840\n",
      "Batch loss: 0.5578309297561646 batch: 143/840\n",
      "Batch loss: 0.6313391327857971 batch: 144/840\n",
      "Batch loss: 0.7791110277175903 batch: 145/840\n",
      "Batch loss: 0.7240590453147888 batch: 146/840\n",
      "Batch loss: 0.5452578663825989 batch: 147/840\n",
      "Batch loss: 0.655192494392395 batch: 148/840\n",
      "Batch loss: 0.7258913516998291 batch: 149/840\n",
      "Batch loss: 0.7656686902046204 batch: 150/840\n",
      "Batch loss: 0.6193640232086182 batch: 151/840\n",
      "Batch loss: 0.6787059903144836 batch: 152/840\n",
      "Batch loss: 0.5295735001564026 batch: 153/840\n",
      "Batch loss: 0.873755693435669 batch: 154/840\n",
      "Batch loss: 0.5923499464988708 batch: 155/840\n",
      "Batch loss: 0.5554658770561218 batch: 156/840\n",
      "Batch loss: 0.7097892165184021 batch: 157/840\n",
      "Batch loss: 0.49912700057029724 batch: 158/840\n",
      "Batch loss: 0.5745890140533447 batch: 159/840\n",
      "Batch loss: 0.560745120048523 batch: 160/840\n",
      "Batch loss: 0.6486131548881531 batch: 161/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8055455088615417 batch: 162/840\n",
      "Batch loss: 0.8526853322982788 batch: 163/840\n",
      "Batch loss: 0.4948110580444336 batch: 164/840\n",
      "Batch loss: 0.6111969351768494 batch: 165/840\n",
      "Batch loss: 0.6300873756408691 batch: 166/840\n",
      "Batch loss: 0.7273434996604919 batch: 167/840\n",
      "Batch loss: 0.592089831829071 batch: 168/840\n",
      "Batch loss: 0.5644845366477966 batch: 169/840\n",
      "Batch loss: 0.7071936726570129 batch: 170/840\n",
      "Batch loss: 0.6894271969795227 batch: 171/840\n",
      "Batch loss: 0.7669909596443176 batch: 172/840\n",
      "Batch loss: 0.563789427280426 batch: 173/840\n",
      "Batch loss: 0.6264147758483887 batch: 174/840\n",
      "Batch loss: 0.6515375375747681 batch: 175/840\n",
      "Batch loss: 0.7897191643714905 batch: 176/840\n",
      "Batch loss: 0.6838936805725098 batch: 177/840\n",
      "Batch loss: 0.6111046075820923 batch: 178/840\n",
      "Batch loss: 0.7797077298164368 batch: 179/840\n",
      "Batch loss: 0.5645722150802612 batch: 180/840\n",
      "Batch loss: 0.5818890333175659 batch: 181/840\n",
      "Batch loss: 0.6301219463348389 batch: 182/840\n",
      "Batch loss: 0.7318995594978333 batch: 183/840\n",
      "Batch loss: 0.5103947520256042 batch: 184/840\n",
      "Batch loss: 0.5313145518302917 batch: 185/840\n",
      "Batch loss: 0.5409291982650757 batch: 186/840\n",
      "Batch loss: 0.7304352521896362 batch: 187/840\n",
      "Batch loss: 0.6144669055938721 batch: 188/840\n",
      "Batch loss: 0.6969654560089111 batch: 189/840\n",
      "Batch loss: 0.6882447004318237 batch: 190/840\n",
      "Batch loss: 0.8382846117019653 batch: 191/840\n",
      "Batch loss: 0.5479690432548523 batch: 192/840\n",
      "Batch loss: 0.5401067733764648 batch: 193/840\n",
      "Batch loss: 0.4546408951282501 batch: 194/840\n",
      "Batch loss: 0.6508830785751343 batch: 195/840\n",
      "Batch loss: 0.7450392246246338 batch: 196/840\n",
      "Batch loss: 0.6798925995826721 batch: 197/840\n",
      "Batch loss: 0.5834998488426208 batch: 198/840\n",
      "Batch loss: 0.6420061588287354 batch: 199/840\n",
      "Batch loss: 0.808563232421875 batch: 200/840\n",
      "Batch loss: 0.5336841940879822 batch: 201/840\n",
      "Batch loss: 0.5998263359069824 batch: 202/840\n",
      "Batch loss: 0.6332675814628601 batch: 203/840\n",
      "Batch loss: 0.710997462272644 batch: 204/840\n",
      "Batch loss: 0.728432297706604 batch: 205/840\n",
      "Batch loss: 0.7532259225845337 batch: 206/840\n",
      "Batch loss: 0.6128024458885193 batch: 207/840\n",
      "Batch loss: 0.7228003144264221 batch: 208/840\n",
      "Batch loss: 0.6103166341781616 batch: 209/840\n",
      "Batch loss: 0.5985656380653381 batch: 210/840\n",
      "Batch loss: 0.5163418650627136 batch: 211/840\n",
      "Batch loss: 0.6315719485282898 batch: 212/840\n",
      "Batch loss: 0.6976892352104187 batch: 213/840\n",
      "Batch loss: 0.7311553359031677 batch: 214/840\n",
      "Batch loss: 0.7831034064292908 batch: 215/840\n",
      "Batch loss: 0.6690928936004639 batch: 216/840\n",
      "Batch loss: 0.6207399964332581 batch: 217/840\n",
      "Batch loss: 0.7288849353790283 batch: 218/840\n",
      "Batch loss: 0.6333940029144287 batch: 219/840\n",
      "Batch loss: 0.8470423221588135 batch: 220/840\n",
      "Batch loss: 0.622408926486969 batch: 221/840\n",
      "Batch loss: 0.7624351978302002 batch: 222/840\n",
      "Batch loss: 0.647672176361084 batch: 223/840\n",
      "Batch loss: 0.7119609117507935 batch: 224/840\n",
      "Batch loss: 0.722464382648468 batch: 225/840\n",
      "Batch loss: 0.7707647085189819 batch: 226/840\n",
      "Batch loss: 0.6948212385177612 batch: 227/840\n",
      "Batch loss: 0.568926990032196 batch: 228/840\n",
      "Batch loss: 0.5745319128036499 batch: 229/840\n",
      "Batch loss: 0.679155707359314 batch: 230/840\n",
      "Batch loss: 0.5733568072319031 batch: 231/840\n",
      "Batch loss: 0.6103163957595825 batch: 232/840\n",
      "Batch loss: 0.8540319800376892 batch: 233/840\n",
      "Batch loss: 0.5791156888008118 batch: 234/840\n",
      "Batch loss: 0.5896663665771484 batch: 235/840\n",
      "Batch loss: 0.7749969959259033 batch: 236/840\n",
      "Batch loss: 0.5689256191253662 batch: 237/840\n",
      "Batch loss: 0.8396617770195007 batch: 238/840\n",
      "Batch loss: 0.6484330892562866 batch: 239/840\n",
      "Batch loss: 0.6273399591445923 batch: 240/840\n",
      "Batch loss: 0.7082788348197937 batch: 241/840\n",
      "Batch loss: 0.6475258469581604 batch: 242/840\n",
      "Batch loss: 0.5821616649627686 batch: 243/840\n",
      "Batch loss: 0.7535585165023804 batch: 244/840\n",
      "Batch loss: 0.5039025545120239 batch: 245/840\n",
      "Batch loss: 0.663362443447113 batch: 246/840\n",
      "Batch loss: 0.8099774122238159 batch: 247/840\n",
      "Batch loss: 0.7784122228622437 batch: 248/840\n",
      "Batch loss: 0.992618203163147 batch: 249/840\n",
      "Batch loss: 0.5372372269630432 batch: 250/840\n",
      "Batch loss: 0.6136429905891418 batch: 251/840\n",
      "Batch loss: 0.6303190588951111 batch: 252/840\n",
      "Batch loss: 0.6787398457527161 batch: 253/840\n",
      "Batch loss: 0.7691569328308105 batch: 254/840\n",
      "Batch loss: 0.7042590379714966 batch: 255/840\n",
      "Batch loss: 0.7310745120048523 batch: 256/840\n",
      "Batch loss: 0.597533643245697 batch: 257/840\n",
      "Batch loss: 0.7620802521705627 batch: 258/840\n",
      "Batch loss: 0.6125026345252991 batch: 259/840\n",
      "Batch loss: 0.5649226307868958 batch: 260/840\n",
      "Batch loss: 0.563365638256073 batch: 261/840\n",
      "Batch loss: 0.49944618344306946 batch: 262/840\n",
      "Batch loss: 0.6443873047828674 batch: 263/840\n",
      "Batch loss: 0.6307184100151062 batch: 264/840\n",
      "Batch loss: 0.7425140142440796 batch: 265/840\n",
      "Batch loss: 0.6251440644264221 batch: 266/840\n",
      "Batch loss: 0.6934234499931335 batch: 267/840\n",
      "Batch loss: 0.7124192118644714 batch: 268/840\n",
      "Batch loss: 0.5686278343200684 batch: 269/840\n",
      "Batch loss: 0.637373149394989 batch: 270/840\n",
      "Batch loss: 0.6125648021697998 batch: 271/840\n",
      "Batch loss: 0.7476955652236938 batch: 272/840\n",
      "Batch loss: 0.8087285757064819 batch: 273/840\n",
      "Batch loss: 0.6843302249908447 batch: 274/840\n",
      "Batch loss: 0.7866818308830261 batch: 275/840\n",
      "Batch loss: 0.5998620390892029 batch: 276/840\n",
      "Batch loss: 0.6608609557151794 batch: 277/840\n",
      "Batch loss: 0.8399088382720947 batch: 278/840\n",
      "Batch loss: 0.7584282755851746 batch: 279/840\n",
      "Batch loss: 0.7910279631614685 batch: 280/840\n",
      "Batch loss: 0.640702486038208 batch: 281/840\n",
      "Batch loss: 0.539933979511261 batch: 282/840\n",
      "Batch loss: 0.61688631772995 batch: 283/840\n",
      "Batch loss: 0.5714269280433655 batch: 284/840\n",
      "Batch loss: 0.5237123966217041 batch: 285/840\n",
      "Batch loss: 0.6506938338279724 batch: 286/840\n",
      "Batch loss: 0.53575599193573 batch: 287/840\n",
      "Batch loss: 0.5757118463516235 batch: 288/840\n",
      "Batch loss: 0.8456674814224243 batch: 289/840\n",
      "Batch loss: 0.8018932938575745 batch: 290/840\n",
      "Batch loss: 0.7820891737937927 batch: 291/840\n",
      "Batch loss: 0.5629794001579285 batch: 292/840\n",
      "Batch loss: 0.6587616801261902 batch: 293/840\n",
      "Batch loss: 0.7731917500495911 batch: 294/840\n",
      "Batch loss: 0.5923469662666321 batch: 295/840\n",
      "Batch loss: 0.6595145463943481 batch: 296/840\n",
      "Batch loss: 0.6871139407157898 batch: 297/840\n",
      "Batch loss: 0.6782267689704895 batch: 298/840\n",
      "Batch loss: 0.6245079636573792 batch: 299/840\n",
      "Batch loss: 0.8013704419136047 batch: 300/840\n",
      "Batch loss: 0.7044625878334045 batch: 301/840\n",
      "Batch loss: 0.6407842040061951 batch: 302/840\n",
      "Batch loss: 0.7859790325164795 batch: 303/840\n",
      "Batch loss: 0.649467408657074 batch: 304/840\n",
      "Batch loss: 0.5352523326873779 batch: 305/840\n",
      "Batch loss: 0.7060136198997498 batch: 306/840\n",
      "Batch loss: 0.5991559624671936 batch: 307/840\n",
      "Batch loss: 0.8036422729492188 batch: 308/840\n",
      "Batch loss: 0.5777233839035034 batch: 309/840\n",
      "Batch loss: 0.8921323418617249 batch: 310/840\n",
      "Batch loss: 0.6512722969055176 batch: 311/840\n",
      "Batch loss: 0.7321298122406006 batch: 312/840\n",
      "Batch loss: 0.7064468860626221 batch: 313/840\n",
      "Batch loss: 0.5630528926849365 batch: 314/840\n",
      "Batch loss: 0.6953601837158203 batch: 315/840\n",
      "Batch loss: 0.6303985118865967 batch: 316/840\n",
      "Batch loss: 0.720857560634613 batch: 317/840\n",
      "Batch loss: 0.7590441107749939 batch: 318/840\n",
      "Batch loss: 0.7422415018081665 batch: 319/840\n",
      "Batch loss: 0.6318797469139099 batch: 320/840\n",
      "Batch loss: 0.6097332239151001 batch: 321/840\n",
      "Batch loss: 0.6258555054664612 batch: 322/840\n",
      "Batch loss: 0.7467612624168396 batch: 323/840\n",
      "Batch loss: 0.7373157739639282 batch: 324/840\n",
      "Batch loss: 0.561779260635376 batch: 325/840\n",
      "Batch loss: 0.6053300499916077 batch: 326/840\n",
      "Batch loss: 0.5063046813011169 batch: 327/840\n",
      "Batch loss: 0.8005856871604919 batch: 328/840\n",
      "Batch loss: 0.7502833008766174 batch: 329/840\n",
      "Batch loss: 0.6717022061347961 batch: 330/840\n",
      "Batch loss: 0.7744457721710205 batch: 331/840\n",
      "Batch loss: 0.6493514180183411 batch: 332/840\n",
      "Batch loss: 0.673545241355896 batch: 333/840\n",
      "Batch loss: 0.7331529259681702 batch: 334/840\n",
      "Batch loss: 0.6381990909576416 batch: 335/840\n",
      "Batch loss: 0.6708759069442749 batch: 336/840\n",
      "Batch loss: 0.895317554473877 batch: 337/840\n",
      "Batch loss: 0.7135608196258545 batch: 338/840\n",
      "Batch loss: 0.5805525183677673 batch: 339/840\n",
      "Batch loss: 0.7954812049865723 batch: 340/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5393564701080322 batch: 341/840\n",
      "Batch loss: 0.5791621208190918 batch: 342/840\n",
      "Batch loss: 0.8212456703186035 batch: 343/840\n",
      "Batch loss: 0.691765308380127 batch: 344/840\n",
      "Batch loss: 0.43591389060020447 batch: 345/840\n",
      "Batch loss: 0.6217524409294128 batch: 346/840\n",
      "Batch loss: 0.6984596252441406 batch: 347/840\n",
      "Batch loss: 0.6453769207000732 batch: 348/840\n",
      "Batch loss: 0.7100834846496582 batch: 349/840\n",
      "Batch loss: 0.5771466493606567 batch: 350/840\n",
      "Batch loss: 0.7074903249740601 batch: 351/840\n",
      "Batch loss: 0.7597488164901733 batch: 352/840\n",
      "Batch loss: 0.8080952167510986 batch: 353/840\n",
      "Batch loss: 0.6019771695137024 batch: 354/840\n",
      "Batch loss: 0.5836263298988342 batch: 355/840\n",
      "Batch loss: 0.6068894863128662 batch: 356/840\n",
      "Batch loss: 0.5619048476219177 batch: 357/840\n",
      "Batch loss: 0.9108259677886963 batch: 358/840\n",
      "Batch loss: 0.5716696977615356 batch: 359/840\n",
      "Batch loss: 0.9013494253158569 batch: 360/840\n",
      "Batch loss: 0.8102597594261169 batch: 361/840\n",
      "Batch loss: 0.5468874573707581 batch: 362/840\n",
      "Batch loss: 0.49778202176094055 batch: 363/840\n",
      "Batch loss: 0.7627338171005249 batch: 364/840\n",
      "Batch loss: 0.5456343293190002 batch: 365/840\n",
      "Batch loss: 0.7170369625091553 batch: 366/840\n",
      "Batch loss: 0.48602649569511414 batch: 367/840\n",
      "Batch loss: 0.8291847705841064 batch: 368/840\n",
      "Batch loss: 0.6309696435928345 batch: 369/840\n",
      "Batch loss: 0.8518025875091553 batch: 370/840\n",
      "Batch loss: 0.6774284839630127 batch: 371/840\n",
      "Batch loss: 0.5316429734230042 batch: 372/840\n",
      "Batch loss: 0.7661010026931763 batch: 373/840\n",
      "Batch loss: 0.7162462472915649 batch: 374/840\n",
      "Batch loss: 0.5234076380729675 batch: 375/840\n",
      "Batch loss: 0.5877513289451599 batch: 376/840\n",
      "Batch loss: 0.6966211199760437 batch: 377/840\n",
      "Batch loss: 0.7106175422668457 batch: 378/840\n",
      "Batch loss: 0.5754316449165344 batch: 379/840\n",
      "Batch loss: 0.9840682148933411 batch: 380/840\n",
      "Batch loss: 0.8701531291007996 batch: 381/840\n",
      "Batch loss: 0.8057776689529419 batch: 382/840\n",
      "Batch loss: 0.7215025424957275 batch: 383/840\n",
      "Batch loss: 0.6119314432144165 batch: 384/840\n",
      "Batch loss: 0.7380624413490295 batch: 385/840\n",
      "Batch loss: 0.7630196213722229 batch: 386/840\n",
      "Batch loss: 0.649884819984436 batch: 387/840\n",
      "Batch loss: 0.7901115417480469 batch: 388/840\n",
      "Batch loss: 0.599256694316864 batch: 389/840\n",
      "Batch loss: 0.8780872821807861 batch: 390/840\n",
      "Batch loss: 0.71985924243927 batch: 391/840\n",
      "Batch loss: 0.5343344211578369 batch: 392/840\n",
      "Batch loss: 0.5057488083839417 batch: 393/840\n",
      "Batch loss: 0.7452318668365479 batch: 394/840\n",
      "Batch loss: 0.6040912866592407 batch: 395/840\n",
      "Batch loss: 0.6574905514717102 batch: 396/840\n",
      "Batch loss: 0.6640942096710205 batch: 397/840\n",
      "Batch loss: 0.7499387264251709 batch: 398/840\n",
      "Batch loss: 0.45918065309524536 batch: 399/840\n",
      "Batch loss: 0.6056952476501465 batch: 400/840\n",
      "Batch loss: 0.6908972263336182 batch: 401/840\n",
      "Batch loss: 0.5732905864715576 batch: 402/840\n",
      "Batch loss: 0.6148589849472046 batch: 403/840\n",
      "Batch loss: 0.5641529560089111 batch: 404/840\n",
      "Batch loss: 0.7611299753189087 batch: 405/840\n",
      "Batch loss: 0.6582345366477966 batch: 406/840\n",
      "Batch loss: 0.5792036652565002 batch: 407/840\n",
      "Batch loss: 0.7406706809997559 batch: 408/840\n",
      "Batch loss: 0.7451518177986145 batch: 409/840\n",
      "Batch loss: 0.7225003838539124 batch: 410/840\n",
      "Batch loss: 0.6479372978210449 batch: 411/840\n",
      "Batch loss: 0.7348377704620361 batch: 412/840\n",
      "Batch loss: 0.7532176375389099 batch: 413/840\n",
      "Batch loss: 0.5759634375572205 batch: 414/840\n",
      "Batch loss: 0.7899202108383179 batch: 415/840\n",
      "Batch loss: 0.4454193413257599 batch: 416/840\n",
      "Batch loss: 0.706239640712738 batch: 417/840\n",
      "Batch loss: 0.8667216300964355 batch: 418/840\n",
      "Batch loss: 0.7518168687820435 batch: 419/840\n",
      "Batch loss: 0.7419865131378174 batch: 420/840\n",
      "Batch loss: 0.6040948629379272 batch: 421/840\n",
      "Batch loss: 0.6499083042144775 batch: 422/840\n",
      "Batch loss: 0.6141567826271057 batch: 423/840\n",
      "Batch loss: 0.7313035726547241 batch: 424/840\n",
      "Batch loss: 0.7897422313690186 batch: 425/840\n",
      "Batch loss: 0.6035574078559875 batch: 426/840\n",
      "Batch loss: 0.6091293692588806 batch: 427/840\n",
      "Batch loss: 0.700481116771698 batch: 428/840\n",
      "Batch loss: 0.6124027967453003 batch: 429/840\n",
      "Batch loss: 0.8211553692817688 batch: 430/840\n",
      "Batch loss: 0.556688666343689 batch: 431/840\n",
      "Batch loss: 0.6778416633605957 batch: 432/840\n",
      "Batch loss: 0.46514928340911865 batch: 433/840\n",
      "Batch loss: 0.6235085129737854 batch: 434/840\n",
      "Batch loss: 0.8503655791282654 batch: 435/840\n",
      "Batch loss: 0.6610072255134583 batch: 436/840\n",
      "Batch loss: 0.7922542095184326 batch: 437/840\n",
      "Batch loss: 0.4074805974960327 batch: 438/840\n",
      "Batch loss: 0.7066138982772827 batch: 439/840\n",
      "Batch loss: 0.723958432674408 batch: 440/840\n",
      "Batch loss: 0.7870781421661377 batch: 441/840\n",
      "Batch loss: 0.7100499868392944 batch: 442/840\n",
      "Batch loss: 0.6666779518127441 batch: 443/840\n",
      "Batch loss: 0.7036826610565186 batch: 444/840\n",
      "Batch loss: 0.5826128125190735 batch: 445/840\n",
      "Batch loss: 0.7398027181625366 batch: 446/840\n",
      "Batch loss: 0.7070688009262085 batch: 447/840\n",
      "Batch loss: 0.7428818345069885 batch: 448/840\n",
      "Batch loss: 0.577948272228241 batch: 449/840\n",
      "Batch loss: 0.6562036871910095 batch: 450/840\n",
      "Batch loss: 0.8185612559318542 batch: 451/840\n",
      "Batch loss: 0.7412586808204651 batch: 452/840\n",
      "Batch loss: 0.7101884484291077 batch: 453/840\n",
      "Batch loss: 0.6589282155036926 batch: 454/840\n",
      "Batch loss: 0.6945684552192688 batch: 455/840\n",
      "Batch loss: 0.611594557762146 batch: 456/840\n",
      "Batch loss: 0.6732745170593262 batch: 457/840\n",
      "Batch loss: 0.6450904607772827 batch: 458/840\n",
      "Batch loss: 0.5567727088928223 batch: 459/840\n",
      "Batch loss: 0.4453246295452118 batch: 460/840\n",
      "Batch loss: 0.7099666595458984 batch: 461/840\n",
      "Batch loss: 0.6946277022361755 batch: 462/840\n",
      "Batch loss: 0.8021658062934875 batch: 463/840\n",
      "Batch loss: 0.5224119424819946 batch: 464/840\n",
      "Batch loss: 0.984839677810669 batch: 465/840\n",
      "Batch loss: 0.6515871286392212 batch: 466/840\n",
      "Batch loss: 0.8253361582756042 batch: 467/840\n",
      "Batch loss: 0.6410252451896667 batch: 468/840\n",
      "Batch loss: 0.5419092178344727 batch: 469/840\n",
      "Batch loss: 0.6594933271408081 batch: 470/840\n",
      "Batch loss: 0.7413829565048218 batch: 471/840\n",
      "Batch loss: 0.7482587695121765 batch: 472/840\n",
      "Batch loss: 0.7975959181785583 batch: 473/840\n",
      "Batch loss: 0.5602424144744873 batch: 474/840\n",
      "Batch loss: 0.6452203989028931 batch: 475/840\n",
      "Batch loss: 0.6057410836219788 batch: 476/840\n",
      "Batch loss: 0.4989984929561615 batch: 477/840\n",
      "Batch loss: 0.6594129204750061 batch: 478/840\n",
      "Batch loss: 0.5690715312957764 batch: 479/840\n",
      "Batch loss: 0.6989727020263672 batch: 480/840\n",
      "Batch loss: 0.7060373425483704 batch: 481/840\n",
      "Batch loss: 0.648192286491394 batch: 482/840\n",
      "Batch loss: 0.7837880849838257 batch: 483/840\n",
      "Batch loss: 0.546903669834137 batch: 484/840\n",
      "Batch loss: 0.6560665369033813 batch: 485/840\n",
      "Batch loss: 0.7133265733718872 batch: 486/840\n",
      "Batch loss: 0.5102633833885193 batch: 487/840\n",
      "Batch loss: 0.6714050769805908 batch: 488/840\n",
      "Batch loss: 0.6696580648422241 batch: 489/840\n",
      "Batch loss: 0.7244837284088135 batch: 490/840\n",
      "Batch loss: 0.4987180233001709 batch: 491/840\n",
      "Batch loss: 0.7244848608970642 batch: 492/840\n",
      "Batch loss: 0.7847307324409485 batch: 493/840\n",
      "Batch loss: 0.4506928622722626 batch: 494/840\n",
      "Batch loss: 0.8073765635490417 batch: 495/840\n",
      "Batch loss: 0.6314924955368042 batch: 496/840\n",
      "Batch loss: 0.6916081309318542 batch: 497/840\n",
      "Batch loss: 0.523723840713501 batch: 498/840\n",
      "Batch loss: 0.7576763033866882 batch: 499/840\n",
      "Batch loss: 0.6414938569068909 batch: 500/840\n",
      "Batch loss: 0.4962000250816345 batch: 501/840\n",
      "Batch loss: 0.7209928035736084 batch: 502/840\n",
      "Batch loss: 0.5216266512870789 batch: 503/840\n",
      "Batch loss: 0.6695737242698669 batch: 504/840\n",
      "Batch loss: 0.7840448617935181 batch: 505/840\n",
      "Batch loss: 0.5335889458656311 batch: 506/840\n",
      "Batch loss: 0.7832791209220886 batch: 507/840\n",
      "Batch loss: 0.5560196042060852 batch: 508/840\n",
      "Batch loss: 0.7335250973701477 batch: 509/840\n",
      "Batch loss: 0.718132495880127 batch: 510/840\n",
      "Batch loss: 0.5983782410621643 batch: 511/840\n",
      "Batch loss: 0.6049350500106812 batch: 512/840\n",
      "Batch loss: 0.5953409075737 batch: 513/840\n",
      "Batch loss: 0.731273353099823 batch: 514/840\n",
      "Batch loss: 0.525320291519165 batch: 515/840\n",
      "Batch loss: 0.9086540341377258 batch: 516/840\n",
      "Batch loss: 0.6791911125183105 batch: 517/840\n",
      "Batch loss: 0.6779746413230896 batch: 518/840\n",
      "Batch loss: 0.8064096570014954 batch: 519/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7167176604270935 batch: 520/840\n",
      "Batch loss: 0.7690946459770203 batch: 521/840\n",
      "Batch loss: 0.6057003736495972 batch: 522/840\n",
      "Batch loss: 0.5384803414344788 batch: 523/840\n",
      "Batch loss: 0.6711328029632568 batch: 524/840\n",
      "Batch loss: 0.6166236996650696 batch: 525/840\n",
      "Batch loss: 0.7826942205429077 batch: 526/840\n",
      "Batch loss: 0.6991108655929565 batch: 527/840\n",
      "Batch loss: 0.592963457107544 batch: 528/840\n",
      "Batch loss: 0.5513065457344055 batch: 529/840\n",
      "Batch loss: 0.5628508925437927 batch: 530/840\n",
      "Batch loss: 0.6286256909370422 batch: 531/840\n",
      "Batch loss: 0.4899005591869354 batch: 532/840\n",
      "Batch loss: 0.6869046688079834 batch: 533/840\n",
      "Batch loss: 0.6446962952613831 batch: 534/840\n",
      "Batch loss: 0.7142298817634583 batch: 535/840\n",
      "Batch loss: 0.7137712240219116 batch: 536/840\n",
      "Batch loss: 0.7373092174530029 batch: 537/840\n",
      "Batch loss: 0.6155681014060974 batch: 538/840\n",
      "Batch loss: 0.617030143737793 batch: 539/840\n",
      "Batch loss: 0.7862457036972046 batch: 540/840\n",
      "Batch loss: 0.6259289979934692 batch: 541/840\n",
      "Batch loss: 0.6706520915031433 batch: 542/840\n",
      "Batch loss: 0.45789188146591187 batch: 543/840\n",
      "Batch loss: 0.7369605302810669 batch: 544/840\n",
      "Batch loss: 0.6159692406654358 batch: 545/840\n",
      "Batch loss: 0.7011238932609558 batch: 546/840\n",
      "Batch loss: 0.6542739272117615 batch: 547/840\n",
      "Batch loss: 0.47659024596214294 batch: 548/840\n",
      "Batch loss: 0.48789745569229126 batch: 549/840\n",
      "Batch loss: 0.5141378045082092 batch: 550/840\n",
      "Batch loss: 0.6261225938796997 batch: 551/840\n",
      "Batch loss: 0.6313766837120056 batch: 552/840\n",
      "Batch loss: 0.6508033871650696 batch: 553/840\n",
      "Batch loss: 0.6925060153007507 batch: 554/840\n",
      "Batch loss: 0.7043871283531189 batch: 555/840\n",
      "Batch loss: 0.6661534905433655 batch: 556/840\n",
      "Batch loss: 0.6379587054252625 batch: 557/840\n",
      "Batch loss: 0.6979573965072632 batch: 558/840\n",
      "Batch loss: 0.6057749390602112 batch: 559/840\n",
      "Batch loss: 0.6344029903411865 batch: 560/840\n",
      "Batch loss: 0.655392050743103 batch: 561/840\n",
      "Batch loss: 0.5445970296859741 batch: 562/840\n",
      "Batch loss: 0.4889090061187744 batch: 563/840\n",
      "Batch loss: 0.6523417830467224 batch: 564/840\n",
      "Batch loss: 0.7520204782485962 batch: 565/840\n",
      "Batch loss: 0.7033494710922241 batch: 566/840\n",
      "Batch loss: 0.7145190238952637 batch: 567/840\n",
      "Batch loss: 0.6043339371681213 batch: 568/840\n",
      "Batch loss: 0.6791520118713379 batch: 569/840\n",
      "Batch loss: 0.46748027205467224 batch: 570/840\n",
      "Batch loss: 0.6683049201965332 batch: 571/840\n",
      "Batch loss: 0.7654505372047424 batch: 572/840\n",
      "Batch loss: 0.6518812775611877 batch: 573/840\n",
      "Batch loss: 0.7580068111419678 batch: 574/840\n",
      "Batch loss: 0.5259227156639099 batch: 575/840\n",
      "Batch loss: 0.7326400279998779 batch: 576/840\n",
      "Batch loss: 0.6265782713890076 batch: 577/840\n",
      "Batch loss: 0.7457125186920166 batch: 578/840\n",
      "Batch loss: 0.5893328189849854 batch: 579/840\n",
      "Batch loss: 0.7322673201560974 batch: 580/840\n",
      "Batch loss: 0.6677314043045044 batch: 581/840\n",
      "Batch loss: 0.7604107856750488 batch: 582/840\n",
      "Batch loss: 0.5488142967224121 batch: 583/840\n",
      "Batch loss: 0.8021907210350037 batch: 584/840\n",
      "Batch loss: 0.6851984262466431 batch: 585/840\n",
      "Batch loss: 0.6737388372421265 batch: 586/840\n",
      "Batch loss: 0.6876089572906494 batch: 587/840\n",
      "Batch loss: 0.5596317052841187 batch: 588/840\n",
      "Batch loss: 0.8281107544898987 batch: 589/840\n",
      "Batch loss: 0.5675554871559143 batch: 590/840\n",
      "Batch loss: 0.4707554876804352 batch: 591/840\n",
      "Batch loss: 0.572927713394165 batch: 592/840\n",
      "Batch loss: 0.7482057213783264 batch: 593/840\n",
      "Batch loss: 0.7275305986404419 batch: 594/840\n",
      "Batch loss: 0.46029385924339294 batch: 595/840\n",
      "Batch loss: 0.5008952021598816 batch: 596/840\n",
      "Batch loss: 0.572573184967041 batch: 597/840\n",
      "Batch loss: 0.5303345918655396 batch: 598/840\n",
      "Batch loss: 0.6726426482200623 batch: 599/840\n",
      "Batch loss: 0.7917066812515259 batch: 600/840\n",
      "Batch loss: 0.7138418555259705 batch: 601/840\n",
      "Batch loss: 0.5714566707611084 batch: 602/840\n",
      "Batch loss: 0.6534757018089294 batch: 603/840\n",
      "Batch loss: 0.7839952111244202 batch: 604/840\n",
      "Batch loss: 0.8310518860816956 batch: 605/840\n",
      "Batch loss: 0.8638436198234558 batch: 606/840\n",
      "Batch loss: 0.662477433681488 batch: 607/840\n",
      "Batch loss: 0.633350133895874 batch: 608/840\n",
      "Batch loss: 0.45028847455978394 batch: 609/840\n",
      "Batch loss: 0.723051130771637 batch: 610/840\n",
      "Batch loss: 0.6737548112869263 batch: 611/840\n",
      "Batch loss: 0.806816816329956 batch: 612/840\n",
      "Batch loss: 0.7078296542167664 batch: 613/840\n",
      "Batch loss: 0.7440127730369568 batch: 614/840\n",
      "Batch loss: 0.6254810094833374 batch: 615/840\n",
      "Batch loss: 0.5479253530502319 batch: 616/840\n",
      "Batch loss: 0.6079022288322449 batch: 617/840\n",
      "Batch loss: 0.6926430463790894 batch: 618/840\n",
      "Batch loss: 0.8174885511398315 batch: 619/840\n",
      "Batch loss: 0.6740795373916626 batch: 620/840\n",
      "Batch loss: 0.7092229723930359 batch: 621/840\n",
      "Batch loss: 0.7457681894302368 batch: 622/840\n",
      "Batch loss: 0.6931520104408264 batch: 623/840\n",
      "Batch loss: 0.7843824625015259 batch: 624/840\n",
      "Batch loss: 0.6938657164573669 batch: 625/840\n",
      "Batch loss: 0.6628496050834656 batch: 626/840\n",
      "Batch loss: 0.6035131216049194 batch: 627/840\n",
      "Batch loss: 0.7015402913093567 batch: 628/840\n",
      "Batch loss: 0.662036657333374 batch: 629/840\n",
      "Batch loss: 0.6848746538162231 batch: 630/840\n",
      "Batch loss: 0.7389309406280518 batch: 631/840\n",
      "Batch loss: 0.6265669465065002 batch: 632/840\n",
      "Batch loss: 0.5806835889816284 batch: 633/840\n",
      "Batch loss: 0.7776480913162231 batch: 634/840\n",
      "Batch loss: 0.6046913862228394 batch: 635/840\n",
      "Batch loss: 0.6081206798553467 batch: 636/840\n",
      "Batch loss: 0.568398118019104 batch: 637/840\n",
      "Batch loss: 0.6644163727760315 batch: 638/840\n",
      "Batch loss: 0.6677922010421753 batch: 639/840\n",
      "Batch loss: 0.7620920538902283 batch: 640/840\n",
      "Batch loss: 0.8330228328704834 batch: 641/840\n",
      "Batch loss: 0.6124322414398193 batch: 642/840\n",
      "Batch loss: 1.0041042566299438 batch: 643/840\n",
      "Batch loss: 0.6397931575775146 batch: 644/840\n",
      "Batch loss: 0.5407095551490784 batch: 645/840\n",
      "Batch loss: 0.612989068031311 batch: 646/840\n",
      "Batch loss: 0.7055810689926147 batch: 647/840\n",
      "Batch loss: 0.6750120520591736 batch: 648/840\n",
      "Batch loss: 0.5862414836883545 batch: 649/840\n",
      "Batch loss: 0.5280277132987976 batch: 650/840\n",
      "Batch loss: 0.6880375742912292 batch: 651/840\n",
      "Batch loss: 0.7250780463218689 batch: 652/840\n",
      "Batch loss: 0.6215495467185974 batch: 653/840\n",
      "Batch loss: 0.827726423740387 batch: 654/840\n",
      "Batch loss: 0.5642338395118713 batch: 655/840\n",
      "Batch loss: 0.6819469928741455 batch: 656/840\n",
      "Batch loss: 0.5459190607070923 batch: 657/840\n",
      "Batch loss: 0.6950334906578064 batch: 658/840\n",
      "Batch loss: 0.7037609815597534 batch: 659/840\n",
      "Batch loss: 0.5448965430259705 batch: 660/840\n",
      "Batch loss: 0.7386176586151123 batch: 661/840\n",
      "Batch loss: 0.6078054904937744 batch: 662/840\n",
      "Batch loss: 0.6255228519439697 batch: 663/840\n",
      "Batch loss: 0.5919354557991028 batch: 664/840\n",
      "Batch loss: 0.7688488960266113 batch: 665/840\n",
      "Batch loss: 0.6335257887840271 batch: 666/840\n",
      "Batch loss: 0.8373423218727112 batch: 667/840\n",
      "Batch loss: 0.627683162689209 batch: 668/840\n",
      "Batch loss: 0.5270505547523499 batch: 669/840\n",
      "Batch loss: 0.6652369499206543 batch: 670/840\n",
      "Batch loss: 0.6323064565658569 batch: 671/840\n",
      "Batch loss: 0.7581493258476257 batch: 672/840\n",
      "Batch loss: 0.7815131545066833 batch: 673/840\n",
      "Batch loss: 0.9454973340034485 batch: 674/840\n",
      "Batch loss: 0.6966735124588013 batch: 675/840\n",
      "Batch loss: 0.5533939003944397 batch: 676/840\n",
      "Batch loss: 0.697333812713623 batch: 677/840\n",
      "Batch loss: 0.6656415462493896 batch: 678/840\n",
      "Batch loss: 0.8980386257171631 batch: 679/840\n",
      "Batch loss: 0.6960514187812805 batch: 680/840\n",
      "Batch loss: 0.8005968332290649 batch: 681/840\n",
      "Batch loss: 0.5882728695869446 batch: 682/840\n",
      "Batch loss: 0.6101349592208862 batch: 683/840\n",
      "Batch loss: 0.8223640322685242 batch: 684/840\n",
      "Batch loss: 0.6907307505607605 batch: 685/840\n",
      "Batch loss: 0.7665377259254456 batch: 686/840\n",
      "Batch loss: 0.7911548614501953 batch: 687/840\n",
      "Batch loss: 0.6055740714073181 batch: 688/840\n",
      "Batch loss: 0.6675022840499878 batch: 689/840\n",
      "Batch loss: 0.5757557153701782 batch: 690/840\n",
      "Batch loss: 0.7295191287994385 batch: 691/840\n",
      "Batch loss: 0.6915239691734314 batch: 692/840\n",
      "Batch loss: 0.6722008585929871 batch: 693/840\n",
      "Batch loss: 0.8716204762458801 batch: 694/840\n",
      "Batch loss: 0.7735043168067932 batch: 695/840\n",
      "Batch loss: 0.6455048322677612 batch: 696/840\n",
      "Batch loss: 0.6789077520370483 batch: 697/840\n",
      "Batch loss: 0.538398265838623 batch: 698/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7416943311691284 batch: 699/840\n",
      "Batch loss: 0.7269849181175232 batch: 700/840\n",
      "Batch loss: 0.8738850355148315 batch: 701/840\n",
      "Batch loss: 0.5809977054595947 batch: 702/840\n",
      "Batch loss: 0.5549842715263367 batch: 703/840\n",
      "Batch loss: 0.7913877964019775 batch: 704/840\n",
      "Batch loss: 0.6765732765197754 batch: 705/840\n",
      "Batch loss: 0.6729186177253723 batch: 706/840\n",
      "Batch loss: 0.6869041323661804 batch: 707/840\n",
      "Batch loss: 0.7430425882339478 batch: 708/840\n",
      "Batch loss: 0.7930179834365845 batch: 709/840\n",
      "Batch loss: 0.7631410360336304 batch: 710/840\n",
      "Batch loss: 0.49731120467185974 batch: 711/840\n",
      "Batch loss: 0.7682493329048157 batch: 712/840\n",
      "Batch loss: 0.631742000579834 batch: 713/840\n",
      "Batch loss: 0.6991836428642273 batch: 714/840\n",
      "Batch loss: 0.7876458168029785 batch: 715/840\n",
      "Batch loss: 0.7220051288604736 batch: 716/840\n",
      "Batch loss: 0.6779417991638184 batch: 717/840\n",
      "Batch loss: 0.6698852777481079 batch: 718/840\n",
      "Batch loss: 0.4665464460849762 batch: 719/840\n",
      "Batch loss: 0.5797619223594666 batch: 720/840\n",
      "Batch loss: 0.686551034450531 batch: 721/840\n",
      "Batch loss: 0.7253842353820801 batch: 722/840\n",
      "Batch loss: 0.4747060537338257 batch: 723/840\n",
      "Batch loss: 0.7137206792831421 batch: 724/840\n",
      "Batch loss: 0.6132251620292664 batch: 725/840\n",
      "Batch loss: 0.6037855744361877 batch: 726/840\n",
      "Batch loss: 0.7580405473709106 batch: 727/840\n",
      "Batch loss: 0.8051555156707764 batch: 728/840\n",
      "Batch loss: 0.6820836663246155 batch: 729/840\n",
      "Batch loss: 0.6699376702308655 batch: 730/840\n",
      "Batch loss: 0.49274981021881104 batch: 731/840\n",
      "Batch loss: 0.8201907873153687 batch: 732/840\n",
      "Batch loss: 0.6508485674858093 batch: 733/840\n",
      "Batch loss: 0.5950610041618347 batch: 734/840\n",
      "Batch loss: 0.6086148619651794 batch: 735/840\n",
      "Batch loss: 0.6237545609474182 batch: 736/840\n",
      "Batch loss: 0.7091502547264099 batch: 737/840\n",
      "Batch loss: 0.5579733848571777 batch: 738/840\n",
      "Batch loss: 0.7899843454360962 batch: 739/840\n",
      "Batch loss: 0.7430877089500427 batch: 740/840\n",
      "Batch loss: 0.5079784989356995 batch: 741/840\n",
      "Batch loss: 0.6320860385894775 batch: 742/840\n",
      "Batch loss: 0.5094702243804932 batch: 743/840\n",
      "Batch loss: 0.5973995327949524 batch: 744/840\n",
      "Batch loss: 0.6292659640312195 batch: 745/840\n",
      "Batch loss: 0.6283223032951355 batch: 746/840\n",
      "Batch loss: 0.7742183804512024 batch: 747/840\n",
      "Batch loss: 0.6718264818191528 batch: 748/840\n",
      "Batch loss: 0.6176334023475647 batch: 749/840\n",
      "Batch loss: 0.5105289816856384 batch: 750/840\n",
      "Batch loss: 0.7013963460922241 batch: 751/840\n",
      "Batch loss: 0.8588721752166748 batch: 752/840\n",
      "Batch loss: 0.6192128658294678 batch: 753/840\n",
      "Batch loss: 0.7119999527931213 batch: 754/840\n",
      "Batch loss: 0.6216949820518494 batch: 755/840\n",
      "Batch loss: 0.5210278630256653 batch: 756/840\n",
      "Batch loss: 0.8171290755271912 batch: 757/840\n",
      "Batch loss: 0.6592044830322266 batch: 758/840\n",
      "Batch loss: 0.6146625876426697 batch: 759/840\n",
      "Batch loss: 0.6084794402122498 batch: 760/840\n",
      "Batch loss: 0.587904155254364 batch: 761/840\n",
      "Batch loss: 0.7698978185653687 batch: 762/840\n",
      "Batch loss: 0.580777645111084 batch: 763/840\n",
      "Batch loss: 0.6972757577896118 batch: 764/840\n",
      "Batch loss: 0.48633238673210144 batch: 765/840\n",
      "Batch loss: 0.5135074853897095 batch: 766/840\n",
      "Batch loss: 0.7486589550971985 batch: 767/840\n",
      "Batch loss: 0.7837965488433838 batch: 768/840\n",
      "Batch loss: 0.7194850444793701 batch: 769/840\n",
      "Batch loss: 0.5945953726768494 batch: 770/840\n",
      "Batch loss: 0.6608965992927551 batch: 771/840\n",
      "Batch loss: 0.6659670472145081 batch: 772/840\n",
      "Batch loss: 0.5642642974853516 batch: 773/840\n",
      "Batch loss: 0.46213406324386597 batch: 774/840\n",
      "Batch loss: 0.5913911461830139 batch: 775/840\n",
      "Batch loss: 0.5541769862174988 batch: 776/840\n",
      "Batch loss: 0.5395271182060242 batch: 777/840\n",
      "Batch loss: 0.4791565239429474 batch: 778/840\n",
      "Batch loss: 0.7441255450248718 batch: 779/840\n",
      "Batch loss: 0.5659030675888062 batch: 780/840\n",
      "Batch loss: 0.770453691482544 batch: 781/840\n",
      "Batch loss: 0.6772854328155518 batch: 782/840\n",
      "Batch loss: 0.5207470059394836 batch: 783/840\n",
      "Batch loss: 0.7712628245353699 batch: 784/840\n",
      "Batch loss: 0.5758941173553467 batch: 785/840\n",
      "Batch loss: 0.7924472689628601 batch: 786/840\n",
      "Batch loss: 0.6358855962753296 batch: 787/840\n",
      "Batch loss: 0.7637870907783508 batch: 788/840\n",
      "Batch loss: 0.7705594897270203 batch: 789/840\n",
      "Batch loss: 0.7949985265731812 batch: 790/840\n",
      "Batch loss: 0.6804578304290771 batch: 791/840\n",
      "Batch loss: 0.4569440186023712 batch: 792/840\n",
      "Batch loss: 0.5852242708206177 batch: 793/840\n",
      "Batch loss: 0.6818675398826599 batch: 794/840\n",
      "Batch loss: 0.7391211986541748 batch: 795/840\n",
      "Batch loss: 0.6922296285629272 batch: 796/840\n",
      "Batch loss: 0.7755659222602844 batch: 797/840\n",
      "Batch loss: 0.6570724248886108 batch: 798/840\n",
      "Batch loss: 0.6466385722160339 batch: 799/840\n",
      "Batch loss: 0.5862010717391968 batch: 800/840\n",
      "Batch loss: 0.8760162591934204 batch: 801/840\n",
      "Batch loss: 0.48676013946533203 batch: 802/840\n",
      "Batch loss: 0.5511940121650696 batch: 803/840\n",
      "Batch loss: 0.6898212432861328 batch: 804/840\n",
      "Batch loss: 0.5580154657363892 batch: 805/840\n",
      "Batch loss: 0.6340872049331665 batch: 806/840\n",
      "Batch loss: 0.595282793045044 batch: 807/840\n",
      "Batch loss: 0.6412134766578674 batch: 808/840\n",
      "Batch loss: 0.6656831502914429 batch: 809/840\n",
      "Batch loss: 0.5509278774261475 batch: 810/840\n",
      "Batch loss: 0.5662599802017212 batch: 811/840\n",
      "Batch loss: 0.6546202301979065 batch: 812/840\n",
      "Batch loss: 0.5368027687072754 batch: 813/840\n",
      "Batch loss: 0.6969314813613892 batch: 814/840\n",
      "Batch loss: 0.6679428219795227 batch: 815/840\n",
      "Batch loss: 0.6834139823913574 batch: 816/840\n",
      "Batch loss: 0.6953280568122864 batch: 817/840\n",
      "Batch loss: 0.6403626799583435 batch: 818/840\n",
      "Batch loss: 0.5460317730903625 batch: 819/840\n",
      "Batch loss: 0.6920501589775085 batch: 820/840\n",
      "Batch loss: 0.6386290788650513 batch: 821/840\n",
      "Batch loss: 0.6356033086776733 batch: 822/840\n",
      "Batch loss: 0.8008231520652771 batch: 823/840\n",
      "Batch loss: 0.7812426686286926 batch: 824/840\n",
      "Batch loss: 0.6338187456130981 batch: 825/840\n",
      "Batch loss: 0.5532727241516113 batch: 826/840\n",
      "Batch loss: 0.5776223540306091 batch: 827/840\n",
      "Batch loss: 0.7155188918113708 batch: 828/840\n",
      "Batch loss: 0.6463446021080017 batch: 829/840\n",
      "Batch loss: 0.6782060861587524 batch: 830/840\n",
      "Batch loss: 0.6075801253318787 batch: 831/840\n",
      "Batch loss: 0.7342447638511658 batch: 832/840\n",
      "Batch loss: 0.710483193397522 batch: 833/840\n",
      "Batch loss: 0.5782282948493958 batch: 834/840\n",
      "Batch loss: 0.6080584526062012 batch: 835/840\n",
      "Batch loss: 0.6462249159812927 batch: 836/840\n",
      "Batch loss: 0.6766570210456848 batch: 837/840\n",
      "Batch loss: 0.7251611351966858 batch: 838/840\n",
      "Batch loss: 0.6520530581474304 batch: 839/840\n",
      "Batch loss: 0.6374151706695557 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 4/15..  Training Loss: 0.007..  Test Loss: 0.005..  Test Accuracy: 0.808\n",
      "Running epoch 5/15\n",
      "Batch loss: 0.5576475858688354 batch: 1/840\n",
      "Batch loss: 1.074262261390686 batch: 2/840\n",
      "Batch loss: 0.6981188058853149 batch: 3/840\n",
      "Batch loss: 0.6922661662101746 batch: 4/840\n",
      "Batch loss: 0.6539745330810547 batch: 5/840\n",
      "Batch loss: 0.546034038066864 batch: 6/840\n",
      "Batch loss: 0.5279750823974609 batch: 7/840\n",
      "Batch loss: 0.7277286648750305 batch: 8/840\n",
      "Batch loss: 0.6246364712715149 batch: 9/840\n",
      "Batch loss: 0.5952658653259277 batch: 10/840\n",
      "Batch loss: 0.6103188395500183 batch: 11/840\n",
      "Batch loss: 0.6054365038871765 batch: 12/840\n",
      "Batch loss: 0.5704494714736938 batch: 13/840\n",
      "Batch loss: 0.6569434404373169 batch: 14/840\n",
      "Batch loss: 0.5896061658859253 batch: 15/840\n",
      "Batch loss: 0.5325726270675659 batch: 16/840\n",
      "Batch loss: 0.5576989650726318 batch: 17/840\n",
      "Batch loss: 0.7444626092910767 batch: 18/840\n",
      "Batch loss: 0.7163824439048767 batch: 19/840\n",
      "Batch loss: 0.7107425928115845 batch: 20/840\n",
      "Batch loss: 0.7581619024276733 batch: 21/840\n",
      "Batch loss: 0.5811477899551392 batch: 22/840\n",
      "Batch loss: 0.6124337315559387 batch: 23/840\n",
      "Batch loss: 0.5542964935302734 batch: 24/840\n",
      "Batch loss: 0.5197584629058838 batch: 25/840\n",
      "Batch loss: 0.7116819024085999 batch: 26/840\n",
      "Batch loss: 0.7311483025550842 batch: 27/840\n",
      "Batch loss: 0.6755580306053162 batch: 28/840\n",
      "Batch loss: 0.6410043239593506 batch: 29/840\n",
      "Batch loss: 0.6132904291152954 batch: 30/840\n",
      "Batch loss: 0.5892517566680908 batch: 31/840\n",
      "Batch loss: 0.637982964515686 batch: 32/840\n",
      "Batch loss: 0.7332155704498291 batch: 33/840\n",
      "Batch loss: 0.5467442870140076 batch: 34/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5553137063980103 batch: 35/840\n",
      "Batch loss: 0.547267735004425 batch: 36/840\n",
      "Batch loss: 0.6446153521537781 batch: 37/840\n",
      "Batch loss: 0.7532804608345032 batch: 38/840\n",
      "Batch loss: 0.6299211382865906 batch: 39/840\n",
      "Batch loss: 0.5711548328399658 batch: 40/840\n",
      "Batch loss: 0.7376341819763184 batch: 41/840\n",
      "Batch loss: 0.7272316813468933 batch: 42/840\n",
      "Batch loss: 0.5959916114807129 batch: 43/840\n",
      "Batch loss: 0.6881881952285767 batch: 44/840\n",
      "Batch loss: 0.6869748830795288 batch: 45/840\n",
      "Batch loss: 0.5083778500556946 batch: 46/840\n",
      "Batch loss: 0.549892783164978 batch: 47/840\n",
      "Batch loss: 0.6762640476226807 batch: 48/840\n",
      "Batch loss: 0.6384792923927307 batch: 49/840\n",
      "Batch loss: 0.684328019618988 batch: 50/840\n",
      "Batch loss: 0.7534106969833374 batch: 51/840\n",
      "Batch loss: 0.7040141820907593 batch: 52/840\n",
      "Batch loss: 0.5750580430030823 batch: 53/840\n",
      "Batch loss: 0.592047929763794 batch: 54/840\n",
      "Batch loss: 0.594737708568573 batch: 55/840\n",
      "Batch loss: 0.5907946228981018 batch: 56/840\n",
      "Batch loss: 0.7689990401268005 batch: 57/840\n",
      "Batch loss: 0.5694923996925354 batch: 58/840\n",
      "Batch loss: 0.6231485605239868 batch: 59/840\n",
      "Batch loss: 0.5746103525161743 batch: 60/840\n",
      "Batch loss: 0.9177908301353455 batch: 61/840\n",
      "Batch loss: 0.7709454298019409 batch: 62/840\n",
      "Batch loss: 0.6278478503227234 batch: 63/840\n",
      "Batch loss: 0.6551820635795593 batch: 64/840\n",
      "Batch loss: 0.5979943871498108 batch: 65/840\n",
      "Batch loss: 0.6494338512420654 batch: 66/840\n",
      "Batch loss: 0.7106356620788574 batch: 67/840\n",
      "Batch loss: 0.7021384239196777 batch: 68/840\n",
      "Batch loss: 0.7588775157928467 batch: 69/840\n",
      "Batch loss: 0.6165828704833984 batch: 70/840\n",
      "Batch loss: 0.743045449256897 batch: 71/840\n",
      "Batch loss: 0.8794558048248291 batch: 72/840\n",
      "Batch loss: 0.8066565990447998 batch: 73/840\n",
      "Batch loss: 0.6123185753822327 batch: 74/840\n",
      "Batch loss: 0.689664900302887 batch: 75/840\n",
      "Batch loss: 0.44176480174064636 batch: 76/840\n",
      "Batch loss: 0.6079925298690796 batch: 77/840\n",
      "Batch loss: 0.8811651468276978 batch: 78/840\n",
      "Batch loss: 0.6253269910812378 batch: 79/840\n",
      "Batch loss: 0.7569841146469116 batch: 80/840\n",
      "Batch loss: 0.5802474021911621 batch: 81/840\n",
      "Batch loss: 0.7786105871200562 batch: 82/840\n",
      "Batch loss: 0.509421706199646 batch: 83/840\n",
      "Batch loss: 0.7087010145187378 batch: 84/840\n",
      "Batch loss: 0.6845076084136963 batch: 85/840\n",
      "Batch loss: 1.073310136795044 batch: 86/840\n",
      "Batch loss: 0.5010038614273071 batch: 87/840\n",
      "Batch loss: 0.515251636505127 batch: 88/840\n",
      "Batch loss: 0.5228665471076965 batch: 89/840\n",
      "Batch loss: 0.559683620929718 batch: 90/840\n",
      "Batch loss: 0.6129360198974609 batch: 91/840\n",
      "Batch loss: 0.6156232953071594 batch: 92/840\n",
      "Batch loss: 0.7305769324302673 batch: 93/840\n",
      "Batch loss: 0.5734337568283081 batch: 94/840\n",
      "Batch loss: 0.6511157155036926 batch: 95/840\n",
      "Batch loss: 0.6177282929420471 batch: 96/840\n",
      "Batch loss: 0.6712436676025391 batch: 97/840\n",
      "Batch loss: 0.7941197752952576 batch: 98/840\n",
      "Batch loss: 0.6259621381759644 batch: 99/840\n",
      "Batch loss: 0.6717895269393921 batch: 100/840\n",
      "Batch loss: 0.5275867581367493 batch: 101/840\n",
      "Batch loss: 0.4691292941570282 batch: 102/840\n",
      "Batch loss: 0.6344815492630005 batch: 103/840\n",
      "Batch loss: 0.5425907373428345 batch: 104/840\n",
      "Batch loss: 0.5735757946968079 batch: 105/840\n",
      "Batch loss: 0.713043749332428 batch: 106/840\n",
      "Batch loss: 0.5170965790748596 batch: 107/840\n",
      "Batch loss: 0.7519557476043701 batch: 108/840\n",
      "Batch loss: 0.6864544153213501 batch: 109/840\n",
      "Batch loss: 0.5048671960830688 batch: 110/840\n",
      "Batch loss: 0.4831468164920807 batch: 111/840\n",
      "Batch loss: 0.7485452890396118 batch: 112/840\n",
      "Batch loss: 0.723965048789978 batch: 113/840\n",
      "Batch loss: 0.6297357082366943 batch: 114/840\n",
      "Batch loss: 0.7012079358100891 batch: 115/840\n",
      "Batch loss: 0.6029764413833618 batch: 116/840\n",
      "Batch loss: 0.6577638983726501 batch: 117/840\n",
      "Batch loss: 0.45536187291145325 batch: 118/840\n",
      "Batch loss: 0.6930286884307861 batch: 119/840\n",
      "Batch loss: 0.5414382219314575 batch: 120/840\n",
      "Batch loss: 0.7517291903495789 batch: 121/840\n",
      "Batch loss: 0.7465444207191467 batch: 122/840\n",
      "Batch loss: 0.5217483043670654 batch: 123/840\n",
      "Batch loss: 0.6004552841186523 batch: 124/840\n",
      "Batch loss: 0.529574990272522 batch: 125/840\n",
      "Batch loss: 0.7196484208106995 batch: 126/840\n",
      "Batch loss: 0.7110944986343384 batch: 127/840\n",
      "Batch loss: 0.7073361277580261 batch: 128/840\n",
      "Batch loss: 0.7430258393287659 batch: 129/840\n",
      "Batch loss: 0.6065340042114258 batch: 130/840\n",
      "Batch loss: 0.7489368915557861 batch: 131/840\n",
      "Batch loss: 1.2400082349777222 batch: 132/840\n",
      "Batch loss: 0.7948396801948547 batch: 133/840\n",
      "Batch loss: 0.6250574588775635 batch: 134/840\n",
      "Batch loss: 0.6024020314216614 batch: 135/840\n",
      "Batch loss: 0.7394249439239502 batch: 136/840\n",
      "Batch loss: 0.6317024827003479 batch: 137/840\n",
      "Batch loss: 0.616268515586853 batch: 138/840\n",
      "Batch loss: 0.561343789100647 batch: 139/840\n",
      "Batch loss: 0.6672735810279846 batch: 140/840\n",
      "Batch loss: 0.5652211904525757 batch: 141/840\n",
      "Batch loss: 0.580367386341095 batch: 142/840\n",
      "Batch loss: 0.6511909365653992 batch: 143/840\n",
      "Batch loss: 0.5841090679168701 batch: 144/840\n",
      "Batch loss: 0.8576616048812866 batch: 145/840\n",
      "Batch loss: 0.7964657545089722 batch: 146/840\n",
      "Batch loss: 0.5824859738349915 batch: 147/840\n",
      "Batch loss: 0.8177032470703125 batch: 148/840\n",
      "Batch loss: 0.6693203449249268 batch: 149/840\n",
      "Batch loss: 0.6434007883071899 batch: 150/840\n",
      "Batch loss: 0.589569628238678 batch: 151/840\n",
      "Batch loss: 0.666158139705658 batch: 152/840\n",
      "Batch loss: 0.5803529024124146 batch: 153/840\n",
      "Batch loss: 0.6771237850189209 batch: 154/840\n",
      "Batch loss: 0.6030092239379883 batch: 155/840\n",
      "Batch loss: 0.7055261135101318 batch: 156/840\n",
      "Batch loss: 0.6163502335548401 batch: 157/840\n",
      "Batch loss: 0.548058032989502 batch: 158/840\n",
      "Batch loss: 0.5561482906341553 batch: 159/840\n",
      "Batch loss: 0.6358674168586731 batch: 160/840\n",
      "Batch loss: 0.6942718625068665 batch: 161/840\n",
      "Batch loss: 0.6772263050079346 batch: 162/840\n",
      "Batch loss: 0.631008505821228 batch: 163/840\n",
      "Batch loss: 0.47859272360801697 batch: 164/840\n",
      "Batch loss: 0.6941531896591187 batch: 165/840\n",
      "Batch loss: 0.5255507230758667 batch: 166/840\n",
      "Batch loss: 0.6758455038070679 batch: 167/840\n",
      "Batch loss: 0.6516655087471008 batch: 168/840\n",
      "Batch loss: 0.5743560791015625 batch: 169/840\n",
      "Batch loss: 0.7944241166114807 batch: 170/840\n",
      "Batch loss: 0.6507737636566162 batch: 171/840\n",
      "Batch loss: 0.8338829278945923 batch: 172/840\n",
      "Batch loss: 0.6452392339706421 batch: 173/840\n",
      "Batch loss: 0.5892031788825989 batch: 174/840\n",
      "Batch loss: 0.6079529523849487 batch: 175/840\n",
      "Batch loss: 0.820544421672821 batch: 176/840\n",
      "Batch loss: 0.6911842823028564 batch: 177/840\n",
      "Batch loss: 0.7651296854019165 batch: 178/840\n",
      "Batch loss: 0.6293900012969971 batch: 179/840\n",
      "Batch loss: 0.5783495903015137 batch: 180/840\n",
      "Batch loss: 0.5527352690696716 batch: 181/840\n",
      "Batch loss: 0.664518415927887 batch: 182/840\n",
      "Batch loss: 0.5770166516304016 batch: 183/840\n",
      "Batch loss: 0.5790047645568848 batch: 184/840\n",
      "Batch loss: 0.45561301708221436 batch: 185/840\n",
      "Batch loss: 0.6255784034729004 batch: 186/840\n",
      "Batch loss: 0.6515230536460876 batch: 187/840\n",
      "Batch loss: 0.6455799341201782 batch: 188/840\n",
      "Batch loss: 0.5752015709877014 batch: 189/840\n",
      "Batch loss: 0.6854360699653625 batch: 190/840\n",
      "Batch loss: 0.7691418528556824 batch: 191/840\n",
      "Batch loss: 0.6120997071266174 batch: 192/840\n",
      "Batch loss: 0.5123757719993591 batch: 193/840\n",
      "Batch loss: 0.358018696308136 batch: 194/840\n",
      "Batch loss: 0.6162603497505188 batch: 195/840\n",
      "Batch loss: 0.6912705898284912 batch: 196/840\n",
      "Batch loss: 0.6346877813339233 batch: 197/840\n",
      "Batch loss: 0.5439764261245728 batch: 198/840\n",
      "Batch loss: 0.6377611756324768 batch: 199/840\n",
      "Batch loss: 0.8281884789466858 batch: 200/840\n",
      "Batch loss: 0.6046531200408936 batch: 201/840\n",
      "Batch loss: 0.6428617835044861 batch: 202/840\n",
      "Batch loss: 0.5386370420455933 batch: 203/840\n",
      "Batch loss: 0.7215172052383423 batch: 204/840\n",
      "Batch loss: 0.7303135395050049 batch: 205/840\n",
      "Batch loss: 0.6996253728866577 batch: 206/840\n",
      "Batch loss: 0.6546691060066223 batch: 207/840\n",
      "Batch loss: 0.623607873916626 batch: 208/840\n",
      "Batch loss: 0.6184679865837097 batch: 209/840\n",
      "Batch loss: 0.6253526210784912 batch: 210/840\n",
      "Batch loss: 0.4598645865917206 batch: 211/840\n",
      "Batch loss: 0.5733277797698975 batch: 212/840\n",
      "Batch loss: 0.750885009765625 batch: 213/840\n",
      "Batch loss: 0.7845690250396729 batch: 214/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6859768629074097 batch: 215/840\n",
      "Batch loss: 0.6991797089576721 batch: 216/840\n",
      "Batch loss: 0.6782758235931396 batch: 217/840\n",
      "Batch loss: 0.6809844374656677 batch: 218/840\n",
      "Batch loss: 0.6858030557632446 batch: 219/840\n",
      "Batch loss: 0.9294419288635254 batch: 220/840\n",
      "Batch loss: 0.6737663745880127 batch: 221/840\n",
      "Batch loss: 0.6714615821838379 batch: 222/840\n",
      "Batch loss: 0.5512919425964355 batch: 223/840\n",
      "Batch loss: 0.8526037335395813 batch: 224/840\n",
      "Batch loss: 0.6670998334884644 batch: 225/840\n",
      "Batch loss: 0.7660945057868958 batch: 226/840\n",
      "Batch loss: 0.8043345808982849 batch: 227/840\n",
      "Batch loss: 0.5440222024917603 batch: 228/840\n",
      "Batch loss: 0.5156587362289429 batch: 229/840\n",
      "Batch loss: 0.6406474113464355 batch: 230/840\n",
      "Batch loss: 0.5344092845916748 batch: 231/840\n",
      "Batch loss: 0.6147727966308594 batch: 232/840\n",
      "Batch loss: 0.6722660660743713 batch: 233/840\n",
      "Batch loss: 0.7668278217315674 batch: 234/840\n",
      "Batch loss: 0.6226369738578796 batch: 235/840\n",
      "Batch loss: 0.7421488165855408 batch: 236/840\n",
      "Batch loss: 0.5057778358459473 batch: 237/840\n",
      "Batch loss: 0.6895812749862671 batch: 238/840\n",
      "Batch loss: 0.7362169623374939 batch: 239/840\n",
      "Batch loss: 0.7110412120819092 batch: 240/840\n",
      "Batch loss: 0.7355815172195435 batch: 241/840\n",
      "Batch loss: 0.6617482900619507 batch: 242/840\n",
      "Batch loss: 0.6881502270698547 batch: 243/840\n",
      "Batch loss: 0.8242912888526917 batch: 244/840\n",
      "Batch loss: 0.48784512281417847 batch: 245/840\n",
      "Batch loss: 0.5821460485458374 batch: 246/840\n",
      "Batch loss: 0.7768516540527344 batch: 247/840\n",
      "Batch loss: 0.7160273790359497 batch: 248/840\n",
      "Batch loss: 0.8285505175590515 batch: 249/840\n",
      "Batch loss: 0.5355020761489868 batch: 250/840\n",
      "Batch loss: 0.6471255421638489 batch: 251/840\n",
      "Batch loss: 0.5714888572692871 batch: 252/840\n",
      "Batch loss: 0.6793000102043152 batch: 253/840\n",
      "Batch loss: 0.7373387813568115 batch: 254/840\n",
      "Batch loss: 0.6297116875648499 batch: 255/840\n",
      "Batch loss: 0.7117129564285278 batch: 256/840\n",
      "Batch loss: 0.5785542130470276 batch: 257/840\n",
      "Batch loss: 0.7700614929199219 batch: 258/840\n",
      "Batch loss: 0.6329227685928345 batch: 259/840\n",
      "Batch loss: 0.5555452108383179 batch: 260/840\n",
      "Batch loss: 0.5994978547096252 batch: 261/840\n",
      "Batch loss: 0.49735501408576965 batch: 262/840\n",
      "Batch loss: 0.5722049474716187 batch: 263/840\n",
      "Batch loss: 0.5470601916313171 batch: 264/840\n",
      "Batch loss: 0.7142077684402466 batch: 265/840\n",
      "Batch loss: 0.5464870929718018 batch: 266/840\n",
      "Batch loss: 0.6878505945205688 batch: 267/840\n",
      "Batch loss: 0.6323967576026917 batch: 268/840\n",
      "Batch loss: 0.5273112654685974 batch: 269/840\n",
      "Batch loss: 0.6090486645698547 batch: 270/840\n",
      "Batch loss: 0.5991947650909424 batch: 271/840\n",
      "Batch loss: 0.76247239112854 batch: 272/840\n",
      "Batch loss: 0.714867115020752 batch: 273/840\n",
      "Batch loss: 0.6649771332740784 batch: 274/840\n",
      "Batch loss: 0.7666789293289185 batch: 275/840\n",
      "Batch loss: 0.504416286945343 batch: 276/840\n",
      "Batch loss: 0.5585353374481201 batch: 277/840\n",
      "Batch loss: 0.7741764187812805 batch: 278/840\n",
      "Batch loss: 0.8201912045478821 batch: 279/840\n",
      "Batch loss: 0.8722261786460876 batch: 280/840\n",
      "Batch loss: 0.5630881786346436 batch: 281/840\n",
      "Batch loss: 0.6519677639007568 batch: 282/840\n",
      "Batch loss: 0.7500597238540649 batch: 283/840\n",
      "Batch loss: 0.44188567996025085 batch: 284/840\n",
      "Batch loss: 0.5449387431144714 batch: 285/840\n",
      "Batch loss: 0.5959506034851074 batch: 286/840\n",
      "Batch loss: 0.4638402462005615 batch: 287/840\n",
      "Batch loss: 0.5986448526382446 batch: 288/840\n",
      "Batch loss: 0.8791549801826477 batch: 289/840\n",
      "Batch loss: 0.857506275177002 batch: 290/840\n",
      "Batch loss: 0.8062984347343445 batch: 291/840\n",
      "Batch loss: 0.6024148464202881 batch: 292/840\n",
      "Batch loss: 0.7464478015899658 batch: 293/840\n",
      "Batch loss: 0.5940485000610352 batch: 294/840\n",
      "Batch loss: 0.5630688071250916 batch: 295/840\n",
      "Batch loss: 0.7467426061630249 batch: 296/840\n",
      "Batch loss: 0.6360602378845215 batch: 297/840\n",
      "Batch loss: 0.6847087144851685 batch: 298/840\n",
      "Batch loss: 0.601772665977478 batch: 299/840\n",
      "Batch loss: 0.8472728729248047 batch: 300/840\n",
      "Batch loss: 0.7013434767723083 batch: 301/840\n",
      "Batch loss: 0.6760920882225037 batch: 302/840\n",
      "Batch loss: 0.7635756134986877 batch: 303/840\n",
      "Batch loss: 0.6069025993347168 batch: 304/840\n",
      "Batch loss: 0.5682925581932068 batch: 305/840\n",
      "Batch loss: 0.7371788024902344 batch: 306/840\n",
      "Batch loss: 0.48096588253974915 batch: 307/840\n",
      "Batch loss: 0.6815482378005981 batch: 308/840\n",
      "Batch loss: 0.6174023747444153 batch: 309/840\n",
      "Batch loss: 0.9007840752601624 batch: 310/840\n",
      "Batch loss: 0.7003787755966187 batch: 311/840\n",
      "Batch loss: 0.7009822130203247 batch: 312/840\n",
      "Batch loss: 0.7701127529144287 batch: 313/840\n",
      "Batch loss: 0.5385069251060486 batch: 314/840\n",
      "Batch loss: 0.661048412322998 batch: 315/840\n",
      "Batch loss: 0.4934290945529938 batch: 316/840\n",
      "Batch loss: 0.7934315204620361 batch: 317/840\n",
      "Batch loss: 0.6352955102920532 batch: 318/840\n",
      "Batch loss: 0.7604313492774963 batch: 319/840\n",
      "Batch loss: 0.6594257950782776 batch: 320/840\n",
      "Batch loss: 0.6543107032775879 batch: 321/840\n",
      "Batch loss: 0.762830376625061 batch: 322/840\n",
      "Batch loss: 0.7330037951469421 batch: 323/840\n",
      "Batch loss: 0.754243791103363 batch: 324/840\n",
      "Batch loss: 0.5646869540214539 batch: 325/840\n",
      "Batch loss: 0.630785346031189 batch: 326/840\n",
      "Batch loss: 0.5567302107810974 batch: 327/840\n",
      "Batch loss: 0.8167403340339661 batch: 328/840\n",
      "Batch loss: 0.7240191698074341 batch: 329/840\n",
      "Batch loss: 0.6609154343605042 batch: 330/840\n",
      "Batch loss: 0.7052819728851318 batch: 331/840\n",
      "Batch loss: 0.6556170582771301 batch: 332/840\n",
      "Batch loss: 0.5739328861236572 batch: 333/840\n",
      "Batch loss: 0.7367759943008423 batch: 334/840\n",
      "Batch loss: 0.6022569537162781 batch: 335/840\n",
      "Batch loss: 0.7285261750221252 batch: 336/840\n",
      "Batch loss: 0.7997615933418274 batch: 337/840\n",
      "Batch loss: 0.7348971366882324 batch: 338/840\n",
      "Batch loss: 0.6575078368186951 batch: 339/840\n",
      "Batch loss: 0.8188313841819763 batch: 340/840\n",
      "Batch loss: 0.6368504166603088 batch: 341/840\n",
      "Batch loss: 0.5135520696640015 batch: 342/840\n",
      "Batch loss: 0.9031776189804077 batch: 343/840\n",
      "Batch loss: 0.6263133883476257 batch: 344/840\n",
      "Batch loss: 0.4622958302497864 batch: 345/840\n",
      "Batch loss: 0.6503222584724426 batch: 346/840\n",
      "Batch loss: 0.7769143581390381 batch: 347/840\n",
      "Batch loss: 0.635028600692749 batch: 348/840\n",
      "Batch loss: 0.5713695287704468 batch: 349/840\n",
      "Batch loss: 0.5348128080368042 batch: 350/840\n",
      "Batch loss: 0.6759195923805237 batch: 351/840\n",
      "Batch loss: 0.6662173271179199 batch: 352/840\n",
      "Batch loss: 0.6864722371101379 batch: 353/840\n",
      "Batch loss: 0.6685733199119568 batch: 354/840\n",
      "Batch loss: 0.586560845375061 batch: 355/840\n",
      "Batch loss: 0.5979289412498474 batch: 356/840\n",
      "Batch loss: 0.5707971453666687 batch: 357/840\n",
      "Batch loss: 0.897739589214325 batch: 358/840\n",
      "Batch loss: 0.5092233419418335 batch: 359/840\n",
      "Batch loss: 0.8856388926506042 batch: 360/840\n",
      "Batch loss: 0.6526398658752441 batch: 361/840\n",
      "Batch loss: 0.5912660360336304 batch: 362/840\n",
      "Batch loss: 0.5864055752754211 batch: 363/840\n",
      "Batch loss: 0.758846640586853 batch: 364/840\n",
      "Batch loss: 0.592460036277771 batch: 365/840\n",
      "Batch loss: 0.7225911617279053 batch: 366/840\n",
      "Batch loss: 0.39174267649650574 batch: 367/840\n",
      "Batch loss: 0.862601637840271 batch: 368/840\n",
      "Batch loss: 0.6040643453598022 batch: 369/840\n",
      "Batch loss: 0.7196823358535767 batch: 370/840\n",
      "Batch loss: 0.6135584712028503 batch: 371/840\n",
      "Batch loss: 0.4689926207065582 batch: 372/840\n",
      "Batch loss: 0.7290188670158386 batch: 373/840\n",
      "Batch loss: 0.6928699016571045 batch: 374/840\n",
      "Batch loss: 0.6088442206382751 batch: 375/840\n",
      "Batch loss: 0.4690110385417938 batch: 376/840\n",
      "Batch loss: 0.6695973873138428 batch: 377/840\n",
      "Batch loss: 0.6590074300765991 batch: 378/840\n",
      "Batch loss: 0.5517866015434265 batch: 379/840\n",
      "Batch loss: 0.8439856767654419 batch: 380/840\n",
      "Batch loss: 0.9578734040260315 batch: 381/840\n",
      "Batch loss: 0.7931050062179565 batch: 382/840\n",
      "Batch loss: 0.7174583673477173 batch: 383/840\n",
      "Batch loss: 0.6219635605812073 batch: 384/840\n",
      "Batch loss: 0.7189337015151978 batch: 385/840\n",
      "Batch loss: 0.6761101484298706 batch: 386/840\n",
      "Batch loss: 0.6056203842163086 batch: 387/840\n",
      "Batch loss: 0.7709102630615234 batch: 388/840\n",
      "Batch loss: 0.5904181003570557 batch: 389/840\n",
      "Batch loss: 0.8718942999839783 batch: 390/840\n",
      "Batch loss: 0.6477551460266113 batch: 391/840\n",
      "Batch loss: 0.6224398612976074 batch: 392/840\n",
      "Batch loss: 0.438108891248703 batch: 393/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7345939874649048 batch: 394/840\n",
      "Batch loss: 0.5910273194313049 batch: 395/840\n",
      "Batch loss: 0.6479747891426086 batch: 396/840\n",
      "Batch loss: 0.603110671043396 batch: 397/840\n",
      "Batch loss: 0.7764988541603088 batch: 398/840\n",
      "Batch loss: 0.4449108839035034 batch: 399/840\n",
      "Batch loss: 0.6228424906730652 batch: 400/840\n",
      "Batch loss: 0.652009129524231 batch: 401/840\n",
      "Batch loss: 0.5601206421852112 batch: 402/840\n",
      "Batch loss: 0.644626259803772 batch: 403/840\n",
      "Batch loss: 0.6018016338348389 batch: 404/840\n",
      "Batch loss: 0.537771999835968 batch: 405/840\n",
      "Batch loss: 0.6725877523422241 batch: 406/840\n",
      "Batch loss: 0.5965656638145447 batch: 407/840\n",
      "Batch loss: 0.7520586252212524 batch: 408/840\n",
      "Batch loss: 0.6969004273414612 batch: 409/840\n",
      "Batch loss: 0.7321940064430237 batch: 410/840\n",
      "Batch loss: 0.7318657040596008 batch: 411/840\n",
      "Batch loss: 0.8395887017250061 batch: 412/840\n",
      "Batch loss: 0.7033280730247498 batch: 413/840\n",
      "Batch loss: 0.6510676741600037 batch: 414/840\n",
      "Batch loss: 0.9329430460929871 batch: 415/840\n",
      "Batch loss: 0.5061418414115906 batch: 416/840\n",
      "Batch loss: 0.6612134575843811 batch: 417/840\n",
      "Batch loss: 0.846304714679718 batch: 418/840\n",
      "Batch loss: 0.6679781079292297 batch: 419/840\n",
      "Batch loss: 0.6951597332954407 batch: 420/840\n",
      "Batch loss: 0.49440306425094604 batch: 421/840\n",
      "Batch loss: 0.6148686408996582 batch: 422/840\n",
      "Batch loss: 0.6375332474708557 batch: 423/840\n",
      "Batch loss: 0.6952133774757385 batch: 424/840\n",
      "Batch loss: 0.7508761882781982 batch: 425/840\n",
      "Batch loss: 0.6256619691848755 batch: 426/840\n",
      "Batch loss: 0.7383155822753906 batch: 427/840\n",
      "Batch loss: 0.7602963447570801 batch: 428/840\n",
      "Batch loss: 0.5665620565414429 batch: 429/840\n",
      "Batch loss: 0.8263814449310303 batch: 430/840\n",
      "Batch loss: 0.6407334804534912 batch: 431/840\n",
      "Batch loss: 0.6774618029594421 batch: 432/840\n",
      "Batch loss: 0.45680728554725647 batch: 433/840\n",
      "Batch loss: 0.6031068563461304 batch: 434/840\n",
      "Batch loss: 0.7566065192222595 batch: 435/840\n",
      "Batch loss: 0.6200162768363953 batch: 436/840\n",
      "Batch loss: 0.7079839110374451 batch: 437/840\n",
      "Batch loss: 0.3768993318080902 batch: 438/840\n",
      "Batch loss: 0.6766345500946045 batch: 439/840\n",
      "Batch loss: 0.7866652607917786 batch: 440/840\n",
      "Batch loss: 0.6179479956626892 batch: 441/840\n",
      "Batch loss: 0.7539970278739929 batch: 442/840\n",
      "Batch loss: 0.5933376550674438 batch: 443/840\n",
      "Batch loss: 0.677663266658783 batch: 444/840\n",
      "Batch loss: 0.5861918330192566 batch: 445/840\n",
      "Batch loss: 0.7473076581954956 batch: 446/840\n",
      "Batch loss: 0.7076768279075623 batch: 447/840\n",
      "Batch loss: 0.7721246480941772 batch: 448/840\n",
      "Batch loss: 0.5052064657211304 batch: 449/840\n",
      "Batch loss: 0.6550821661949158 batch: 450/840\n",
      "Batch loss: 0.7781834602355957 batch: 451/840\n",
      "Batch loss: 0.7661043405532837 batch: 452/840\n",
      "Batch loss: 0.6458780169487 batch: 453/840\n",
      "Batch loss: 0.659739077091217 batch: 454/840\n",
      "Batch loss: 0.6450521349906921 batch: 455/840\n",
      "Batch loss: 0.732283353805542 batch: 456/840\n",
      "Batch loss: 0.6584355235099792 batch: 457/840\n",
      "Batch loss: 0.6029708385467529 batch: 458/840\n",
      "Batch loss: 0.5894356966018677 batch: 459/840\n",
      "Batch loss: 0.4937534034252167 batch: 460/840\n",
      "Batch loss: 0.7201312780380249 batch: 461/840\n",
      "Batch loss: 0.6917359232902527 batch: 462/840\n",
      "Batch loss: 0.8034558296203613 batch: 463/840\n",
      "Batch loss: 0.5298892855644226 batch: 464/840\n",
      "Batch loss: 0.9105015397071838 batch: 465/840\n",
      "Batch loss: 0.6682237982749939 batch: 466/840\n",
      "Batch loss: 0.9936187863349915 batch: 467/840\n",
      "Batch loss: 0.5602394342422485 batch: 468/840\n",
      "Batch loss: 0.5810866951942444 batch: 469/840\n",
      "Batch loss: 0.6117379665374756 batch: 470/840\n",
      "Batch loss: 0.6299469470977783 batch: 471/840\n",
      "Batch loss: 0.7776394486427307 batch: 472/840\n",
      "Batch loss: 0.7176315784454346 batch: 473/840\n",
      "Batch loss: 0.5882217288017273 batch: 474/840\n",
      "Batch loss: 0.6279010772705078 batch: 475/840\n",
      "Batch loss: 0.6661180853843689 batch: 476/840\n",
      "Batch loss: 0.5459529757499695 batch: 477/840\n",
      "Batch loss: 0.5988069772720337 batch: 478/840\n",
      "Batch loss: 0.5918056964874268 batch: 479/840\n",
      "Batch loss: 0.7102453708648682 batch: 480/840\n",
      "Batch loss: 0.6611930727958679 batch: 481/840\n",
      "Batch loss: 0.6722288727760315 batch: 482/840\n",
      "Batch loss: 0.8515562415122986 batch: 483/840\n",
      "Batch loss: 0.5675894618034363 batch: 484/840\n",
      "Batch loss: 0.6471593976020813 batch: 485/840\n",
      "Batch loss: 0.6298452615737915 batch: 486/840\n",
      "Batch loss: 0.4034357964992523 batch: 487/840\n",
      "Batch loss: 0.6890853643417358 batch: 488/840\n",
      "Batch loss: 0.6274408102035522 batch: 489/840\n",
      "Batch loss: 0.6845178008079529 batch: 490/840\n",
      "Batch loss: 0.3959035873413086 batch: 491/840\n",
      "Batch loss: 0.8939723968505859 batch: 492/840\n",
      "Batch loss: 0.6868401169776917 batch: 493/840\n",
      "Batch loss: 0.41647475957870483 batch: 494/840\n",
      "Batch loss: 0.7548953890800476 batch: 495/840\n",
      "Batch loss: 0.6442213654518127 batch: 496/840\n",
      "Batch loss: 0.7600091695785522 batch: 497/840\n",
      "Batch loss: 0.5527200102806091 batch: 498/840\n",
      "Batch loss: 0.7675630450248718 batch: 499/840\n",
      "Batch loss: 0.6371462345123291 batch: 500/840\n",
      "Batch loss: 0.585002601146698 batch: 501/840\n",
      "Batch loss: 0.7552800178527832 batch: 502/840\n",
      "Batch loss: 0.5067822933197021 batch: 503/840\n",
      "Batch loss: 0.6874606609344482 batch: 504/840\n",
      "Batch loss: 0.7343412041664124 batch: 505/840\n",
      "Batch loss: 0.6691672801971436 batch: 506/840\n",
      "Batch loss: 0.8315305113792419 batch: 507/840\n",
      "Batch loss: 0.5528198480606079 batch: 508/840\n",
      "Batch loss: 0.6629657745361328 batch: 509/840\n",
      "Batch loss: 0.5889033079147339 batch: 510/840\n",
      "Batch loss: 0.5562590956687927 batch: 511/840\n",
      "Batch loss: 0.7903072237968445 batch: 512/840\n",
      "Batch loss: 0.6199682354927063 batch: 513/840\n",
      "Batch loss: 0.7248103618621826 batch: 514/840\n",
      "Batch loss: 0.5277380347251892 batch: 515/840\n",
      "Batch loss: 0.6829717755317688 batch: 516/840\n",
      "Batch loss: 0.6263736486434937 batch: 517/840\n",
      "Batch loss: 0.6667910218238831 batch: 518/840\n",
      "Batch loss: 0.8587108850479126 batch: 519/840\n",
      "Batch loss: 0.5782562494277954 batch: 520/840\n",
      "Batch loss: 0.6134236454963684 batch: 521/840\n",
      "Batch loss: 0.5868345499038696 batch: 522/840\n",
      "Batch loss: 0.4692569077014923 batch: 523/840\n",
      "Batch loss: 0.6511329412460327 batch: 524/840\n",
      "Batch loss: 0.6977231502532959 batch: 525/840\n",
      "Batch loss: 0.7630549073219299 batch: 526/840\n",
      "Batch loss: 0.7798963189125061 batch: 527/840\n",
      "Batch loss: 0.6963056325912476 batch: 528/840\n",
      "Batch loss: 0.5534203052520752 batch: 529/840\n",
      "Batch loss: 0.6859226226806641 batch: 530/840\n",
      "Batch loss: 0.5872577428817749 batch: 531/840\n",
      "Batch loss: 0.4508224129676819 batch: 532/840\n",
      "Batch loss: 0.7086886763572693 batch: 533/840\n",
      "Batch loss: 0.7194183468818665 batch: 534/840\n",
      "Batch loss: 0.6019004583358765 batch: 535/840\n",
      "Batch loss: 0.8949230313301086 batch: 536/840\n",
      "Batch loss: 0.6627991199493408 batch: 537/840\n",
      "Batch loss: 0.602260410785675 batch: 538/840\n",
      "Batch loss: 0.6231849193572998 batch: 539/840\n",
      "Batch loss: 0.778102695941925 batch: 540/840\n",
      "Batch loss: 0.5737347602844238 batch: 541/840\n",
      "Batch loss: 0.6906365156173706 batch: 542/840\n",
      "Batch loss: 0.4841456711292267 batch: 543/840\n",
      "Batch loss: 0.6812137365341187 batch: 544/840\n",
      "Batch loss: 0.7449018955230713 batch: 545/840\n",
      "Batch loss: 0.6208360195159912 batch: 546/840\n",
      "Batch loss: 0.6515896320343018 batch: 547/840\n",
      "Batch loss: 0.5071609616279602 batch: 548/840\n",
      "Batch loss: 0.5755045413970947 batch: 549/840\n",
      "Batch loss: 0.6409092545509338 batch: 550/840\n",
      "Batch loss: 0.5932726263999939 batch: 551/840\n",
      "Batch loss: 0.5277904272079468 batch: 552/840\n",
      "Batch loss: 0.7610070705413818 batch: 553/840\n",
      "Batch loss: 0.7690660357475281 batch: 554/840\n",
      "Batch loss: 0.610140323638916 batch: 555/840\n",
      "Batch loss: 0.6498798131942749 batch: 556/840\n",
      "Batch loss: 0.6651431322097778 batch: 557/840\n",
      "Batch loss: 0.5804977416992188 batch: 558/840\n",
      "Batch loss: 0.7979828119277954 batch: 559/840\n",
      "Batch loss: 0.678391695022583 batch: 560/840\n",
      "Batch loss: 0.6360915303230286 batch: 561/840\n",
      "Batch loss: 0.6203647255897522 batch: 562/840\n",
      "Batch loss: 0.46124371886253357 batch: 563/840\n",
      "Batch loss: 0.6718169450759888 batch: 564/840\n",
      "Batch loss: 0.8187614679336548 batch: 565/840\n",
      "Batch loss: 0.7001640200614929 batch: 566/840\n",
      "Batch loss: 0.6701110601425171 batch: 567/840\n",
      "Batch loss: 0.5541591048240662 batch: 568/840\n",
      "Batch loss: 0.6958848834037781 batch: 569/840\n",
      "Batch loss: 0.4051813781261444 batch: 570/840\n",
      "Batch loss: 0.7611845135688782 batch: 571/840\n",
      "Batch loss: 0.6757606267929077 batch: 572/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.656150221824646 batch: 573/840\n",
      "Batch loss: 0.7795054912567139 batch: 574/840\n",
      "Batch loss: 0.5167266726493835 batch: 575/840\n",
      "Batch loss: 0.7259087562561035 batch: 576/840\n",
      "Batch loss: 0.6271551251411438 batch: 577/840\n",
      "Batch loss: 0.750383734703064 batch: 578/840\n",
      "Batch loss: 0.6894859075546265 batch: 579/840\n",
      "Batch loss: 0.8437957167625427 batch: 580/840\n",
      "Batch loss: 0.7548210024833679 batch: 581/840\n",
      "Batch loss: 0.8586009740829468 batch: 582/840\n",
      "Batch loss: 0.6151660084724426 batch: 583/840\n",
      "Batch loss: 0.7065261006355286 batch: 584/840\n",
      "Batch loss: 0.6819703578948975 batch: 585/840\n",
      "Batch loss: 0.6843703985214233 batch: 586/840\n",
      "Batch loss: 0.6434952020645142 batch: 587/840\n",
      "Batch loss: 0.49952012300491333 batch: 588/840\n",
      "Batch loss: 0.7644034028053284 batch: 589/840\n",
      "Batch loss: 0.554949164390564 batch: 590/840\n",
      "Batch loss: 0.4679270088672638 batch: 591/840\n",
      "Batch loss: 0.5980646014213562 batch: 592/840\n",
      "Batch loss: 0.7976739406585693 batch: 593/840\n",
      "Batch loss: 0.6787362694740295 batch: 594/840\n",
      "Batch loss: 0.45024722814559937 batch: 595/840\n",
      "Batch loss: 0.5602824091911316 batch: 596/840\n",
      "Batch loss: 0.5499177575111389 batch: 597/840\n",
      "Batch loss: 0.5093665719032288 batch: 598/840\n",
      "Batch loss: 0.5466963052749634 batch: 599/840\n",
      "Batch loss: 0.6769907474517822 batch: 600/840\n",
      "Batch loss: 0.7551906704902649 batch: 601/840\n",
      "Batch loss: 0.5599700808525085 batch: 602/840\n",
      "Batch loss: 0.6197702288627625 batch: 603/840\n",
      "Batch loss: 0.7780841588973999 batch: 604/840\n",
      "Batch loss: 0.8523745536804199 batch: 605/840\n",
      "Batch loss: 0.8355969190597534 batch: 606/840\n",
      "Batch loss: 0.6582448482513428 batch: 607/840\n",
      "Batch loss: 0.7235580682754517 batch: 608/840\n",
      "Batch loss: 0.5501939654350281 batch: 609/840\n",
      "Batch loss: 0.7389907240867615 batch: 610/840\n",
      "Batch loss: 0.6788640022277832 batch: 611/840\n",
      "Batch loss: 0.6995313763618469 batch: 612/840\n",
      "Batch loss: 0.6886137127876282 batch: 613/840\n",
      "Batch loss: 0.698517918586731 batch: 614/840\n",
      "Batch loss: 0.6162468194961548 batch: 615/840\n",
      "Batch loss: 0.584744393825531 batch: 616/840\n",
      "Batch loss: 0.5048888921737671 batch: 617/840\n",
      "Batch loss: 0.6728412508964539 batch: 618/840\n",
      "Batch loss: 0.7358516454696655 batch: 619/840\n",
      "Batch loss: 0.6069002151489258 batch: 620/840\n",
      "Batch loss: 0.7127435207366943 batch: 621/840\n",
      "Batch loss: 0.6675792932510376 batch: 622/840\n",
      "Batch loss: 0.5412858724594116 batch: 623/840\n",
      "Batch loss: 0.8919780850410461 batch: 624/840\n",
      "Batch loss: 0.6576987504959106 batch: 625/840\n",
      "Batch loss: 0.5505960583686829 batch: 626/840\n",
      "Batch loss: 0.6475474834442139 batch: 627/840\n",
      "Batch loss: 0.6849014759063721 batch: 628/840\n",
      "Batch loss: 0.6546329259872437 batch: 629/840\n",
      "Batch loss: 0.7185606360435486 batch: 630/840\n",
      "Batch loss: 0.7159990668296814 batch: 631/840\n",
      "Batch loss: 0.6928489208221436 batch: 632/840\n",
      "Batch loss: 0.597139298915863 batch: 633/840\n",
      "Batch loss: 0.7457689046859741 batch: 634/840\n",
      "Batch loss: 0.6493092179298401 batch: 635/840\n",
      "Batch loss: 0.5910604596138 batch: 636/840\n",
      "Batch loss: 0.5186030864715576 batch: 637/840\n",
      "Batch loss: 0.6889356970787048 batch: 638/840\n",
      "Batch loss: 0.6531295776367188 batch: 639/840\n",
      "Batch loss: 0.5969009399414062 batch: 640/840\n",
      "Batch loss: 0.8046010732650757 batch: 641/840\n",
      "Batch loss: 0.6137198805809021 batch: 642/840\n",
      "Batch loss: 1.0404598712921143 batch: 643/840\n",
      "Batch loss: 0.5534742474555969 batch: 644/840\n",
      "Batch loss: 0.6039261817932129 batch: 645/840\n",
      "Batch loss: 0.5458519458770752 batch: 646/840\n",
      "Batch loss: 0.7357521653175354 batch: 647/840\n",
      "Batch loss: 0.6507548689842224 batch: 648/840\n",
      "Batch loss: 0.7234146595001221 batch: 649/840\n",
      "Batch loss: 0.5901943445205688 batch: 650/840\n",
      "Batch loss: 0.5794776678085327 batch: 651/840\n",
      "Batch loss: 0.8062310218811035 batch: 652/840\n",
      "Batch loss: 0.48448440432548523 batch: 653/840\n",
      "Batch loss: 0.8676561117172241 batch: 654/840\n",
      "Batch loss: 0.5928875803947449 batch: 655/840\n",
      "Batch loss: 0.8191848993301392 batch: 656/840\n",
      "Batch loss: 0.6271962523460388 batch: 657/840\n",
      "Batch loss: 0.7485415935516357 batch: 658/840\n",
      "Batch loss: 0.6278812289237976 batch: 659/840\n",
      "Batch loss: 0.5694518685340881 batch: 660/840\n",
      "Batch loss: 0.7335023283958435 batch: 661/840\n",
      "Batch loss: 0.6652538180351257 batch: 662/840\n",
      "Batch loss: 0.5643132925033569 batch: 663/840\n",
      "Batch loss: 0.7219548225402832 batch: 664/840\n",
      "Batch loss: 0.7300395965576172 batch: 665/840\n",
      "Batch loss: 0.7555928826332092 batch: 666/840\n",
      "Batch loss: 0.9152669310569763 batch: 667/840\n",
      "Batch loss: 0.5970762968063354 batch: 668/840\n",
      "Batch loss: 0.6291519999504089 batch: 669/840\n",
      "Batch loss: 0.7807609438896179 batch: 670/840\n",
      "Batch loss: 0.6162122488021851 batch: 671/840\n",
      "Batch loss: 0.6791744232177734 batch: 672/840\n",
      "Batch loss: 0.8894177079200745 batch: 673/840\n",
      "Batch loss: 0.759936511516571 batch: 674/840\n",
      "Batch loss: 0.6364536285400391 batch: 675/840\n",
      "Batch loss: 0.5824753642082214 batch: 676/840\n",
      "Batch loss: 0.5643472075462341 batch: 677/840\n",
      "Batch loss: 0.6553198099136353 batch: 678/840\n",
      "Batch loss: 0.7929583787918091 batch: 679/840\n",
      "Batch loss: 0.6185489892959595 batch: 680/840\n",
      "Batch loss: 0.7370302081108093 batch: 681/840\n",
      "Batch loss: 0.59312504529953 batch: 682/840\n",
      "Batch loss: 0.5107610821723938 batch: 683/840\n",
      "Batch loss: 0.8055129051208496 batch: 684/840\n",
      "Batch loss: 0.7130784392356873 batch: 685/840\n",
      "Batch loss: 0.8200277090072632 batch: 686/840\n",
      "Batch loss: 0.7514030933380127 batch: 687/840\n",
      "Batch loss: 0.6975242495536804 batch: 688/840\n",
      "Batch loss: 0.5749652981758118 batch: 689/840\n",
      "Batch loss: 0.5541152358055115 batch: 690/840\n",
      "Batch loss: 0.6215600371360779 batch: 691/840\n",
      "Batch loss: 0.6357659697532654 batch: 692/840\n",
      "Batch loss: 0.665438175201416 batch: 693/840\n",
      "Batch loss: 0.7427319288253784 batch: 694/840\n",
      "Batch loss: 0.6630600094795227 batch: 695/840\n",
      "Batch loss: 0.7152851819992065 batch: 696/840\n",
      "Batch loss: 0.6234151721000671 batch: 697/840\n",
      "Batch loss: 0.6751649379730225 batch: 698/840\n",
      "Batch loss: 0.7334191799163818 batch: 699/840\n",
      "Batch loss: 0.6711111664772034 batch: 700/840\n",
      "Batch loss: 0.7339352369308472 batch: 701/840\n",
      "Batch loss: 0.6430001258850098 batch: 702/840\n",
      "Batch loss: 0.5330235362052917 batch: 703/840\n",
      "Batch loss: 0.7647234201431274 batch: 704/840\n",
      "Batch loss: 0.5601646900177002 batch: 705/840\n",
      "Batch loss: 0.7192075848579407 batch: 706/840\n",
      "Batch loss: 0.6404776573181152 batch: 707/840\n",
      "Batch loss: 0.7400529384613037 batch: 708/840\n",
      "Batch loss: 0.7634978294372559 batch: 709/840\n",
      "Batch loss: 0.7935099601745605 batch: 710/840\n",
      "Batch loss: 0.47584763169288635 batch: 711/840\n",
      "Batch loss: 0.7150646448135376 batch: 712/840\n",
      "Batch loss: 0.5506978631019592 batch: 713/840\n",
      "Batch loss: 0.7298853993415833 batch: 714/840\n",
      "Batch loss: 0.8092837333679199 batch: 715/840\n",
      "Batch loss: 0.7103534936904907 batch: 716/840\n",
      "Batch loss: 0.5478464365005493 batch: 717/840\n",
      "Batch loss: 0.6369516253471375 batch: 718/840\n",
      "Batch loss: 0.45942923426628113 batch: 719/840\n",
      "Batch loss: 0.5454267263412476 batch: 720/840\n",
      "Batch loss: 0.7543792724609375 batch: 721/840\n",
      "Batch loss: 0.725563108921051 batch: 722/840\n",
      "Batch loss: 0.5638654828071594 batch: 723/840\n",
      "Batch loss: 0.7528751492500305 batch: 724/840\n",
      "Batch loss: 0.6247961521148682 batch: 725/840\n",
      "Batch loss: 0.6489140391349792 batch: 726/840\n",
      "Batch loss: 0.8465183973312378 batch: 727/840\n",
      "Batch loss: 0.7398495674133301 batch: 728/840\n",
      "Batch loss: 0.6860585808753967 batch: 729/840\n",
      "Batch loss: 0.6519638895988464 batch: 730/840\n",
      "Batch loss: 0.5511876940727234 batch: 731/840\n",
      "Batch loss: 0.9399339556694031 batch: 732/840\n",
      "Batch loss: 0.6930890083312988 batch: 733/840\n",
      "Batch loss: 0.6681844592094421 batch: 734/840\n",
      "Batch loss: 0.6048929691314697 batch: 735/840\n",
      "Batch loss: 0.7104897499084473 batch: 736/840\n",
      "Batch loss: 0.6392602920532227 batch: 737/840\n",
      "Batch loss: 0.5195401906967163 batch: 738/840\n",
      "Batch loss: 0.7040551900863647 batch: 739/840\n",
      "Batch loss: 0.8172253966331482 batch: 740/840\n",
      "Batch loss: 0.637834370136261 batch: 741/840\n",
      "Batch loss: 0.553602933883667 batch: 742/840\n",
      "Batch loss: 0.46747273206710815 batch: 743/840\n",
      "Batch loss: 0.5557677745819092 batch: 744/840\n",
      "Batch loss: 0.6301500797271729 batch: 745/840\n",
      "Batch loss: 0.6183401346206665 batch: 746/840\n",
      "Batch loss: 0.6391815543174744 batch: 747/840\n",
      "Batch loss: 0.7899317145347595 batch: 748/840\n",
      "Batch loss: 0.5213009119033813 batch: 749/840\n",
      "Batch loss: 0.5552691221237183 batch: 750/840\n",
      "Batch loss: 0.71518874168396 batch: 751/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8145378828048706 batch: 752/840\n",
      "Batch loss: 0.5652704834938049 batch: 753/840\n",
      "Batch loss: 0.6685783267021179 batch: 754/840\n",
      "Batch loss: 0.6702832579612732 batch: 755/840\n",
      "Batch loss: 0.5927320718765259 batch: 756/840\n",
      "Batch loss: 0.6473495960235596 batch: 757/840\n",
      "Batch loss: 0.6631720662117004 batch: 758/840\n",
      "Batch loss: 0.5276720523834229 batch: 759/840\n",
      "Batch loss: 0.6075963973999023 batch: 760/840\n",
      "Batch loss: 0.5651214122772217 batch: 761/840\n",
      "Batch loss: 0.8148454427719116 batch: 762/840\n",
      "Batch loss: 0.38564586639404297 batch: 763/840\n",
      "Batch loss: 0.6904526352882385 batch: 764/840\n",
      "Batch loss: 0.5302625894546509 batch: 765/840\n",
      "Batch loss: 0.5483038425445557 batch: 766/840\n",
      "Batch loss: 0.6895551085472107 batch: 767/840\n",
      "Batch loss: 0.6501303315162659 batch: 768/840\n",
      "Batch loss: 0.8633359670639038 batch: 769/840\n",
      "Batch loss: 0.5449306964874268 batch: 770/840\n",
      "Batch loss: 0.6466947793960571 batch: 771/840\n",
      "Batch loss: 0.5897641181945801 batch: 772/840\n",
      "Batch loss: 0.5487260818481445 batch: 773/840\n",
      "Batch loss: 0.5072406530380249 batch: 774/840\n",
      "Batch loss: 0.5498449206352234 batch: 775/840\n",
      "Batch loss: 0.573380172252655 batch: 776/840\n",
      "Batch loss: 0.5728092193603516 batch: 777/840\n",
      "Batch loss: 0.49387219548225403 batch: 778/840\n",
      "Batch loss: 0.7360808849334717 batch: 779/840\n",
      "Batch loss: 0.5937699675559998 batch: 780/840\n",
      "Batch loss: 0.6860910058021545 batch: 781/840\n",
      "Batch loss: 0.6672508120536804 batch: 782/840\n",
      "Batch loss: 0.549121081829071 batch: 783/840\n",
      "Batch loss: 0.7521575689315796 batch: 784/840\n",
      "Batch loss: 0.5735012888908386 batch: 785/840\n",
      "Batch loss: 0.7176650166511536 batch: 786/840\n",
      "Batch loss: 0.622219443321228 batch: 787/840\n",
      "Batch loss: 0.6977083086967468 batch: 788/840\n",
      "Batch loss: 0.6568061709403992 batch: 789/840\n",
      "Batch loss: 0.6667852997779846 batch: 790/840\n",
      "Batch loss: 0.6807142496109009 batch: 791/840\n",
      "Batch loss: 0.4586069881916046 batch: 792/840\n",
      "Batch loss: 0.6124240159988403 batch: 793/840\n",
      "Batch loss: 0.7291174530982971 batch: 794/840\n",
      "Batch loss: 0.6967217922210693 batch: 795/840\n",
      "Batch loss: 0.7657825350761414 batch: 796/840\n",
      "Batch loss: 0.6789363622665405 batch: 797/840\n",
      "Batch loss: 0.588072657585144 batch: 798/840\n",
      "Batch loss: 0.7695910930633545 batch: 799/840\n",
      "Batch loss: 0.6282835602760315 batch: 800/840\n",
      "Batch loss: 0.7373631000518799 batch: 801/840\n",
      "Batch loss: 0.5949417948722839 batch: 802/840\n",
      "Batch loss: 0.524946391582489 batch: 803/840\n",
      "Batch loss: 0.64277184009552 batch: 804/840\n",
      "Batch loss: 0.5473802089691162 batch: 805/840\n",
      "Batch loss: 0.615090012550354 batch: 806/840\n",
      "Batch loss: 0.614031970500946 batch: 807/840\n",
      "Batch loss: 0.6136566996574402 batch: 808/840\n",
      "Batch loss: 0.6557481288909912 batch: 809/840\n",
      "Batch loss: 0.614166259765625 batch: 810/840\n",
      "Batch loss: 0.5903039574623108 batch: 811/840\n",
      "Batch loss: 0.7312210202217102 batch: 812/840\n",
      "Batch loss: 0.5623680949211121 batch: 813/840\n",
      "Batch loss: 0.6391364932060242 batch: 814/840\n",
      "Batch loss: 0.6961365342140198 batch: 815/840\n",
      "Batch loss: 0.5988809466362 batch: 816/840\n",
      "Batch loss: 0.740019679069519 batch: 817/840\n",
      "Batch loss: 0.6793495416641235 batch: 818/840\n",
      "Batch loss: 0.552200436592102 batch: 819/840\n",
      "Batch loss: 0.7056140899658203 batch: 820/840\n",
      "Batch loss: 0.649421215057373 batch: 821/840\n",
      "Batch loss: 0.6934136748313904 batch: 822/840\n",
      "Batch loss: 0.7296501994132996 batch: 823/840\n",
      "Batch loss: 0.7515964508056641 batch: 824/840\n",
      "Batch loss: 0.7601261138916016 batch: 825/840\n",
      "Batch loss: 0.5486146211624146 batch: 826/840\n",
      "Batch loss: 0.6156942248344421 batch: 827/840\n",
      "Batch loss: 0.7425209283828735 batch: 828/840\n",
      "Batch loss: 0.5559021234512329 batch: 829/840\n",
      "Batch loss: 0.6660346984863281 batch: 830/840\n",
      "Batch loss: 0.5693060755729675 batch: 831/840\n",
      "Batch loss: 0.7216464877128601 batch: 832/840\n",
      "Batch loss: 0.7770317196846008 batch: 833/840\n",
      "Batch loss: 0.6745837330818176 batch: 834/840\n",
      "Batch loss: 0.4504276514053345 batch: 835/840\n",
      "Batch loss: 0.5683481693267822 batch: 836/840\n",
      "Batch loss: 0.6281469464302063 batch: 837/840\n",
      "Batch loss: 0.7732208967208862 batch: 838/840\n",
      "Batch loss: 0.5309184789657593 batch: 839/840\n",
      "Batch loss: 0.6494854688644409 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 5/15..  Training Loss: 0.007..  Test Loss: 0.005..  Test Accuracy: 0.822\n",
      "Running epoch 6/15\n",
      "Batch loss: 0.4427791237831116 batch: 1/840\n",
      "Batch loss: 1.0577385425567627 batch: 2/840\n",
      "Batch loss: 0.64284348487854 batch: 3/840\n",
      "Batch loss: 0.6610840559005737 batch: 4/840\n",
      "Batch loss: 0.6248111128807068 batch: 5/840\n",
      "Batch loss: 0.4566439390182495 batch: 6/840\n",
      "Batch loss: 0.6284620761871338 batch: 7/840\n",
      "Batch loss: 0.5164722204208374 batch: 8/840\n",
      "Batch loss: 0.5493101477622986 batch: 9/840\n",
      "Batch loss: 0.6524710655212402 batch: 10/840\n",
      "Batch loss: 0.6023821830749512 batch: 11/840\n",
      "Batch loss: 0.5896358489990234 batch: 12/840\n",
      "Batch loss: 0.5389100313186646 batch: 13/840\n",
      "Batch loss: 0.6428219079971313 batch: 14/840\n",
      "Batch loss: 0.5765491724014282 batch: 15/840\n",
      "Batch loss: 0.5305447578430176 batch: 16/840\n",
      "Batch loss: 0.48257124423980713 batch: 17/840\n",
      "Batch loss: 0.6573668122291565 batch: 18/840\n",
      "Batch loss: 0.7196542620658875 batch: 19/840\n",
      "Batch loss: 0.7289421558380127 batch: 20/840\n",
      "Batch loss: 0.7260549664497375 batch: 21/840\n",
      "Batch loss: 0.6496372818946838 batch: 22/840\n",
      "Batch loss: 0.6478361487388611 batch: 23/840\n",
      "Batch loss: 0.5231642127037048 batch: 24/840\n",
      "Batch loss: 0.6202481389045715 batch: 25/840\n",
      "Batch loss: 0.7341275215148926 batch: 26/840\n",
      "Batch loss: 0.7190302014350891 batch: 27/840\n",
      "Batch loss: 0.705524206161499 batch: 28/840\n",
      "Batch loss: 0.6501732468605042 batch: 29/840\n",
      "Batch loss: 0.5468518733978271 batch: 30/840\n",
      "Batch loss: 0.6027087569236755 batch: 31/840\n",
      "Batch loss: 0.5432612895965576 batch: 32/840\n",
      "Batch loss: 0.6552445888519287 batch: 33/840\n",
      "Batch loss: 0.5386765003204346 batch: 34/840\n",
      "Batch loss: 0.6682060360908508 batch: 35/840\n",
      "Batch loss: 0.6673434376716614 batch: 36/840\n",
      "Batch loss: 0.7163692712783813 batch: 37/840\n",
      "Batch loss: 0.6846441626548767 batch: 38/840\n",
      "Batch loss: 0.7087934613227844 batch: 39/840\n",
      "Batch loss: 0.5083931088447571 batch: 40/840\n",
      "Batch loss: 0.6734166741371155 batch: 41/840\n",
      "Batch loss: 0.6581780314445496 batch: 42/840\n",
      "Batch loss: 0.6979359984397888 batch: 43/840\n",
      "Batch loss: 0.6649924516677856 batch: 44/840\n",
      "Batch loss: 0.7050987482070923 batch: 45/840\n",
      "Batch loss: 0.5520228147506714 batch: 46/840\n",
      "Batch loss: 0.5231970548629761 batch: 47/840\n",
      "Batch loss: 0.6102287173271179 batch: 48/840\n",
      "Batch loss: 0.631365954875946 batch: 49/840\n",
      "Batch loss: 0.7041054368019104 batch: 50/840\n",
      "Batch loss: 0.802341103553772 batch: 51/840\n",
      "Batch loss: 0.6531246900558472 batch: 52/840\n",
      "Batch loss: 0.5714917182922363 batch: 53/840\n",
      "Batch loss: 0.6508570313453674 batch: 54/840\n",
      "Batch loss: 0.6319523453712463 batch: 55/840\n",
      "Batch loss: 0.7244227528572083 batch: 56/840\n",
      "Batch loss: 0.6868084669113159 batch: 57/840\n",
      "Batch loss: 0.4796222448348999 batch: 58/840\n",
      "Batch loss: 0.6307342648506165 batch: 59/840\n",
      "Batch loss: 0.5392623543739319 batch: 60/840\n",
      "Batch loss: 0.8089649677276611 batch: 61/840\n",
      "Batch loss: 0.6916823387145996 batch: 62/840\n",
      "Batch loss: 0.5783832669258118 batch: 63/840\n",
      "Batch loss: 0.7847622632980347 batch: 64/840\n",
      "Batch loss: 0.5509134531021118 batch: 65/840\n",
      "Batch loss: 0.6789810061454773 batch: 66/840\n",
      "Batch loss: 0.722990095615387 batch: 67/840\n",
      "Batch loss: 0.6783484816551208 batch: 68/840\n",
      "Batch loss: 0.6824244856834412 batch: 69/840\n",
      "Batch loss: 0.6246034502983093 batch: 70/840\n",
      "Batch loss: 0.7205767631530762 batch: 71/840\n",
      "Batch loss: 0.779075026512146 batch: 72/840\n",
      "Batch loss: 0.6390345096588135 batch: 73/840\n",
      "Batch loss: 0.6631450057029724 batch: 74/840\n",
      "Batch loss: 0.667595386505127 batch: 75/840\n",
      "Batch loss: 0.43769779801368713 batch: 76/840\n",
      "Batch loss: 0.704552412033081 batch: 77/840\n",
      "Batch loss: 0.7800388336181641 batch: 78/840\n",
      "Batch loss: 0.6109054684638977 batch: 79/840\n",
      "Batch loss: 0.6861116290092468 batch: 80/840\n",
      "Batch loss: 0.5936882495880127 batch: 81/840\n",
      "Batch loss: 0.6285815834999084 batch: 82/840\n",
      "Batch loss: 0.48747575283050537 batch: 83/840\n",
      "Batch loss: 0.708152711391449 batch: 84/840\n",
      "Batch loss: 0.6220013499259949 batch: 85/840\n",
      "Batch loss: 0.8152748942375183 batch: 86/840\n",
      "Batch loss: 0.5453541278839111 batch: 87/840\n",
      "Batch loss: 0.48351025581359863 batch: 88/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4273456931114197 batch: 89/840\n",
      "Batch loss: 0.49756741523742676 batch: 90/840\n",
      "Batch loss: 0.6277928352355957 batch: 91/840\n",
      "Batch loss: 0.6013364195823669 batch: 92/840\n",
      "Batch loss: 0.621572732925415 batch: 93/840\n",
      "Batch loss: 0.5959766507148743 batch: 94/840\n",
      "Batch loss: 0.4871208071708679 batch: 95/840\n",
      "Batch loss: 0.6136190891265869 batch: 96/840\n",
      "Batch loss: 0.6936027407646179 batch: 97/840\n",
      "Batch loss: 0.8742967844009399 batch: 98/840\n",
      "Batch loss: 0.529708981513977 batch: 99/840\n",
      "Batch loss: 0.6611769199371338 batch: 100/840\n",
      "Batch loss: 0.5891065001487732 batch: 101/840\n",
      "Batch loss: 0.48619404435157776 batch: 102/840\n",
      "Batch loss: 0.6648203134536743 batch: 103/840\n",
      "Batch loss: 0.4931459426879883 batch: 104/840\n",
      "Batch loss: 0.5454641580581665 batch: 105/840\n",
      "Batch loss: 0.6422783732414246 batch: 106/840\n",
      "Batch loss: 0.6101815700531006 batch: 107/840\n",
      "Batch loss: 0.80327969789505 batch: 108/840\n",
      "Batch loss: 0.599432110786438 batch: 109/840\n",
      "Batch loss: 0.5376288294792175 batch: 110/840\n",
      "Batch loss: 0.5453454256057739 batch: 111/840\n",
      "Batch loss: 0.6925486922264099 batch: 112/840\n",
      "Batch loss: 0.7447683215141296 batch: 113/840\n",
      "Batch loss: 0.5852773189544678 batch: 114/840\n",
      "Batch loss: 0.5892994403839111 batch: 115/840\n",
      "Batch loss: 0.5674786567687988 batch: 116/840\n",
      "Batch loss: 0.6326403617858887 batch: 117/840\n",
      "Batch loss: 0.47676360607147217 batch: 118/840\n",
      "Batch loss: 0.8171194195747375 batch: 119/840\n",
      "Batch loss: 0.5992203950881958 batch: 120/840\n",
      "Batch loss: 0.7004770040512085 batch: 121/840\n",
      "Batch loss: 0.7750688195228577 batch: 122/840\n",
      "Batch loss: 0.5592843890190125 batch: 123/840\n",
      "Batch loss: 0.6683327555656433 batch: 124/840\n",
      "Batch loss: 0.6216224431991577 batch: 125/840\n",
      "Batch loss: 0.5890533924102783 batch: 126/840\n",
      "Batch loss: 0.7206562757492065 batch: 127/840\n",
      "Batch loss: 0.7064852118492126 batch: 128/840\n",
      "Batch loss: 0.8078311085700989 batch: 129/840\n",
      "Batch loss: 0.6370584964752197 batch: 130/840\n",
      "Batch loss: 0.7997585535049438 batch: 131/840\n",
      "Batch loss: 1.011234164237976 batch: 132/840\n",
      "Batch loss: 0.8103521466255188 batch: 133/840\n",
      "Batch loss: 0.5671385526657104 batch: 134/840\n",
      "Batch loss: 0.5471674203872681 batch: 135/840\n",
      "Batch loss: 0.7229723930358887 batch: 136/840\n",
      "Batch loss: 0.6432016491889954 batch: 137/840\n",
      "Batch loss: 0.5628909468650818 batch: 138/840\n",
      "Batch loss: 0.5945526361465454 batch: 139/840\n",
      "Batch loss: 0.6731825470924377 batch: 140/840\n",
      "Batch loss: 0.5132843255996704 batch: 141/840\n",
      "Batch loss: 0.6067240834236145 batch: 142/840\n",
      "Batch loss: 0.5854052901268005 batch: 143/840\n",
      "Batch loss: 0.5641764402389526 batch: 144/840\n",
      "Batch loss: 0.7389586567878723 batch: 145/840\n",
      "Batch loss: 0.703859806060791 batch: 146/840\n",
      "Batch loss: 0.5247370004653931 batch: 147/840\n",
      "Batch loss: 0.7443997859954834 batch: 148/840\n",
      "Batch loss: 0.7154947519302368 batch: 149/840\n",
      "Batch loss: 0.642770528793335 batch: 150/840\n",
      "Batch loss: 0.7061426639556885 batch: 151/840\n",
      "Batch loss: 0.5842702984809875 batch: 152/840\n",
      "Batch loss: 0.49079930782318115 batch: 153/840\n",
      "Batch loss: 0.7176451086997986 batch: 154/840\n",
      "Batch loss: 0.5478666424751282 batch: 155/840\n",
      "Batch loss: 0.6453020572662354 batch: 156/840\n",
      "Batch loss: 0.6705985069274902 batch: 157/840\n",
      "Batch loss: 0.5694016814231873 batch: 158/840\n",
      "Batch loss: 0.5399797558784485 batch: 159/840\n",
      "Batch loss: 0.6057393550872803 batch: 160/840\n",
      "Batch loss: 0.7136102318763733 batch: 161/840\n",
      "Batch loss: 0.7493037581443787 batch: 162/840\n",
      "Batch loss: 0.7916649580001831 batch: 163/840\n",
      "Batch loss: 0.46947625279426575 batch: 164/840\n",
      "Batch loss: 0.7449190020561218 batch: 165/840\n",
      "Batch loss: 0.5791305899620056 batch: 166/840\n",
      "Batch loss: 0.7552075982093811 batch: 167/840\n",
      "Batch loss: 0.6921785473823547 batch: 168/840\n",
      "Batch loss: 0.5660135746002197 batch: 169/840\n",
      "Batch loss: 0.7861835956573486 batch: 170/840\n",
      "Batch loss: 0.7037910223007202 batch: 171/840\n",
      "Batch loss: 0.7482841610908508 batch: 172/840\n",
      "Batch loss: 0.6748127937316895 batch: 173/840\n",
      "Batch loss: 0.6534422039985657 batch: 174/840\n",
      "Batch loss: 0.5795272588729858 batch: 175/840\n",
      "Batch loss: 0.7264260649681091 batch: 176/840\n",
      "Batch loss: 0.6512619853019714 batch: 177/840\n",
      "Batch loss: 0.7820056676864624 batch: 178/840\n",
      "Batch loss: 0.6504443883895874 batch: 179/840\n",
      "Batch loss: 0.5440006852149963 batch: 180/840\n",
      "Batch loss: 0.5920050740242004 batch: 181/840\n",
      "Batch loss: 0.5410888195037842 batch: 182/840\n",
      "Batch loss: 0.6638121604919434 batch: 183/840\n",
      "Batch loss: 0.47903117537498474 batch: 184/840\n",
      "Batch loss: 0.4965284466743469 batch: 185/840\n",
      "Batch loss: 0.5158738493919373 batch: 186/840\n",
      "Batch loss: 0.6973487138748169 batch: 187/840\n",
      "Batch loss: 0.6447361707687378 batch: 188/840\n",
      "Batch loss: 0.7395555973052979 batch: 189/840\n",
      "Batch loss: 0.6908037662506104 batch: 190/840\n",
      "Batch loss: 0.8582199215888977 batch: 191/840\n",
      "Batch loss: 0.4733089804649353 batch: 192/840\n",
      "Batch loss: 0.5262660384178162 batch: 193/840\n",
      "Batch loss: 0.48133063316345215 batch: 194/840\n",
      "Batch loss: 0.6298543214797974 batch: 195/840\n",
      "Batch loss: 0.7835521101951599 batch: 196/840\n",
      "Batch loss: 0.5327398180961609 batch: 197/840\n",
      "Batch loss: 0.501495897769928 batch: 198/840\n",
      "Batch loss: 0.5920141339302063 batch: 199/840\n",
      "Batch loss: 0.7837663888931274 batch: 200/840\n",
      "Batch loss: 0.669575035572052 batch: 201/840\n",
      "Batch loss: 0.6334240436553955 batch: 202/840\n",
      "Batch loss: 0.5876715183258057 batch: 203/840\n",
      "Batch loss: 0.7997834086418152 batch: 204/840\n",
      "Batch loss: 0.7485499382019043 batch: 205/840\n",
      "Batch loss: 0.6490949392318726 batch: 206/840\n",
      "Batch loss: 0.5743077397346497 batch: 207/840\n",
      "Batch loss: 0.6047419905662537 batch: 208/840\n",
      "Batch loss: 0.6303954124450684 batch: 209/840\n",
      "Batch loss: 0.5811023116111755 batch: 210/840\n",
      "Batch loss: 0.5406386256217957 batch: 211/840\n",
      "Batch loss: 0.6299581527709961 batch: 212/840\n",
      "Batch loss: 0.7884486317634583 batch: 213/840\n",
      "Batch loss: 0.825346052646637 batch: 214/840\n",
      "Batch loss: 0.6356912851333618 batch: 215/840\n",
      "Batch loss: 0.650533139705658 batch: 216/840\n",
      "Batch loss: 0.5757441520690918 batch: 217/840\n",
      "Batch loss: 0.7106042504310608 batch: 218/840\n",
      "Batch loss: 0.6706544756889343 batch: 219/840\n",
      "Batch loss: 0.9983170032501221 batch: 220/840\n",
      "Batch loss: 0.6520171165466309 batch: 221/840\n",
      "Batch loss: 0.6658908724784851 batch: 222/840\n",
      "Batch loss: 0.622029185295105 batch: 223/840\n",
      "Batch loss: 0.7657694220542908 batch: 224/840\n",
      "Batch loss: 0.658933699131012 batch: 225/840\n",
      "Batch loss: 0.6911741495132446 batch: 226/840\n",
      "Batch loss: 0.7351888418197632 batch: 227/840\n",
      "Batch loss: 0.47562935948371887 batch: 228/840\n",
      "Batch loss: 0.4709925949573517 batch: 229/840\n",
      "Batch loss: 0.5957607626914978 batch: 230/840\n",
      "Batch loss: 0.6049935221672058 batch: 231/840\n",
      "Batch loss: 0.6214052438735962 batch: 232/840\n",
      "Batch loss: 0.7230568528175354 batch: 233/840\n",
      "Batch loss: 0.6347805857658386 batch: 234/840\n",
      "Batch loss: 0.6579006314277649 batch: 235/840\n",
      "Batch loss: 0.6338129639625549 batch: 236/840\n",
      "Batch loss: 0.586527943611145 batch: 237/840\n",
      "Batch loss: 0.7888216376304626 batch: 238/840\n",
      "Batch loss: 0.6100289821624756 batch: 239/840\n",
      "Batch loss: 0.634688138961792 batch: 240/840\n",
      "Batch loss: 0.7265248894691467 batch: 241/840\n",
      "Batch loss: 0.6475470066070557 batch: 242/840\n",
      "Batch loss: 0.6107156276702881 batch: 243/840\n",
      "Batch loss: 0.7060156464576721 batch: 244/840\n",
      "Batch loss: 0.4931323230266571 batch: 245/840\n",
      "Batch loss: 0.6896995306015015 batch: 246/840\n",
      "Batch loss: 0.751095175743103 batch: 247/840\n",
      "Batch loss: 0.7331769466400146 batch: 248/840\n",
      "Batch loss: 0.8148584961891174 batch: 249/840\n",
      "Batch loss: 0.5485062003135681 batch: 250/840\n",
      "Batch loss: 0.6183608770370483 batch: 251/840\n",
      "Batch loss: 0.6728734374046326 batch: 252/840\n",
      "Batch loss: 0.6347609162330627 batch: 253/840\n",
      "Batch loss: 0.7360260486602783 batch: 254/840\n",
      "Batch loss: 0.6186694502830505 batch: 255/840\n",
      "Batch loss: 0.6302557587623596 batch: 256/840\n",
      "Batch loss: 0.5855318307876587 batch: 257/840\n",
      "Batch loss: 0.7383038401603699 batch: 258/840\n",
      "Batch loss: 0.5595707893371582 batch: 259/840\n",
      "Batch loss: 0.5650416016578674 batch: 260/840\n",
      "Batch loss: 0.5253629088401794 batch: 261/840\n",
      "Batch loss: 0.4533044397830963 batch: 262/840\n",
      "Batch loss: 0.6862708926200867 batch: 263/840\n",
      "Batch loss: 0.5427812337875366 batch: 264/840\n",
      "Batch loss: 0.6353601217269897 batch: 265/840\n",
      "Batch loss: 0.4929039478302002 batch: 266/840\n",
      "Batch loss: 0.7243418097496033 batch: 267/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5435997843742371 batch: 268/840\n",
      "Batch loss: 0.5709262490272522 batch: 269/840\n",
      "Batch loss: 0.6061674952507019 batch: 270/840\n",
      "Batch loss: 0.5108600854873657 batch: 271/840\n",
      "Batch loss: 0.8558752536773682 batch: 272/840\n",
      "Batch loss: 0.6923969388008118 batch: 273/840\n",
      "Batch loss: 0.5989536643028259 batch: 274/840\n",
      "Batch loss: 0.7667465209960938 batch: 275/840\n",
      "Batch loss: 0.6629545092582703 batch: 276/840\n",
      "Batch loss: 0.5671050548553467 batch: 277/840\n",
      "Batch loss: 0.8004733324050903 batch: 278/840\n",
      "Batch loss: 0.6689884066581726 batch: 279/840\n",
      "Batch loss: 0.7161937952041626 batch: 280/840\n",
      "Batch loss: 0.6179115772247314 batch: 281/840\n",
      "Batch loss: 0.5336389541625977 batch: 282/840\n",
      "Batch loss: 0.771929919719696 batch: 283/840\n",
      "Batch loss: 0.4995283782482147 batch: 284/840\n",
      "Batch loss: 0.4961470067501068 batch: 285/840\n",
      "Batch loss: 0.6402315497398376 batch: 286/840\n",
      "Batch loss: 0.4910798668861389 batch: 287/840\n",
      "Batch loss: 0.642548143863678 batch: 288/840\n",
      "Batch loss: 0.8573712110519409 batch: 289/840\n",
      "Batch loss: 0.773802638053894 batch: 290/840\n",
      "Batch loss: 0.8708131313323975 batch: 291/840\n",
      "Batch loss: 0.5743513107299805 batch: 292/840\n",
      "Batch loss: 0.713755190372467 batch: 293/840\n",
      "Batch loss: 0.5708158612251282 batch: 294/840\n",
      "Batch loss: 0.5651620030403137 batch: 295/840\n",
      "Batch loss: 0.6037203073501587 batch: 296/840\n",
      "Batch loss: 0.7191534638404846 batch: 297/840\n",
      "Batch loss: 0.6857086420059204 batch: 298/840\n",
      "Batch loss: 0.6810958385467529 batch: 299/840\n",
      "Batch loss: 0.7767140865325928 batch: 300/840\n",
      "Batch loss: 0.8230865597724915 batch: 301/840\n",
      "Batch loss: 0.6252658367156982 batch: 302/840\n",
      "Batch loss: 0.7581771016120911 batch: 303/840\n",
      "Batch loss: 0.5827934741973877 batch: 304/840\n",
      "Batch loss: 0.5418897867202759 batch: 305/840\n",
      "Batch loss: 0.7125977873802185 batch: 306/840\n",
      "Batch loss: 0.5533897280693054 batch: 307/840\n",
      "Batch loss: 0.7355562448501587 batch: 308/840\n",
      "Batch loss: 0.682129979133606 batch: 309/840\n",
      "Batch loss: 0.8905746936798096 batch: 310/840\n",
      "Batch loss: 0.6567731499671936 batch: 311/840\n",
      "Batch loss: 0.7226932048797607 batch: 312/840\n",
      "Batch loss: 0.6755117774009705 batch: 313/840\n",
      "Batch loss: 0.5373058319091797 batch: 314/840\n",
      "Batch loss: 0.5616914629936218 batch: 315/840\n",
      "Batch loss: 0.4297991096973419 batch: 316/840\n",
      "Batch loss: 0.7285537719726562 batch: 317/840\n",
      "Batch loss: 0.6234760880470276 batch: 318/840\n",
      "Batch loss: 0.7005738615989685 batch: 319/840\n",
      "Batch loss: 0.522662341594696 batch: 320/840\n",
      "Batch loss: 0.548228919506073 batch: 321/840\n",
      "Batch loss: 0.7312858700752258 batch: 322/840\n",
      "Batch loss: 0.6443438529968262 batch: 323/840\n",
      "Batch loss: 0.7073708176612854 batch: 324/840\n",
      "Batch loss: 0.5187333822250366 batch: 325/840\n",
      "Batch loss: 0.7692974209785461 batch: 326/840\n",
      "Batch loss: 0.49512404203414917 batch: 327/840\n",
      "Batch loss: 0.7918998003005981 batch: 328/840\n",
      "Batch loss: 0.7735955119132996 batch: 329/840\n",
      "Batch loss: 0.6315653324127197 batch: 330/840\n",
      "Batch loss: 0.7738349437713623 batch: 331/840\n",
      "Batch loss: 0.6319593191146851 batch: 332/840\n",
      "Batch loss: 0.5452044010162354 batch: 333/840\n",
      "Batch loss: 0.7366662621498108 batch: 334/840\n",
      "Batch loss: 0.5686017870903015 batch: 335/840\n",
      "Batch loss: 0.6362873315811157 batch: 336/840\n",
      "Batch loss: 0.8532363772392273 batch: 337/840\n",
      "Batch loss: 0.7813506126403809 batch: 338/840\n",
      "Batch loss: 0.6060869693756104 batch: 339/840\n",
      "Batch loss: 0.8512327075004578 batch: 340/840\n",
      "Batch loss: 0.5118380784988403 batch: 341/840\n",
      "Batch loss: 0.609093427658081 batch: 342/840\n",
      "Batch loss: 0.7757450342178345 batch: 343/840\n",
      "Batch loss: 0.6197965741157532 batch: 344/840\n",
      "Batch loss: 0.4061219096183777 batch: 345/840\n",
      "Batch loss: 0.6359948515892029 batch: 346/840\n",
      "Batch loss: 0.6804583668708801 batch: 347/840\n",
      "Batch loss: 0.632725715637207 batch: 348/840\n",
      "Batch loss: 0.6728441715240479 batch: 349/840\n",
      "Batch loss: 0.48966923356056213 batch: 350/840\n",
      "Batch loss: 0.6312282085418701 batch: 351/840\n",
      "Batch loss: 0.7527974843978882 batch: 352/840\n",
      "Batch loss: 0.7393817901611328 batch: 353/840\n",
      "Batch loss: 0.5872254371643066 batch: 354/840\n",
      "Batch loss: 0.6202728748321533 batch: 355/840\n",
      "Batch loss: 0.5910630822181702 batch: 356/840\n",
      "Batch loss: 0.5177690386772156 batch: 357/840\n",
      "Batch loss: 0.9234930276870728 batch: 358/840\n",
      "Batch loss: 0.5600301623344421 batch: 359/840\n",
      "Batch loss: 0.6969939470291138 batch: 360/840\n",
      "Batch loss: 0.7608994245529175 batch: 361/840\n",
      "Batch loss: 0.5275318026542664 batch: 362/840\n",
      "Batch loss: 0.6469342708587646 batch: 363/840\n",
      "Batch loss: 0.7561547756195068 batch: 364/840\n",
      "Batch loss: 0.5477359890937805 batch: 365/840\n",
      "Batch loss: 0.6208765506744385 batch: 366/840\n",
      "Batch loss: 0.5258787870407104 batch: 367/840\n",
      "Batch loss: 0.7833674550056458 batch: 368/840\n",
      "Batch loss: 0.6726873517036438 batch: 369/840\n",
      "Batch loss: 0.7916128039360046 batch: 370/840\n",
      "Batch loss: 0.6822961568832397 batch: 371/840\n",
      "Batch loss: 0.47547826170921326 batch: 372/840\n",
      "Batch loss: 0.6080743074417114 batch: 373/840\n",
      "Batch loss: 0.6931324601173401 batch: 374/840\n",
      "Batch loss: 0.5454888343811035 batch: 375/840\n",
      "Batch loss: 0.44436702132225037 batch: 376/840\n",
      "Batch loss: 0.7600715160369873 batch: 377/840\n",
      "Batch loss: 0.6522443890571594 batch: 378/840\n",
      "Batch loss: 0.5170013308525085 batch: 379/840\n",
      "Batch loss: 0.7951309680938721 batch: 380/840\n",
      "Batch loss: 0.9612484574317932 batch: 381/840\n",
      "Batch loss: 0.6780532598495483 batch: 382/840\n",
      "Batch loss: 0.7030230164527893 batch: 383/840\n",
      "Batch loss: 0.6324485540390015 batch: 384/840\n",
      "Batch loss: 0.6437693238258362 batch: 385/840\n",
      "Batch loss: 0.7066855430603027 batch: 386/840\n",
      "Batch loss: 0.5752226114273071 batch: 387/840\n",
      "Batch loss: 0.6892290711402893 batch: 388/840\n",
      "Batch loss: 0.5539736747741699 batch: 389/840\n",
      "Batch loss: 0.8326975107192993 batch: 390/840\n",
      "Batch loss: 0.6070355772972107 batch: 391/840\n",
      "Batch loss: 0.5680962204933167 batch: 392/840\n",
      "Batch loss: 0.48093074560165405 batch: 393/840\n",
      "Batch loss: 0.7619759440422058 batch: 394/840\n",
      "Batch loss: 0.6246589422225952 batch: 395/840\n",
      "Batch loss: 0.6647646427154541 batch: 396/840\n",
      "Batch loss: 0.5899198651313782 batch: 397/840\n",
      "Batch loss: 0.7780020833015442 batch: 398/840\n",
      "Batch loss: 0.46448612213134766 batch: 399/840\n",
      "Batch loss: 0.5718801617622375 batch: 400/840\n",
      "Batch loss: 0.6877569556236267 batch: 401/840\n",
      "Batch loss: 0.5282734036445618 batch: 402/840\n",
      "Batch loss: 0.6592990756034851 batch: 403/840\n",
      "Batch loss: 0.5808974504470825 batch: 404/840\n",
      "Batch loss: 0.629147469997406 batch: 405/840\n",
      "Batch loss: 0.6156587600708008 batch: 406/840\n",
      "Batch loss: 0.5445139408111572 batch: 407/840\n",
      "Batch loss: 0.7985191345214844 batch: 408/840\n",
      "Batch loss: 0.6870334148406982 batch: 409/840\n",
      "Batch loss: 0.7507280707359314 batch: 410/840\n",
      "Batch loss: 0.6658082008361816 batch: 411/840\n",
      "Batch loss: 0.779462993144989 batch: 412/840\n",
      "Batch loss: 0.735136866569519 batch: 413/840\n",
      "Batch loss: 0.6330938339233398 batch: 414/840\n",
      "Batch loss: 0.8322679996490479 batch: 415/840\n",
      "Batch loss: 0.5441551804542542 batch: 416/840\n",
      "Batch loss: 0.7692745327949524 batch: 417/840\n",
      "Batch loss: 0.837364912033081 batch: 418/840\n",
      "Batch loss: 0.7158857583999634 batch: 419/840\n",
      "Batch loss: 0.7195382714271545 batch: 420/840\n",
      "Batch loss: 0.5528310537338257 batch: 421/840\n",
      "Batch loss: 0.5640355348587036 batch: 422/840\n",
      "Batch loss: 0.6887861490249634 batch: 423/840\n",
      "Batch loss: 0.7135075926780701 batch: 424/840\n",
      "Batch loss: 0.7388296723365784 batch: 425/840\n",
      "Batch loss: 0.6722004413604736 batch: 426/840\n",
      "Batch loss: 0.6257297992706299 batch: 427/840\n",
      "Batch loss: 0.7208126783370972 batch: 428/840\n",
      "Batch loss: 0.5398879051208496 batch: 429/840\n",
      "Batch loss: 0.7952010631561279 batch: 430/840\n",
      "Batch loss: 0.6741282939910889 batch: 431/840\n",
      "Batch loss: 0.5960460901260376 batch: 432/840\n",
      "Batch loss: 0.5357022881507874 batch: 433/840\n",
      "Batch loss: 0.5545645356178284 batch: 434/840\n",
      "Batch loss: 0.781685471534729 batch: 435/840\n",
      "Batch loss: 0.6975224018096924 batch: 436/840\n",
      "Batch loss: 0.7556021213531494 batch: 437/840\n",
      "Batch loss: 0.4734402000904083 batch: 438/840\n",
      "Batch loss: 0.5791715383529663 batch: 439/840\n",
      "Batch loss: 0.7362424731254578 batch: 440/840\n",
      "Batch loss: 0.6414436101913452 batch: 441/840\n",
      "Batch loss: 0.7648074626922607 batch: 442/840\n",
      "Batch loss: 0.651685357093811 batch: 443/840\n",
      "Batch loss: 0.676707923412323 batch: 444/840\n",
      "Batch loss: 0.6303725242614746 batch: 445/840\n",
      "Batch loss: 0.7929897904396057 batch: 446/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.636339545249939 batch: 447/840\n",
      "Batch loss: 0.7086731195449829 batch: 448/840\n",
      "Batch loss: 0.5532137751579285 batch: 449/840\n",
      "Batch loss: 0.6442967057228088 batch: 450/840\n",
      "Batch loss: 0.7028977274894714 batch: 451/840\n",
      "Batch loss: 0.7765887379646301 batch: 452/840\n",
      "Batch loss: 0.6512401700019836 batch: 453/840\n",
      "Batch loss: 0.5835393667221069 batch: 454/840\n",
      "Batch loss: 0.6583204865455627 batch: 455/840\n",
      "Batch loss: 0.6039807796478271 batch: 456/840\n",
      "Batch loss: 0.543761670589447 batch: 457/840\n",
      "Batch loss: 0.6709083318710327 batch: 458/840\n",
      "Batch loss: 0.672484815120697 batch: 459/840\n",
      "Batch loss: 0.5074782371520996 batch: 460/840\n",
      "Batch loss: 0.7323715090751648 batch: 461/840\n",
      "Batch loss: 0.6828070878982544 batch: 462/840\n",
      "Batch loss: 0.8199707269668579 batch: 463/840\n",
      "Batch loss: 0.45494842529296875 batch: 464/840\n",
      "Batch loss: 0.904721200466156 batch: 465/840\n",
      "Batch loss: 0.6809939742088318 batch: 466/840\n",
      "Batch loss: 0.8221043348312378 batch: 467/840\n",
      "Batch loss: 0.6063264012336731 batch: 468/840\n",
      "Batch loss: 0.6292956471443176 batch: 469/840\n",
      "Batch loss: 0.6614074110984802 batch: 470/840\n",
      "Batch loss: 0.7490280270576477 batch: 471/840\n",
      "Batch loss: 0.6895444393157959 batch: 472/840\n",
      "Batch loss: 0.6742106080055237 batch: 473/840\n",
      "Batch loss: 0.6011437773704529 batch: 474/840\n",
      "Batch loss: 0.6236380338668823 batch: 475/840\n",
      "Batch loss: 0.6710294485092163 batch: 476/840\n",
      "Batch loss: 0.5413771867752075 batch: 477/840\n",
      "Batch loss: 0.5596311092376709 batch: 478/840\n",
      "Batch loss: 0.511132538318634 batch: 479/840\n",
      "Batch loss: 0.6039208769798279 batch: 480/840\n",
      "Batch loss: 0.6602520942687988 batch: 481/840\n",
      "Batch loss: 0.7004439830780029 batch: 482/840\n",
      "Batch loss: 0.7378423810005188 batch: 483/840\n",
      "Batch loss: 0.6098974347114563 batch: 484/840\n",
      "Batch loss: 0.5791708827018738 batch: 485/840\n",
      "Batch loss: 0.6624462008476257 batch: 486/840\n",
      "Batch loss: 0.5192240476608276 batch: 487/840\n",
      "Batch loss: 0.6198405623435974 batch: 488/840\n",
      "Batch loss: 0.591937243938446 batch: 489/840\n",
      "Batch loss: 0.7375438213348389 batch: 490/840\n",
      "Batch loss: 0.3965136706829071 batch: 491/840\n",
      "Batch loss: 0.7623504400253296 batch: 492/840\n",
      "Batch loss: 0.7084364891052246 batch: 493/840\n",
      "Batch loss: 0.4315682351589203 batch: 494/840\n",
      "Batch loss: 0.7036900520324707 batch: 495/840\n",
      "Batch loss: 0.6878423094749451 batch: 496/840\n",
      "Batch loss: 0.6804498434066772 batch: 497/840\n",
      "Batch loss: 0.489009827375412 batch: 498/840\n",
      "Batch loss: 0.5769636631011963 batch: 499/840\n",
      "Batch loss: 0.5927760601043701 batch: 500/840\n",
      "Batch loss: 0.546172022819519 batch: 501/840\n",
      "Batch loss: 0.6935961842536926 batch: 502/840\n",
      "Batch loss: 0.5107674598693848 batch: 503/840\n",
      "Batch loss: 0.5878217220306396 batch: 504/840\n",
      "Batch loss: 0.7806567549705505 batch: 505/840\n",
      "Batch loss: 0.5370041131973267 batch: 506/840\n",
      "Batch loss: 0.8041580319404602 batch: 507/840\n",
      "Batch loss: 0.620310366153717 batch: 508/840\n",
      "Batch loss: 0.7751017212867737 batch: 509/840\n",
      "Batch loss: 0.6451423764228821 batch: 510/840\n",
      "Batch loss: 0.5889772176742554 batch: 511/840\n",
      "Batch loss: 0.7299354076385498 batch: 512/840\n",
      "Batch loss: 0.586275041103363 batch: 513/840\n",
      "Batch loss: 0.6604963541030884 batch: 514/840\n",
      "Batch loss: 0.4536445140838623 batch: 515/840\n",
      "Batch loss: 0.6669771075248718 batch: 516/840\n",
      "Batch loss: 0.654039740562439 batch: 517/840\n",
      "Batch loss: 0.585945725440979 batch: 518/840\n",
      "Batch loss: 0.7112500071525574 batch: 519/840\n",
      "Batch loss: 0.605008065700531 batch: 520/840\n",
      "Batch loss: 0.7903016805648804 batch: 521/840\n",
      "Batch loss: 0.510465145111084 batch: 522/840\n",
      "Batch loss: 0.5489230751991272 batch: 523/840\n",
      "Batch loss: 0.6487815380096436 batch: 524/840\n",
      "Batch loss: 0.6648113131523132 batch: 525/840\n",
      "Batch loss: 0.7470545768737793 batch: 526/840\n",
      "Batch loss: 0.6413627862930298 batch: 527/840\n",
      "Batch loss: 0.6617627143859863 batch: 528/840\n",
      "Batch loss: 0.5560178756713867 batch: 529/840\n",
      "Batch loss: 0.5972720980644226 batch: 530/840\n",
      "Batch loss: 0.5813930034637451 batch: 531/840\n",
      "Batch loss: 0.44834980368614197 batch: 532/840\n",
      "Batch loss: 0.7520504593849182 batch: 533/840\n",
      "Batch loss: 0.5736697912216187 batch: 534/840\n",
      "Batch loss: 0.6112828850746155 batch: 535/840\n",
      "Batch loss: 0.6730669140815735 batch: 536/840\n",
      "Batch loss: 0.6492143273353577 batch: 537/840\n",
      "Batch loss: 0.47650882601737976 batch: 538/840\n",
      "Batch loss: 0.5686668157577515 batch: 539/840\n",
      "Batch loss: 0.8911743760108948 batch: 540/840\n",
      "Batch loss: 0.6451303362846375 batch: 541/840\n",
      "Batch loss: 0.6784842610359192 batch: 542/840\n",
      "Batch loss: 0.4692623019218445 batch: 543/840\n",
      "Batch loss: 0.6511328220367432 batch: 544/840\n",
      "Batch loss: 0.6972582340240479 batch: 545/840\n",
      "Batch loss: 0.6243957281112671 batch: 546/840\n",
      "Batch loss: 0.7212463617324829 batch: 547/840\n",
      "Batch loss: 0.5514122843742371 batch: 548/840\n",
      "Batch loss: 0.6051757335662842 batch: 549/840\n",
      "Batch loss: 0.5654012560844421 batch: 550/840\n",
      "Batch loss: 0.6570783853530884 batch: 551/840\n",
      "Batch loss: 0.5251586437225342 batch: 552/840\n",
      "Batch loss: 0.7291300296783447 batch: 553/840\n",
      "Batch loss: 0.7305286526679993 batch: 554/840\n",
      "Batch loss: 0.5884832143783569 batch: 555/840\n",
      "Batch loss: 0.6728386878967285 batch: 556/840\n",
      "Batch loss: 0.673999547958374 batch: 557/840\n",
      "Batch loss: 0.6729382276535034 batch: 558/840\n",
      "Batch loss: 0.6387395262718201 batch: 559/840\n",
      "Batch loss: 0.6414124369621277 batch: 560/840\n",
      "Batch loss: 0.61347895860672 batch: 561/840\n",
      "Batch loss: 0.6074053645133972 batch: 562/840\n",
      "Batch loss: 0.47602084279060364 batch: 563/840\n",
      "Batch loss: 0.6949722170829773 batch: 564/840\n",
      "Batch loss: 0.756334125995636 batch: 565/840\n",
      "Batch loss: 0.6493483781814575 batch: 566/840\n",
      "Batch loss: 0.7786383032798767 batch: 567/840\n",
      "Batch loss: 0.6022438406944275 batch: 568/840\n",
      "Batch loss: 0.7196279168128967 batch: 569/840\n",
      "Batch loss: 0.46318501234054565 batch: 570/840\n",
      "Batch loss: 0.6868436932563782 batch: 571/840\n",
      "Batch loss: 0.6890386343002319 batch: 572/840\n",
      "Batch loss: 0.6202488541603088 batch: 573/840\n",
      "Batch loss: 0.6902375221252441 batch: 574/840\n",
      "Batch loss: 0.5774185061454773 batch: 575/840\n",
      "Batch loss: 0.5797051787376404 batch: 576/840\n",
      "Batch loss: 0.6177721619606018 batch: 577/840\n",
      "Batch loss: 0.8178910613059998 batch: 578/840\n",
      "Batch loss: 0.7021980285644531 batch: 579/840\n",
      "Batch loss: 0.8290143609046936 batch: 580/840\n",
      "Batch loss: 0.6990094780921936 batch: 581/840\n",
      "Batch loss: 0.8143937587738037 batch: 582/840\n",
      "Batch loss: 0.5986464023590088 batch: 583/840\n",
      "Batch loss: 0.7892124652862549 batch: 584/840\n",
      "Batch loss: 0.6397903561592102 batch: 585/840\n",
      "Batch loss: 0.616716742515564 batch: 586/840\n",
      "Batch loss: 0.7122653722763062 batch: 587/840\n",
      "Batch loss: 0.5908159017562866 batch: 588/840\n",
      "Batch loss: 0.8461025953292847 batch: 589/840\n",
      "Batch loss: 0.6098546385765076 batch: 590/840\n",
      "Batch loss: 0.5011546611785889 batch: 591/840\n",
      "Batch loss: 0.44578105211257935 batch: 592/840\n",
      "Batch loss: 0.6834196448326111 batch: 593/840\n",
      "Batch loss: 0.7060520052909851 batch: 594/840\n",
      "Batch loss: 0.34010207653045654 batch: 595/840\n",
      "Batch loss: 0.45629599690437317 batch: 596/840\n",
      "Batch loss: 0.5443628430366516 batch: 597/840\n",
      "Batch loss: 0.40094590187072754 batch: 598/840\n",
      "Batch loss: 0.6432642340660095 batch: 599/840\n",
      "Batch loss: 0.6826204061508179 batch: 600/840\n",
      "Batch loss: 0.7980536818504333 batch: 601/840\n",
      "Batch loss: 0.5105248093605042 batch: 602/840\n",
      "Batch loss: 0.6359168887138367 batch: 603/840\n",
      "Batch loss: 0.6085730791091919 batch: 604/840\n",
      "Batch loss: 0.8401634693145752 batch: 605/840\n",
      "Batch loss: 0.8221222758293152 batch: 606/840\n",
      "Batch loss: 0.715262234210968 batch: 607/840\n",
      "Batch loss: 0.6655756235122681 batch: 608/840\n",
      "Batch loss: 0.5602356791496277 batch: 609/840\n",
      "Batch loss: 0.8414802551269531 batch: 610/840\n",
      "Batch loss: 0.6462727189064026 batch: 611/840\n",
      "Batch loss: 0.7560555338859558 batch: 612/840\n",
      "Batch loss: 0.8358043432235718 batch: 613/840\n",
      "Batch loss: 0.695248007774353 batch: 614/840\n",
      "Batch loss: 0.5789650678634644 batch: 615/840\n",
      "Batch loss: 0.6083530187606812 batch: 616/840\n",
      "Batch loss: 0.4192081689834595 batch: 617/840\n",
      "Batch loss: 0.568108081817627 batch: 618/840\n",
      "Batch loss: 0.7855862975120544 batch: 619/840\n",
      "Batch loss: 0.626331627368927 batch: 620/840\n",
      "Batch loss: 0.7733395099639893 batch: 621/840\n",
      "Batch loss: 0.7392498850822449 batch: 622/840\n",
      "Batch loss: 0.5865130424499512 batch: 623/840\n",
      "Batch loss: 0.8188086748123169 batch: 624/840\n",
      "Batch loss: 0.7162477374076843 batch: 625/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5647445321083069 batch: 626/840\n",
      "Batch loss: 0.576361894607544 batch: 627/840\n",
      "Batch loss: 0.6583991050720215 batch: 628/840\n",
      "Batch loss: 0.5965865850448608 batch: 629/840\n",
      "Batch loss: 0.6124563813209534 batch: 630/840\n",
      "Batch loss: 0.6473554968833923 batch: 631/840\n",
      "Batch loss: 0.7086635828018188 batch: 632/840\n",
      "Batch loss: 0.5127256512641907 batch: 633/840\n",
      "Batch loss: 0.6960954070091248 batch: 634/840\n",
      "Batch loss: 0.5632712841033936 batch: 635/840\n",
      "Batch loss: 0.5921258330345154 batch: 636/840\n",
      "Batch loss: 0.5026082992553711 batch: 637/840\n",
      "Batch loss: 0.683290421962738 batch: 638/840\n",
      "Batch loss: 0.6440385580062866 batch: 639/840\n",
      "Batch loss: 0.6322287321090698 batch: 640/840\n",
      "Batch loss: 0.7800689935684204 batch: 641/840\n",
      "Batch loss: 0.573424756526947 batch: 642/840\n",
      "Batch loss: 1.0185548067092896 batch: 643/840\n",
      "Batch loss: 0.6422721743583679 batch: 644/840\n",
      "Batch loss: 0.5797909498214722 batch: 645/840\n",
      "Batch loss: 0.6838799118995667 batch: 646/840\n",
      "Batch loss: 0.7534171342849731 batch: 647/840\n",
      "Batch loss: 0.667603611946106 batch: 648/840\n",
      "Batch loss: 0.6193262338638306 batch: 649/840\n",
      "Batch loss: 0.5594245791435242 batch: 650/840\n",
      "Batch loss: 0.5699360370635986 batch: 651/840\n",
      "Batch loss: 0.6532782912254333 batch: 652/840\n",
      "Batch loss: 0.5660365223884583 batch: 653/840\n",
      "Batch loss: 0.9384874701499939 batch: 654/840\n",
      "Batch loss: 0.5719814300537109 batch: 655/840\n",
      "Batch loss: 0.6650614738464355 batch: 656/840\n",
      "Batch loss: 0.6430889368057251 batch: 657/840\n",
      "Batch loss: 0.6886972188949585 batch: 658/840\n",
      "Batch loss: 0.6633937954902649 batch: 659/840\n",
      "Batch loss: 0.726774275302887 batch: 660/840\n",
      "Batch loss: 0.7184497714042664 batch: 661/840\n",
      "Batch loss: 0.6352951526641846 batch: 662/840\n",
      "Batch loss: 0.553671658039093 batch: 663/840\n",
      "Batch loss: 0.6575353741645813 batch: 664/840\n",
      "Batch loss: 0.6869665384292603 batch: 665/840\n",
      "Batch loss: 0.6207258105278015 batch: 666/840\n",
      "Batch loss: 0.7381486296653748 batch: 667/840\n",
      "Batch loss: 0.536876380443573 batch: 668/840\n",
      "Batch loss: 0.5969077348709106 batch: 669/840\n",
      "Batch loss: 0.6358956694602966 batch: 670/840\n",
      "Batch loss: 0.632819414138794 batch: 671/840\n",
      "Batch loss: 0.6038539409637451 batch: 672/840\n",
      "Batch loss: 0.8144927024841309 batch: 673/840\n",
      "Batch loss: 0.822446346282959 batch: 674/840\n",
      "Batch loss: 0.6284465193748474 batch: 675/840\n",
      "Batch loss: 0.49777859449386597 batch: 676/840\n",
      "Batch loss: 0.6953803300857544 batch: 677/840\n",
      "Batch loss: 0.7604956030845642 batch: 678/840\n",
      "Batch loss: 0.8340761065483093 batch: 679/840\n",
      "Batch loss: 0.6868633031845093 batch: 680/840\n",
      "Batch loss: 0.724358081817627 batch: 681/840\n",
      "Batch loss: 0.6906272172927856 batch: 682/840\n",
      "Batch loss: 0.5784551501274109 batch: 683/840\n",
      "Batch loss: 0.825766384601593 batch: 684/840\n",
      "Batch loss: 0.6972206234931946 batch: 685/840\n",
      "Batch loss: 0.8240220546722412 batch: 686/840\n",
      "Batch loss: 0.7557879686355591 batch: 687/840\n",
      "Batch loss: 0.650726854801178 batch: 688/840\n",
      "Batch loss: 0.5562285780906677 batch: 689/840\n",
      "Batch loss: 0.5416309833526611 batch: 690/840\n",
      "Batch loss: 0.6622003316879272 batch: 691/840\n",
      "Batch loss: 0.6548200845718384 batch: 692/840\n",
      "Batch loss: 0.6729276180267334 batch: 693/840\n",
      "Batch loss: 0.7211247086524963 batch: 694/840\n",
      "Batch loss: 0.7516255974769592 batch: 695/840\n",
      "Batch loss: 0.5998575687408447 batch: 696/840\n",
      "Batch loss: 0.6805558800697327 batch: 697/840\n",
      "Batch loss: 0.615719199180603 batch: 698/840\n",
      "Batch loss: 0.7064415812492371 batch: 699/840\n",
      "Batch loss: 0.7391304969787598 batch: 700/840\n",
      "Batch loss: 0.7634431719779968 batch: 701/840\n",
      "Batch loss: 0.6384137272834778 batch: 702/840\n",
      "Batch loss: 0.6196831464767456 batch: 703/840\n",
      "Batch loss: 0.8862274289131165 batch: 704/840\n",
      "Batch loss: 0.6265158653259277 batch: 705/840\n",
      "Batch loss: 0.6265519261360168 batch: 706/840\n",
      "Batch loss: 0.7379148006439209 batch: 707/840\n",
      "Batch loss: 0.7537234425544739 batch: 708/840\n",
      "Batch loss: 0.7417618036270142 batch: 709/840\n",
      "Batch loss: 0.7070399522781372 batch: 710/840\n",
      "Batch loss: 0.45755621790885925 batch: 711/840\n",
      "Batch loss: 0.6993865966796875 batch: 712/840\n",
      "Batch loss: 0.6459407210350037 batch: 713/840\n",
      "Batch loss: 0.6163696050643921 batch: 714/840\n",
      "Batch loss: 0.8591667413711548 batch: 715/840\n",
      "Batch loss: 0.6489431858062744 batch: 716/840\n",
      "Batch loss: 0.6321430206298828 batch: 717/840\n",
      "Batch loss: 0.63008052110672 batch: 718/840\n",
      "Batch loss: 0.47273489832878113 batch: 719/840\n",
      "Batch loss: 0.5453336238861084 batch: 720/840\n",
      "Batch loss: 0.8056730628013611 batch: 721/840\n",
      "Batch loss: 0.8743040561676025 batch: 722/840\n",
      "Batch loss: 0.5730441808700562 batch: 723/840\n",
      "Batch loss: 0.7383199334144592 batch: 724/840\n",
      "Batch loss: 0.6598154306411743 batch: 725/840\n",
      "Batch loss: 0.6194438338279724 batch: 726/840\n",
      "Batch loss: 0.7717258930206299 batch: 727/840\n",
      "Batch loss: 0.8214147686958313 batch: 728/840\n",
      "Batch loss: 0.5648561120033264 batch: 729/840\n",
      "Batch loss: 0.6120448112487793 batch: 730/840\n",
      "Batch loss: 0.5518209934234619 batch: 731/840\n",
      "Batch loss: 0.7980049252510071 batch: 732/840\n",
      "Batch loss: 0.615145206451416 batch: 733/840\n",
      "Batch loss: 0.6063286662101746 batch: 734/840\n",
      "Batch loss: 0.6027598977088928 batch: 735/840\n",
      "Batch loss: 0.7250328063964844 batch: 736/840\n",
      "Batch loss: 0.7284079790115356 batch: 737/840\n",
      "Batch loss: 0.4884064197540283 batch: 738/840\n",
      "Batch loss: 0.6786816120147705 batch: 739/840\n",
      "Batch loss: 0.7617895603179932 batch: 740/840\n",
      "Batch loss: 0.5254208445549011 batch: 741/840\n",
      "Batch loss: 0.5292760729789734 batch: 742/840\n",
      "Batch loss: 0.39118102192878723 batch: 743/840\n",
      "Batch loss: 0.5783783793449402 batch: 744/840\n",
      "Batch loss: 0.6028093099594116 batch: 745/840\n",
      "Batch loss: 0.6064770221710205 batch: 746/840\n",
      "Batch loss: 0.6231358051300049 batch: 747/840\n",
      "Batch loss: 0.7423005700111389 batch: 748/840\n",
      "Batch loss: 0.6272719502449036 batch: 749/840\n",
      "Batch loss: 0.5226281881332397 batch: 750/840\n",
      "Batch loss: 0.6810934543609619 batch: 751/840\n",
      "Batch loss: 0.8778481483459473 batch: 752/840\n",
      "Batch loss: 0.525437593460083 batch: 753/840\n",
      "Batch loss: 0.7034143805503845 batch: 754/840\n",
      "Batch loss: 0.6821646690368652 batch: 755/840\n",
      "Batch loss: 0.5704973936080933 batch: 756/840\n",
      "Batch loss: 0.6546870470046997 batch: 757/840\n",
      "Batch loss: 0.5885108709335327 batch: 758/840\n",
      "Batch loss: 0.5215510725975037 batch: 759/840\n",
      "Batch loss: 0.5782721042633057 batch: 760/840\n",
      "Batch loss: 0.5578876733779907 batch: 761/840\n",
      "Batch loss: 0.8428516387939453 batch: 762/840\n",
      "Batch loss: 0.5214850902557373 batch: 763/840\n",
      "Batch loss: 0.7869952917098999 batch: 764/840\n",
      "Batch loss: 0.5379506349563599 batch: 765/840\n",
      "Batch loss: 0.5776470303535461 batch: 766/840\n",
      "Batch loss: 0.7299731373786926 batch: 767/840\n",
      "Batch loss: 0.72312331199646 batch: 768/840\n",
      "Batch loss: 0.604495644569397 batch: 769/840\n",
      "Batch loss: 0.5439364314079285 batch: 770/840\n",
      "Batch loss: 0.5366885662078857 batch: 771/840\n",
      "Batch loss: 0.6275190114974976 batch: 772/840\n",
      "Batch loss: 0.5742002725601196 batch: 773/840\n",
      "Batch loss: 0.49926283955574036 batch: 774/840\n",
      "Batch loss: 0.5675792098045349 batch: 775/840\n",
      "Batch loss: 0.5494810342788696 batch: 776/840\n",
      "Batch loss: 0.560888946056366 batch: 777/840\n",
      "Batch loss: 0.4455196261405945 batch: 778/840\n",
      "Batch loss: 0.7139601111412048 batch: 779/840\n",
      "Batch loss: 0.5974530577659607 batch: 780/840\n",
      "Batch loss: 0.6499314308166504 batch: 781/840\n",
      "Batch loss: 0.6057044863700867 batch: 782/840\n",
      "Batch loss: 0.5154740810394287 batch: 783/840\n",
      "Batch loss: 0.6690409183502197 batch: 784/840\n",
      "Batch loss: 0.566133439540863 batch: 785/840\n",
      "Batch loss: 0.691982090473175 batch: 786/840\n",
      "Batch loss: 0.5679447054862976 batch: 787/840\n",
      "Batch loss: 0.7891698479652405 batch: 788/840\n",
      "Batch loss: 0.7294090390205383 batch: 789/840\n",
      "Batch loss: 0.7722811698913574 batch: 790/840\n",
      "Batch loss: 0.5661606192588806 batch: 791/840\n",
      "Batch loss: 0.4103761911392212 batch: 792/840\n",
      "Batch loss: 0.6368871331214905 batch: 793/840\n",
      "Batch loss: 0.6524325609207153 batch: 794/840\n",
      "Batch loss: 0.6393938064575195 batch: 795/840\n",
      "Batch loss: 0.7487133145332336 batch: 796/840\n",
      "Batch loss: 0.8312733173370361 batch: 797/840\n",
      "Batch loss: 0.707491397857666 batch: 798/840\n",
      "Batch loss: 0.6709442138671875 batch: 799/840\n",
      "Batch loss: 0.5929656028747559 batch: 800/840\n",
      "Batch loss: 0.6971408724784851 batch: 801/840\n",
      "Batch loss: 0.39194586873054504 batch: 802/840\n",
      "Batch loss: 0.5076488256454468 batch: 803/840\n",
      "Batch loss: 0.6691423058509827 batch: 804/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5908075571060181 batch: 805/840\n",
      "Batch loss: 0.6861249804496765 batch: 806/840\n",
      "Batch loss: 0.6320688724517822 batch: 807/840\n",
      "Batch loss: 0.6443777680397034 batch: 808/840\n",
      "Batch loss: 0.5998581051826477 batch: 809/840\n",
      "Batch loss: 0.6267279982566833 batch: 810/840\n",
      "Batch loss: 0.5462456345558167 batch: 811/840\n",
      "Batch loss: 0.6458823680877686 batch: 812/840\n",
      "Batch loss: 0.4515721797943115 batch: 813/840\n",
      "Batch loss: 0.6888258457183838 batch: 814/840\n",
      "Batch loss: 0.6393541693687439 batch: 815/840\n",
      "Batch loss: 0.6975163817405701 batch: 816/840\n",
      "Batch loss: 0.6042117476463318 batch: 817/840\n",
      "Batch loss: 0.6168645024299622 batch: 818/840\n",
      "Batch loss: 0.529721200466156 batch: 819/840\n",
      "Batch loss: 0.6453955769538879 batch: 820/840\n",
      "Batch loss: 0.683186948299408 batch: 821/840\n",
      "Batch loss: 0.6761329174041748 batch: 822/840\n",
      "Batch loss: 0.6958857774734497 batch: 823/840\n",
      "Batch loss: 0.7436498403549194 batch: 824/840\n",
      "Batch loss: 0.6622024774551392 batch: 825/840\n",
      "Batch loss: 0.5878279805183411 batch: 826/840\n",
      "Batch loss: 0.5623992681503296 batch: 827/840\n",
      "Batch loss: 0.8069545030593872 batch: 828/840\n",
      "Batch loss: 0.6174829602241516 batch: 829/840\n",
      "Batch loss: 0.7824080586433411 batch: 830/840\n",
      "Batch loss: 0.6434537768363953 batch: 831/840\n",
      "Batch loss: 0.77278733253479 batch: 832/840\n",
      "Batch loss: 0.7115546464920044 batch: 833/840\n",
      "Batch loss: 0.6346166133880615 batch: 834/840\n",
      "Batch loss: 0.5272562503814697 batch: 835/840\n",
      "Batch loss: 0.5337109565734863 batch: 836/840\n",
      "Batch loss: 0.7192419171333313 batch: 837/840\n",
      "Batch loss: 0.8019983768463135 batch: 838/840\n",
      "Batch loss: 0.5635089874267578 batch: 839/840\n",
      "Batch loss: 0.7120628356933594 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 6/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.826\n",
      "Running epoch 7/15\n",
      "Batch loss: 0.5709648132324219 batch: 1/840\n",
      "Batch loss: 1.1030778884887695 batch: 2/840\n",
      "Batch loss: 0.6089138984680176 batch: 3/840\n",
      "Batch loss: 0.5786057710647583 batch: 4/840\n",
      "Batch loss: 0.6374487280845642 batch: 5/840\n",
      "Batch loss: 0.42577987909317017 batch: 6/840\n",
      "Batch loss: 0.5250431895256042 batch: 7/840\n",
      "Batch loss: 0.6497412919998169 batch: 8/840\n",
      "Batch loss: 0.550657331943512 batch: 9/840\n",
      "Batch loss: 0.6289719343185425 batch: 10/840\n",
      "Batch loss: 0.5493304133415222 batch: 11/840\n",
      "Batch loss: 0.5218162536621094 batch: 12/840\n",
      "Batch loss: 0.545586884021759 batch: 13/840\n",
      "Batch loss: 0.6334134340286255 batch: 14/840\n",
      "Batch loss: 0.5996279120445251 batch: 15/840\n",
      "Batch loss: 0.5236691236495972 batch: 16/840\n",
      "Batch loss: 0.5430002808570862 batch: 17/840\n",
      "Batch loss: 0.7320389747619629 batch: 18/840\n",
      "Batch loss: 0.6276296973228455 batch: 19/840\n",
      "Batch loss: 0.8326326012611389 batch: 20/840\n",
      "Batch loss: 0.6765657663345337 batch: 21/840\n",
      "Batch loss: 0.5481697916984558 batch: 22/840\n",
      "Batch loss: 0.6750845313072205 batch: 23/840\n",
      "Batch loss: 0.5419452786445618 batch: 24/840\n",
      "Batch loss: 0.6366824507713318 batch: 25/840\n",
      "Batch loss: 0.7574360370635986 batch: 26/840\n",
      "Batch loss: 0.6115626692771912 batch: 27/840\n",
      "Batch loss: 0.7246123552322388 batch: 28/840\n",
      "Batch loss: 0.8061103224754333 batch: 29/840\n",
      "Batch loss: 0.5983794927597046 batch: 30/840\n",
      "Batch loss: 0.6463040709495544 batch: 31/840\n",
      "Batch loss: 0.5967994332313538 batch: 32/840\n",
      "Batch loss: 0.692426860332489 batch: 33/840\n",
      "Batch loss: 0.5772900581359863 batch: 34/840\n",
      "Batch loss: 0.6560205817222595 batch: 35/840\n",
      "Batch loss: 0.5551370978355408 batch: 36/840\n",
      "Batch loss: 0.6974178552627563 batch: 37/840\n",
      "Batch loss: 0.6273386478424072 batch: 38/840\n",
      "Batch loss: 0.6678430438041687 batch: 39/840\n",
      "Batch loss: 0.6021298766136169 batch: 40/840\n",
      "Batch loss: 0.7519933581352234 batch: 41/840\n",
      "Batch loss: 0.6552875638008118 batch: 42/840\n",
      "Batch loss: 0.6431053280830383 batch: 43/840\n",
      "Batch loss: 0.6723336577415466 batch: 44/840\n",
      "Batch loss: 0.6939937472343445 batch: 45/840\n",
      "Batch loss: 0.538140058517456 batch: 46/840\n",
      "Batch loss: 0.4991813600063324 batch: 47/840\n",
      "Batch loss: 0.6798657774925232 batch: 48/840\n",
      "Batch loss: 0.6546993851661682 batch: 49/840\n",
      "Batch loss: 0.6509755849838257 batch: 50/840\n",
      "Batch loss: 0.6866243481636047 batch: 51/840\n",
      "Batch loss: 0.7651522159576416 batch: 52/840\n",
      "Batch loss: 0.5973700881004333 batch: 53/840\n",
      "Batch loss: 0.7065171599388123 batch: 54/840\n",
      "Batch loss: 0.6591717600822449 batch: 55/840\n",
      "Batch loss: 0.5859135389328003 batch: 56/840\n",
      "Batch loss: 0.6672688126564026 batch: 57/840\n",
      "Batch loss: 0.5670709609985352 batch: 58/840\n",
      "Batch loss: 0.5596886277198792 batch: 59/840\n",
      "Batch loss: 0.5404140949249268 batch: 60/840\n",
      "Batch loss: 0.8673567175865173 batch: 61/840\n",
      "Batch loss: 0.6405292749404907 batch: 62/840\n",
      "Batch loss: 0.5658876895904541 batch: 63/840\n",
      "Batch loss: 0.6584418416023254 batch: 64/840\n",
      "Batch loss: 0.5221930742263794 batch: 65/840\n",
      "Batch loss: 0.6908062696456909 batch: 66/840\n",
      "Batch loss: 0.634023129940033 batch: 67/840\n",
      "Batch loss: 0.6048822999000549 batch: 68/840\n",
      "Batch loss: 0.711967945098877 batch: 69/840\n",
      "Batch loss: 0.6512483358383179 batch: 70/840\n",
      "Batch loss: 0.7938758134841919 batch: 71/840\n",
      "Batch loss: 0.7674853801727295 batch: 72/840\n",
      "Batch loss: 0.6502633094787598 batch: 73/840\n",
      "Batch loss: 0.5675289034843445 batch: 74/840\n",
      "Batch loss: 0.6910108923912048 batch: 75/840\n",
      "Batch loss: 0.4403237998485565 batch: 76/840\n",
      "Batch loss: 0.6436483263969421 batch: 77/840\n",
      "Batch loss: 0.7287301421165466 batch: 78/840\n",
      "Batch loss: 0.5999414324760437 batch: 79/840\n",
      "Batch loss: 0.6305249333381653 batch: 80/840\n",
      "Batch loss: 0.4475507438182831 batch: 81/840\n",
      "Batch loss: 0.6553889513015747 batch: 82/840\n",
      "Batch loss: 0.5449706315994263 batch: 83/840\n",
      "Batch loss: 0.7368735671043396 batch: 84/840\n",
      "Batch loss: 0.6220842003822327 batch: 85/840\n",
      "Batch loss: 1.2030482292175293 batch: 86/840\n",
      "Batch loss: 0.5417267084121704 batch: 87/840\n",
      "Batch loss: 0.5352670550346375 batch: 88/840\n",
      "Batch loss: 0.4907957911491394 batch: 89/840\n",
      "Batch loss: 0.5455800890922546 batch: 90/840\n",
      "Batch loss: 0.5743022561073303 batch: 91/840\n",
      "Batch loss: 0.560980498790741 batch: 92/840\n",
      "Batch loss: 0.6645829081535339 batch: 93/840\n",
      "Batch loss: 0.6464946866035461 batch: 94/840\n",
      "Batch loss: 0.64168381690979 batch: 95/840\n",
      "Batch loss: 0.6211162805557251 batch: 96/840\n",
      "Batch loss: 0.6703132390975952 batch: 97/840\n",
      "Batch loss: 0.7237786650657654 batch: 98/840\n",
      "Batch loss: 0.5943559408187866 batch: 99/840\n",
      "Batch loss: 0.6438949704170227 batch: 100/840\n",
      "Batch loss: 0.5886241793632507 batch: 101/840\n",
      "Batch loss: 0.4758157730102539 batch: 102/840\n",
      "Batch loss: 0.6738863587379456 batch: 103/840\n",
      "Batch loss: 0.5037522315979004 batch: 104/840\n",
      "Batch loss: 0.6836622357368469 batch: 105/840\n",
      "Batch loss: 0.6912097930908203 batch: 106/840\n",
      "Batch loss: 0.5438364148139954 batch: 107/840\n",
      "Batch loss: 0.7802836894989014 batch: 108/840\n",
      "Batch loss: 0.656058669090271 batch: 109/840\n",
      "Batch loss: 0.5124607086181641 batch: 110/840\n",
      "Batch loss: 0.5064674615859985 batch: 111/840\n",
      "Batch loss: 0.6585208773612976 batch: 112/840\n",
      "Batch loss: 0.7331754565238953 batch: 113/840\n",
      "Batch loss: 0.5248286724090576 batch: 114/840\n",
      "Batch loss: 0.651109516620636 batch: 115/840\n",
      "Batch loss: 0.6234719157218933 batch: 116/840\n",
      "Batch loss: 0.6846804618835449 batch: 117/840\n",
      "Batch loss: 0.46248528361320496 batch: 118/840\n",
      "Batch loss: 0.726345419883728 batch: 119/840\n",
      "Batch loss: 0.5850338935852051 batch: 120/840\n",
      "Batch loss: 0.7067732214927673 batch: 121/840\n",
      "Batch loss: 0.8610847592353821 batch: 122/840\n",
      "Batch loss: 0.4936790466308594 batch: 123/840\n",
      "Batch loss: 0.5562310218811035 batch: 124/840\n",
      "Batch loss: 0.6349481344223022 batch: 125/840\n",
      "Batch loss: 0.7475749850273132 batch: 126/840\n",
      "Batch loss: 0.6277144551277161 batch: 127/840\n",
      "Batch loss: 0.623802125453949 batch: 128/840\n",
      "Batch loss: 0.7530741691589355 batch: 129/840\n",
      "Batch loss: 0.526330828666687 batch: 130/840\n",
      "Batch loss: 0.6671264171600342 batch: 131/840\n",
      "Batch loss: 1.1428219079971313 batch: 132/840\n",
      "Batch loss: 0.7483886480331421 batch: 133/840\n",
      "Batch loss: 0.5774515271186829 batch: 134/840\n",
      "Batch loss: 0.5526972413063049 batch: 135/840\n",
      "Batch loss: 0.69753098487854 batch: 136/840\n",
      "Batch loss: 0.6434727311134338 batch: 137/840\n",
      "Batch loss: 0.5649803280830383 batch: 138/840\n",
      "Batch loss: 0.6066953539848328 batch: 139/840\n",
      "Batch loss: 0.6286898255348206 batch: 140/840\n",
      "Batch loss: 0.5427082777023315 batch: 141/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.617235004901886 batch: 142/840\n",
      "Batch loss: 0.6275566220283508 batch: 143/840\n",
      "Batch loss: 0.5123825669288635 batch: 144/840\n",
      "Batch loss: 0.7076588273048401 batch: 145/840\n",
      "Batch loss: 0.7337955236434937 batch: 146/840\n",
      "Batch loss: 0.5931075215339661 batch: 147/840\n",
      "Batch loss: 0.6785932183265686 batch: 148/840\n",
      "Batch loss: 0.6694176197052002 batch: 149/840\n",
      "Batch loss: 0.6968677043914795 batch: 150/840\n",
      "Batch loss: 0.6138027310371399 batch: 151/840\n",
      "Batch loss: 0.584904670715332 batch: 152/840\n",
      "Batch loss: 0.5637467503547668 batch: 153/840\n",
      "Batch loss: 0.7191519141197205 batch: 154/840\n",
      "Batch loss: 0.5358077883720398 batch: 155/840\n",
      "Batch loss: 0.5569371581077576 batch: 156/840\n",
      "Batch loss: 0.6332311034202576 batch: 157/840\n",
      "Batch loss: 0.591666579246521 batch: 158/840\n",
      "Batch loss: 0.5717006921768188 batch: 159/840\n",
      "Batch loss: 0.5851190090179443 batch: 160/840\n",
      "Batch loss: 0.6754816174507141 batch: 161/840\n",
      "Batch loss: 0.6878563165664673 batch: 162/840\n",
      "Batch loss: 0.6785523295402527 batch: 163/840\n",
      "Batch loss: 0.4649832248687744 batch: 164/840\n",
      "Batch loss: 0.5790196061134338 batch: 165/840\n",
      "Batch loss: 0.6299583315849304 batch: 166/840\n",
      "Batch loss: 0.710025429725647 batch: 167/840\n",
      "Batch loss: 0.611667811870575 batch: 168/840\n",
      "Batch loss: 0.6079668998718262 batch: 169/840\n",
      "Batch loss: 0.6402617692947388 batch: 170/840\n",
      "Batch loss: 0.619454026222229 batch: 171/840\n",
      "Batch loss: 0.8016031384468079 batch: 172/840\n",
      "Batch loss: 0.6264302730560303 batch: 173/840\n",
      "Batch loss: 0.6922266483306885 batch: 174/840\n",
      "Batch loss: 0.5735363364219666 batch: 175/840\n",
      "Batch loss: 0.7054312825202942 batch: 176/840\n",
      "Batch loss: 0.6332536935806274 batch: 177/840\n",
      "Batch loss: 0.681161105632782 batch: 178/840\n",
      "Batch loss: 0.6863117218017578 batch: 179/840\n",
      "Batch loss: 0.6314709186553955 batch: 180/840\n",
      "Batch loss: 0.6303591728210449 batch: 181/840\n",
      "Batch loss: 0.6351163983345032 batch: 182/840\n",
      "Batch loss: 0.6341617703437805 batch: 183/840\n",
      "Batch loss: 0.5662977695465088 batch: 184/840\n",
      "Batch loss: 0.5577966570854187 batch: 185/840\n",
      "Batch loss: 0.5232274532318115 batch: 186/840\n",
      "Batch loss: 0.610721230506897 batch: 187/840\n",
      "Batch loss: 0.5684091448783875 batch: 188/840\n",
      "Batch loss: 0.650777280330658 batch: 189/840\n",
      "Batch loss: 0.7988709211349487 batch: 190/840\n",
      "Batch loss: 0.7011765837669373 batch: 191/840\n",
      "Batch loss: 0.4842875301837921 batch: 192/840\n",
      "Batch loss: 0.6546790599822998 batch: 193/840\n",
      "Batch loss: 0.4349280297756195 batch: 194/840\n",
      "Batch loss: 0.6529418230056763 batch: 195/840\n",
      "Batch loss: 0.7706522345542908 batch: 196/840\n",
      "Batch loss: 0.6659131646156311 batch: 197/840\n",
      "Batch loss: 0.5250908732414246 batch: 198/840\n",
      "Batch loss: 0.6679684519767761 batch: 199/840\n",
      "Batch loss: 0.8161227703094482 batch: 200/840\n",
      "Batch loss: 0.5435023307800293 batch: 201/840\n",
      "Batch loss: 0.6364103555679321 batch: 202/840\n",
      "Batch loss: 0.5392619371414185 batch: 203/840\n",
      "Batch loss: 0.6765679121017456 batch: 204/840\n",
      "Batch loss: 0.7779157757759094 batch: 205/840\n",
      "Batch loss: 0.6517800092697144 batch: 206/840\n",
      "Batch loss: 0.5958288311958313 batch: 207/840\n",
      "Batch loss: 0.6020084619522095 batch: 208/840\n",
      "Batch loss: 0.6032416224479675 batch: 209/840\n",
      "Batch loss: 0.6522551774978638 batch: 210/840\n",
      "Batch loss: 0.5992787480354309 batch: 211/840\n",
      "Batch loss: 0.6169359683990479 batch: 212/840\n",
      "Batch loss: 0.6969335675239563 batch: 213/840\n",
      "Batch loss: 0.8692418932914734 batch: 214/840\n",
      "Batch loss: 0.6973483562469482 batch: 215/840\n",
      "Batch loss: 0.5763964056968689 batch: 216/840\n",
      "Batch loss: 0.5403406023979187 batch: 217/840\n",
      "Batch loss: 0.6468042731285095 batch: 218/840\n",
      "Batch loss: 0.6083358526229858 batch: 219/840\n",
      "Batch loss: 0.8605177402496338 batch: 220/840\n",
      "Batch loss: 0.5622709393501282 batch: 221/840\n",
      "Batch loss: 0.7178025841712952 batch: 222/840\n",
      "Batch loss: 0.583894670009613 batch: 223/840\n",
      "Batch loss: 0.7239288091659546 batch: 224/840\n",
      "Batch loss: 0.7315001487731934 batch: 225/840\n",
      "Batch loss: 0.664671003818512 batch: 226/840\n",
      "Batch loss: 0.813810408115387 batch: 227/840\n",
      "Batch loss: 0.4579671621322632 batch: 228/840\n",
      "Batch loss: 0.5448158979415894 batch: 229/840\n",
      "Batch loss: 0.699013352394104 batch: 230/840\n",
      "Batch loss: 0.46274957060813904 batch: 231/840\n",
      "Batch loss: 0.6804159283638 batch: 232/840\n",
      "Batch loss: 0.6873989105224609 batch: 233/840\n",
      "Batch loss: 0.5892499685287476 batch: 234/840\n",
      "Batch loss: 0.7044241428375244 batch: 235/840\n",
      "Batch loss: 0.7184139490127563 batch: 236/840\n",
      "Batch loss: 0.5567940473556519 batch: 237/840\n",
      "Batch loss: 0.8149288296699524 batch: 238/840\n",
      "Batch loss: 0.7082035541534424 batch: 239/840\n",
      "Batch loss: 0.6897788047790527 batch: 240/840\n",
      "Batch loss: 0.7112328410148621 batch: 241/840\n",
      "Batch loss: 0.6207559108734131 batch: 242/840\n",
      "Batch loss: 0.677238941192627 batch: 243/840\n",
      "Batch loss: 0.7937489151954651 batch: 244/840\n",
      "Batch loss: 0.42530471086502075 batch: 245/840\n",
      "Batch loss: 0.6653264760971069 batch: 246/840\n",
      "Batch loss: 0.6642965078353882 batch: 247/840\n",
      "Batch loss: 0.6907752752304077 batch: 248/840\n",
      "Batch loss: 0.8183778524398804 batch: 249/840\n",
      "Batch loss: 0.5694994926452637 batch: 250/840\n",
      "Batch loss: 0.5834177136421204 batch: 251/840\n",
      "Batch loss: 0.5280511975288391 batch: 252/840\n",
      "Batch loss: 0.6864110827445984 batch: 253/840\n",
      "Batch loss: 0.7416823506355286 batch: 254/840\n",
      "Batch loss: 0.6013436913490295 batch: 255/840\n",
      "Batch loss: 0.6292271614074707 batch: 256/840\n",
      "Batch loss: 0.5954334139823914 batch: 257/840\n",
      "Batch loss: 0.7568342089653015 batch: 258/840\n",
      "Batch loss: 0.4816952645778656 batch: 259/840\n",
      "Batch loss: 0.5602381229400635 batch: 260/840\n",
      "Batch loss: 0.6423175930976868 batch: 261/840\n",
      "Batch loss: 0.4626861810684204 batch: 262/840\n",
      "Batch loss: 0.6606723666191101 batch: 263/840\n",
      "Batch loss: 0.6464968323707581 batch: 264/840\n",
      "Batch loss: 0.6906046867370605 batch: 265/840\n",
      "Batch loss: 0.538600742816925 batch: 266/840\n",
      "Batch loss: 0.7358994483947754 batch: 267/840\n",
      "Batch loss: 0.5424865484237671 batch: 268/840\n",
      "Batch loss: 0.5364407896995544 batch: 269/840\n",
      "Batch loss: 0.5676164031028748 batch: 270/840\n",
      "Batch loss: 0.5459401607513428 batch: 271/840\n",
      "Batch loss: 0.7737840414047241 batch: 272/840\n",
      "Batch loss: 0.885267972946167 batch: 273/840\n",
      "Batch loss: 0.6122360229492188 batch: 274/840\n",
      "Batch loss: 0.7429586052894592 batch: 275/840\n",
      "Batch loss: 0.6002721190452576 batch: 276/840\n",
      "Batch loss: 0.5300956964492798 batch: 277/840\n",
      "Batch loss: 0.8114519715309143 batch: 278/840\n",
      "Batch loss: 0.7839530110359192 batch: 279/840\n",
      "Batch loss: 0.7268072366714478 batch: 280/840\n",
      "Batch loss: 0.5830326676368713 batch: 281/840\n",
      "Batch loss: 0.6167712807655334 batch: 282/840\n",
      "Batch loss: 0.6806455850601196 batch: 283/840\n",
      "Batch loss: 0.5122262835502625 batch: 284/840\n",
      "Batch loss: 0.5460365414619446 batch: 285/840\n",
      "Batch loss: 0.7184427380561829 batch: 286/840\n",
      "Batch loss: 0.3997747302055359 batch: 287/840\n",
      "Batch loss: 0.6355633735656738 batch: 288/840\n",
      "Batch loss: 0.774721622467041 batch: 289/840\n",
      "Batch loss: 0.7269738912582397 batch: 290/840\n",
      "Batch loss: 0.7219573855400085 batch: 291/840\n",
      "Batch loss: 0.6193224191665649 batch: 292/840\n",
      "Batch loss: 0.7604551911354065 batch: 293/840\n",
      "Batch loss: 0.6257626414299011 batch: 294/840\n",
      "Batch loss: 0.5383694767951965 batch: 295/840\n",
      "Batch loss: 0.7664808034896851 batch: 296/840\n",
      "Batch loss: 0.7630065679550171 batch: 297/840\n",
      "Batch loss: 0.7520781755447388 batch: 298/840\n",
      "Batch loss: 0.5561292171478271 batch: 299/840\n",
      "Batch loss: 0.8723035454750061 batch: 300/840\n",
      "Batch loss: 0.7494620680809021 batch: 301/840\n",
      "Batch loss: 0.6492491364479065 batch: 302/840\n",
      "Batch loss: 0.7919862866401672 batch: 303/840\n",
      "Batch loss: 0.5279520153999329 batch: 304/840\n",
      "Batch loss: 0.6096018552780151 batch: 305/840\n",
      "Batch loss: 0.6749257445335388 batch: 306/840\n",
      "Batch loss: 0.5630313158035278 batch: 307/840\n",
      "Batch loss: 0.8349484205245972 batch: 308/840\n",
      "Batch loss: 0.5963736772537231 batch: 309/840\n",
      "Batch loss: 0.7870162725448608 batch: 310/840\n",
      "Batch loss: 0.6009953618049622 batch: 311/840\n",
      "Batch loss: 0.7664231657981873 batch: 312/840\n",
      "Batch loss: 0.76102614402771 batch: 313/840\n",
      "Batch loss: 0.5854833722114563 batch: 314/840\n",
      "Batch loss: 0.5775927305221558 batch: 315/840\n",
      "Batch loss: 0.5259283185005188 batch: 316/840\n",
      "Batch loss: 0.7563386559486389 batch: 317/840\n",
      "Batch loss: 0.6869723796844482 batch: 318/840\n",
      "Batch loss: 0.6978259086608887 batch: 319/840\n",
      "Batch loss: 0.531298041343689 batch: 320/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5292178392410278 batch: 321/840\n",
      "Batch loss: 0.7555029988288879 batch: 322/840\n",
      "Batch loss: 0.8196893334388733 batch: 323/840\n",
      "Batch loss: 0.6750500202178955 batch: 324/840\n",
      "Batch loss: 0.5271837115287781 batch: 325/840\n",
      "Batch loss: 0.5799279808998108 batch: 326/840\n",
      "Batch loss: 0.5284938216209412 batch: 327/840\n",
      "Batch loss: 0.8976287841796875 batch: 328/840\n",
      "Batch loss: 0.7130608558654785 batch: 329/840\n",
      "Batch loss: 0.6223104596138 batch: 330/840\n",
      "Batch loss: 0.7257717847824097 batch: 331/840\n",
      "Batch loss: 0.6819815635681152 batch: 332/840\n",
      "Batch loss: 0.6146687269210815 batch: 333/840\n",
      "Batch loss: 0.7319801449775696 batch: 334/840\n",
      "Batch loss: 0.5656142830848694 batch: 335/840\n",
      "Batch loss: 0.815721333026886 batch: 336/840\n",
      "Batch loss: 0.8373140692710876 batch: 337/840\n",
      "Batch loss: 0.6743718981742859 batch: 338/840\n",
      "Batch loss: 0.5682758092880249 batch: 339/840\n",
      "Batch loss: 0.7881045341491699 batch: 340/840\n",
      "Batch loss: 0.5675820112228394 batch: 341/840\n",
      "Batch loss: 0.48436182737350464 batch: 342/840\n",
      "Batch loss: 0.8499706387519836 batch: 343/840\n",
      "Batch loss: 0.5966794490814209 batch: 344/840\n",
      "Batch loss: 0.5064555406570435 batch: 345/840\n",
      "Batch loss: 0.5821129083633423 batch: 346/840\n",
      "Batch loss: 0.7116487622261047 batch: 347/840\n",
      "Batch loss: 0.5555675625801086 batch: 348/840\n",
      "Batch loss: 0.6614831686019897 batch: 349/840\n",
      "Batch loss: 0.5861234068870544 batch: 350/840\n",
      "Batch loss: 0.6182787418365479 batch: 351/840\n",
      "Batch loss: 0.640482485294342 batch: 352/840\n",
      "Batch loss: 0.7514700293540955 batch: 353/840\n",
      "Batch loss: 0.6186257600784302 batch: 354/840\n",
      "Batch loss: 0.5316941142082214 batch: 355/840\n",
      "Batch loss: 0.667888343334198 batch: 356/840\n",
      "Batch loss: 0.542559802532196 batch: 357/840\n",
      "Batch loss: 0.9224643111228943 batch: 358/840\n",
      "Batch loss: 0.6226885318756104 batch: 359/840\n",
      "Batch loss: 0.7964179515838623 batch: 360/840\n",
      "Batch loss: 0.7036740183830261 batch: 361/840\n",
      "Batch loss: 0.5156801342964172 batch: 362/840\n",
      "Batch loss: 0.5411044955253601 batch: 363/840\n",
      "Batch loss: 0.6582807898521423 batch: 364/840\n",
      "Batch loss: 0.5481505393981934 batch: 365/840\n",
      "Batch loss: 0.5722653865814209 batch: 366/840\n",
      "Batch loss: 0.48075780272483826 batch: 367/840\n",
      "Batch loss: 0.8118645548820496 batch: 368/840\n",
      "Batch loss: 0.6479403972625732 batch: 369/840\n",
      "Batch loss: 0.6795668601989746 batch: 370/840\n",
      "Batch loss: 0.6897702813148499 batch: 371/840\n",
      "Batch loss: 0.5143030881881714 batch: 372/840\n",
      "Batch loss: 0.6386053562164307 batch: 373/840\n",
      "Batch loss: 0.6829317212104797 batch: 374/840\n",
      "Batch loss: 0.5991946458816528 batch: 375/840\n",
      "Batch loss: 0.603059709072113 batch: 376/840\n",
      "Batch loss: 0.7516549825668335 batch: 377/840\n",
      "Batch loss: 0.5072377920150757 batch: 378/840\n",
      "Batch loss: 0.5606902241706848 batch: 379/840\n",
      "Batch loss: 0.8080239295959473 batch: 380/840\n",
      "Batch loss: 0.8941977620124817 batch: 381/840\n",
      "Batch loss: 0.7502934336662292 batch: 382/840\n",
      "Batch loss: 0.6826096177101135 batch: 383/840\n",
      "Batch loss: 0.5839645862579346 batch: 384/840\n",
      "Batch loss: 0.7403245568275452 batch: 385/840\n",
      "Batch loss: 0.8152364492416382 batch: 386/840\n",
      "Batch loss: 0.626849889755249 batch: 387/840\n",
      "Batch loss: 0.7797127366065979 batch: 388/840\n",
      "Batch loss: 0.5933694243431091 batch: 389/840\n",
      "Batch loss: 0.8937498331069946 batch: 390/840\n",
      "Batch loss: 0.7754369378089905 batch: 391/840\n",
      "Batch loss: 0.6048688292503357 batch: 392/840\n",
      "Batch loss: 0.5053113698959351 batch: 393/840\n",
      "Batch loss: 0.6858460903167725 batch: 394/840\n",
      "Batch loss: 0.6181448698043823 batch: 395/840\n",
      "Batch loss: 0.6559234857559204 batch: 396/840\n",
      "Batch loss: 0.5992431044578552 batch: 397/840\n",
      "Batch loss: 0.6610549688339233 batch: 398/840\n",
      "Batch loss: 0.46904799342155457 batch: 399/840\n",
      "Batch loss: 0.5837483406066895 batch: 400/840\n",
      "Batch loss: 0.8170451521873474 batch: 401/840\n",
      "Batch loss: 0.5359739661216736 batch: 402/840\n",
      "Batch loss: 0.6570939421653748 batch: 403/840\n",
      "Batch loss: 0.5770530104637146 batch: 404/840\n",
      "Batch loss: 0.57376629114151 batch: 405/840\n",
      "Batch loss: 0.6706852912902832 batch: 406/840\n",
      "Batch loss: 0.643984317779541 batch: 407/840\n",
      "Batch loss: 0.5959054231643677 batch: 408/840\n",
      "Batch loss: 0.7585865259170532 batch: 409/840\n",
      "Batch loss: 0.7683122158050537 batch: 410/840\n",
      "Batch loss: 0.5960739254951477 batch: 411/840\n",
      "Batch loss: 0.7757831811904907 batch: 412/840\n",
      "Batch loss: 0.7038189768791199 batch: 413/840\n",
      "Batch loss: 0.5786711573600769 batch: 414/840\n",
      "Batch loss: 0.9196168780326843 batch: 415/840\n",
      "Batch loss: 0.5308797359466553 batch: 416/840\n",
      "Batch loss: 0.6007914543151855 batch: 417/840\n",
      "Batch loss: 0.7376317381858826 batch: 418/840\n",
      "Batch loss: 0.7246309518814087 batch: 419/840\n",
      "Batch loss: 0.7282619476318359 batch: 420/840\n",
      "Batch loss: 0.5378038883209229 batch: 421/840\n",
      "Batch loss: 0.6222115755081177 batch: 422/840\n",
      "Batch loss: 0.6118895411491394 batch: 423/840\n",
      "Batch loss: 0.8123460412025452 batch: 424/840\n",
      "Batch loss: 0.7253207564353943 batch: 425/840\n",
      "Batch loss: 0.6017680168151855 batch: 426/840\n",
      "Batch loss: 0.5973221063613892 batch: 427/840\n",
      "Batch loss: 0.7269734740257263 batch: 428/840\n",
      "Batch loss: 0.557272732257843 batch: 429/840\n",
      "Batch loss: 0.7518962621688843 batch: 430/840\n",
      "Batch loss: 0.6837667226791382 batch: 431/840\n",
      "Batch loss: 0.6263042092323303 batch: 432/840\n",
      "Batch loss: 0.5259963870048523 batch: 433/840\n",
      "Batch loss: 0.5737271308898926 batch: 434/840\n",
      "Batch loss: 0.7782942056655884 batch: 435/840\n",
      "Batch loss: 0.5721579194068909 batch: 436/840\n",
      "Batch loss: 0.7150410413742065 batch: 437/840\n",
      "Batch loss: 0.43987685441970825 batch: 438/840\n",
      "Batch loss: 0.5828715562820435 batch: 439/840\n",
      "Batch loss: 0.7165267467498779 batch: 440/840\n",
      "Batch loss: 0.5935258269309998 batch: 441/840\n",
      "Batch loss: 0.6733680963516235 batch: 442/840\n",
      "Batch loss: 0.6127563714981079 batch: 443/840\n",
      "Batch loss: 0.6998282670974731 batch: 444/840\n",
      "Batch loss: 0.5941201448440552 batch: 445/840\n",
      "Batch loss: 0.7883711457252502 batch: 446/840\n",
      "Batch loss: 0.7860987782478333 batch: 447/840\n",
      "Batch loss: 0.7906144857406616 batch: 448/840\n",
      "Batch loss: 0.5540926456451416 batch: 449/840\n",
      "Batch loss: 0.6808439493179321 batch: 450/840\n",
      "Batch loss: 0.6371021866798401 batch: 451/840\n",
      "Batch loss: 0.686668872833252 batch: 452/840\n",
      "Batch loss: 0.6044151782989502 batch: 453/840\n",
      "Batch loss: 0.6107905507087708 batch: 454/840\n",
      "Batch loss: 0.6549685001373291 batch: 455/840\n",
      "Batch loss: 0.5304587483406067 batch: 456/840\n",
      "Batch loss: 0.5909923911094666 batch: 457/840\n",
      "Batch loss: 0.6451926231384277 batch: 458/840\n",
      "Batch loss: 0.5037850737571716 batch: 459/840\n",
      "Batch loss: 0.48150208592414856 batch: 460/840\n",
      "Batch loss: 0.6906142234802246 batch: 461/840\n",
      "Batch loss: 0.653928279876709 batch: 462/840\n",
      "Batch loss: 0.8487632870674133 batch: 463/840\n",
      "Batch loss: 0.45002949237823486 batch: 464/840\n",
      "Batch loss: 0.9069033265113831 batch: 465/840\n",
      "Batch loss: 0.6471363306045532 batch: 466/840\n",
      "Batch loss: 0.880522608757019 batch: 467/840\n",
      "Batch loss: 0.5974186062812805 batch: 468/840\n",
      "Batch loss: 0.5206358432769775 batch: 469/840\n",
      "Batch loss: 0.601131796836853 batch: 470/840\n",
      "Batch loss: 0.6804284453392029 batch: 471/840\n",
      "Batch loss: 0.6910699605941772 batch: 472/840\n",
      "Batch loss: 0.7179622650146484 batch: 473/840\n",
      "Batch loss: 0.5941975116729736 batch: 474/840\n",
      "Batch loss: 0.6574150323867798 batch: 475/840\n",
      "Batch loss: 0.6802299618721008 batch: 476/840\n",
      "Batch loss: 0.5208990573883057 batch: 477/840\n",
      "Batch loss: 0.4618496298789978 batch: 478/840\n",
      "Batch loss: 0.4466748833656311 batch: 479/840\n",
      "Batch loss: 0.6212098598480225 batch: 480/840\n",
      "Batch loss: 0.667600154876709 batch: 481/840\n",
      "Batch loss: 0.6416140794754028 batch: 482/840\n",
      "Batch loss: 0.6648815274238586 batch: 483/840\n",
      "Batch loss: 0.5691714286804199 batch: 484/840\n",
      "Batch loss: 0.5960108637809753 batch: 485/840\n",
      "Batch loss: 0.6446300745010376 batch: 486/840\n",
      "Batch loss: 0.4199869632720947 batch: 487/840\n",
      "Batch loss: 0.6427903771400452 batch: 488/840\n",
      "Batch loss: 0.6855975389480591 batch: 489/840\n",
      "Batch loss: 0.7422719597816467 batch: 490/840\n",
      "Batch loss: 0.4747777283191681 batch: 491/840\n",
      "Batch loss: 0.7633628845214844 batch: 492/840\n",
      "Batch loss: 0.6189452409744263 batch: 493/840\n",
      "Batch loss: 0.4174266755580902 batch: 494/840\n",
      "Batch loss: 0.7281481027603149 batch: 495/840\n",
      "Batch loss: 0.6631322503089905 batch: 496/840\n",
      "Batch loss: 0.7435570359230042 batch: 497/840\n",
      "Batch loss: 0.4619179666042328 batch: 498/840\n",
      "Batch loss: 0.7895164489746094 batch: 499/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6276779770851135 batch: 500/840\n",
      "Batch loss: 0.5198490023612976 batch: 501/840\n",
      "Batch loss: 0.7441518902778625 batch: 502/840\n",
      "Batch loss: 0.48566871881484985 batch: 503/840\n",
      "Batch loss: 0.6064630746841431 batch: 504/840\n",
      "Batch loss: 0.7661152482032776 batch: 505/840\n",
      "Batch loss: 0.5896593332290649 batch: 506/840\n",
      "Batch loss: 0.7413894534111023 batch: 507/840\n",
      "Batch loss: 0.5950033068656921 batch: 508/840\n",
      "Batch loss: 0.655758798122406 batch: 509/840\n",
      "Batch loss: 0.7134829163551331 batch: 510/840\n",
      "Batch loss: 0.4748131036758423 batch: 511/840\n",
      "Batch loss: 0.6362482309341431 batch: 512/840\n",
      "Batch loss: 0.5462290048599243 batch: 513/840\n",
      "Batch loss: 0.7183212041854858 batch: 514/840\n",
      "Batch loss: 0.5413353443145752 batch: 515/840\n",
      "Batch loss: 0.6891316175460815 batch: 516/840\n",
      "Batch loss: 0.5990265011787415 batch: 517/840\n",
      "Batch loss: 0.6191167831420898 batch: 518/840\n",
      "Batch loss: 0.7804262638092041 batch: 519/840\n",
      "Batch loss: 0.5558063387870789 batch: 520/840\n",
      "Batch loss: 0.6955779790878296 batch: 521/840\n",
      "Batch loss: 0.7268189787864685 batch: 522/840\n",
      "Batch loss: 0.49369126558303833 batch: 523/840\n",
      "Batch loss: 0.6884227991104126 batch: 524/840\n",
      "Batch loss: 0.5957916975021362 batch: 525/840\n",
      "Batch loss: 0.8110994100570679 batch: 526/840\n",
      "Batch loss: 0.7427738904953003 batch: 527/840\n",
      "Batch loss: 0.6095733046531677 batch: 528/840\n",
      "Batch loss: 0.5093529224395752 batch: 529/840\n",
      "Batch loss: 0.6681008338928223 batch: 530/840\n",
      "Batch loss: 0.5548457503318787 batch: 531/840\n",
      "Batch loss: 0.4854000210762024 batch: 532/840\n",
      "Batch loss: 0.6638998985290527 batch: 533/840\n",
      "Batch loss: 0.6497337222099304 batch: 534/840\n",
      "Batch loss: 0.5962560176849365 batch: 535/840\n",
      "Batch loss: 0.7738337516784668 batch: 536/840\n",
      "Batch loss: 0.5915484428405762 batch: 537/840\n",
      "Batch loss: 0.5732245445251465 batch: 538/840\n",
      "Batch loss: 0.544075071811676 batch: 539/840\n",
      "Batch loss: 0.7367690801620483 batch: 540/840\n",
      "Batch loss: 0.6921236515045166 batch: 541/840\n",
      "Batch loss: 0.7977545857429504 batch: 542/840\n",
      "Batch loss: 0.46114039421081543 batch: 543/840\n",
      "Batch loss: 0.6652393937110901 batch: 544/840\n",
      "Batch loss: 0.6169988512992859 batch: 545/840\n",
      "Batch loss: 0.5938172936439514 batch: 546/840\n",
      "Batch loss: 0.5908908843994141 batch: 547/840\n",
      "Batch loss: 0.47782671451568604 batch: 548/840\n",
      "Batch loss: 0.6372015476226807 batch: 549/840\n",
      "Batch loss: 0.5360086560249329 batch: 550/840\n",
      "Batch loss: 0.6102691888809204 batch: 551/840\n",
      "Batch loss: 0.4952876567840576 batch: 552/840\n",
      "Batch loss: 0.713803768157959 batch: 553/840\n",
      "Batch loss: 0.5766637325286865 batch: 554/840\n",
      "Batch loss: 0.6234041452407837 batch: 555/840\n",
      "Batch loss: 0.5666022300720215 batch: 556/840\n",
      "Batch loss: 0.5871455073356628 batch: 557/840\n",
      "Batch loss: 0.6937547922134399 batch: 558/840\n",
      "Batch loss: 0.608640730381012 batch: 559/840\n",
      "Batch loss: 0.6651586294174194 batch: 560/840\n",
      "Batch loss: 0.5537728667259216 batch: 561/840\n",
      "Batch loss: 0.6569433808326721 batch: 562/840\n",
      "Batch loss: 0.4788658022880554 batch: 563/840\n",
      "Batch loss: 0.6968801021575928 batch: 564/840\n",
      "Batch loss: 0.781059980392456 batch: 565/840\n",
      "Batch loss: 0.6978955268859863 batch: 566/840\n",
      "Batch loss: 0.8664994835853577 batch: 567/840\n",
      "Batch loss: 0.5964851379394531 batch: 568/840\n",
      "Batch loss: 0.6084027290344238 batch: 569/840\n",
      "Batch loss: 0.4445919096469879 batch: 570/840\n",
      "Batch loss: 0.6294340491294861 batch: 571/840\n",
      "Batch loss: 0.6792845129966736 batch: 572/840\n",
      "Batch loss: 0.6574609875679016 batch: 573/840\n",
      "Batch loss: 0.7721561193466187 batch: 574/840\n",
      "Batch loss: 0.5692801475524902 batch: 575/840\n",
      "Batch loss: 0.6161585450172424 batch: 576/840\n",
      "Batch loss: 0.5418342351913452 batch: 577/840\n",
      "Batch loss: 0.7180194854736328 batch: 578/840\n",
      "Batch loss: 0.591215193271637 batch: 579/840\n",
      "Batch loss: 0.8728615045547485 batch: 580/840\n",
      "Batch loss: 0.6727045178413391 batch: 581/840\n",
      "Batch loss: 0.8005272150039673 batch: 582/840\n",
      "Batch loss: 0.4954127371311188 batch: 583/840\n",
      "Batch loss: 0.7497552633285522 batch: 584/840\n",
      "Batch loss: 0.5899814367294312 batch: 585/840\n",
      "Batch loss: 0.6174929141998291 batch: 586/840\n",
      "Batch loss: 0.588767945766449 batch: 587/840\n",
      "Batch loss: 0.6275469064712524 batch: 588/840\n",
      "Batch loss: 0.7973417043685913 batch: 589/840\n",
      "Batch loss: 0.553413450717926 batch: 590/840\n",
      "Batch loss: 0.45655399560928345 batch: 591/840\n",
      "Batch loss: 0.5906376838684082 batch: 592/840\n",
      "Batch loss: 0.7400618195533752 batch: 593/840\n",
      "Batch loss: 0.5014061331748962 batch: 594/840\n",
      "Batch loss: 0.4421583414077759 batch: 595/840\n",
      "Batch loss: 0.47416383028030396 batch: 596/840\n",
      "Batch loss: 0.5811131596565247 batch: 597/840\n",
      "Batch loss: 0.438755065202713 batch: 598/840\n",
      "Batch loss: 0.5656000375747681 batch: 599/840\n",
      "Batch loss: 0.7111135125160217 batch: 600/840\n",
      "Batch loss: 0.7686660289764404 batch: 601/840\n",
      "Batch loss: 0.5568882822990417 batch: 602/840\n",
      "Batch loss: 0.5894761085510254 batch: 603/840\n",
      "Batch loss: 0.6511878967285156 batch: 604/840\n",
      "Batch loss: 0.8399825096130371 batch: 605/840\n",
      "Batch loss: 0.7587845325469971 batch: 606/840\n",
      "Batch loss: 0.7308220863342285 batch: 607/840\n",
      "Batch loss: 0.7649915218353271 batch: 608/840\n",
      "Batch loss: 0.530440092086792 batch: 609/840\n",
      "Batch loss: 0.7121700048446655 batch: 610/840\n",
      "Batch loss: 0.8052239418029785 batch: 611/840\n",
      "Batch loss: 0.7126628160476685 batch: 612/840\n",
      "Batch loss: 0.6506633162498474 batch: 613/840\n",
      "Batch loss: 0.6215384602546692 batch: 614/840\n",
      "Batch loss: 0.6293721795082092 batch: 615/840\n",
      "Batch loss: 0.516669511795044 batch: 616/840\n",
      "Batch loss: 0.5263872742652893 batch: 617/840\n",
      "Batch loss: 0.589577317237854 batch: 618/840\n",
      "Batch loss: 0.8536900281906128 batch: 619/840\n",
      "Batch loss: 0.606524646282196 batch: 620/840\n",
      "Batch loss: 0.651582658290863 batch: 621/840\n",
      "Batch loss: 0.6545377969741821 batch: 622/840\n",
      "Batch loss: 0.636722207069397 batch: 623/840\n",
      "Batch loss: 0.834757387638092 batch: 624/840\n",
      "Batch loss: 0.606955349445343 batch: 625/840\n",
      "Batch loss: 0.6253261566162109 batch: 626/840\n",
      "Batch loss: 0.5204188823699951 batch: 627/840\n",
      "Batch loss: 0.6221547722816467 batch: 628/840\n",
      "Batch loss: 0.713726818561554 batch: 629/840\n",
      "Batch loss: 0.613476037979126 batch: 630/840\n",
      "Batch loss: 0.702456533908844 batch: 631/840\n",
      "Batch loss: 0.6939460039138794 batch: 632/840\n",
      "Batch loss: 0.6494579315185547 batch: 633/840\n",
      "Batch loss: 0.6580744385719299 batch: 634/840\n",
      "Batch loss: 0.580546498298645 batch: 635/840\n",
      "Batch loss: 0.5490623712539673 batch: 636/840\n",
      "Batch loss: 0.49038514494895935 batch: 637/840\n",
      "Batch loss: 0.7535250186920166 batch: 638/840\n",
      "Batch loss: 0.761810302734375 batch: 639/840\n",
      "Batch loss: 0.5804468989372253 batch: 640/840\n",
      "Batch loss: 0.75177401304245 batch: 641/840\n",
      "Batch loss: 0.5078227519989014 batch: 642/840\n",
      "Batch loss: 0.9741442203521729 batch: 643/840\n",
      "Batch loss: 0.6283223628997803 batch: 644/840\n",
      "Batch loss: 0.524117648601532 batch: 645/840\n",
      "Batch loss: 0.5751835107803345 batch: 646/840\n",
      "Batch loss: 0.7481020092964172 batch: 647/840\n",
      "Batch loss: 0.7041773796081543 batch: 648/840\n",
      "Batch loss: 0.6209824085235596 batch: 649/840\n",
      "Batch loss: 0.5256218910217285 batch: 650/840\n",
      "Batch loss: 0.6370797753334045 batch: 651/840\n",
      "Batch loss: 0.6085742115974426 batch: 652/840\n",
      "Batch loss: 0.4958714246749878 batch: 653/840\n",
      "Batch loss: 0.8578867316246033 batch: 654/840\n",
      "Batch loss: 0.557511031627655 batch: 655/840\n",
      "Batch loss: 0.6488236784934998 batch: 656/840\n",
      "Batch loss: 0.5456349849700928 batch: 657/840\n",
      "Batch loss: 0.6757360696792603 batch: 658/840\n",
      "Batch loss: 0.7244516015052795 batch: 659/840\n",
      "Batch loss: 0.5907646417617798 batch: 660/840\n",
      "Batch loss: 0.7314589023590088 batch: 661/840\n",
      "Batch loss: 0.6468281745910645 batch: 662/840\n",
      "Batch loss: 0.5201566815376282 batch: 663/840\n",
      "Batch loss: 0.6699867248535156 batch: 664/840\n",
      "Batch loss: 0.6549898386001587 batch: 665/840\n",
      "Batch loss: 0.697869062423706 batch: 666/840\n",
      "Batch loss: 0.7873736619949341 batch: 667/840\n",
      "Batch loss: 0.6783437132835388 batch: 668/840\n",
      "Batch loss: 0.5397817492485046 batch: 669/840\n",
      "Batch loss: 0.6795867085456848 batch: 670/840\n",
      "Batch loss: 0.6287242770195007 batch: 671/840\n",
      "Batch loss: 0.5103703141212463 batch: 672/840\n",
      "Batch loss: 0.7558572888374329 batch: 673/840\n",
      "Batch loss: 0.750302255153656 batch: 674/840\n",
      "Batch loss: 0.6447227597236633 batch: 675/840\n",
      "Batch loss: 0.58421790599823 batch: 676/840\n",
      "Batch loss: 0.6750376224517822 batch: 677/840\n",
      "Batch loss: 0.6612523794174194 batch: 678/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8069524168968201 batch: 679/840\n",
      "Batch loss: 0.6602084636688232 batch: 680/840\n",
      "Batch loss: 0.647300124168396 batch: 681/840\n",
      "Batch loss: 0.6055466532707214 batch: 682/840\n",
      "Batch loss: 0.5354856252670288 batch: 683/840\n",
      "Batch loss: 0.8109566569328308 batch: 684/840\n",
      "Batch loss: 0.6712053418159485 batch: 685/840\n",
      "Batch loss: 0.6825747489929199 batch: 686/840\n",
      "Batch loss: 0.7098442912101746 batch: 687/840\n",
      "Batch loss: 0.6371276378631592 batch: 688/840\n",
      "Batch loss: 0.5666394233703613 batch: 689/840\n",
      "Batch loss: 0.5731914639472961 batch: 690/840\n",
      "Batch loss: 0.6444064974784851 batch: 691/840\n",
      "Batch loss: 0.5869895219802856 batch: 692/840\n",
      "Batch loss: 0.6465593576431274 batch: 693/840\n",
      "Batch loss: 0.7953531742095947 batch: 694/840\n",
      "Batch loss: 0.8226582407951355 batch: 695/840\n",
      "Batch loss: 0.6007908582687378 batch: 696/840\n",
      "Batch loss: 0.6721212863922119 batch: 697/840\n",
      "Batch loss: 0.6237316727638245 batch: 698/840\n",
      "Batch loss: 0.7440046072006226 batch: 699/840\n",
      "Batch loss: 0.7729341387748718 batch: 700/840\n",
      "Batch loss: 0.857451856136322 batch: 701/840\n",
      "Batch loss: 0.6321194171905518 batch: 702/840\n",
      "Batch loss: 0.47479483485221863 batch: 703/840\n",
      "Batch loss: 0.7029556035995483 batch: 704/840\n",
      "Batch loss: 0.5990222692489624 batch: 705/840\n",
      "Batch loss: 0.5936660766601562 batch: 706/840\n",
      "Batch loss: 0.6184207797050476 batch: 707/840\n",
      "Batch loss: 0.6140437722206116 batch: 708/840\n",
      "Batch loss: 0.6552755832672119 batch: 709/840\n",
      "Batch loss: 0.8328386545181274 batch: 710/840\n",
      "Batch loss: 0.4501416087150574 batch: 711/840\n",
      "Batch loss: 0.701049268245697 batch: 712/840\n",
      "Batch loss: 0.5883428454399109 batch: 713/840\n",
      "Batch loss: 0.6703964471817017 batch: 714/840\n",
      "Batch loss: 0.8127490282058716 batch: 715/840\n",
      "Batch loss: 0.6212634444236755 batch: 716/840\n",
      "Batch loss: 0.7207769751548767 batch: 717/840\n",
      "Batch loss: 0.5844826698303223 batch: 718/840\n",
      "Batch loss: 0.4838086664676666 batch: 719/840\n",
      "Batch loss: 0.5432713627815247 batch: 720/840\n",
      "Batch loss: 0.7130946516990662 batch: 721/840\n",
      "Batch loss: 0.7633966207504272 batch: 722/840\n",
      "Batch loss: 0.5293710827827454 batch: 723/840\n",
      "Batch loss: 0.7036890983581543 batch: 724/840\n",
      "Batch loss: 0.6854230761528015 batch: 725/840\n",
      "Batch loss: 0.6812272071838379 batch: 726/840\n",
      "Batch loss: 0.8903337717056274 batch: 727/840\n",
      "Batch loss: 0.7498289346694946 batch: 728/840\n",
      "Batch loss: 0.6686390042304993 batch: 729/840\n",
      "Batch loss: 0.7065771222114563 batch: 730/840\n",
      "Batch loss: 0.5219370126724243 batch: 731/840\n",
      "Batch loss: 0.7908115983009338 batch: 732/840\n",
      "Batch loss: 0.6271607875823975 batch: 733/840\n",
      "Batch loss: 0.6539736986160278 batch: 734/840\n",
      "Batch loss: 0.6600738763809204 batch: 735/840\n",
      "Batch loss: 0.7174165844917297 batch: 736/840\n",
      "Batch loss: 0.6832330226898193 batch: 737/840\n",
      "Batch loss: 0.5600235462188721 batch: 738/840\n",
      "Batch loss: 0.6882997155189514 batch: 739/840\n",
      "Batch loss: 0.7024974226951599 batch: 740/840\n",
      "Batch loss: 0.5201850533485413 batch: 741/840\n",
      "Batch loss: 0.5079082250595093 batch: 742/840\n",
      "Batch loss: 0.5069512724876404 batch: 743/840\n",
      "Batch loss: 0.5185856223106384 batch: 744/840\n",
      "Batch loss: 0.5974175930023193 batch: 745/840\n",
      "Batch loss: 0.7045333981513977 batch: 746/840\n",
      "Batch loss: 0.7278239727020264 batch: 747/840\n",
      "Batch loss: 0.6763189435005188 batch: 748/840\n",
      "Batch loss: 0.5428524613380432 batch: 749/840\n",
      "Batch loss: 0.5373184084892273 batch: 750/840\n",
      "Batch loss: 0.7232205271720886 batch: 751/840\n",
      "Batch loss: 0.8685796856880188 batch: 752/840\n",
      "Batch loss: 0.49410805106163025 batch: 753/840\n",
      "Batch loss: 0.673022985458374 batch: 754/840\n",
      "Batch loss: 0.5962781310081482 batch: 755/840\n",
      "Batch loss: 0.47774845361709595 batch: 756/840\n",
      "Batch loss: 0.7603409290313721 batch: 757/840\n",
      "Batch loss: 0.713762640953064 batch: 758/840\n",
      "Batch loss: 0.5775997638702393 batch: 759/840\n",
      "Batch loss: 0.5028247237205505 batch: 760/840\n",
      "Batch loss: 0.561984658241272 batch: 761/840\n",
      "Batch loss: 0.8045998215675354 batch: 762/840\n",
      "Batch loss: 0.4339669942855835 batch: 763/840\n",
      "Batch loss: 0.6196963787078857 batch: 764/840\n",
      "Batch loss: 0.4815136790275574 batch: 765/840\n",
      "Batch loss: 0.6295186877250671 batch: 766/840\n",
      "Batch loss: 0.8008703589439392 batch: 767/840\n",
      "Batch loss: 0.6971662044525146 batch: 768/840\n",
      "Batch loss: 0.6200082302093506 batch: 769/840\n",
      "Batch loss: 0.571692705154419 batch: 770/840\n",
      "Batch loss: 0.6643333435058594 batch: 771/840\n",
      "Batch loss: 0.557532787322998 batch: 772/840\n",
      "Batch loss: 0.5643251538276672 batch: 773/840\n",
      "Batch loss: 0.4644261300563812 batch: 774/840\n",
      "Batch loss: 0.6150056719779968 batch: 775/840\n",
      "Batch loss: 0.5518075823783875 batch: 776/840\n",
      "Batch loss: 0.5701661109924316 batch: 777/840\n",
      "Batch loss: 0.38466253876686096 batch: 778/840\n",
      "Batch loss: 0.7224602699279785 batch: 779/840\n",
      "Batch loss: 0.5760841965675354 batch: 780/840\n",
      "Batch loss: 0.6087930202484131 batch: 781/840\n",
      "Batch loss: 0.5509400963783264 batch: 782/840\n",
      "Batch loss: 0.57013338804245 batch: 783/840\n",
      "Batch loss: 0.6918087601661682 batch: 784/840\n",
      "Batch loss: 0.6163525581359863 batch: 785/840\n",
      "Batch loss: 0.6268275380134583 batch: 786/840\n",
      "Batch loss: 0.5304977297782898 batch: 787/840\n",
      "Batch loss: 0.6480234265327454 batch: 788/840\n",
      "Batch loss: 0.6966550946235657 batch: 789/840\n",
      "Batch loss: 0.6382980942726135 batch: 790/840\n",
      "Batch loss: 0.7336750626564026 batch: 791/840\n",
      "Batch loss: 0.4182857275009155 batch: 792/840\n",
      "Batch loss: 0.6686455011367798 batch: 793/840\n",
      "Batch loss: 0.6712117791175842 batch: 794/840\n",
      "Batch loss: 0.6371572017669678 batch: 795/840\n",
      "Batch loss: 0.6963874101638794 batch: 796/840\n",
      "Batch loss: 0.7334774732589722 batch: 797/840\n",
      "Batch loss: 0.6193171143531799 batch: 798/840\n",
      "Batch loss: 0.6963441371917725 batch: 799/840\n",
      "Batch loss: 0.5029644966125488 batch: 800/840\n",
      "Batch loss: 0.6806167364120483 batch: 801/840\n",
      "Batch loss: 0.5220948457717896 batch: 802/840\n",
      "Batch loss: 0.45471617579460144 batch: 803/840\n",
      "Batch loss: 0.7740231156349182 batch: 804/840\n",
      "Batch loss: 0.5421090722084045 batch: 805/840\n",
      "Batch loss: 0.6465129852294922 batch: 806/840\n",
      "Batch loss: 0.6355805397033691 batch: 807/840\n",
      "Batch loss: 0.6995715498924255 batch: 808/840\n",
      "Batch loss: 0.6284462213516235 batch: 809/840\n",
      "Batch loss: 0.622922420501709 batch: 810/840\n",
      "Batch loss: 0.5786053538322449 batch: 811/840\n",
      "Batch loss: 0.6976893544197083 batch: 812/840\n",
      "Batch loss: 0.5618670582771301 batch: 813/840\n",
      "Batch loss: 0.520894467830658 batch: 814/840\n",
      "Batch loss: 0.7840369939804077 batch: 815/840\n",
      "Batch loss: 0.6549965739250183 batch: 816/840\n",
      "Batch loss: 0.7116983532905579 batch: 817/840\n",
      "Batch loss: 0.6845954060554504 batch: 818/840\n",
      "Batch loss: 0.5744466185569763 batch: 819/840\n",
      "Batch loss: 0.7415162920951843 batch: 820/840\n",
      "Batch loss: 0.7505612373352051 batch: 821/840\n",
      "Batch loss: 0.6298381686210632 batch: 822/840\n",
      "Batch loss: 0.793806254863739 batch: 823/840\n",
      "Batch loss: 0.7997392416000366 batch: 824/840\n",
      "Batch loss: 0.7963678240776062 batch: 825/840\n",
      "Batch loss: 0.5583323836326599 batch: 826/840\n",
      "Batch loss: 0.5694411993026733 batch: 827/840\n",
      "Batch loss: 0.6815072894096375 batch: 828/840\n",
      "Batch loss: 0.5711588859558105 batch: 829/840\n",
      "Batch loss: 0.6839062571525574 batch: 830/840\n",
      "Batch loss: 0.6595587730407715 batch: 831/840\n",
      "Batch loss: 0.8114175200462341 batch: 832/840\n",
      "Batch loss: 0.7219017148017883 batch: 833/840\n",
      "Batch loss: 0.6075285077095032 batch: 834/840\n",
      "Batch loss: 0.5059248208999634 batch: 835/840\n",
      "Batch loss: 0.5777762532234192 batch: 836/840\n",
      "Batch loss: 0.652372419834137 batch: 837/840\n",
      "Batch loss: 0.6731007099151611 batch: 838/840\n",
      "Batch loss: 0.6388776898384094 batch: 839/840\n",
      "Batch loss: 0.6443740725517273 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 7/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.812\n",
      "Running epoch 8/15\n",
      "Batch loss: 0.5128111243247986 batch: 1/840\n",
      "Batch loss: 0.9548673033714294 batch: 2/840\n",
      "Batch loss: 0.6011151671409607 batch: 3/840\n",
      "Batch loss: 0.6650046706199646 batch: 4/840\n",
      "Batch loss: 0.5870201587677002 batch: 5/840\n",
      "Batch loss: 0.5618301033973694 batch: 6/840\n",
      "Batch loss: 0.5422947406768799 batch: 7/840\n",
      "Batch loss: 0.5696503520011902 batch: 8/840\n",
      "Batch loss: 0.5286530256271362 batch: 9/840\n",
      "Batch loss: 0.651623547077179 batch: 10/840\n",
      "Batch loss: 0.6232947707176208 batch: 11/840\n",
      "Batch loss: 0.6350938081741333 batch: 12/840\n",
      "Batch loss: 0.5176449418067932 batch: 13/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6568712592124939 batch: 14/840\n",
      "Batch loss: 0.6296234130859375 batch: 15/840\n",
      "Batch loss: 0.4546438157558441 batch: 16/840\n",
      "Batch loss: 0.48572295904159546 batch: 17/840\n",
      "Batch loss: 0.7581895589828491 batch: 18/840\n",
      "Batch loss: 0.6506277322769165 batch: 19/840\n",
      "Batch loss: 0.8572757244110107 batch: 20/840\n",
      "Batch loss: 0.7043890953063965 batch: 21/840\n",
      "Batch loss: 0.5363996624946594 batch: 22/840\n",
      "Batch loss: 0.612775981426239 batch: 23/840\n",
      "Batch loss: 0.5060285329818726 batch: 24/840\n",
      "Batch loss: 0.5380575060844421 batch: 25/840\n",
      "Batch loss: 0.6935567259788513 batch: 26/840\n",
      "Batch loss: 0.7029228210449219 batch: 27/840\n",
      "Batch loss: 0.7013592720031738 batch: 28/840\n",
      "Batch loss: 0.6477252244949341 batch: 29/840\n",
      "Batch loss: 0.4914020299911499 batch: 30/840\n",
      "Batch loss: 0.5624921321868896 batch: 31/840\n",
      "Batch loss: 0.6138068437576294 batch: 32/840\n",
      "Batch loss: 0.6311284303665161 batch: 33/840\n",
      "Batch loss: 0.5980865955352783 batch: 34/840\n",
      "Batch loss: 0.5652652978897095 batch: 35/840\n",
      "Batch loss: 0.4944818913936615 batch: 36/840\n",
      "Batch loss: 0.7387317419052124 batch: 37/840\n",
      "Batch loss: 0.6478815674781799 batch: 38/840\n",
      "Batch loss: 0.7202050089836121 batch: 39/840\n",
      "Batch loss: 0.5193273425102234 batch: 40/840\n",
      "Batch loss: 0.7539154887199402 batch: 41/840\n",
      "Batch loss: 0.5978882312774658 batch: 42/840\n",
      "Batch loss: 0.5821096301078796 batch: 43/840\n",
      "Batch loss: 0.6894686222076416 batch: 44/840\n",
      "Batch loss: 0.619533360004425 batch: 45/840\n",
      "Batch loss: 0.5432897806167603 batch: 46/840\n",
      "Batch loss: 0.5557215809822083 batch: 47/840\n",
      "Batch loss: 0.6501308679580688 batch: 48/840\n",
      "Batch loss: 0.6592495441436768 batch: 49/840\n",
      "Batch loss: 0.6228220462799072 batch: 50/840\n",
      "Batch loss: 0.7533465027809143 batch: 51/840\n",
      "Batch loss: 0.7214202284812927 batch: 52/840\n",
      "Batch loss: 0.5328546762466431 batch: 53/840\n",
      "Batch loss: 0.6200778484344482 batch: 54/840\n",
      "Batch loss: 0.6841332912445068 batch: 55/840\n",
      "Batch loss: 0.6250196695327759 batch: 56/840\n",
      "Batch loss: 0.6879875063896179 batch: 57/840\n",
      "Batch loss: 0.594144880771637 batch: 58/840\n",
      "Batch loss: 0.5745400190353394 batch: 59/840\n",
      "Batch loss: 0.5070312023162842 batch: 60/840\n",
      "Batch loss: 0.7853067517280579 batch: 61/840\n",
      "Batch loss: 0.6228654384613037 batch: 62/840\n",
      "Batch loss: 0.5512235760688782 batch: 63/840\n",
      "Batch loss: 0.6837093830108643 batch: 64/840\n",
      "Batch loss: 0.5736846327781677 batch: 65/840\n",
      "Batch loss: 0.6721099615097046 batch: 66/840\n",
      "Batch loss: 0.7080584168434143 batch: 67/840\n",
      "Batch loss: 0.6124699115753174 batch: 68/840\n",
      "Batch loss: 0.7873956561088562 batch: 69/840\n",
      "Batch loss: 0.6061660051345825 batch: 70/840\n",
      "Batch loss: 0.7953594326972961 batch: 71/840\n",
      "Batch loss: 0.8291816115379333 batch: 72/840\n",
      "Batch loss: 0.6851860880851746 batch: 73/840\n",
      "Batch loss: 0.5450896620750427 batch: 74/840\n",
      "Batch loss: 0.7028921246528625 batch: 75/840\n",
      "Batch loss: 0.4601759612560272 batch: 76/840\n",
      "Batch loss: 0.6512637138366699 batch: 77/840\n",
      "Batch loss: 0.8332604169845581 batch: 78/840\n",
      "Batch loss: 0.5980370044708252 batch: 79/840\n",
      "Batch loss: 0.7022064328193665 batch: 80/840\n",
      "Batch loss: 0.5835548639297485 batch: 81/840\n",
      "Batch loss: 0.6848793029785156 batch: 82/840\n",
      "Batch loss: 0.5279572010040283 batch: 83/840\n",
      "Batch loss: 0.6559535264968872 batch: 84/840\n",
      "Batch loss: 0.6630269885063171 batch: 85/840\n",
      "Batch loss: 0.9872961640357971 batch: 86/840\n",
      "Batch loss: 0.5566626191139221 batch: 87/840\n",
      "Batch loss: 0.5108455419540405 batch: 88/840\n",
      "Batch loss: 0.4829652011394501 batch: 89/840\n",
      "Batch loss: 0.514866292476654 batch: 90/840\n",
      "Batch loss: 0.6013532876968384 batch: 91/840\n",
      "Batch loss: 0.6488902568817139 batch: 92/840\n",
      "Batch loss: 0.6283096075057983 batch: 93/840\n",
      "Batch loss: 0.5962232351303101 batch: 94/840\n",
      "Batch loss: 0.7217710614204407 batch: 95/840\n",
      "Batch loss: 0.5430904626846313 batch: 96/840\n",
      "Batch loss: 0.6716234087944031 batch: 97/840\n",
      "Batch loss: 0.758811891078949 batch: 98/840\n",
      "Batch loss: 0.5400448441505432 batch: 99/840\n",
      "Batch loss: 0.6644431948661804 batch: 100/840\n",
      "Batch loss: 0.5885051488876343 batch: 101/840\n",
      "Batch loss: 0.589226245880127 batch: 102/840\n",
      "Batch loss: 0.6171228885650635 batch: 103/840\n",
      "Batch loss: 0.47750771045684814 batch: 104/840\n",
      "Batch loss: 0.580852210521698 batch: 105/840\n",
      "Batch loss: 0.6462547779083252 batch: 106/840\n",
      "Batch loss: 0.6789847612380981 batch: 107/840\n",
      "Batch loss: 0.7670633792877197 batch: 108/840\n",
      "Batch loss: 0.6271665692329407 batch: 109/840\n",
      "Batch loss: 0.5104203224182129 batch: 110/840\n",
      "Batch loss: 0.5267784595489502 batch: 111/840\n",
      "Batch loss: 0.7449449896812439 batch: 112/840\n",
      "Batch loss: 0.7858293652534485 batch: 113/840\n",
      "Batch loss: 0.5781019330024719 batch: 114/840\n",
      "Batch loss: 0.5813971161842346 batch: 115/840\n",
      "Batch loss: 0.6116781234741211 batch: 116/840\n",
      "Batch loss: 0.6271927952766418 batch: 117/840\n",
      "Batch loss: 0.4613778293132782 batch: 118/840\n",
      "Batch loss: 0.6783550381660461 batch: 119/840\n",
      "Batch loss: 0.5000003576278687 batch: 120/840\n",
      "Batch loss: 0.7083615660667419 batch: 121/840\n",
      "Batch loss: 0.8633304834365845 batch: 122/840\n",
      "Batch loss: 0.5172955393791199 batch: 123/840\n",
      "Batch loss: 0.5498222708702087 batch: 124/840\n",
      "Batch loss: 0.5698021650314331 batch: 125/840\n",
      "Batch loss: 0.6761744618415833 batch: 126/840\n",
      "Batch loss: 0.6750956177711487 batch: 127/840\n",
      "Batch loss: 0.6940518021583557 batch: 128/840\n",
      "Batch loss: 0.6891047954559326 batch: 129/840\n",
      "Batch loss: 0.5219240784645081 batch: 130/840\n",
      "Batch loss: 0.6825932860374451 batch: 131/840\n",
      "Batch loss: 1.0042521953582764 batch: 132/840\n",
      "Batch loss: 0.7299321889877319 batch: 133/840\n",
      "Batch loss: 0.6260238885879517 batch: 134/840\n",
      "Batch loss: 0.5162122845649719 batch: 135/840\n",
      "Batch loss: 0.774965226650238 batch: 136/840\n",
      "Batch loss: 0.6088712811470032 batch: 137/840\n",
      "Batch loss: 0.5250747799873352 batch: 138/840\n",
      "Batch loss: 0.5528392195701599 batch: 139/840\n",
      "Batch loss: 0.5810466408729553 batch: 140/840\n",
      "Batch loss: 0.4642254114151001 batch: 141/840\n",
      "Batch loss: 0.6289286613464355 batch: 142/840\n",
      "Batch loss: 0.5096028447151184 batch: 143/840\n",
      "Batch loss: 0.5455350279808044 batch: 144/840\n",
      "Batch loss: 0.7744268178939819 batch: 145/840\n",
      "Batch loss: 0.5905451774597168 batch: 146/840\n",
      "Batch loss: 0.5638038516044617 batch: 147/840\n",
      "Batch loss: 0.7884527444839478 batch: 148/840\n",
      "Batch loss: 0.620334267616272 batch: 149/840\n",
      "Batch loss: 0.6666066646575928 batch: 150/840\n",
      "Batch loss: 0.6164736151695251 batch: 151/840\n",
      "Batch loss: 0.6853533387184143 batch: 152/840\n",
      "Batch loss: 0.6115180253982544 batch: 153/840\n",
      "Batch loss: 0.687507688999176 batch: 154/840\n",
      "Batch loss: 0.613734781742096 batch: 155/840\n",
      "Batch loss: 0.6308721899986267 batch: 156/840\n",
      "Batch loss: 0.7458432912826538 batch: 157/840\n",
      "Batch loss: 0.5393956899642944 batch: 158/840\n",
      "Batch loss: 0.5556967258453369 batch: 159/840\n",
      "Batch loss: 0.5520035028457642 batch: 160/840\n",
      "Batch loss: 0.7148211598396301 batch: 161/840\n",
      "Batch loss: 0.7513863444328308 batch: 162/840\n",
      "Batch loss: 0.8407770395278931 batch: 163/840\n",
      "Batch loss: 0.41386932134628296 batch: 164/840\n",
      "Batch loss: 0.6991456747055054 batch: 165/840\n",
      "Batch loss: 0.58739173412323 batch: 166/840\n",
      "Batch loss: 0.6870185732841492 batch: 167/840\n",
      "Batch loss: 0.6418299078941345 batch: 168/840\n",
      "Batch loss: 0.592741072177887 batch: 169/840\n",
      "Batch loss: 0.7982550859451294 batch: 170/840\n",
      "Batch loss: 0.6753292679786682 batch: 171/840\n",
      "Batch loss: 0.7114159464836121 batch: 172/840\n",
      "Batch loss: 0.6697240471839905 batch: 173/840\n",
      "Batch loss: 0.5247454047203064 batch: 174/840\n",
      "Batch loss: 0.5993502736091614 batch: 175/840\n",
      "Batch loss: 0.742666482925415 batch: 176/840\n",
      "Batch loss: 0.6964554786682129 batch: 177/840\n",
      "Batch loss: 0.793745756149292 batch: 178/840\n",
      "Batch loss: 0.6836678385734558 batch: 179/840\n",
      "Batch loss: 0.5766265392303467 batch: 180/840\n",
      "Batch loss: 0.4970545470714569 batch: 181/840\n",
      "Batch loss: 0.5974594950675964 batch: 182/840\n",
      "Batch loss: 0.6308817863464355 batch: 183/840\n",
      "Batch loss: 0.5471920967102051 batch: 184/840\n",
      "Batch loss: 0.482420951128006 batch: 185/840\n",
      "Batch loss: 0.44370418787002563 batch: 186/840\n",
      "Batch loss: 0.6758627891540527 batch: 187/840\n",
      "Batch loss: 0.5240183472633362 batch: 188/840\n",
      "Batch loss: 0.609661340713501 batch: 189/840\n",
      "Batch loss: 0.5423316359519958 batch: 190/840\n",
      "Batch loss: 0.7638024091720581 batch: 191/840\n",
      "Batch loss: 0.6079984903335571 batch: 192/840\n",
      "Batch loss: 0.5728359818458557 batch: 193/840\n",
      "Batch loss: 0.5366203784942627 batch: 194/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6100456118583679 batch: 195/840\n",
      "Batch loss: 0.756122887134552 batch: 196/840\n",
      "Batch loss: 0.6547669768333435 batch: 197/840\n",
      "Batch loss: 0.4722850024700165 batch: 198/840\n",
      "Batch loss: 0.655456006526947 batch: 199/840\n",
      "Batch loss: 0.8042426109313965 batch: 200/840\n",
      "Batch loss: 0.5270053744316101 batch: 201/840\n",
      "Batch loss: 0.6068867444992065 batch: 202/840\n",
      "Batch loss: 0.6071258783340454 batch: 203/840\n",
      "Batch loss: 0.7038097977638245 batch: 204/840\n",
      "Batch loss: 0.6880822777748108 batch: 205/840\n",
      "Batch loss: 0.6491600871086121 batch: 206/840\n",
      "Batch loss: 0.5509613156318665 batch: 207/840\n",
      "Batch loss: 0.6924440860748291 batch: 208/840\n",
      "Batch loss: 0.6627405285835266 batch: 209/840\n",
      "Batch loss: 0.5289117693901062 batch: 210/840\n",
      "Batch loss: 0.47052496671676636 batch: 211/840\n",
      "Batch loss: 0.5426260828971863 batch: 212/840\n",
      "Batch loss: 0.6512241959571838 batch: 213/840\n",
      "Batch loss: 0.758857011795044 batch: 214/840\n",
      "Batch loss: 0.6841474175453186 batch: 215/840\n",
      "Batch loss: 0.5296105742454529 batch: 216/840\n",
      "Batch loss: 0.5560452938079834 batch: 217/840\n",
      "Batch loss: 0.7355831265449524 batch: 218/840\n",
      "Batch loss: 0.6217175722122192 batch: 219/840\n",
      "Batch loss: 0.9117429852485657 batch: 220/840\n",
      "Batch loss: 0.5337100028991699 batch: 221/840\n",
      "Batch loss: 0.6882537603378296 batch: 222/840\n",
      "Batch loss: 0.6339002847671509 batch: 223/840\n",
      "Batch loss: 0.8520715832710266 batch: 224/840\n",
      "Batch loss: 0.7145794630050659 batch: 225/840\n",
      "Batch loss: 0.7467772960662842 batch: 226/840\n",
      "Batch loss: 0.7811605334281921 batch: 227/840\n",
      "Batch loss: 0.478240430355072 batch: 228/840\n",
      "Batch loss: 0.49657881259918213 batch: 229/840\n",
      "Batch loss: 0.5630367398262024 batch: 230/840\n",
      "Batch loss: 0.5254836082458496 batch: 231/840\n",
      "Batch loss: 0.5807086825370789 batch: 232/840\n",
      "Batch loss: 0.7056688666343689 batch: 233/840\n",
      "Batch loss: 0.636397123336792 batch: 234/840\n",
      "Batch loss: 0.6188623905181885 batch: 235/840\n",
      "Batch loss: 0.6856730580329895 batch: 236/840\n",
      "Batch loss: 0.5533219575881958 batch: 237/840\n",
      "Batch loss: 0.6670284867286682 batch: 238/840\n",
      "Batch loss: 0.6602880358695984 batch: 239/840\n",
      "Batch loss: 0.551135241985321 batch: 240/840\n",
      "Batch loss: 0.7450410723686218 batch: 241/840\n",
      "Batch loss: 0.6874421834945679 batch: 242/840\n",
      "Batch loss: 0.6053969264030457 batch: 243/840\n",
      "Batch loss: 0.7505248188972473 batch: 244/840\n",
      "Batch loss: 0.43699198961257935 batch: 245/840\n",
      "Batch loss: 0.6138367652893066 batch: 246/840\n",
      "Batch loss: 0.7114616632461548 batch: 247/840\n",
      "Batch loss: 0.7395645380020142 batch: 248/840\n",
      "Batch loss: 0.8904611468315125 batch: 249/840\n",
      "Batch loss: 0.5109243392944336 batch: 250/840\n",
      "Batch loss: 0.5889716744422913 batch: 251/840\n",
      "Batch loss: 0.5236707925796509 batch: 252/840\n",
      "Batch loss: 0.6290516257286072 batch: 253/840\n",
      "Batch loss: 0.7749561071395874 batch: 254/840\n",
      "Batch loss: 0.6234575510025024 batch: 255/840\n",
      "Batch loss: 0.639094352722168 batch: 256/840\n",
      "Batch loss: 0.5978379249572754 batch: 257/840\n",
      "Batch loss: 0.7948617339134216 batch: 258/840\n",
      "Batch loss: 0.5169516801834106 batch: 259/840\n",
      "Batch loss: 0.4444468319416046 batch: 260/840\n",
      "Batch loss: 0.6360113620758057 batch: 261/840\n",
      "Batch loss: 0.47538816928863525 batch: 262/840\n",
      "Batch loss: 0.6124269366264343 batch: 263/840\n",
      "Batch loss: 0.6310022473335266 batch: 264/840\n",
      "Batch loss: 0.75091552734375 batch: 265/840\n",
      "Batch loss: 0.608864426612854 batch: 266/840\n",
      "Batch loss: 0.7118677496910095 batch: 267/840\n",
      "Batch loss: 0.49979594349861145 batch: 268/840\n",
      "Batch loss: 0.5513308644294739 batch: 269/840\n",
      "Batch loss: 0.6063387393951416 batch: 270/840\n",
      "Batch loss: 0.6669006943702698 batch: 271/840\n",
      "Batch loss: 0.9047394394874573 batch: 272/840\n",
      "Batch loss: 0.6289561986923218 batch: 273/840\n",
      "Batch loss: 0.5795419812202454 batch: 274/840\n",
      "Batch loss: 0.7358126044273376 batch: 275/840\n",
      "Batch loss: 0.5803314447402954 batch: 276/840\n",
      "Batch loss: 0.6084353923797607 batch: 277/840\n",
      "Batch loss: 0.728380024433136 batch: 278/840\n",
      "Batch loss: 0.6887983679771423 batch: 279/840\n",
      "Batch loss: 0.7005833387374878 batch: 280/840\n",
      "Batch loss: 0.5484254360198975 batch: 281/840\n",
      "Batch loss: 0.6141672134399414 batch: 282/840\n",
      "Batch loss: 0.6281553506851196 batch: 283/840\n",
      "Batch loss: 0.504077136516571 batch: 284/840\n",
      "Batch loss: 0.5332781076431274 batch: 285/840\n",
      "Batch loss: 0.6748799681663513 batch: 286/840\n",
      "Batch loss: 0.47527220845222473 batch: 287/840\n",
      "Batch loss: 0.5603943467140198 batch: 288/840\n",
      "Batch loss: 0.7911734580993652 batch: 289/840\n",
      "Batch loss: 0.7677627801895142 batch: 290/840\n",
      "Batch loss: 0.813340961933136 batch: 291/840\n",
      "Batch loss: 0.5136921405792236 batch: 292/840\n",
      "Batch loss: 0.6962223649024963 batch: 293/840\n",
      "Batch loss: 0.6030760407447815 batch: 294/840\n",
      "Batch loss: 0.535897433757782 batch: 295/840\n",
      "Batch loss: 0.6593750715255737 batch: 296/840\n",
      "Batch loss: 0.631087064743042 batch: 297/840\n",
      "Batch loss: 0.643197774887085 batch: 298/840\n",
      "Batch loss: 0.5123768448829651 batch: 299/840\n",
      "Batch loss: 0.8565994501113892 batch: 300/840\n",
      "Batch loss: 0.6560291051864624 batch: 301/840\n",
      "Batch loss: 0.5821976065635681 batch: 302/840\n",
      "Batch loss: 0.7374840378761292 batch: 303/840\n",
      "Batch loss: 0.5474157929420471 batch: 304/840\n",
      "Batch loss: 0.5860393643379211 batch: 305/840\n",
      "Batch loss: 0.6008128523826599 batch: 306/840\n",
      "Batch loss: 0.5235388278961182 batch: 307/840\n",
      "Batch loss: 0.8066163063049316 batch: 308/840\n",
      "Batch loss: 0.6270027756690979 batch: 309/840\n",
      "Batch loss: 0.8264867663383484 batch: 310/840\n",
      "Batch loss: 0.7351660132408142 batch: 311/840\n",
      "Batch loss: 0.7218633890151978 batch: 312/840\n",
      "Batch loss: 0.7683191895484924 batch: 313/840\n",
      "Batch loss: 0.4809969663619995 batch: 314/840\n",
      "Batch loss: 0.6515759229660034 batch: 315/840\n",
      "Batch loss: 0.5515299439430237 batch: 316/840\n",
      "Batch loss: 0.6664415001869202 batch: 317/840\n",
      "Batch loss: 0.634474515914917 batch: 318/840\n",
      "Batch loss: 0.58404940366745 batch: 319/840\n",
      "Batch loss: 0.647600531578064 batch: 320/840\n",
      "Batch loss: 0.5668606758117676 batch: 321/840\n",
      "Batch loss: 0.6858119368553162 batch: 322/840\n",
      "Batch loss: 0.6640471816062927 batch: 323/840\n",
      "Batch loss: 0.7169957160949707 batch: 324/840\n",
      "Batch loss: 0.5065438747406006 batch: 325/840\n",
      "Batch loss: 0.5879951119422913 batch: 326/840\n",
      "Batch loss: 0.41949859261512756 batch: 327/840\n",
      "Batch loss: 0.7942771911621094 batch: 328/840\n",
      "Batch loss: 0.7122738361358643 batch: 329/840\n",
      "Batch loss: 0.6949917674064636 batch: 330/840\n",
      "Batch loss: 0.6375444531440735 batch: 331/840\n",
      "Batch loss: 0.6621589064598083 batch: 332/840\n",
      "Batch loss: 0.5877184867858887 batch: 333/840\n",
      "Batch loss: 0.6188361644744873 batch: 334/840\n",
      "Batch loss: 0.6312813758850098 batch: 335/840\n",
      "Batch loss: 0.6852208971977234 batch: 336/840\n",
      "Batch loss: 0.8250558376312256 batch: 337/840\n",
      "Batch loss: 0.7640107870101929 batch: 338/840\n",
      "Batch loss: 0.5312594175338745 batch: 339/840\n",
      "Batch loss: 0.7877328395843506 batch: 340/840\n",
      "Batch loss: 0.5613624453544617 batch: 341/840\n",
      "Batch loss: 0.5918743014335632 batch: 342/840\n",
      "Batch loss: 0.805877685546875 batch: 343/840\n",
      "Batch loss: 0.6479107737541199 batch: 344/840\n",
      "Batch loss: 0.4609880745410919 batch: 345/840\n",
      "Batch loss: 0.6274882555007935 batch: 346/840\n",
      "Batch loss: 0.6103746294975281 batch: 347/840\n",
      "Batch loss: 0.630774736404419 batch: 348/840\n",
      "Batch loss: 0.6847634315490723 batch: 349/840\n",
      "Batch loss: 0.45685139298439026 batch: 350/840\n",
      "Batch loss: 0.7477538585662842 batch: 351/840\n",
      "Batch loss: 0.6501067280769348 batch: 352/840\n",
      "Batch loss: 0.6386587023735046 batch: 353/840\n",
      "Batch loss: 0.6710692048072815 batch: 354/840\n",
      "Batch loss: 0.5383981466293335 batch: 355/840\n",
      "Batch loss: 0.6149774789810181 batch: 356/840\n",
      "Batch loss: 0.5128980875015259 batch: 357/840\n",
      "Batch loss: 0.9200528860092163 batch: 358/840\n",
      "Batch loss: 0.6109926700592041 batch: 359/840\n",
      "Batch loss: 0.8950120806694031 batch: 360/840\n",
      "Batch loss: 0.6149871945381165 batch: 361/840\n",
      "Batch loss: 0.6411935687065125 batch: 362/840\n",
      "Batch loss: 0.6431906223297119 batch: 363/840\n",
      "Batch loss: 0.7086677551269531 batch: 364/840\n",
      "Batch loss: 0.5767311453819275 batch: 365/840\n",
      "Batch loss: 0.5924399495124817 batch: 366/840\n",
      "Batch loss: 0.5009058117866516 batch: 367/840\n",
      "Batch loss: 0.8020642995834351 batch: 368/840\n",
      "Batch loss: 0.5946717262268066 batch: 369/840\n",
      "Batch loss: 0.6878343820571899 batch: 370/840\n",
      "Batch loss: 0.6062782406806946 batch: 371/840\n",
      "Batch loss: 0.5169615745544434 batch: 372/840\n",
      "Batch loss: 0.6597989797592163 batch: 373/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6950103044509888 batch: 374/840\n",
      "Batch loss: 0.58251953125 batch: 375/840\n",
      "Batch loss: 0.5506236553192139 batch: 376/840\n",
      "Batch loss: 0.5947486758232117 batch: 377/840\n",
      "Batch loss: 0.5970924496650696 batch: 378/840\n",
      "Batch loss: 0.5059450268745422 batch: 379/840\n",
      "Batch loss: 0.7921249270439148 batch: 380/840\n",
      "Batch loss: 0.7739621996879578 batch: 381/840\n",
      "Batch loss: 0.7752132415771484 batch: 382/840\n",
      "Batch loss: 0.6751151084899902 batch: 383/840\n",
      "Batch loss: 0.6006791591644287 batch: 384/840\n",
      "Batch loss: 0.6985270977020264 batch: 385/840\n",
      "Batch loss: 0.707266628742218 batch: 386/840\n",
      "Batch loss: 0.6318323016166687 batch: 387/840\n",
      "Batch loss: 0.702701210975647 batch: 388/840\n",
      "Batch loss: 0.5843770503997803 batch: 389/840\n",
      "Batch loss: 0.7741184234619141 batch: 390/840\n",
      "Batch loss: 0.7277212738990784 batch: 391/840\n",
      "Batch loss: 0.5786960124969482 batch: 392/840\n",
      "Batch loss: 0.4499836266040802 batch: 393/840\n",
      "Batch loss: 0.7524163722991943 batch: 394/840\n",
      "Batch loss: 0.5092244148254395 batch: 395/840\n",
      "Batch loss: 0.6885395050048828 batch: 396/840\n",
      "Batch loss: 0.5826348662376404 batch: 397/840\n",
      "Batch loss: 0.6463583111763 batch: 398/840\n",
      "Batch loss: 0.45144152641296387 batch: 399/840\n",
      "Batch loss: 0.6001462936401367 batch: 400/840\n",
      "Batch loss: 0.7334311008453369 batch: 401/840\n",
      "Batch loss: 0.5403964519500732 batch: 402/840\n",
      "Batch loss: 0.6097347736358643 batch: 403/840\n",
      "Batch loss: 0.6495705246925354 batch: 404/840\n",
      "Batch loss: 0.6139786243438721 batch: 405/840\n",
      "Batch loss: 0.6608815789222717 batch: 406/840\n",
      "Batch loss: 0.5540510416030884 batch: 407/840\n",
      "Batch loss: 0.7441685199737549 batch: 408/840\n",
      "Batch loss: 0.6713257431983948 batch: 409/840\n",
      "Batch loss: 0.8265271782875061 batch: 410/840\n",
      "Batch loss: 0.6187098026275635 batch: 411/840\n",
      "Batch loss: 0.6751717329025269 batch: 412/840\n",
      "Batch loss: 0.6298096179962158 batch: 413/840\n",
      "Batch loss: 0.5987210869789124 batch: 414/840\n",
      "Batch loss: 0.9023919105529785 batch: 415/840\n",
      "Batch loss: 0.4413343071937561 batch: 416/840\n",
      "Batch loss: 0.5743884444236755 batch: 417/840\n",
      "Batch loss: 0.7418853044509888 batch: 418/840\n",
      "Batch loss: 0.7351320385932922 batch: 419/840\n",
      "Batch loss: 0.7620505690574646 batch: 420/840\n",
      "Batch loss: 0.5278995037078857 batch: 421/840\n",
      "Batch loss: 0.5861415266990662 batch: 422/840\n",
      "Batch loss: 0.6290108561515808 batch: 423/840\n",
      "Batch loss: 0.6483418345451355 batch: 424/840\n",
      "Batch loss: 0.7163377404212952 batch: 425/840\n",
      "Batch loss: 0.5260224342346191 batch: 426/840\n",
      "Batch loss: 0.6351656913757324 batch: 427/840\n",
      "Batch loss: 0.734892725944519 batch: 428/840\n",
      "Batch loss: 0.5384059548377991 batch: 429/840\n",
      "Batch loss: 0.8295059204101562 batch: 430/840\n",
      "Batch loss: 0.6652156710624695 batch: 431/840\n",
      "Batch loss: 0.5547491908073425 batch: 432/840\n",
      "Batch loss: 0.3946911692619324 batch: 433/840\n",
      "Batch loss: 0.5595471858978271 batch: 434/840\n",
      "Batch loss: 0.8154705762863159 batch: 435/840\n",
      "Batch loss: 0.551982045173645 batch: 436/840\n",
      "Batch loss: 0.8011026978492737 batch: 437/840\n",
      "Batch loss: 0.4248652160167694 batch: 438/840\n",
      "Batch loss: 0.6358175873756409 batch: 439/840\n",
      "Batch loss: 0.6759458780288696 batch: 440/840\n",
      "Batch loss: 0.7571753859519958 batch: 441/840\n",
      "Batch loss: 0.654792845249176 batch: 442/840\n",
      "Batch loss: 0.6235220432281494 batch: 443/840\n",
      "Batch loss: 0.6343485116958618 batch: 444/840\n",
      "Batch loss: 0.5721696615219116 batch: 445/840\n",
      "Batch loss: 0.7296584248542786 batch: 446/840\n",
      "Batch loss: 0.7586239576339722 batch: 447/840\n",
      "Batch loss: 0.7352136373519897 batch: 448/840\n",
      "Batch loss: 0.5243989825248718 batch: 449/840\n",
      "Batch loss: 0.6125354170799255 batch: 450/840\n",
      "Batch loss: 0.7054579854011536 batch: 451/840\n",
      "Batch loss: 0.7419140338897705 batch: 452/840\n",
      "Batch loss: 0.6257967948913574 batch: 453/840\n",
      "Batch loss: 0.512904703617096 batch: 454/840\n",
      "Batch loss: 0.6940696239471436 batch: 455/840\n",
      "Batch loss: 0.6174683570861816 batch: 456/840\n",
      "Batch loss: 0.6131125092506409 batch: 457/840\n",
      "Batch loss: 0.6158431172370911 batch: 458/840\n",
      "Batch loss: 0.584854245185852 batch: 459/840\n",
      "Batch loss: 0.5207778811454773 batch: 460/840\n",
      "Batch loss: 0.6977701783180237 batch: 461/840\n",
      "Batch loss: 0.6300509572029114 batch: 462/840\n",
      "Batch loss: 0.8351242542266846 batch: 463/840\n",
      "Batch loss: 0.49059849977493286 batch: 464/840\n",
      "Batch loss: 0.9362572431564331 batch: 465/840\n",
      "Batch loss: 0.6676695346832275 batch: 466/840\n",
      "Batch loss: 0.9055573344230652 batch: 467/840\n",
      "Batch loss: 0.542641818523407 batch: 468/840\n",
      "Batch loss: 0.549261212348938 batch: 469/840\n",
      "Batch loss: 0.6532405614852905 batch: 470/840\n",
      "Batch loss: 0.6800944805145264 batch: 471/840\n",
      "Batch loss: 0.7075679302215576 batch: 472/840\n",
      "Batch loss: 0.6782256960868835 batch: 473/840\n",
      "Batch loss: 0.6117275953292847 batch: 474/840\n",
      "Batch loss: 0.6472597718238831 batch: 475/840\n",
      "Batch loss: 0.5821405649185181 batch: 476/840\n",
      "Batch loss: 0.48928746581077576 batch: 477/840\n",
      "Batch loss: 0.4486038088798523 batch: 478/840\n",
      "Batch loss: 0.470508873462677 batch: 479/840\n",
      "Batch loss: 0.6727488040924072 batch: 480/840\n",
      "Batch loss: 0.6903244256973267 batch: 481/840\n",
      "Batch loss: 0.6837485432624817 batch: 482/840\n",
      "Batch loss: 0.8176692724227905 batch: 483/840\n",
      "Batch loss: 0.6721621751785278 batch: 484/840\n",
      "Batch loss: 0.5631776452064514 batch: 485/840\n",
      "Batch loss: 0.6762894988059998 batch: 486/840\n",
      "Batch loss: 0.4905852973461151 batch: 487/840\n",
      "Batch loss: 0.6507008075714111 batch: 488/840\n",
      "Batch loss: 0.6022608876228333 batch: 489/840\n",
      "Batch loss: 0.7543479800224304 batch: 490/840\n",
      "Batch loss: 0.40588366985321045 batch: 491/840\n",
      "Batch loss: 0.6862834095954895 batch: 492/840\n",
      "Batch loss: 0.6041942834854126 batch: 493/840\n",
      "Batch loss: 0.40902113914489746 batch: 494/840\n",
      "Batch loss: 0.7598133683204651 batch: 495/840\n",
      "Batch loss: 0.6255684494972229 batch: 496/840\n",
      "Batch loss: 0.7084730267524719 batch: 497/840\n",
      "Batch loss: 0.4604382812976837 batch: 498/840\n",
      "Batch loss: 0.7205734252929688 batch: 499/840\n",
      "Batch loss: 0.5967740416526794 batch: 500/840\n",
      "Batch loss: 0.5505401492118835 batch: 501/840\n",
      "Batch loss: 0.5784646272659302 batch: 502/840\n",
      "Batch loss: 0.5270225405693054 batch: 503/840\n",
      "Batch loss: 0.7294197082519531 batch: 504/840\n",
      "Batch loss: 0.7538618445396423 batch: 505/840\n",
      "Batch loss: 0.5219528079032898 batch: 506/840\n",
      "Batch loss: 0.6802753210067749 batch: 507/840\n",
      "Batch loss: 0.6614090204238892 batch: 508/840\n",
      "Batch loss: 0.7407369017601013 batch: 509/840\n",
      "Batch loss: 0.6581206321716309 batch: 510/840\n",
      "Batch loss: 0.4833451807498932 batch: 511/840\n",
      "Batch loss: 0.5640300512313843 batch: 512/840\n",
      "Batch loss: 0.5592382550239563 batch: 513/840\n",
      "Batch loss: 0.7883748412132263 batch: 514/840\n",
      "Batch loss: 0.43570277094841003 batch: 515/840\n",
      "Batch loss: 0.6773020029067993 batch: 516/840\n",
      "Batch loss: 0.6795194149017334 batch: 517/840\n",
      "Batch loss: 0.6606696248054504 batch: 518/840\n",
      "Batch loss: 0.7349486351013184 batch: 519/840\n",
      "Batch loss: 0.6490892171859741 batch: 520/840\n",
      "Batch loss: 0.6945838928222656 batch: 521/840\n",
      "Batch loss: 0.6090558171272278 batch: 522/840\n",
      "Batch loss: 0.4657961130142212 batch: 523/840\n",
      "Batch loss: 0.6956377625465393 batch: 524/840\n",
      "Batch loss: 0.6152979731559753 batch: 525/840\n",
      "Batch loss: 0.628535270690918 batch: 526/840\n",
      "Batch loss: 0.5694553852081299 batch: 527/840\n",
      "Batch loss: 0.5689449906349182 batch: 528/840\n",
      "Batch loss: 0.496592253446579 batch: 529/840\n",
      "Batch loss: 0.5939871668815613 batch: 530/840\n",
      "Batch loss: 0.5058181881904602 batch: 531/840\n",
      "Batch loss: 0.3957855701446533 batch: 532/840\n",
      "Batch loss: 0.7466510534286499 batch: 533/840\n",
      "Batch loss: 0.6477225422859192 batch: 534/840\n",
      "Batch loss: 0.5638877749443054 batch: 535/840\n",
      "Batch loss: 0.8032858371734619 batch: 536/840\n",
      "Batch loss: 0.6614334583282471 batch: 537/840\n",
      "Batch loss: 0.531877875328064 batch: 538/840\n",
      "Batch loss: 0.5927383899688721 batch: 539/840\n",
      "Batch loss: 0.8021265268325806 batch: 540/840\n",
      "Batch loss: 0.6580359935760498 batch: 541/840\n",
      "Batch loss: 0.6078060865402222 batch: 542/840\n",
      "Batch loss: 0.4566941559314728 batch: 543/840\n",
      "Batch loss: 0.6346369385719299 batch: 544/840\n",
      "Batch loss: 0.5862240195274353 batch: 545/840\n",
      "Batch loss: 0.6425704956054688 batch: 546/840\n",
      "Batch loss: 0.6344125270843506 batch: 547/840\n",
      "Batch loss: 0.5028977990150452 batch: 548/840\n",
      "Batch loss: 0.5747265219688416 batch: 549/840\n",
      "Batch loss: 0.4882573187351227 batch: 550/840\n",
      "Batch loss: 0.6076930165290833 batch: 551/840\n",
      "Batch loss: 0.5421692132949829 batch: 552/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6754202842712402 batch: 553/840\n",
      "Batch loss: 0.541429340839386 batch: 554/840\n",
      "Batch loss: 0.6940234899520874 batch: 555/840\n",
      "Batch loss: 0.6613345146179199 batch: 556/840\n",
      "Batch loss: 0.6002936363220215 batch: 557/840\n",
      "Batch loss: 0.6536957621574402 batch: 558/840\n",
      "Batch loss: 0.5243127942085266 batch: 559/840\n",
      "Batch loss: 0.7536185383796692 batch: 560/840\n",
      "Batch loss: 0.6129105687141418 batch: 561/840\n",
      "Batch loss: 0.5706142783164978 batch: 562/840\n",
      "Batch loss: 0.48910441994667053 batch: 563/840\n",
      "Batch loss: 0.6633398532867432 batch: 564/840\n",
      "Batch loss: 0.6747941374778748 batch: 565/840\n",
      "Batch loss: 0.8206098675727844 batch: 566/840\n",
      "Batch loss: 0.8087664842605591 batch: 567/840\n",
      "Batch loss: 0.6687734127044678 batch: 568/840\n",
      "Batch loss: 0.653992235660553 batch: 569/840\n",
      "Batch loss: 0.40633055567741394 batch: 570/840\n",
      "Batch loss: 0.7254889607429504 batch: 571/840\n",
      "Batch loss: 0.7042340636253357 batch: 572/840\n",
      "Batch loss: 0.6295430660247803 batch: 573/840\n",
      "Batch loss: 0.7542447447776794 batch: 574/840\n",
      "Batch loss: 0.44722476601600647 batch: 575/840\n",
      "Batch loss: 0.6478039622306824 batch: 576/840\n",
      "Batch loss: 0.5677650570869446 batch: 577/840\n",
      "Batch loss: 0.8119224309921265 batch: 578/840\n",
      "Batch loss: 0.5901545286178589 batch: 579/840\n",
      "Batch loss: 0.8547584414482117 batch: 580/840\n",
      "Batch loss: 0.6949098706245422 batch: 581/840\n",
      "Batch loss: 0.8310518860816956 batch: 582/840\n",
      "Batch loss: 0.5756945013999939 batch: 583/840\n",
      "Batch loss: 0.7443556785583496 batch: 584/840\n",
      "Batch loss: 0.6627311110496521 batch: 585/840\n",
      "Batch loss: 0.6370465159416199 batch: 586/840\n",
      "Batch loss: 0.6379265785217285 batch: 587/840\n",
      "Batch loss: 0.5545445680618286 batch: 588/840\n",
      "Batch loss: 0.7732437252998352 batch: 589/840\n",
      "Batch loss: 0.5394793152809143 batch: 590/840\n",
      "Batch loss: 0.4362233877182007 batch: 591/840\n",
      "Batch loss: 0.48998382687568665 batch: 592/840\n",
      "Batch loss: 0.6788117289543152 batch: 593/840\n",
      "Batch loss: 0.6910362243652344 batch: 594/840\n",
      "Batch loss: 0.3517408072948456 batch: 595/840\n",
      "Batch loss: 0.5282801985740662 batch: 596/840\n",
      "Batch loss: 0.6674201488494873 batch: 597/840\n",
      "Batch loss: 0.468922883272171 batch: 598/840\n",
      "Batch loss: 0.5190978050231934 batch: 599/840\n",
      "Batch loss: 0.7331921458244324 batch: 600/840\n",
      "Batch loss: 0.6708659529685974 batch: 601/840\n",
      "Batch loss: 0.5696110725402832 batch: 602/840\n",
      "Batch loss: 0.5931873917579651 batch: 603/840\n",
      "Batch loss: 0.5839389562606812 batch: 604/840\n",
      "Batch loss: 0.8620595335960388 batch: 605/840\n",
      "Batch loss: 0.7768409848213196 batch: 606/840\n",
      "Batch loss: 0.7212597131729126 batch: 607/840\n",
      "Batch loss: 0.6328830122947693 batch: 608/840\n",
      "Batch loss: 0.5365079641342163 batch: 609/840\n",
      "Batch loss: 0.7041205763816833 batch: 610/840\n",
      "Batch loss: 0.7281132340431213 batch: 611/840\n",
      "Batch loss: 0.7282586693763733 batch: 612/840\n",
      "Batch loss: 0.7065761089324951 batch: 613/840\n",
      "Batch loss: 0.6617617011070251 batch: 614/840\n",
      "Batch loss: 0.5464808940887451 batch: 615/840\n",
      "Batch loss: 0.5140575170516968 batch: 616/840\n",
      "Batch loss: 0.5269164443016052 batch: 617/840\n",
      "Batch loss: 0.6009019613265991 batch: 618/840\n",
      "Batch loss: 0.7663611769676208 batch: 619/840\n",
      "Batch loss: 0.47877976298332214 batch: 620/840\n",
      "Batch loss: 0.7065886855125427 batch: 621/840\n",
      "Batch loss: 0.6650624871253967 batch: 622/840\n",
      "Batch loss: 0.6641265153884888 batch: 623/840\n",
      "Batch loss: 0.8205538988113403 batch: 624/840\n",
      "Batch loss: 0.7360163927078247 batch: 625/840\n",
      "Batch loss: 0.676708996295929 batch: 626/840\n",
      "Batch loss: 0.5510892868041992 batch: 627/840\n",
      "Batch loss: 0.7143524289131165 batch: 628/840\n",
      "Batch loss: 0.6760152578353882 batch: 629/840\n",
      "Batch loss: 0.7005516290664673 batch: 630/840\n",
      "Batch loss: 0.5888760089874268 batch: 631/840\n",
      "Batch loss: 0.6742052435874939 batch: 632/840\n",
      "Batch loss: 0.5965539813041687 batch: 633/840\n",
      "Batch loss: 0.5754886269569397 batch: 634/840\n",
      "Batch loss: 0.5193946957588196 batch: 635/840\n",
      "Batch loss: 0.5477371215820312 batch: 636/840\n",
      "Batch loss: 0.44439274072647095 batch: 637/840\n",
      "Batch loss: 0.6678550839424133 batch: 638/840\n",
      "Batch loss: 0.7212159037590027 batch: 639/840\n",
      "Batch loss: 0.6429563164710999 batch: 640/840\n",
      "Batch loss: 0.9451457262039185 batch: 641/840\n",
      "Batch loss: 0.5529916882514954 batch: 642/840\n",
      "Batch loss: 1.0147366523742676 batch: 643/840\n",
      "Batch loss: 0.633291482925415 batch: 644/840\n",
      "Batch loss: 0.5699343681335449 batch: 645/840\n",
      "Batch loss: 0.5314952731132507 batch: 646/840\n",
      "Batch loss: 0.7094234228134155 batch: 647/840\n",
      "Batch loss: 0.7619422078132629 batch: 648/840\n",
      "Batch loss: 0.5717331171035767 batch: 649/840\n",
      "Batch loss: 0.5180701613426208 batch: 650/840\n",
      "Batch loss: 0.6570606231689453 batch: 651/840\n",
      "Batch loss: 0.666256308555603 batch: 652/840\n",
      "Batch loss: 0.5457398891448975 batch: 653/840\n",
      "Batch loss: 0.8084059357643127 batch: 654/840\n",
      "Batch loss: 0.6377151012420654 batch: 655/840\n",
      "Batch loss: 0.7102484703063965 batch: 656/840\n",
      "Batch loss: 0.6723938584327698 batch: 657/840\n",
      "Batch loss: 0.7509786486625671 batch: 658/840\n",
      "Batch loss: 0.716604471206665 batch: 659/840\n",
      "Batch loss: 0.5762666463851929 batch: 660/840\n",
      "Batch loss: 0.7291757464408875 batch: 661/840\n",
      "Batch loss: 0.6644465923309326 batch: 662/840\n",
      "Batch loss: 0.49115076661109924 batch: 663/840\n",
      "Batch loss: 0.560172438621521 batch: 664/840\n",
      "Batch loss: 0.7044457793235779 batch: 665/840\n",
      "Batch loss: 0.6429381370544434 batch: 666/840\n",
      "Batch loss: 0.6677320599555969 batch: 667/840\n",
      "Batch loss: 0.7118233442306519 batch: 668/840\n",
      "Batch loss: 0.5810787677764893 batch: 669/840\n",
      "Batch loss: 0.6918520331382751 batch: 670/840\n",
      "Batch loss: 0.6610472798347473 batch: 671/840\n",
      "Batch loss: 0.5902615189552307 batch: 672/840\n",
      "Batch loss: 0.7235674858093262 batch: 673/840\n",
      "Batch loss: 0.8931844234466553 batch: 674/840\n",
      "Batch loss: 0.5422303676605225 batch: 675/840\n",
      "Batch loss: 0.517580509185791 batch: 676/840\n",
      "Batch loss: 0.5844162702560425 batch: 677/840\n",
      "Batch loss: 0.7530702948570251 batch: 678/840\n",
      "Batch loss: 0.8320772647857666 batch: 679/840\n",
      "Batch loss: 0.660532534122467 batch: 680/840\n",
      "Batch loss: 0.6942828893661499 batch: 681/840\n",
      "Batch loss: 0.5967377424240112 batch: 682/840\n",
      "Batch loss: 0.4977582097053528 batch: 683/840\n",
      "Batch loss: 0.8070029616355896 batch: 684/840\n",
      "Batch loss: 0.6293387413024902 batch: 685/840\n",
      "Batch loss: 0.7754269242286682 batch: 686/840\n",
      "Batch loss: 0.6489070653915405 batch: 687/840\n",
      "Batch loss: 0.6023315191268921 batch: 688/840\n",
      "Batch loss: 0.5951921939849854 batch: 689/840\n",
      "Batch loss: 0.550735592842102 batch: 690/840\n",
      "Batch loss: 0.6469448804855347 batch: 691/840\n",
      "Batch loss: 0.6167688369750977 batch: 692/840\n",
      "Batch loss: 0.6065544486045837 batch: 693/840\n",
      "Batch loss: 0.8258855938911438 batch: 694/840\n",
      "Batch loss: 0.7379267811775208 batch: 695/840\n",
      "Batch loss: 0.6458595395088196 batch: 696/840\n",
      "Batch loss: 0.6925046443939209 batch: 697/840\n",
      "Batch loss: 0.5744650363922119 batch: 698/840\n",
      "Batch loss: 0.7124172449111938 batch: 699/840\n",
      "Batch loss: 0.7162750363349915 batch: 700/840\n",
      "Batch loss: 0.7688170075416565 batch: 701/840\n",
      "Batch loss: 0.5565046072006226 batch: 702/840\n",
      "Batch loss: 0.6408361196517944 batch: 703/840\n",
      "Batch loss: 0.7376033663749695 batch: 704/840\n",
      "Batch loss: 0.6220942139625549 batch: 705/840\n",
      "Batch loss: 0.5600770711898804 batch: 706/840\n",
      "Batch loss: 0.7889516949653625 batch: 707/840\n",
      "Batch loss: 0.7623723745346069 batch: 708/840\n",
      "Batch loss: 0.7202586531639099 batch: 709/840\n",
      "Batch loss: 0.7877023220062256 batch: 710/840\n",
      "Batch loss: 0.4320884048938751 batch: 711/840\n",
      "Batch loss: 0.6665486693382263 batch: 712/840\n",
      "Batch loss: 0.518699049949646 batch: 713/840\n",
      "Batch loss: 0.618338406085968 batch: 714/840\n",
      "Batch loss: 0.7853578925132751 batch: 715/840\n",
      "Batch loss: 0.7550447583198547 batch: 716/840\n",
      "Batch loss: 0.6521439552307129 batch: 717/840\n",
      "Batch loss: 0.5792748332023621 batch: 718/840\n",
      "Batch loss: 0.4456826448440552 batch: 719/840\n",
      "Batch loss: 0.5528707504272461 batch: 720/840\n",
      "Batch loss: 0.7656733989715576 batch: 721/840\n",
      "Batch loss: 0.7401716113090515 batch: 722/840\n",
      "Batch loss: 0.5606415271759033 batch: 723/840\n",
      "Batch loss: 0.7160631418228149 batch: 724/840\n",
      "Batch loss: 0.5635524392127991 batch: 725/840\n",
      "Batch loss: 0.701865553855896 batch: 726/840\n",
      "Batch loss: 0.8343378305435181 batch: 727/840\n",
      "Batch loss: 0.7212944626808167 batch: 728/840\n",
      "Batch loss: 0.6481443047523499 batch: 729/840\n",
      "Batch loss: 0.5855706930160522 batch: 730/840\n",
      "Batch loss: 0.5035767555236816 batch: 731/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8091269135475159 batch: 732/840\n",
      "Batch loss: 0.6594135165214539 batch: 733/840\n",
      "Batch loss: 0.5829812288284302 batch: 734/840\n",
      "Batch loss: 0.5434207916259766 batch: 735/840\n",
      "Batch loss: 0.7424229383468628 batch: 736/840\n",
      "Batch loss: 0.6686879992485046 batch: 737/840\n",
      "Batch loss: 0.4742799997329712 batch: 738/840\n",
      "Batch loss: 0.7717787027359009 batch: 739/840\n",
      "Batch loss: 0.651549756526947 batch: 740/840\n",
      "Batch loss: 0.5491855144500732 batch: 741/840\n",
      "Batch loss: 0.5602543354034424 batch: 742/840\n",
      "Batch loss: 0.44938868284225464 batch: 743/840\n",
      "Batch loss: 0.4995308816432953 batch: 744/840\n",
      "Batch loss: 0.5449714660644531 batch: 745/840\n",
      "Batch loss: 0.595782995223999 batch: 746/840\n",
      "Batch loss: 0.7084508538246155 batch: 747/840\n",
      "Batch loss: 0.6481449007987976 batch: 748/840\n",
      "Batch loss: 0.5973760485649109 batch: 749/840\n",
      "Batch loss: 0.5755674242973328 batch: 750/840\n",
      "Batch loss: 0.7710118889808655 batch: 751/840\n",
      "Batch loss: 0.8273326754570007 batch: 752/840\n",
      "Batch loss: 0.4902692139148712 batch: 753/840\n",
      "Batch loss: 0.6465353965759277 batch: 754/840\n",
      "Batch loss: 0.6007705330848694 batch: 755/840\n",
      "Batch loss: 0.5691342353820801 batch: 756/840\n",
      "Batch loss: 0.7042222023010254 batch: 757/840\n",
      "Batch loss: 0.5512230396270752 batch: 758/840\n",
      "Batch loss: 0.5494673252105713 batch: 759/840\n",
      "Batch loss: 0.5621391534805298 batch: 760/840\n",
      "Batch loss: 0.5492712259292603 batch: 761/840\n",
      "Batch loss: 0.7709680795669556 batch: 762/840\n",
      "Batch loss: 0.4586610794067383 batch: 763/840\n",
      "Batch loss: 0.6774532794952393 batch: 764/840\n",
      "Batch loss: 0.5040139555931091 batch: 765/840\n",
      "Batch loss: 0.5297149419784546 batch: 766/840\n",
      "Batch loss: 0.7836582660675049 batch: 767/840\n",
      "Batch loss: 0.7105072140693665 batch: 768/840\n",
      "Batch loss: 0.6826052665710449 batch: 769/840\n",
      "Batch loss: 0.5728565454483032 batch: 770/840\n",
      "Batch loss: 0.6118748188018799 batch: 771/840\n",
      "Batch loss: 0.6148011684417725 batch: 772/840\n",
      "Batch loss: 0.5686990022659302 batch: 773/840\n",
      "Batch loss: 0.5024869441986084 batch: 774/840\n",
      "Batch loss: 0.5429863929748535 batch: 775/840\n",
      "Batch loss: 0.567298173904419 batch: 776/840\n",
      "Batch loss: 0.5612900257110596 batch: 777/840\n",
      "Batch loss: 0.49022936820983887 batch: 778/840\n",
      "Batch loss: 0.627288818359375 batch: 779/840\n",
      "Batch loss: 0.5643578171730042 batch: 780/840\n",
      "Batch loss: 0.5702828764915466 batch: 781/840\n",
      "Batch loss: 0.6859006285667419 batch: 782/840\n",
      "Batch loss: 0.4701229929924011 batch: 783/840\n",
      "Batch loss: 0.727378785610199 batch: 784/840\n",
      "Batch loss: 0.5586891770362854 batch: 785/840\n",
      "Batch loss: 0.6049075126647949 batch: 786/840\n",
      "Batch loss: 0.6046062111854553 batch: 787/840\n",
      "Batch loss: 0.7555950284004211 batch: 788/840\n",
      "Batch loss: 0.7515838742256165 batch: 789/840\n",
      "Batch loss: 0.6312552094459534 batch: 790/840\n",
      "Batch loss: 0.6321010589599609 batch: 791/840\n",
      "Batch loss: 0.3865014612674713 batch: 792/840\n",
      "Batch loss: 0.5931577682495117 batch: 793/840\n",
      "Batch loss: 0.6005823612213135 batch: 794/840\n",
      "Batch loss: 0.6243168115615845 batch: 795/840\n",
      "Batch loss: 0.7261375188827515 batch: 796/840\n",
      "Batch loss: 0.6901390552520752 batch: 797/840\n",
      "Batch loss: 0.7115092277526855 batch: 798/840\n",
      "Batch loss: 0.6551702618598938 batch: 799/840\n",
      "Batch loss: 0.5123776197433472 batch: 800/840\n",
      "Batch loss: 0.7067309617996216 batch: 801/840\n",
      "Batch loss: 0.47986751794815063 batch: 802/840\n",
      "Batch loss: 0.4314838647842407 batch: 803/840\n",
      "Batch loss: 0.632016658782959 batch: 804/840\n",
      "Batch loss: 0.493897408246994 batch: 805/840\n",
      "Batch loss: 0.64646315574646 batch: 806/840\n",
      "Batch loss: 0.600757360458374 batch: 807/840\n",
      "Batch loss: 0.552214503288269 batch: 808/840\n",
      "Batch loss: 0.69975745677948 batch: 809/840\n",
      "Batch loss: 0.5758298635482788 batch: 810/840\n",
      "Batch loss: 0.516801118850708 batch: 811/840\n",
      "Batch loss: 0.7210677862167358 batch: 812/840\n",
      "Batch loss: 0.5130940079689026 batch: 813/840\n",
      "Batch loss: 0.6214785575866699 batch: 814/840\n",
      "Batch loss: 0.7905107140541077 batch: 815/840\n",
      "Batch loss: 0.6656181812286377 batch: 816/840\n",
      "Batch loss: 0.743142306804657 batch: 817/840\n",
      "Batch loss: 0.6585374474525452 batch: 818/840\n",
      "Batch loss: 0.47324272990226746 batch: 819/840\n",
      "Batch loss: 0.6770607233047485 batch: 820/840\n",
      "Batch loss: 0.7119659185409546 batch: 821/840\n",
      "Batch loss: 0.602100133895874 batch: 822/840\n",
      "Batch loss: 0.7676972150802612 batch: 823/840\n",
      "Batch loss: 0.8251024484634399 batch: 824/840\n",
      "Batch loss: 0.7770984172821045 batch: 825/840\n",
      "Batch loss: 0.6224884986877441 batch: 826/840\n",
      "Batch loss: 0.5191988348960876 batch: 827/840\n",
      "Batch loss: 0.6534721851348877 batch: 828/840\n",
      "Batch loss: 0.5416591167449951 batch: 829/840\n",
      "Batch loss: 0.6203359365463257 batch: 830/840\n",
      "Batch loss: 0.6184760332107544 batch: 831/840\n",
      "Batch loss: 0.7197799682617188 batch: 832/840\n",
      "Batch loss: 0.7508650422096252 batch: 833/840\n",
      "Batch loss: 0.6609027981758118 batch: 834/840\n",
      "Batch loss: 0.4837178885936737 batch: 835/840\n",
      "Batch loss: 0.6018986105918884 batch: 836/840\n",
      "Batch loss: 0.6085525751113892 batch: 837/840\n",
      "Batch loss: 0.6546699404716492 batch: 838/840\n",
      "Batch loss: 0.5248937606811523 batch: 839/840\n",
      "Batch loss: 0.6634663939476013 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 8/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.806\n",
      "Running epoch 9/15\n",
      "Batch loss: 0.46720263361930847 batch: 1/840\n",
      "Batch loss: 1.0408592224121094 batch: 2/840\n",
      "Batch loss: 0.5417430996894836 batch: 3/840\n",
      "Batch loss: 0.5443840622901917 batch: 4/840\n",
      "Batch loss: 0.551097571849823 batch: 5/840\n",
      "Batch loss: 0.5295368432998657 batch: 6/840\n",
      "Batch loss: 0.6012303829193115 batch: 7/840\n",
      "Batch loss: 0.6103538274765015 batch: 8/840\n",
      "Batch loss: 0.5110390186309814 batch: 9/840\n",
      "Batch loss: 0.5599561929702759 batch: 10/840\n",
      "Batch loss: 0.5831788182258606 batch: 11/840\n",
      "Batch loss: 0.6176919341087341 batch: 12/840\n",
      "Batch loss: 0.5633613467216492 batch: 13/840\n",
      "Batch loss: 0.6592355370521545 batch: 14/840\n",
      "Batch loss: 0.5872313380241394 batch: 15/840\n",
      "Batch loss: 0.5403078198432922 batch: 16/840\n",
      "Batch loss: 0.4774814546108246 batch: 17/840\n",
      "Batch loss: 0.6605782508850098 batch: 18/840\n",
      "Batch loss: 0.6618796586990356 batch: 19/840\n",
      "Batch loss: 0.7750101685523987 batch: 20/840\n",
      "Batch loss: 0.7389453649520874 batch: 21/840\n",
      "Batch loss: 0.5828856825828552 batch: 22/840\n",
      "Batch loss: 0.5732805132865906 batch: 23/840\n",
      "Batch loss: 0.5425131320953369 batch: 24/840\n",
      "Batch loss: 0.5150629878044128 batch: 25/840\n",
      "Batch loss: 0.6963707804679871 batch: 26/840\n",
      "Batch loss: 0.5917718410491943 batch: 27/840\n",
      "Batch loss: 0.7525048851966858 batch: 28/840\n",
      "Batch loss: 0.6910750865936279 batch: 29/840\n",
      "Batch loss: 0.5332231521606445 batch: 30/840\n",
      "Batch loss: 0.5956131219863892 batch: 31/840\n",
      "Batch loss: 0.6351809501647949 batch: 32/840\n",
      "Batch loss: 0.589358389377594 batch: 33/840\n",
      "Batch loss: 0.5904303193092346 batch: 34/840\n",
      "Batch loss: 0.6284778118133545 batch: 35/840\n",
      "Batch loss: 0.5360521674156189 batch: 36/840\n",
      "Batch loss: 0.7765170335769653 batch: 37/840\n",
      "Batch loss: 0.7147554159164429 batch: 38/840\n",
      "Batch loss: 0.6684427857398987 batch: 39/840\n",
      "Batch loss: 0.544118344783783 batch: 40/840\n",
      "Batch loss: 0.7523673176765442 batch: 41/840\n",
      "Batch loss: 0.6949252486228943 batch: 42/840\n",
      "Batch loss: 0.5981256365776062 batch: 43/840\n",
      "Batch loss: 0.6327112317085266 batch: 44/840\n",
      "Batch loss: 0.6149328351020813 batch: 45/840\n",
      "Batch loss: 0.5614184737205505 batch: 46/840\n",
      "Batch loss: 0.5062647461891174 batch: 47/840\n",
      "Batch loss: 0.5324628353118896 batch: 48/840\n",
      "Batch loss: 0.632505476474762 batch: 49/840\n",
      "Batch loss: 0.6285065412521362 batch: 50/840\n",
      "Batch loss: 0.6711486577987671 batch: 51/840\n",
      "Batch loss: 0.6612865328788757 batch: 52/840\n",
      "Batch loss: 0.5347152948379517 batch: 53/840\n",
      "Batch loss: 0.5816335082054138 batch: 54/840\n",
      "Batch loss: 0.6325497627258301 batch: 55/840\n",
      "Batch loss: 0.5468953847885132 batch: 56/840\n",
      "Batch loss: 0.7303562760353088 batch: 57/840\n",
      "Batch loss: 0.5903746485710144 batch: 58/840\n",
      "Batch loss: 0.6116459369659424 batch: 59/840\n",
      "Batch loss: 0.46725738048553467 batch: 60/840\n",
      "Batch loss: 0.8085153102874756 batch: 61/840\n",
      "Batch loss: 0.7693091034889221 batch: 62/840\n",
      "Batch loss: 0.5733938217163086 batch: 63/840\n",
      "Batch loss: 0.5485685467720032 batch: 64/840\n",
      "Batch loss: 0.4877672493457794 batch: 65/840\n",
      "Batch loss: 0.7208139300346375 batch: 66/840\n",
      "Batch loss: 0.7109994292259216 batch: 67/840\n",
      "Batch loss: 0.5740928053855896 batch: 68/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.726094663143158 batch: 69/840\n",
      "Batch loss: 0.6016557216644287 batch: 70/840\n",
      "Batch loss: 0.8730433583259583 batch: 71/840\n",
      "Batch loss: 0.783261775970459 batch: 72/840\n",
      "Batch loss: 0.6854133605957031 batch: 73/840\n",
      "Batch loss: 0.6141713857650757 batch: 74/840\n",
      "Batch loss: 0.6643725037574768 batch: 75/840\n",
      "Batch loss: 0.43803471326828003 batch: 76/840\n",
      "Batch loss: 0.6693471670150757 batch: 77/840\n",
      "Batch loss: 0.7206944823265076 batch: 78/840\n",
      "Batch loss: 0.6168774366378784 batch: 79/840\n",
      "Batch loss: 0.6286092400550842 batch: 80/840\n",
      "Batch loss: 0.5526739358901978 batch: 81/840\n",
      "Batch loss: 0.610927402973175 batch: 82/840\n",
      "Batch loss: 0.5013668537139893 batch: 83/840\n",
      "Batch loss: 0.8300849795341492 batch: 84/840\n",
      "Batch loss: 0.685204267501831 batch: 85/840\n",
      "Batch loss: 0.9446369409561157 batch: 86/840\n",
      "Batch loss: 0.563470721244812 batch: 87/840\n",
      "Batch loss: 0.44857245683670044 batch: 88/840\n",
      "Batch loss: 0.499497652053833 batch: 89/840\n",
      "Batch loss: 0.5819955468177795 batch: 90/840\n",
      "Batch loss: 0.5885211229324341 batch: 91/840\n",
      "Batch loss: 0.6645589470863342 batch: 92/840\n",
      "Batch loss: 0.6516312956809998 batch: 93/840\n",
      "Batch loss: 0.6007585525512695 batch: 94/840\n",
      "Batch loss: 0.5833510756492615 batch: 95/840\n",
      "Batch loss: 0.6375114321708679 batch: 96/840\n",
      "Batch loss: 0.6397817730903625 batch: 97/840\n",
      "Batch loss: 0.7646418213844299 batch: 98/840\n",
      "Batch loss: 0.5357962846755981 batch: 99/840\n",
      "Batch loss: 0.6275628209114075 batch: 100/840\n",
      "Batch loss: 0.577477216720581 batch: 101/840\n",
      "Batch loss: 0.5862029790878296 batch: 102/840\n",
      "Batch loss: 0.6221598386764526 batch: 103/840\n",
      "Batch loss: 0.5311405658721924 batch: 104/840\n",
      "Batch loss: 0.47299060225486755 batch: 105/840\n",
      "Batch loss: 0.6188169717788696 batch: 106/840\n",
      "Batch loss: 0.564983606338501 batch: 107/840\n",
      "Batch loss: 0.7508234977722168 batch: 108/840\n",
      "Batch loss: 0.6467620134353638 batch: 109/840\n",
      "Batch loss: 0.4969959259033203 batch: 110/840\n",
      "Batch loss: 0.5419116616249084 batch: 111/840\n",
      "Batch loss: 0.7590692043304443 batch: 112/840\n",
      "Batch loss: 0.6796351075172424 batch: 113/840\n",
      "Batch loss: 0.5895488262176514 batch: 114/840\n",
      "Batch loss: 0.5403008460998535 batch: 115/840\n",
      "Batch loss: 0.5938689708709717 batch: 116/840\n",
      "Batch loss: 0.5596352219581604 batch: 117/840\n",
      "Batch loss: 0.49695098400115967 batch: 118/840\n",
      "Batch loss: 0.7308501601219177 batch: 119/840\n",
      "Batch loss: 0.588097333908081 batch: 120/840\n",
      "Batch loss: 0.6943177580833435 batch: 121/840\n",
      "Batch loss: 0.8194280862808228 batch: 122/840\n",
      "Batch loss: 0.5373057126998901 batch: 123/840\n",
      "Batch loss: 0.529222309589386 batch: 124/840\n",
      "Batch loss: 0.5766758322715759 batch: 125/840\n",
      "Batch loss: 0.6766937375068665 batch: 126/840\n",
      "Batch loss: 0.6598759293556213 batch: 127/840\n",
      "Batch loss: 0.6353331804275513 batch: 128/840\n",
      "Batch loss: 0.6623866558074951 batch: 129/840\n",
      "Batch loss: 0.6130945086479187 batch: 130/840\n",
      "Batch loss: 0.7066322565078735 batch: 131/840\n",
      "Batch loss: 1.003556251525879 batch: 132/840\n",
      "Batch loss: 0.7326041460037231 batch: 133/840\n",
      "Batch loss: 0.5925211906433105 batch: 134/840\n",
      "Batch loss: 0.6189296841621399 batch: 135/840\n",
      "Batch loss: 0.6705384850502014 batch: 136/840\n",
      "Batch loss: 0.5816745162010193 batch: 137/840\n",
      "Batch loss: 0.4990716874599457 batch: 138/840\n",
      "Batch loss: 0.5566556453704834 batch: 139/840\n",
      "Batch loss: 0.6004728674888611 batch: 140/840\n",
      "Batch loss: 0.5152343511581421 batch: 141/840\n",
      "Batch loss: 0.563664972782135 batch: 142/840\n",
      "Batch loss: 0.5220342874526978 batch: 143/840\n",
      "Batch loss: 0.5329620838165283 batch: 144/840\n",
      "Batch loss: 0.8052594661712646 batch: 145/840\n",
      "Batch loss: 0.7767630219459534 batch: 146/840\n",
      "Batch loss: 0.5245287418365479 batch: 147/840\n",
      "Batch loss: 0.8402785658836365 batch: 148/840\n",
      "Batch loss: 0.6218195557594299 batch: 149/840\n",
      "Batch loss: 0.6388680338859558 batch: 150/840\n",
      "Batch loss: 0.5590250492095947 batch: 151/840\n",
      "Batch loss: 0.6389867067337036 batch: 152/840\n",
      "Batch loss: 0.46252527832984924 batch: 153/840\n",
      "Batch loss: 0.7392736673355103 batch: 154/840\n",
      "Batch loss: 0.5379275679588318 batch: 155/840\n",
      "Batch loss: 0.6769118309020996 batch: 156/840\n",
      "Batch loss: 0.6956487894058228 batch: 157/840\n",
      "Batch loss: 0.49961498379707336 batch: 158/840\n",
      "Batch loss: 0.5220372676849365 batch: 159/840\n",
      "Batch loss: 0.6233189105987549 batch: 160/840\n",
      "Batch loss: 0.7220887541770935 batch: 161/840\n",
      "Batch loss: 0.6924154162406921 batch: 162/840\n",
      "Batch loss: 0.6900289058685303 batch: 163/840\n",
      "Batch loss: 0.5129395127296448 batch: 164/840\n",
      "Batch loss: 0.6187071204185486 batch: 165/840\n",
      "Batch loss: 0.49052563309669495 batch: 166/840\n",
      "Batch loss: 0.659264326095581 batch: 167/840\n",
      "Batch loss: 0.6302458047866821 batch: 168/840\n",
      "Batch loss: 0.5610060095787048 batch: 169/840\n",
      "Batch loss: 0.6965833902359009 batch: 170/840\n",
      "Batch loss: 0.5635627508163452 batch: 171/840\n",
      "Batch loss: 0.724855899810791 batch: 172/840\n",
      "Batch loss: 0.5704315304756165 batch: 173/840\n",
      "Batch loss: 0.628697395324707 batch: 174/840\n",
      "Batch loss: 0.5625576972961426 batch: 175/840\n",
      "Batch loss: 0.7547378540039062 batch: 176/840\n",
      "Batch loss: 0.6924172043800354 batch: 177/840\n",
      "Batch loss: 0.6620611548423767 batch: 178/840\n",
      "Batch loss: 0.7298150658607483 batch: 179/840\n",
      "Batch loss: 0.5038092732429504 batch: 180/840\n",
      "Batch loss: 0.5595798492431641 batch: 181/840\n",
      "Batch loss: 0.5277445912361145 batch: 182/840\n",
      "Batch loss: 0.6912953853607178 batch: 183/840\n",
      "Batch loss: 0.5870241522789001 batch: 184/840\n",
      "Batch loss: 0.4195987284183502 batch: 185/840\n",
      "Batch loss: 0.592597246170044 batch: 186/840\n",
      "Batch loss: 0.6088815927505493 batch: 187/840\n",
      "Batch loss: 0.5834867358207703 batch: 188/840\n",
      "Batch loss: 0.5653294920921326 batch: 189/840\n",
      "Batch loss: 0.6381814479827881 batch: 190/840\n",
      "Batch loss: 0.7387114763259888 batch: 191/840\n",
      "Batch loss: 0.4822213053703308 batch: 192/840\n",
      "Batch loss: 0.4724225103855133 batch: 193/840\n",
      "Batch loss: 0.45537468791007996 batch: 194/840\n",
      "Batch loss: 0.5151609182357788 batch: 195/840\n",
      "Batch loss: 0.7187432050704956 batch: 196/840\n",
      "Batch loss: 0.5636433959007263 batch: 197/840\n",
      "Batch loss: 0.4743410348892212 batch: 198/840\n",
      "Batch loss: 0.692157506942749 batch: 199/840\n",
      "Batch loss: 0.923482358455658 batch: 200/840\n",
      "Batch loss: 0.5161690711975098 batch: 201/840\n",
      "Batch loss: 0.5673556327819824 batch: 202/840\n",
      "Batch loss: 0.5670406818389893 batch: 203/840\n",
      "Batch loss: 0.715119481086731 batch: 204/840\n",
      "Batch loss: 0.7232906222343445 batch: 205/840\n",
      "Batch loss: 0.6807544827461243 batch: 206/840\n",
      "Batch loss: 0.6682493686676025 batch: 207/840\n",
      "Batch loss: 0.6469703912734985 batch: 208/840\n",
      "Batch loss: 0.5367289185523987 batch: 209/840\n",
      "Batch loss: 0.5781431198120117 batch: 210/840\n",
      "Batch loss: 0.4495828151702881 batch: 211/840\n",
      "Batch loss: 0.5235375761985779 batch: 212/840\n",
      "Batch loss: 0.6664482951164246 batch: 213/840\n",
      "Batch loss: 0.8076543211936951 batch: 214/840\n",
      "Batch loss: 0.6273176670074463 batch: 215/840\n",
      "Batch loss: 0.6696894764900208 batch: 216/840\n",
      "Batch loss: 0.6810657382011414 batch: 217/840\n",
      "Batch loss: 0.6512558460235596 batch: 218/840\n",
      "Batch loss: 0.6904773116111755 batch: 219/840\n",
      "Batch loss: 0.7544159889221191 batch: 220/840\n",
      "Batch loss: 0.6118414402008057 batch: 221/840\n",
      "Batch loss: 0.7840908169746399 batch: 222/840\n",
      "Batch loss: 0.5773178339004517 batch: 223/840\n",
      "Batch loss: 0.7119357585906982 batch: 224/840\n",
      "Batch loss: 0.6078453063964844 batch: 225/840\n",
      "Batch loss: 0.7250950336456299 batch: 226/840\n",
      "Batch loss: 0.6826357245445251 batch: 227/840\n",
      "Batch loss: 0.46527794003486633 batch: 228/840\n",
      "Batch loss: 0.5081445574760437 batch: 229/840\n",
      "Batch loss: 0.6575700640678406 batch: 230/840\n",
      "Batch loss: 0.5097560286521912 batch: 231/840\n",
      "Batch loss: 0.7012931704521179 batch: 232/840\n",
      "Batch loss: 0.7078735828399658 batch: 233/840\n",
      "Batch loss: 0.6225784420967102 batch: 234/840\n",
      "Batch loss: 0.6406348943710327 batch: 235/840\n",
      "Batch loss: 0.7185800075531006 batch: 236/840\n",
      "Batch loss: 0.6542635560035706 batch: 237/840\n",
      "Batch loss: 0.7244508862495422 batch: 238/840\n",
      "Batch loss: 0.5831475257873535 batch: 239/840\n",
      "Batch loss: 0.6225013732910156 batch: 240/840\n",
      "Batch loss: 0.7297075390815735 batch: 241/840\n",
      "Batch loss: 0.5721263885498047 batch: 242/840\n",
      "Batch loss: 0.5832617282867432 batch: 243/840\n",
      "Batch loss: 0.7841969132423401 batch: 244/840\n",
      "Batch loss: 0.47845128178596497 batch: 245/840\n",
      "Batch loss: 0.6089494228363037 batch: 246/840\n",
      "Batch loss: 0.6881828904151917 batch: 247/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7087507843971252 batch: 248/840\n",
      "Batch loss: 0.6444986462593079 batch: 249/840\n",
      "Batch loss: 0.5033085942268372 batch: 250/840\n",
      "Batch loss: 0.5960314273834229 batch: 251/840\n",
      "Batch loss: 0.5817121267318726 batch: 252/840\n",
      "Batch loss: 0.7002370953559875 batch: 253/840\n",
      "Batch loss: 0.6930816173553467 batch: 254/840\n",
      "Batch loss: 0.6040250658988953 batch: 255/840\n",
      "Batch loss: 0.666151225566864 batch: 256/840\n",
      "Batch loss: 0.5316101908683777 batch: 257/840\n",
      "Batch loss: 0.7246439456939697 batch: 258/840\n",
      "Batch loss: 0.5317834615707397 batch: 259/840\n",
      "Batch loss: 0.4934249520301819 batch: 260/840\n",
      "Batch loss: 0.5574193000793457 batch: 261/840\n",
      "Batch loss: 0.42644762992858887 batch: 262/840\n",
      "Batch loss: 0.6393835544586182 batch: 263/840\n",
      "Batch loss: 0.564897894859314 batch: 264/840\n",
      "Batch loss: 0.6426345705986023 batch: 265/840\n",
      "Batch loss: 0.5702565908432007 batch: 266/840\n",
      "Batch loss: 0.6365152597427368 batch: 267/840\n",
      "Batch loss: 0.5330783724784851 batch: 268/840\n",
      "Batch loss: 0.5758811831474304 batch: 269/840\n",
      "Batch loss: 0.6102586388587952 batch: 270/840\n",
      "Batch loss: 0.6480127573013306 batch: 271/840\n",
      "Batch loss: 0.8386380672454834 batch: 272/840\n",
      "Batch loss: 0.7252113223075867 batch: 273/840\n",
      "Batch loss: 0.5572139620780945 batch: 274/840\n",
      "Batch loss: 0.7327607870101929 batch: 275/840\n",
      "Batch loss: 0.5352132320404053 batch: 276/840\n",
      "Batch loss: 0.6601620316505432 batch: 277/840\n",
      "Batch loss: 0.6721561551094055 batch: 278/840\n",
      "Batch loss: 0.721869707107544 batch: 279/840\n",
      "Batch loss: 0.8063673973083496 batch: 280/840\n",
      "Batch loss: 0.4981197118759155 batch: 281/840\n",
      "Batch loss: 0.5153654217720032 batch: 282/840\n",
      "Batch loss: 0.6291202306747437 batch: 283/840\n",
      "Batch loss: 0.5662968754768372 batch: 284/840\n",
      "Batch loss: 0.4710562229156494 batch: 285/840\n",
      "Batch loss: 0.5889464020729065 batch: 286/840\n",
      "Batch loss: 0.466562956571579 batch: 287/840\n",
      "Batch loss: 0.581459105014801 batch: 288/840\n",
      "Batch loss: 0.7579061388969421 batch: 289/840\n",
      "Batch loss: 0.7178129553794861 batch: 290/840\n",
      "Batch loss: 0.6338863968849182 batch: 291/840\n",
      "Batch loss: 0.5876119136810303 batch: 292/840\n",
      "Batch loss: 0.6693970561027527 batch: 293/840\n",
      "Batch loss: 0.6035677194595337 batch: 294/840\n",
      "Batch loss: 0.4999287724494934 batch: 295/840\n",
      "Batch loss: 0.6385448575019836 batch: 296/840\n",
      "Batch loss: 0.8171305060386658 batch: 297/840\n",
      "Batch loss: 0.8131267428398132 batch: 298/840\n",
      "Batch loss: 0.5232344269752502 batch: 299/840\n",
      "Batch loss: 0.7412283420562744 batch: 300/840\n",
      "Batch loss: 0.7362081408500671 batch: 301/840\n",
      "Batch loss: 0.6223866939544678 batch: 302/840\n",
      "Batch loss: 0.8825304508209229 batch: 303/840\n",
      "Batch loss: 0.5551559329032898 batch: 304/840\n",
      "Batch loss: 0.5126925110816956 batch: 305/840\n",
      "Batch loss: 0.6695334911346436 batch: 306/840\n",
      "Batch loss: 0.6047444939613342 batch: 307/840\n",
      "Batch loss: 0.7012946605682373 batch: 308/840\n",
      "Batch loss: 0.6243370771408081 batch: 309/840\n",
      "Batch loss: 0.7346993088722229 batch: 310/840\n",
      "Batch loss: 0.779778242111206 batch: 311/840\n",
      "Batch loss: 0.697098970413208 batch: 312/840\n",
      "Batch loss: 0.6612234711647034 batch: 313/840\n",
      "Batch loss: 0.45357921719551086 batch: 314/840\n",
      "Batch loss: 0.6578208804130554 batch: 315/840\n",
      "Batch loss: 0.5333945751190186 batch: 316/840\n",
      "Batch loss: 0.7013885378837585 batch: 317/840\n",
      "Batch loss: 0.6084938049316406 batch: 318/840\n",
      "Batch loss: 0.7731484174728394 batch: 319/840\n",
      "Batch loss: 0.5187990665435791 batch: 320/840\n",
      "Batch loss: 0.5753445625305176 batch: 321/840\n",
      "Batch loss: 0.7177255749702454 batch: 322/840\n",
      "Batch loss: 0.7794089317321777 batch: 323/840\n",
      "Batch loss: 0.6192091703414917 batch: 324/840\n",
      "Batch loss: 0.5406458377838135 batch: 325/840\n",
      "Batch loss: 0.6065355539321899 batch: 326/840\n",
      "Batch loss: 0.49999329447746277 batch: 327/840\n",
      "Batch loss: 0.7351995706558228 batch: 328/840\n",
      "Batch loss: 0.6491515636444092 batch: 329/840\n",
      "Batch loss: 0.6540685892105103 batch: 330/840\n",
      "Batch loss: 0.679820716381073 batch: 331/840\n",
      "Batch loss: 0.6678908467292786 batch: 332/840\n",
      "Batch loss: 0.6358050107955933 batch: 333/840\n",
      "Batch loss: 0.6856387257575989 batch: 334/840\n",
      "Batch loss: 0.6450189352035522 batch: 335/840\n",
      "Batch loss: 0.6653608083724976 batch: 336/840\n",
      "Batch loss: 0.8806698322296143 batch: 337/840\n",
      "Batch loss: 0.7247281074523926 batch: 338/840\n",
      "Batch loss: 0.5281143188476562 batch: 339/840\n",
      "Batch loss: 0.7915657162666321 batch: 340/840\n",
      "Batch loss: 0.47628268599510193 batch: 341/840\n",
      "Batch loss: 0.49686622619628906 batch: 342/840\n",
      "Batch loss: 0.8016006350517273 batch: 343/840\n",
      "Batch loss: 0.6507316827774048 batch: 344/840\n",
      "Batch loss: 0.4530785083770752 batch: 345/840\n",
      "Batch loss: 0.6090896129608154 batch: 346/840\n",
      "Batch loss: 0.5621417760848999 batch: 347/840\n",
      "Batch loss: 0.5607439875602722 batch: 348/840\n",
      "Batch loss: 0.5801187753677368 batch: 349/840\n",
      "Batch loss: 0.5820907354354858 batch: 350/840\n",
      "Batch loss: 0.6797874569892883 batch: 351/840\n",
      "Batch loss: 0.6267884373664856 batch: 352/840\n",
      "Batch loss: 0.6605251431465149 batch: 353/840\n",
      "Batch loss: 0.6346376538276672 batch: 354/840\n",
      "Batch loss: 0.5461602210998535 batch: 355/840\n",
      "Batch loss: 0.5482287406921387 batch: 356/840\n",
      "Batch loss: 0.5091579556465149 batch: 357/840\n",
      "Batch loss: 0.768073558807373 batch: 358/840\n",
      "Batch loss: 0.596660315990448 batch: 359/840\n",
      "Batch loss: 0.7261320352554321 batch: 360/840\n",
      "Batch loss: 0.7475717067718506 batch: 361/840\n",
      "Batch loss: 0.5446472764015198 batch: 362/840\n",
      "Batch loss: 0.5893754363059998 batch: 363/840\n",
      "Batch loss: 0.689147412776947 batch: 364/840\n",
      "Batch loss: 0.5456902384757996 batch: 365/840\n",
      "Batch loss: 0.6875455975532532 batch: 366/840\n",
      "Batch loss: 0.47878336906433105 batch: 367/840\n",
      "Batch loss: 0.7714394330978394 batch: 368/840\n",
      "Batch loss: 0.6474930047988892 batch: 369/840\n",
      "Batch loss: 0.7640664577484131 batch: 370/840\n",
      "Batch loss: 0.6692766547203064 batch: 371/840\n",
      "Batch loss: 0.5357468128204346 batch: 372/840\n",
      "Batch loss: 0.5458059906959534 batch: 373/840\n",
      "Batch loss: 0.6717793345451355 batch: 374/840\n",
      "Batch loss: 0.4835397005081177 batch: 375/840\n",
      "Batch loss: 0.5004908442497253 batch: 376/840\n",
      "Batch loss: 0.6054301261901855 batch: 377/840\n",
      "Batch loss: 0.5738227963447571 batch: 378/840\n",
      "Batch loss: 0.49981769919395447 batch: 379/840\n",
      "Batch loss: 0.8595103621482849 batch: 380/840\n",
      "Batch loss: 1.060224175453186 batch: 381/840\n",
      "Batch loss: 0.6785922050476074 batch: 382/840\n",
      "Batch loss: 0.6505854725837708 batch: 383/840\n",
      "Batch loss: 0.6097143292427063 batch: 384/840\n",
      "Batch loss: 0.6597146391868591 batch: 385/840\n",
      "Batch loss: 0.7805274724960327 batch: 386/840\n",
      "Batch loss: 0.5755336880683899 batch: 387/840\n",
      "Batch loss: 0.6476947665214539 batch: 388/840\n",
      "Batch loss: 0.5252013206481934 batch: 389/840\n",
      "Batch loss: 0.9203460812568665 batch: 390/840\n",
      "Batch loss: 0.776908278465271 batch: 391/840\n",
      "Batch loss: 0.6030365228652954 batch: 392/840\n",
      "Batch loss: 0.501693606376648 batch: 393/840\n",
      "Batch loss: 0.7699711322784424 batch: 394/840\n",
      "Batch loss: 0.6636087894439697 batch: 395/840\n",
      "Batch loss: 0.605646014213562 batch: 396/840\n",
      "Batch loss: 0.5570612549781799 batch: 397/840\n",
      "Batch loss: 0.7639591097831726 batch: 398/840\n",
      "Batch loss: 0.4175383746623993 batch: 399/840\n",
      "Batch loss: 0.6302525997161865 batch: 400/840\n",
      "Batch loss: 0.6472092270851135 batch: 401/840\n",
      "Batch loss: 0.4988885819911957 batch: 402/840\n",
      "Batch loss: 0.6092717051506042 batch: 403/840\n",
      "Batch loss: 0.5347450375556946 batch: 404/840\n",
      "Batch loss: 0.6186445355415344 batch: 405/840\n",
      "Batch loss: 0.5636435151100159 batch: 406/840\n",
      "Batch loss: 0.5421745181083679 batch: 407/840\n",
      "Batch loss: 0.721964418888092 batch: 408/840\n",
      "Batch loss: 0.6653881072998047 batch: 409/840\n",
      "Batch loss: 0.7459123134613037 batch: 410/840\n",
      "Batch loss: 0.7331175804138184 batch: 411/840\n",
      "Batch loss: 0.7644063830375671 batch: 412/840\n",
      "Batch loss: 0.648150622844696 batch: 413/840\n",
      "Batch loss: 0.6566672325134277 batch: 414/840\n",
      "Batch loss: 0.9184335470199585 batch: 415/840\n",
      "Batch loss: 0.48588505387306213 batch: 416/840\n",
      "Batch loss: 0.6007694005966187 batch: 417/840\n",
      "Batch loss: 0.7957248091697693 batch: 418/840\n",
      "Batch loss: 0.7179077863693237 batch: 419/840\n",
      "Batch loss: 0.6772482395172119 batch: 420/840\n",
      "Batch loss: 0.45881417393684387 batch: 421/840\n",
      "Batch loss: 0.5124198794364929 batch: 422/840\n",
      "Batch loss: 0.606976330280304 batch: 423/840\n",
      "Batch loss: 0.7746378183364868 batch: 424/840\n",
      "Batch loss: 0.7141510248184204 batch: 425/840\n",
      "Batch loss: 0.6013335585594177 batch: 426/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6328994631767273 batch: 427/840\n",
      "Batch loss: 0.7144467830657959 batch: 428/840\n",
      "Batch loss: 0.5882222056388855 batch: 429/840\n",
      "Batch loss: 0.7345538139343262 batch: 430/840\n",
      "Batch loss: 0.6331673860549927 batch: 431/840\n",
      "Batch loss: 0.6205217838287354 batch: 432/840\n",
      "Batch loss: 0.5064971446990967 batch: 433/840\n",
      "Batch loss: 0.5621923804283142 batch: 434/840\n",
      "Batch loss: 0.6718695163726807 batch: 435/840\n",
      "Batch loss: 0.6152183413505554 batch: 436/840\n",
      "Batch loss: 0.6221634149551392 batch: 437/840\n",
      "Batch loss: 0.5131189227104187 batch: 438/840\n",
      "Batch loss: 0.6231229901313782 batch: 439/840\n",
      "Batch loss: 0.6852729320526123 batch: 440/840\n",
      "Batch loss: 0.7857697010040283 batch: 441/840\n",
      "Batch loss: 0.7132149338722229 batch: 442/840\n",
      "Batch loss: 0.6094950437545776 batch: 443/840\n",
      "Batch loss: 0.6353698372840881 batch: 444/840\n",
      "Batch loss: 0.5493463277816772 batch: 445/840\n",
      "Batch loss: 0.6397413015365601 batch: 446/840\n",
      "Batch loss: 0.6949372887611389 batch: 447/840\n",
      "Batch loss: 0.691754937171936 batch: 448/840\n",
      "Batch loss: 0.598717987537384 batch: 449/840\n",
      "Batch loss: 0.6283212900161743 batch: 450/840\n",
      "Batch loss: 0.680530846118927 batch: 451/840\n",
      "Batch loss: 0.7440336346626282 batch: 452/840\n",
      "Batch loss: 0.7721909284591675 batch: 453/840\n",
      "Batch loss: 0.5697265267372131 batch: 454/840\n",
      "Batch loss: 0.6409485340118408 batch: 455/840\n",
      "Batch loss: 0.621028482913971 batch: 456/840\n",
      "Batch loss: 0.5181517004966736 batch: 457/840\n",
      "Batch loss: 0.6751169562339783 batch: 458/840\n",
      "Batch loss: 0.5094811916351318 batch: 459/840\n",
      "Batch loss: 0.42637377977371216 batch: 460/840\n",
      "Batch loss: 0.7682836651802063 batch: 461/840\n",
      "Batch loss: 0.6302329897880554 batch: 462/840\n",
      "Batch loss: 0.7539163827896118 batch: 463/840\n",
      "Batch loss: 0.5110216736793518 batch: 464/840\n",
      "Batch loss: 0.9628555178642273 batch: 465/840\n",
      "Batch loss: 0.6553127765655518 batch: 466/840\n",
      "Batch loss: 0.882129967212677 batch: 467/840\n",
      "Batch loss: 0.5431798696517944 batch: 468/840\n",
      "Batch loss: 0.5773367285728455 batch: 469/840\n",
      "Batch loss: 0.596451997756958 batch: 470/840\n",
      "Batch loss: 0.6144998073577881 batch: 471/840\n",
      "Batch loss: 0.5997008681297302 batch: 472/840\n",
      "Batch loss: 0.7094058990478516 batch: 473/840\n",
      "Batch loss: 0.5421209931373596 batch: 474/840\n",
      "Batch loss: 0.6227898597717285 batch: 475/840\n",
      "Batch loss: 0.6131453514099121 batch: 476/840\n",
      "Batch loss: 0.5326719284057617 batch: 477/840\n",
      "Batch loss: 0.5856667757034302 batch: 478/840\n",
      "Batch loss: 0.5880134105682373 batch: 479/840\n",
      "Batch loss: 0.624143123626709 batch: 480/840\n",
      "Batch loss: 0.6788923144340515 batch: 481/840\n",
      "Batch loss: 0.6408188343048096 batch: 482/840\n",
      "Batch loss: 0.6973316669464111 batch: 483/840\n",
      "Batch loss: 0.46519744396209717 batch: 484/840\n",
      "Batch loss: 0.6651962399482727 batch: 485/840\n",
      "Batch loss: 0.6220008730888367 batch: 486/840\n",
      "Batch loss: 0.43814021348953247 batch: 487/840\n",
      "Batch loss: 0.5929620265960693 batch: 488/840\n",
      "Batch loss: 0.6297224760055542 batch: 489/840\n",
      "Batch loss: 0.7363564968109131 batch: 490/840\n",
      "Batch loss: 0.4379744827747345 batch: 491/840\n",
      "Batch loss: 0.7565906643867493 batch: 492/840\n",
      "Batch loss: 0.7995694875717163 batch: 493/840\n",
      "Batch loss: 0.4488976001739502 batch: 494/840\n",
      "Batch loss: 0.8084660172462463 batch: 495/840\n",
      "Batch loss: 0.6769286394119263 batch: 496/840\n",
      "Batch loss: 0.6035841107368469 batch: 497/840\n",
      "Batch loss: 0.5223285555839539 batch: 498/840\n",
      "Batch loss: 0.6674758791923523 batch: 499/840\n",
      "Batch loss: 0.639729380607605 batch: 500/840\n",
      "Batch loss: 0.5880715847015381 batch: 501/840\n",
      "Batch loss: 0.729850172996521 batch: 502/840\n",
      "Batch loss: 0.4137401282787323 batch: 503/840\n",
      "Batch loss: 0.6609017848968506 batch: 504/840\n",
      "Batch loss: 0.6762059926986694 batch: 505/840\n",
      "Batch loss: 0.5947221517562866 batch: 506/840\n",
      "Batch loss: 0.6732006669044495 batch: 507/840\n",
      "Batch loss: 0.5864303708076477 batch: 508/840\n",
      "Batch loss: 0.7164042592048645 batch: 509/840\n",
      "Batch loss: 0.5779170989990234 batch: 510/840\n",
      "Batch loss: 0.5035150051116943 batch: 511/840\n",
      "Batch loss: 0.6259011030197144 batch: 512/840\n",
      "Batch loss: 0.5798521041870117 batch: 513/840\n",
      "Batch loss: 0.7219563126564026 batch: 514/840\n",
      "Batch loss: 0.5226141214370728 batch: 515/840\n",
      "Batch loss: 0.6249282956123352 batch: 516/840\n",
      "Batch loss: 0.6151735782623291 batch: 517/840\n",
      "Batch loss: 0.6456295847892761 batch: 518/840\n",
      "Batch loss: 0.7289748191833496 batch: 519/840\n",
      "Batch loss: 0.6547396779060364 batch: 520/840\n",
      "Batch loss: 0.7423204183578491 batch: 521/840\n",
      "Batch loss: 0.5689185857772827 batch: 522/840\n",
      "Batch loss: 0.5803030133247375 batch: 523/840\n",
      "Batch loss: 0.6124246120452881 batch: 524/840\n",
      "Batch loss: 0.6873654723167419 batch: 525/840\n",
      "Batch loss: 0.6957244873046875 batch: 526/840\n",
      "Batch loss: 0.6735248565673828 batch: 527/840\n",
      "Batch loss: 0.6507965922355652 batch: 528/840\n",
      "Batch loss: 0.532738208770752 batch: 529/840\n",
      "Batch loss: 0.6741690039634705 batch: 530/840\n",
      "Batch loss: 0.5082652568817139 batch: 531/840\n",
      "Batch loss: 0.4433060586452484 batch: 532/840\n",
      "Batch loss: 0.6434652209281921 batch: 533/840\n",
      "Batch loss: 0.6610107421875 batch: 534/840\n",
      "Batch loss: 0.6186215281486511 batch: 535/840\n",
      "Batch loss: 0.6628591418266296 batch: 536/840\n",
      "Batch loss: 0.6044527888298035 batch: 537/840\n",
      "Batch loss: 0.5527300238609314 batch: 538/840\n",
      "Batch loss: 0.636502206325531 batch: 539/840\n",
      "Batch loss: 0.7095894813537598 batch: 540/840\n",
      "Batch loss: 0.4884856939315796 batch: 541/840\n",
      "Batch loss: 0.6744760274887085 batch: 542/840\n",
      "Batch loss: 0.4183388948440552 batch: 543/840\n",
      "Batch loss: 0.6248883605003357 batch: 544/840\n",
      "Batch loss: 0.6168196201324463 batch: 545/840\n",
      "Batch loss: 0.5851783752441406 batch: 546/840\n",
      "Batch loss: 0.5676496028900146 batch: 547/840\n",
      "Batch loss: 0.5078338980674744 batch: 548/840\n",
      "Batch loss: 0.5453600287437439 batch: 549/840\n",
      "Batch loss: 0.5066419243812561 batch: 550/840\n",
      "Batch loss: 0.6838148236274719 batch: 551/840\n",
      "Batch loss: 0.5665812492370605 batch: 552/840\n",
      "Batch loss: 0.6268386840820312 batch: 553/840\n",
      "Batch loss: 0.5492491722106934 batch: 554/840\n",
      "Batch loss: 0.6305602788925171 batch: 555/840\n",
      "Batch loss: 0.6541185975074768 batch: 556/840\n",
      "Batch loss: 0.5713297128677368 batch: 557/840\n",
      "Batch loss: 0.6015369296073914 batch: 558/840\n",
      "Batch loss: 0.580501914024353 batch: 559/840\n",
      "Batch loss: 0.6459366083145142 batch: 560/840\n",
      "Batch loss: 0.6161648631095886 batch: 561/840\n",
      "Batch loss: 0.5980812311172485 batch: 562/840\n",
      "Batch loss: 0.49247822165489197 batch: 563/840\n",
      "Batch loss: 0.6508483290672302 batch: 564/840\n",
      "Batch loss: 0.7647207379341125 batch: 565/840\n",
      "Batch loss: 0.7099173665046692 batch: 566/840\n",
      "Batch loss: 0.7666503190994263 batch: 567/840\n",
      "Batch loss: 0.5226521492004395 batch: 568/840\n",
      "Batch loss: 0.593743622303009 batch: 569/840\n",
      "Batch loss: 0.4752233922481537 batch: 570/840\n",
      "Batch loss: 0.7167566418647766 batch: 571/840\n",
      "Batch loss: 0.6764801740646362 batch: 572/840\n",
      "Batch loss: 0.6590465307235718 batch: 573/840\n",
      "Batch loss: 0.7365267276763916 batch: 574/840\n",
      "Batch loss: 0.502659261226654 batch: 575/840\n",
      "Batch loss: 0.5723166465759277 batch: 576/840\n",
      "Batch loss: 0.5284430980682373 batch: 577/840\n",
      "Batch loss: 0.7214901447296143 batch: 578/840\n",
      "Batch loss: 0.7110830545425415 batch: 579/840\n",
      "Batch loss: 0.7723095417022705 batch: 580/840\n",
      "Batch loss: 0.693021833896637 batch: 581/840\n",
      "Batch loss: 0.8698728084564209 batch: 582/840\n",
      "Batch loss: 0.5653095841407776 batch: 583/840\n",
      "Batch loss: 0.7263783812522888 batch: 584/840\n",
      "Batch loss: 0.711723268032074 batch: 585/840\n",
      "Batch loss: 0.7351190447807312 batch: 586/840\n",
      "Batch loss: 0.6298427581787109 batch: 587/840\n",
      "Batch loss: 0.6160877346992493 batch: 588/840\n",
      "Batch loss: 0.6715015172958374 batch: 589/840\n",
      "Batch loss: 0.5681446194648743 batch: 590/840\n",
      "Batch loss: 0.4448176920413971 batch: 591/840\n",
      "Batch loss: 0.565345048904419 batch: 592/840\n",
      "Batch loss: 0.9413208961486816 batch: 593/840\n",
      "Batch loss: 0.6282541155815125 batch: 594/840\n",
      "Batch loss: 0.36272957921028137 batch: 595/840\n",
      "Batch loss: 0.4617297053337097 batch: 596/840\n",
      "Batch loss: 0.5990068316459656 batch: 597/840\n",
      "Batch loss: 0.4196324050426483 batch: 598/840\n",
      "Batch loss: 0.533980131149292 batch: 599/840\n",
      "Batch loss: 0.5819435119628906 batch: 600/840\n",
      "Batch loss: 0.7333360314369202 batch: 601/840\n",
      "Batch loss: 0.5777857303619385 batch: 602/840\n",
      "Batch loss: 0.6273977756500244 batch: 603/840\n",
      "Batch loss: 0.5771207213401794 batch: 604/840\n",
      "Batch loss: 0.7579383850097656 batch: 605/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.8905407786369324 batch: 606/840\n",
      "Batch loss: 0.6224891543388367 batch: 607/840\n",
      "Batch loss: 0.6245429515838623 batch: 608/840\n",
      "Batch loss: 0.46295538544654846 batch: 609/840\n",
      "Batch loss: 0.6474604606628418 batch: 610/840\n",
      "Batch loss: 0.6580077409744263 batch: 611/840\n",
      "Batch loss: 0.6199028491973877 batch: 612/840\n",
      "Batch loss: 0.6779490113258362 batch: 613/840\n",
      "Batch loss: 0.6177284717559814 batch: 614/840\n",
      "Batch loss: 0.5120131969451904 batch: 615/840\n",
      "Batch loss: 0.5481239557266235 batch: 616/840\n",
      "Batch loss: 0.49344608187675476 batch: 617/840\n",
      "Batch loss: 0.5900804400444031 batch: 618/840\n",
      "Batch loss: 0.7559494972229004 batch: 619/840\n",
      "Batch loss: 0.5270478129386902 batch: 620/840\n",
      "Batch loss: 0.700214684009552 batch: 621/840\n",
      "Batch loss: 0.6552430987358093 batch: 622/840\n",
      "Batch loss: 0.5820823907852173 batch: 623/840\n",
      "Batch loss: 0.7617287635803223 batch: 624/840\n",
      "Batch loss: 0.6986879706382751 batch: 625/840\n",
      "Batch loss: 0.57233065366745 batch: 626/840\n",
      "Batch loss: 0.6244034767150879 batch: 627/840\n",
      "Batch loss: 0.7105811834335327 batch: 628/840\n",
      "Batch loss: 0.6230621337890625 batch: 629/840\n",
      "Batch loss: 0.6410508155822754 batch: 630/840\n",
      "Batch loss: 0.8000884056091309 batch: 631/840\n",
      "Batch loss: 0.6701948642730713 batch: 632/840\n",
      "Batch loss: 0.5859460234642029 batch: 633/840\n",
      "Batch loss: 0.7683612108230591 batch: 634/840\n",
      "Batch loss: 0.5701433420181274 batch: 635/840\n",
      "Batch loss: 0.5657908916473389 batch: 636/840\n",
      "Batch loss: 0.5180866122245789 batch: 637/840\n",
      "Batch loss: 0.6776191592216492 batch: 638/840\n",
      "Batch loss: 0.6971752047538757 batch: 639/840\n",
      "Batch loss: 0.5787554383277893 batch: 640/840\n",
      "Batch loss: 0.8839860558509827 batch: 641/840\n",
      "Batch loss: 0.6696061491966248 batch: 642/840\n",
      "Batch loss: 0.8841133713722229 batch: 643/840\n",
      "Batch loss: 0.5773835778236389 batch: 644/840\n",
      "Batch loss: 0.5998947024345398 batch: 645/840\n",
      "Batch loss: 0.6231186985969543 batch: 646/840\n",
      "Batch loss: 0.7159653306007385 batch: 647/840\n",
      "Batch loss: 0.7251223921775818 batch: 648/840\n",
      "Batch loss: 0.6150240898132324 batch: 649/840\n",
      "Batch loss: 0.5479413866996765 batch: 650/840\n",
      "Batch loss: 0.6568450927734375 batch: 651/840\n",
      "Batch loss: 0.646634578704834 batch: 652/840\n",
      "Batch loss: 0.5081339478492737 batch: 653/840\n",
      "Batch loss: 0.8220161199569702 batch: 654/840\n",
      "Batch loss: 0.5166146159172058 batch: 655/840\n",
      "Batch loss: 0.6701714992523193 batch: 656/840\n",
      "Batch loss: 0.5688493251800537 batch: 657/840\n",
      "Batch loss: 0.7071205377578735 batch: 658/840\n",
      "Batch loss: 0.7447535991668701 batch: 659/840\n",
      "Batch loss: 0.5232247710227966 batch: 660/840\n",
      "Batch loss: 0.7114764451980591 batch: 661/840\n",
      "Batch loss: 0.6494996547698975 batch: 662/840\n",
      "Batch loss: 0.5457444190979004 batch: 663/840\n",
      "Batch loss: 0.5854930281639099 batch: 664/840\n",
      "Batch loss: 0.6652767658233643 batch: 665/840\n",
      "Batch loss: 0.6370000243186951 batch: 666/840\n",
      "Batch loss: 0.6670354604721069 batch: 667/840\n",
      "Batch loss: 0.5564190745353699 batch: 668/840\n",
      "Batch loss: 0.5215683579444885 batch: 669/840\n",
      "Batch loss: 0.6575211882591248 batch: 670/840\n",
      "Batch loss: 0.725362241268158 batch: 671/840\n",
      "Batch loss: 0.6533451080322266 batch: 672/840\n",
      "Batch loss: 0.8773183226585388 batch: 673/840\n",
      "Batch loss: 0.7314255237579346 batch: 674/840\n",
      "Batch loss: 0.5599275827407837 batch: 675/840\n",
      "Batch loss: 0.5920734405517578 batch: 676/840\n",
      "Batch loss: 0.6657184362411499 batch: 677/840\n",
      "Batch loss: 0.6335951089859009 batch: 678/840\n",
      "Batch loss: 0.7349227666854858 batch: 679/840\n",
      "Batch loss: 0.6496559381484985 batch: 680/840\n",
      "Batch loss: 0.6333274245262146 batch: 681/840\n",
      "Batch loss: 0.6146484613418579 batch: 682/840\n",
      "Batch loss: 0.5256410241127014 batch: 683/840\n",
      "Batch loss: 0.8869006633758545 batch: 684/840\n",
      "Batch loss: 0.6677177548408508 batch: 685/840\n",
      "Batch loss: 0.7395106554031372 batch: 686/840\n",
      "Batch loss: 0.6916980147361755 batch: 687/840\n",
      "Batch loss: 0.6579289436340332 batch: 688/840\n",
      "Batch loss: 0.592282772064209 batch: 689/840\n",
      "Batch loss: 0.5451173782348633 batch: 690/840\n",
      "Batch loss: 0.6431192755699158 batch: 691/840\n",
      "Batch loss: 0.5949462652206421 batch: 692/840\n",
      "Batch loss: 0.6153609752655029 batch: 693/840\n",
      "Batch loss: 0.7707385420799255 batch: 694/840\n",
      "Batch loss: 0.7738090753555298 batch: 695/840\n",
      "Batch loss: 0.561694324016571 batch: 696/840\n",
      "Batch loss: 0.5958281755447388 batch: 697/840\n",
      "Batch loss: 0.5904597640037537 batch: 698/840\n",
      "Batch loss: 0.8444886803627014 batch: 699/840\n",
      "Batch loss: 0.7711362242698669 batch: 700/840\n",
      "Batch loss: 0.9070918560028076 batch: 701/840\n",
      "Batch loss: 0.6389248371124268 batch: 702/840\n",
      "Batch loss: 0.5205205082893372 batch: 703/840\n",
      "Batch loss: 0.6841217875480652 batch: 704/840\n",
      "Batch loss: 0.5860050916671753 batch: 705/840\n",
      "Batch loss: 0.554890513420105 batch: 706/840\n",
      "Batch loss: 0.6666091680526733 batch: 707/840\n",
      "Batch loss: 0.6604236364364624 batch: 708/840\n",
      "Batch loss: 0.6376886367797852 batch: 709/840\n",
      "Batch loss: 0.7377636432647705 batch: 710/840\n",
      "Batch loss: 0.443945974111557 batch: 711/840\n",
      "Batch loss: 0.719869077205658 batch: 712/840\n",
      "Batch loss: 0.5041095018386841 batch: 713/840\n",
      "Batch loss: 0.6638249158859253 batch: 714/840\n",
      "Batch loss: 0.6516139507293701 batch: 715/840\n",
      "Batch loss: 0.6666204333305359 batch: 716/840\n",
      "Batch loss: 0.6982252597808838 batch: 717/840\n",
      "Batch loss: 0.6134570837020874 batch: 718/840\n",
      "Batch loss: 0.4371623694896698 batch: 719/840\n",
      "Batch loss: 0.529442548751831 batch: 720/840\n",
      "Batch loss: 0.7161738872528076 batch: 721/840\n",
      "Batch loss: 0.7838053703308105 batch: 722/840\n",
      "Batch loss: 0.5167933702468872 batch: 723/840\n",
      "Batch loss: 0.6924545764923096 batch: 724/840\n",
      "Batch loss: 0.6809466481208801 batch: 725/840\n",
      "Batch loss: 0.6774118542671204 batch: 726/840\n",
      "Batch loss: 0.7679069638252258 batch: 727/840\n",
      "Batch loss: 0.705568253993988 batch: 728/840\n",
      "Batch loss: 0.6572784185409546 batch: 729/840\n",
      "Batch loss: 0.6313426494598389 batch: 730/840\n",
      "Batch loss: 0.532088041305542 batch: 731/840\n",
      "Batch loss: 0.785306990146637 batch: 732/840\n",
      "Batch loss: 0.5797634720802307 batch: 733/840\n",
      "Batch loss: 0.5542693734169006 batch: 734/840\n",
      "Batch loss: 0.6646729111671448 batch: 735/840\n",
      "Batch loss: 0.6157766580581665 batch: 736/840\n",
      "Batch loss: 0.692656397819519 batch: 737/840\n",
      "Batch loss: 0.5788323879241943 batch: 738/840\n",
      "Batch loss: 0.7294967770576477 batch: 739/840\n",
      "Batch loss: 0.7701783776283264 batch: 740/840\n",
      "Batch loss: 0.4750896394252777 batch: 741/840\n",
      "Batch loss: 0.542731523513794 batch: 742/840\n",
      "Batch loss: 0.4493386447429657 batch: 743/840\n",
      "Batch loss: 0.4894118905067444 batch: 744/840\n",
      "Batch loss: 0.548532247543335 batch: 745/840\n",
      "Batch loss: 0.6210891604423523 batch: 746/840\n",
      "Batch loss: 0.667487621307373 batch: 747/840\n",
      "Batch loss: 0.6108214855194092 batch: 748/840\n",
      "Batch loss: 0.4532324969768524 batch: 749/840\n",
      "Batch loss: 0.5488715171813965 batch: 750/840\n",
      "Batch loss: 0.6559700965881348 batch: 751/840\n",
      "Batch loss: 0.8128030896186829 batch: 752/840\n",
      "Batch loss: 0.5424684286117554 batch: 753/840\n",
      "Batch loss: 0.6893182992935181 batch: 754/840\n",
      "Batch loss: 0.6081587076187134 batch: 755/840\n",
      "Batch loss: 0.533118486404419 batch: 756/840\n",
      "Batch loss: 0.7526369690895081 batch: 757/840\n",
      "Batch loss: 0.6217887997627258 batch: 758/840\n",
      "Batch loss: 0.5069513916969299 batch: 759/840\n",
      "Batch loss: 0.5940898656845093 batch: 760/840\n",
      "Batch loss: 0.5690922737121582 batch: 761/840\n",
      "Batch loss: 0.8119156360626221 batch: 762/840\n",
      "Batch loss: 0.4909963607788086 batch: 763/840\n",
      "Batch loss: 0.6195173859596252 batch: 764/840\n",
      "Batch loss: 0.4668423533439636 batch: 765/840\n",
      "Batch loss: 0.5721601247787476 batch: 766/840\n",
      "Batch loss: 0.681535542011261 batch: 767/840\n",
      "Batch loss: 0.7161763906478882 batch: 768/840\n",
      "Batch loss: 0.6298815011978149 batch: 769/840\n",
      "Batch loss: 0.498581200838089 batch: 770/840\n",
      "Batch loss: 0.6339809894561768 batch: 771/840\n",
      "Batch loss: 0.5388587713241577 batch: 772/840\n",
      "Batch loss: 0.5467380285263062 batch: 773/840\n",
      "Batch loss: 0.5055928230285645 batch: 774/840\n",
      "Batch loss: 0.5798878073692322 batch: 775/840\n",
      "Batch loss: 0.6065549254417419 batch: 776/840\n",
      "Batch loss: 0.5407194495201111 batch: 777/840\n",
      "Batch loss: 0.44620567560195923 batch: 778/840\n",
      "Batch loss: 0.6831965446472168 batch: 779/840\n",
      "Batch loss: 0.583029568195343 batch: 780/840\n",
      "Batch loss: 0.6099979281425476 batch: 781/840\n",
      "Batch loss: 0.5408332943916321 batch: 782/840\n",
      "Batch loss: 0.4399915635585785 batch: 783/840\n",
      "Batch loss: 0.6496766805648804 batch: 784/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5507640242576599 batch: 785/840\n",
      "Batch loss: 0.6922746300697327 batch: 786/840\n",
      "Batch loss: 0.5607686042785645 batch: 787/840\n",
      "Batch loss: 0.6133044362068176 batch: 788/840\n",
      "Batch loss: 0.7778906226158142 batch: 789/840\n",
      "Batch loss: 0.6819035410881042 batch: 790/840\n",
      "Batch loss: 0.5766227841377258 batch: 791/840\n",
      "Batch loss: 0.3831547200679779 batch: 792/840\n",
      "Batch loss: 0.6479565501213074 batch: 793/840\n",
      "Batch loss: 0.6930228471755981 batch: 794/840\n",
      "Batch loss: 0.6564703583717346 batch: 795/840\n",
      "Batch loss: 0.7637691497802734 batch: 796/840\n",
      "Batch loss: 0.6773735880851746 batch: 797/840\n",
      "Batch loss: 0.5854198932647705 batch: 798/840\n",
      "Batch loss: 0.5863327980041504 batch: 799/840\n",
      "Batch loss: 0.5609017014503479 batch: 800/840\n",
      "Batch loss: 0.7572056651115417 batch: 801/840\n",
      "Batch loss: 0.5263804197311401 batch: 802/840\n",
      "Batch loss: 0.5134181380271912 batch: 803/840\n",
      "Batch loss: 0.7484831213951111 batch: 804/840\n",
      "Batch loss: 0.5787115693092346 batch: 805/840\n",
      "Batch loss: 0.6113894581794739 batch: 806/840\n",
      "Batch loss: 0.6210883855819702 batch: 807/840\n",
      "Batch loss: 0.7202314138412476 batch: 808/840\n",
      "Batch loss: 0.5570850968360901 batch: 809/840\n",
      "Batch loss: 0.5343998670578003 batch: 810/840\n",
      "Batch loss: 0.5468782186508179 batch: 811/840\n",
      "Batch loss: 0.5960303544998169 batch: 812/840\n",
      "Batch loss: 0.5229883193969727 batch: 813/840\n",
      "Batch loss: 0.6990527510643005 batch: 814/840\n",
      "Batch loss: 0.6368551850318909 batch: 815/840\n",
      "Batch loss: 0.6107497215270996 batch: 816/840\n",
      "Batch loss: 0.6708396673202515 batch: 817/840\n",
      "Batch loss: 0.6992252469062805 batch: 818/840\n",
      "Batch loss: 0.47287991642951965 batch: 819/840\n",
      "Batch loss: 0.6510565876960754 batch: 820/840\n",
      "Batch loss: 0.6185251474380493 batch: 821/840\n",
      "Batch loss: 0.6285527348518372 batch: 822/840\n",
      "Batch loss: 0.6929678320884705 batch: 823/840\n",
      "Batch loss: 0.678557276725769 batch: 824/840\n",
      "Batch loss: 0.7349182963371277 batch: 825/840\n",
      "Batch loss: 0.5646944642066956 batch: 826/840\n",
      "Batch loss: 0.593255341053009 batch: 827/840\n",
      "Batch loss: 0.7133530974388123 batch: 828/840\n",
      "Batch loss: 0.6338998079299927 batch: 829/840\n",
      "Batch loss: 0.6362488269805908 batch: 830/840\n",
      "Batch loss: 0.62517249584198 batch: 831/840\n",
      "Batch loss: 0.6545860767364502 batch: 832/840\n",
      "Batch loss: 0.7156873941421509 batch: 833/840\n",
      "Batch loss: 0.654600977897644 batch: 834/840\n",
      "Batch loss: 0.5639805793762207 batch: 835/840\n",
      "Batch loss: 0.6401152014732361 batch: 836/840\n",
      "Batch loss: 0.5936282873153687 batch: 837/840\n",
      "Batch loss: 0.7049600481987 batch: 838/840\n",
      "Batch loss: 0.5707842707633972 batch: 839/840\n",
      "Batch loss: 0.6732403635978699 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 9/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.834\n",
      "Running epoch 10/15\n",
      "Batch loss: 0.5156034827232361 batch: 1/840\n",
      "Batch loss: 0.9341583251953125 batch: 2/840\n",
      "Batch loss: 0.6470323801040649 batch: 3/840\n",
      "Batch loss: 0.5626633167266846 batch: 4/840\n",
      "Batch loss: 0.6379145979881287 batch: 5/840\n",
      "Batch loss: 0.48413124680519104 batch: 6/840\n",
      "Batch loss: 0.5442413687705994 batch: 7/840\n",
      "Batch loss: 0.7210080027580261 batch: 8/840\n",
      "Batch loss: 0.5164779424667358 batch: 9/840\n",
      "Batch loss: 0.5880047082901001 batch: 10/840\n",
      "Batch loss: 0.5885134339332581 batch: 11/840\n",
      "Batch loss: 0.5477438569068909 batch: 12/840\n",
      "Batch loss: 0.5649979114532471 batch: 13/840\n",
      "Batch loss: 0.6307011842727661 batch: 14/840\n",
      "Batch loss: 0.6549804210662842 batch: 15/840\n",
      "Batch loss: 0.49406468868255615 batch: 16/840\n",
      "Batch loss: 0.4550822377204895 batch: 17/840\n",
      "Batch loss: 0.7457714080810547 batch: 18/840\n",
      "Batch loss: 0.7046574950218201 batch: 19/840\n",
      "Batch loss: 0.7068007588386536 batch: 20/840\n",
      "Batch loss: 0.6761229038238525 batch: 21/840\n",
      "Batch loss: 0.5820375680923462 batch: 22/840\n",
      "Batch loss: 0.6032527685165405 batch: 23/840\n",
      "Batch loss: 0.516638994216919 batch: 24/840\n",
      "Batch loss: 0.5061919689178467 batch: 25/840\n",
      "Batch loss: 0.6978904604911804 batch: 26/840\n",
      "Batch loss: 0.6445889472961426 batch: 27/840\n",
      "Batch loss: 0.6575186252593994 batch: 28/840\n",
      "Batch loss: 0.7240080237388611 batch: 29/840\n",
      "Batch loss: 0.5249836444854736 batch: 30/840\n",
      "Batch loss: 0.5960837006568909 batch: 31/840\n",
      "Batch loss: 0.5918891429901123 batch: 32/840\n",
      "Batch loss: 0.5885050296783447 batch: 33/840\n",
      "Batch loss: 0.5906667709350586 batch: 34/840\n",
      "Batch loss: 0.47565406560897827 batch: 35/840\n",
      "Batch loss: 0.4652889668941498 batch: 36/840\n",
      "Batch loss: 0.6947287917137146 batch: 37/840\n",
      "Batch loss: 0.6632173657417297 batch: 38/840\n",
      "Batch loss: 0.6315122842788696 batch: 39/840\n",
      "Batch loss: 0.5887441039085388 batch: 40/840\n",
      "Batch loss: 0.7262008786201477 batch: 41/840\n",
      "Batch loss: 0.606738269329071 batch: 42/840\n",
      "Batch loss: 0.6513682007789612 batch: 43/840\n",
      "Batch loss: 0.6474194526672363 batch: 44/840\n",
      "Batch loss: 0.6365832090377808 batch: 45/840\n",
      "Batch loss: 0.49882152676582336 batch: 46/840\n",
      "Batch loss: 0.4427679181098938 batch: 47/840\n",
      "Batch loss: 0.7035892605781555 batch: 48/840\n",
      "Batch loss: 0.6302986145019531 batch: 49/840\n",
      "Batch loss: 0.6806619763374329 batch: 50/840\n",
      "Batch loss: 0.6796970367431641 batch: 51/840\n",
      "Batch loss: 0.7780486941337585 batch: 52/840\n",
      "Batch loss: 0.6325130462646484 batch: 53/840\n",
      "Batch loss: 0.6437392234802246 batch: 54/840\n",
      "Batch loss: 0.6931252479553223 batch: 55/840\n",
      "Batch loss: 0.5292301774024963 batch: 56/840\n",
      "Batch loss: 0.5974635481834412 batch: 57/840\n",
      "Batch loss: 0.5221115946769714 batch: 58/840\n",
      "Batch loss: 0.5576427578926086 batch: 59/840\n",
      "Batch loss: 0.5578440427780151 batch: 60/840\n",
      "Batch loss: 0.6524484157562256 batch: 61/840\n",
      "Batch loss: 0.7135108709335327 batch: 62/840\n",
      "Batch loss: 0.6336075067520142 batch: 63/840\n",
      "Batch loss: 0.5895082354545593 batch: 64/840\n",
      "Batch loss: 0.5790234804153442 batch: 65/840\n",
      "Batch loss: 0.5617774128913879 batch: 66/840\n",
      "Batch loss: 0.6852574944496155 batch: 67/840\n",
      "Batch loss: 0.5736647844314575 batch: 68/840\n",
      "Batch loss: 0.6803371906280518 batch: 69/840\n",
      "Batch loss: 0.563851535320282 batch: 70/840\n",
      "Batch loss: 0.794110119342804 batch: 71/840\n",
      "Batch loss: 0.6777949333190918 batch: 72/840\n",
      "Batch loss: 0.6254041790962219 batch: 73/840\n",
      "Batch loss: 0.6126729249954224 batch: 74/840\n",
      "Batch loss: 0.6747308969497681 batch: 75/840\n",
      "Batch loss: 0.44298475980758667 batch: 76/840\n",
      "Batch loss: 0.5798364281654358 batch: 77/840\n",
      "Batch loss: 0.826558530330658 batch: 78/840\n",
      "Batch loss: 0.5963432192802429 batch: 79/840\n",
      "Batch loss: 0.6516745686531067 batch: 80/840\n",
      "Batch loss: 0.557762861251831 batch: 81/840\n",
      "Batch loss: 0.6632461547851562 batch: 82/840\n",
      "Batch loss: 0.5036998987197876 batch: 83/840\n",
      "Batch loss: 0.7683155536651611 batch: 84/840\n",
      "Batch loss: 0.6274816393852234 batch: 85/840\n",
      "Batch loss: 0.9212642908096313 batch: 86/840\n",
      "Batch loss: 0.5485422015190125 batch: 87/840\n",
      "Batch loss: 0.5050503611564636 batch: 88/840\n",
      "Batch loss: 0.49499690532684326 batch: 89/840\n",
      "Batch loss: 0.5202465057373047 batch: 90/840\n",
      "Batch loss: 0.5828481316566467 batch: 91/840\n",
      "Batch loss: 0.6644712090492249 batch: 92/840\n",
      "Batch loss: 0.7447974681854248 batch: 93/840\n",
      "Batch loss: 0.6908990740776062 batch: 94/840\n",
      "Batch loss: 0.6392658352851868 batch: 95/840\n",
      "Batch loss: 0.5595837831497192 batch: 96/840\n",
      "Batch loss: 0.6030725836753845 batch: 97/840\n",
      "Batch loss: 0.7563343048095703 batch: 98/840\n",
      "Batch loss: 0.5927428007125854 batch: 99/840\n",
      "Batch loss: 0.5480600595474243 batch: 100/840\n",
      "Batch loss: 0.5144731998443604 batch: 101/840\n",
      "Batch loss: 0.5252973437309265 batch: 102/840\n",
      "Batch loss: 0.6481150984764099 batch: 103/840\n",
      "Batch loss: 0.5125983953475952 batch: 104/840\n",
      "Batch loss: 0.5305662155151367 batch: 105/840\n",
      "Batch loss: 0.6830368638038635 batch: 106/840\n",
      "Batch loss: 0.5619137287139893 batch: 107/840\n",
      "Batch loss: 0.7414697408676147 batch: 108/840\n",
      "Batch loss: 0.6020026206970215 batch: 109/840\n",
      "Batch loss: 0.5692604184150696 batch: 110/840\n",
      "Batch loss: 0.5318817496299744 batch: 111/840\n",
      "Batch loss: 0.8098549842834473 batch: 112/840\n",
      "Batch loss: 0.6526928544044495 batch: 113/840\n",
      "Batch loss: 0.5297205448150635 batch: 114/840\n",
      "Batch loss: 0.6431067585945129 batch: 115/840\n",
      "Batch loss: 0.5252368450164795 batch: 116/840\n",
      "Batch loss: 0.5854933261871338 batch: 117/840\n",
      "Batch loss: 0.43133145570755005 batch: 118/840\n",
      "Batch loss: 0.7354788184165955 batch: 119/840\n",
      "Batch loss: 0.681439995765686 batch: 120/840\n",
      "Batch loss: 0.7172375321388245 batch: 121/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.85808265209198 batch: 122/840\n",
      "Batch loss: 0.47007134556770325 batch: 123/840\n",
      "Batch loss: 0.6022940874099731 batch: 124/840\n",
      "Batch loss: 0.5806018710136414 batch: 125/840\n",
      "Batch loss: 0.6389834880828857 batch: 126/840\n",
      "Batch loss: 0.6662684082984924 batch: 127/840\n",
      "Batch loss: 0.611802339553833 batch: 128/840\n",
      "Batch loss: 0.7805414795875549 batch: 129/840\n",
      "Batch loss: 0.6538532376289368 batch: 130/840\n",
      "Batch loss: 0.6897634267807007 batch: 131/840\n",
      "Batch loss: 1.1014901399612427 batch: 132/840\n",
      "Batch loss: 0.7001873254776001 batch: 133/840\n",
      "Batch loss: 0.5872253179550171 batch: 134/840\n",
      "Batch loss: 0.5693249106407166 batch: 135/840\n",
      "Batch loss: 0.728101909160614 batch: 136/840\n",
      "Batch loss: 0.5534405708312988 batch: 137/840\n",
      "Batch loss: 0.4803513288497925 batch: 138/840\n",
      "Batch loss: 0.678789496421814 batch: 139/840\n",
      "Batch loss: 0.6421746611595154 batch: 140/840\n",
      "Batch loss: 0.5144826769828796 batch: 141/840\n",
      "Batch loss: 0.5039138197898865 batch: 142/840\n",
      "Batch loss: 0.48080652952194214 batch: 143/840\n",
      "Batch loss: 0.5809071660041809 batch: 144/840\n",
      "Batch loss: 0.6554514169692993 batch: 145/840\n",
      "Batch loss: 0.6502225995063782 batch: 146/840\n",
      "Batch loss: 0.5442054271697998 batch: 147/840\n",
      "Batch loss: 0.7705468535423279 batch: 148/840\n",
      "Batch loss: 0.7098168730735779 batch: 149/840\n",
      "Batch loss: 0.7226535677909851 batch: 150/840\n",
      "Batch loss: 0.4311835765838623 batch: 151/840\n",
      "Batch loss: 0.6196616291999817 batch: 152/840\n",
      "Batch loss: 0.46357443928718567 batch: 153/840\n",
      "Batch loss: 0.6084699034690857 batch: 154/840\n",
      "Batch loss: 0.5426554679870605 batch: 155/840\n",
      "Batch loss: 0.6472299695014954 batch: 156/840\n",
      "Batch loss: 0.6951054930686951 batch: 157/840\n",
      "Batch loss: 0.4946034550666809 batch: 158/840\n",
      "Batch loss: 0.5082846283912659 batch: 159/840\n",
      "Batch loss: 0.5373414754867554 batch: 160/840\n",
      "Batch loss: 0.710702121257782 batch: 161/840\n",
      "Batch loss: 0.6031584739685059 batch: 162/840\n",
      "Batch loss: 0.7258912920951843 batch: 163/840\n",
      "Batch loss: 0.4631927013397217 batch: 164/840\n",
      "Batch loss: 0.6078360080718994 batch: 165/840\n",
      "Batch loss: 0.5554391741752625 batch: 166/840\n",
      "Batch loss: 0.6997144818305969 batch: 167/840\n",
      "Batch loss: 0.6293696761131287 batch: 168/840\n",
      "Batch loss: 0.5949747562408447 batch: 169/840\n",
      "Batch loss: 0.7539219856262207 batch: 170/840\n",
      "Batch loss: 0.6262609958648682 batch: 171/840\n",
      "Batch loss: 0.7953009009361267 batch: 172/840\n",
      "Batch loss: 0.5457433462142944 batch: 173/840\n",
      "Batch loss: 0.5086389183998108 batch: 174/840\n",
      "Batch loss: 0.4754925072193146 batch: 175/840\n",
      "Batch loss: 0.6239048838615417 batch: 176/840\n",
      "Batch loss: 0.650941014289856 batch: 177/840\n",
      "Batch loss: 0.7013711333274841 batch: 178/840\n",
      "Batch loss: 0.7786467671394348 batch: 179/840\n",
      "Batch loss: 0.5436460375785828 batch: 180/840\n",
      "Batch loss: 0.570050060749054 batch: 181/840\n",
      "Batch loss: 0.5581393241882324 batch: 182/840\n",
      "Batch loss: 0.6594743132591248 batch: 183/840\n",
      "Batch loss: 0.5624778270721436 batch: 184/840\n",
      "Batch loss: 0.4970320463180542 batch: 185/840\n",
      "Batch loss: 0.47438767552375793 batch: 186/840\n",
      "Batch loss: 0.7411839962005615 batch: 187/840\n",
      "Batch loss: 0.5577676296234131 batch: 188/840\n",
      "Batch loss: 0.581987202167511 batch: 189/840\n",
      "Batch loss: 0.62544846534729 batch: 190/840\n",
      "Batch loss: 0.8190702795982361 batch: 191/840\n",
      "Batch loss: 0.48560860753059387 batch: 192/840\n",
      "Batch loss: 0.47107642889022827 batch: 193/840\n",
      "Batch loss: 0.465311199426651 batch: 194/840\n",
      "Batch loss: 0.5066946148872375 batch: 195/840\n",
      "Batch loss: 0.7612977623939514 batch: 196/840\n",
      "Batch loss: 0.5883588194847107 batch: 197/840\n",
      "Batch loss: 0.6426523327827454 batch: 198/840\n",
      "Batch loss: 0.5752250552177429 batch: 199/840\n",
      "Batch loss: 0.8933876752853394 batch: 200/840\n",
      "Batch loss: 0.45950251817703247 batch: 201/840\n",
      "Batch loss: 0.5450718402862549 batch: 202/840\n",
      "Batch loss: 0.6489993929862976 batch: 203/840\n",
      "Batch loss: 0.7386883497238159 batch: 204/840\n",
      "Batch loss: 0.7348618507385254 batch: 205/840\n",
      "Batch loss: 0.555081307888031 batch: 206/840\n",
      "Batch loss: 0.5973495841026306 batch: 207/840\n",
      "Batch loss: 0.6048895716667175 batch: 208/840\n",
      "Batch loss: 0.5476621389389038 batch: 209/840\n",
      "Batch loss: 0.5602977871894836 batch: 210/840\n",
      "Batch loss: 0.5392705798149109 batch: 211/840\n",
      "Batch loss: 0.5472573637962341 batch: 212/840\n",
      "Batch loss: 0.7398367524147034 batch: 213/840\n",
      "Batch loss: 0.7710826992988586 batch: 214/840\n",
      "Batch loss: 0.6474626064300537 batch: 215/840\n",
      "Batch loss: 0.6350417137145996 batch: 216/840\n",
      "Batch loss: 0.5832796096801758 batch: 217/840\n",
      "Batch loss: 0.6468190550804138 batch: 218/840\n",
      "Batch loss: 0.6628109216690063 batch: 219/840\n",
      "Batch loss: 0.731863260269165 batch: 220/840\n",
      "Batch loss: 0.6052515506744385 batch: 221/840\n",
      "Batch loss: 0.7152622938156128 batch: 222/840\n",
      "Batch loss: 0.5620042681694031 batch: 223/840\n",
      "Batch loss: 0.731839120388031 batch: 224/840\n",
      "Batch loss: 0.7188791632652283 batch: 225/840\n",
      "Batch loss: 0.7566434741020203 batch: 226/840\n",
      "Batch loss: 0.6433821320533752 batch: 227/840\n",
      "Batch loss: 0.4652290642261505 batch: 228/840\n",
      "Batch loss: 0.5143759250640869 batch: 229/840\n",
      "Batch loss: 0.6549944877624512 batch: 230/840\n",
      "Batch loss: 0.5525201559066772 batch: 231/840\n",
      "Batch loss: 0.49014490842819214 batch: 232/840\n",
      "Batch loss: 0.7852862477302551 batch: 233/840\n",
      "Batch loss: 0.6583282351493835 batch: 234/840\n",
      "Batch loss: 0.66693514585495 batch: 235/840\n",
      "Batch loss: 0.6706840395927429 batch: 236/840\n",
      "Batch loss: 0.5459635257720947 batch: 237/840\n",
      "Batch loss: 0.7702005505561829 batch: 238/840\n",
      "Batch loss: 0.6588619351387024 batch: 239/840\n",
      "Batch loss: 0.5923012495040894 batch: 240/840\n",
      "Batch loss: 0.7253470420837402 batch: 241/840\n",
      "Batch loss: 0.6417287588119507 batch: 242/840\n",
      "Batch loss: 0.6200616955757141 batch: 243/840\n",
      "Batch loss: 0.8066010475158691 batch: 244/840\n",
      "Batch loss: 0.43965157866477966 batch: 245/840\n",
      "Batch loss: 0.6451036930084229 batch: 246/840\n",
      "Batch loss: 0.7025034427642822 batch: 247/840\n",
      "Batch loss: 0.647916316986084 batch: 248/840\n",
      "Batch loss: 0.8018209934234619 batch: 249/840\n",
      "Batch loss: 0.43869999051094055 batch: 250/840\n",
      "Batch loss: 0.6212867498397827 batch: 251/840\n",
      "Batch loss: 0.5892409682273865 batch: 252/840\n",
      "Batch loss: 0.6034302115440369 batch: 253/840\n",
      "Batch loss: 0.6965065598487854 batch: 254/840\n",
      "Batch loss: 0.5048626065254211 batch: 255/840\n",
      "Batch loss: 0.7076438665390015 batch: 256/840\n",
      "Batch loss: 0.600963830947876 batch: 257/840\n",
      "Batch loss: 0.7468297481536865 batch: 258/840\n",
      "Batch loss: 0.5696364045143127 batch: 259/840\n",
      "Batch loss: 0.5102096796035767 batch: 260/840\n",
      "Batch loss: 0.5477589964866638 batch: 261/840\n",
      "Batch loss: 0.4164660573005676 batch: 262/840\n",
      "Batch loss: 0.6571597456932068 batch: 263/840\n",
      "Batch loss: 0.5781073570251465 batch: 264/840\n",
      "Batch loss: 0.6601701974868774 batch: 265/840\n",
      "Batch loss: 0.5084666609764099 batch: 266/840\n",
      "Batch loss: 0.6811909675598145 batch: 267/840\n",
      "Batch loss: 0.5724907517433167 batch: 268/840\n",
      "Batch loss: 0.48473864793777466 batch: 269/840\n",
      "Batch loss: 0.5934624075889587 batch: 270/840\n",
      "Batch loss: 0.4868421256542206 batch: 271/840\n",
      "Batch loss: 0.8137737512588501 batch: 272/840\n",
      "Batch loss: 0.7279167771339417 batch: 273/840\n",
      "Batch loss: 0.7796854376792908 batch: 274/840\n",
      "Batch loss: 0.6824537515640259 batch: 275/840\n",
      "Batch loss: 0.480102002620697 batch: 276/840\n",
      "Batch loss: 0.6255128979682922 batch: 277/840\n",
      "Batch loss: 0.8515970706939697 batch: 278/840\n",
      "Batch loss: 0.7094508409500122 batch: 279/840\n",
      "Batch loss: 0.6763995289802551 batch: 280/840\n",
      "Batch loss: 0.5603367686271667 batch: 281/840\n",
      "Batch loss: 0.5352768301963806 batch: 282/840\n",
      "Batch loss: 0.7089381217956543 batch: 283/840\n",
      "Batch loss: 0.4373922646045685 batch: 284/840\n",
      "Batch loss: 0.500007688999176 batch: 285/840\n",
      "Batch loss: 0.5678704977035522 batch: 286/840\n",
      "Batch loss: 0.4730653464794159 batch: 287/840\n",
      "Batch loss: 0.5723750591278076 batch: 288/840\n",
      "Batch loss: 0.7724435329437256 batch: 289/840\n",
      "Batch loss: 0.8841294646263123 batch: 290/840\n",
      "Batch loss: 0.7487313747406006 batch: 291/840\n",
      "Batch loss: 0.6323630809783936 batch: 292/840\n",
      "Batch loss: 0.7307391166687012 batch: 293/840\n",
      "Batch loss: 0.6075073480606079 batch: 294/840\n",
      "Batch loss: 0.5237523913383484 batch: 295/840\n",
      "Batch loss: 0.6780654191970825 batch: 296/840\n",
      "Batch loss: 0.6344388723373413 batch: 297/840\n",
      "Batch loss: 0.7120099663734436 batch: 298/840\n",
      "Batch loss: 0.5571194291114807 batch: 299/840\n",
      "Batch loss: 0.7308780550956726 batch: 300/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7248691320419312 batch: 301/840\n",
      "Batch loss: 0.660478949546814 batch: 302/840\n",
      "Batch loss: 0.6644818186759949 batch: 303/840\n",
      "Batch loss: 0.4748244881629944 batch: 304/840\n",
      "Batch loss: 0.5441550612449646 batch: 305/840\n",
      "Batch loss: 0.6619676947593689 batch: 306/840\n",
      "Batch loss: 0.6131755709648132 batch: 307/840\n",
      "Batch loss: 0.6947618126869202 batch: 308/840\n",
      "Batch loss: 0.600753128528595 batch: 309/840\n",
      "Batch loss: 0.9204639196395874 batch: 310/840\n",
      "Batch loss: 0.7619420886039734 batch: 311/840\n",
      "Batch loss: 0.6731660962104797 batch: 312/840\n",
      "Batch loss: 0.645439863204956 batch: 313/840\n",
      "Batch loss: 0.5992175340652466 batch: 314/840\n",
      "Batch loss: 0.6717221736907959 batch: 315/840\n",
      "Batch loss: 0.41551512479782104 batch: 316/840\n",
      "Batch loss: 0.6989560127258301 batch: 317/840\n",
      "Batch loss: 0.7524411678314209 batch: 318/840\n",
      "Batch loss: 0.7142200469970703 batch: 319/840\n",
      "Batch loss: 0.5410881042480469 batch: 320/840\n",
      "Batch loss: 0.5324452519416809 batch: 321/840\n",
      "Batch loss: 0.7725279927253723 batch: 322/840\n",
      "Batch loss: 0.745714008808136 batch: 323/840\n",
      "Batch loss: 0.5405919551849365 batch: 324/840\n",
      "Batch loss: 0.42661240696907043 batch: 325/840\n",
      "Batch loss: 0.6172152757644653 batch: 326/840\n",
      "Batch loss: 0.5136899948120117 batch: 327/840\n",
      "Batch loss: 0.8008798956871033 batch: 328/840\n",
      "Batch loss: 0.6574237942695618 batch: 329/840\n",
      "Batch loss: 0.6696288585662842 batch: 330/840\n",
      "Batch loss: 0.6591758131980896 batch: 331/840\n",
      "Batch loss: 0.6891536712646484 batch: 332/840\n",
      "Batch loss: 0.5991501808166504 batch: 333/840\n",
      "Batch loss: 0.6305783987045288 batch: 334/840\n",
      "Batch loss: 0.5726598501205444 batch: 335/840\n",
      "Batch loss: 0.6874281167984009 batch: 336/840\n",
      "Batch loss: 0.8549606800079346 batch: 337/840\n",
      "Batch loss: 0.6549007296562195 batch: 338/840\n",
      "Batch loss: 0.5408294200897217 batch: 339/840\n",
      "Batch loss: 0.7732564806938171 batch: 340/840\n",
      "Batch loss: 0.5445316433906555 batch: 341/840\n",
      "Batch loss: 0.48168304562568665 batch: 342/840\n",
      "Batch loss: 0.9169927835464478 batch: 343/840\n",
      "Batch loss: 0.5897066593170166 batch: 344/840\n",
      "Batch loss: 0.47626614570617676 batch: 345/840\n",
      "Batch loss: 0.6149392127990723 batch: 346/840\n",
      "Batch loss: 0.694369912147522 batch: 347/840\n",
      "Batch loss: 0.6418939232826233 batch: 348/840\n",
      "Batch loss: 0.7429834008216858 batch: 349/840\n",
      "Batch loss: 0.5560045838356018 batch: 350/840\n",
      "Batch loss: 0.6497114300727844 batch: 351/840\n",
      "Batch loss: 0.5518596172332764 batch: 352/840\n",
      "Batch loss: 0.7561669945716858 batch: 353/840\n",
      "Batch loss: 0.6491620540618896 batch: 354/840\n",
      "Batch loss: 0.5802427530288696 batch: 355/840\n",
      "Batch loss: 0.5394189357757568 batch: 356/840\n",
      "Batch loss: 0.5457513332366943 batch: 357/840\n",
      "Batch loss: 0.8576971292495728 batch: 358/840\n",
      "Batch loss: 0.5921418070793152 batch: 359/840\n",
      "Batch loss: 0.7934030294418335 batch: 360/840\n",
      "Batch loss: 0.7579109072685242 batch: 361/840\n",
      "Batch loss: 0.6141137480735779 batch: 362/840\n",
      "Batch loss: 0.5443708300590515 batch: 363/840\n",
      "Batch loss: 0.7237794399261475 batch: 364/840\n",
      "Batch loss: 0.5964557528495789 batch: 365/840\n",
      "Batch loss: 0.6260215640068054 batch: 366/840\n",
      "Batch loss: 0.49365857243537903 batch: 367/840\n",
      "Batch loss: 0.6828106045722961 batch: 368/840\n",
      "Batch loss: 0.6126893758773804 batch: 369/840\n",
      "Batch loss: 0.7683783769607544 batch: 370/840\n",
      "Batch loss: 0.66542649269104 batch: 371/840\n",
      "Batch loss: 0.4618566930294037 batch: 372/840\n",
      "Batch loss: 0.5975565314292908 batch: 373/840\n",
      "Batch loss: 0.6995528936386108 batch: 374/840\n",
      "Batch loss: 0.4640723764896393 batch: 375/840\n",
      "Batch loss: 0.42196109890937805 batch: 376/840\n",
      "Batch loss: 0.6778570413589478 batch: 377/840\n",
      "Batch loss: 0.6461448669433594 batch: 378/840\n",
      "Batch loss: 0.5675933957099915 batch: 379/840\n",
      "Batch loss: 0.8500401377677917 batch: 380/840\n",
      "Batch loss: 1.107835054397583 batch: 381/840\n",
      "Batch loss: 0.6730055212974548 batch: 382/840\n",
      "Batch loss: 0.7037563323974609 batch: 383/840\n",
      "Batch loss: 0.6855656504631042 batch: 384/840\n",
      "Batch loss: 0.7334461212158203 batch: 385/840\n",
      "Batch loss: 0.7022340893745422 batch: 386/840\n",
      "Batch loss: 0.5455726981163025 batch: 387/840\n",
      "Batch loss: 0.6374369263648987 batch: 388/840\n",
      "Batch loss: 0.6057223677635193 batch: 389/840\n",
      "Batch loss: 0.7531105279922485 batch: 390/840\n",
      "Batch loss: 0.6524584293365479 batch: 391/840\n",
      "Batch loss: 0.5584481954574585 batch: 392/840\n",
      "Batch loss: 0.5352826714515686 batch: 393/840\n",
      "Batch loss: 0.7147871255874634 batch: 394/840\n",
      "Batch loss: 0.5341747999191284 batch: 395/840\n",
      "Batch loss: 0.6427792906761169 batch: 396/840\n",
      "Batch loss: 0.6111310720443726 batch: 397/840\n",
      "Batch loss: 0.6519888043403625 batch: 398/840\n",
      "Batch loss: 0.45747125148773193 batch: 399/840\n",
      "Batch loss: 0.5488172173500061 batch: 400/840\n",
      "Batch loss: 0.7603127956390381 batch: 401/840\n",
      "Batch loss: 0.5127843618392944 batch: 402/840\n",
      "Batch loss: 0.598891019821167 batch: 403/840\n",
      "Batch loss: 0.6139239072799683 batch: 404/840\n",
      "Batch loss: 0.5437890887260437 batch: 405/840\n",
      "Batch loss: 0.6584349870681763 batch: 406/840\n",
      "Batch loss: 0.5328506231307983 batch: 407/840\n",
      "Batch loss: 0.7096216082572937 batch: 408/840\n",
      "Batch loss: 0.7576215267181396 batch: 409/840\n",
      "Batch loss: 0.7674565315246582 batch: 410/840\n",
      "Batch loss: 0.5460376739501953 batch: 411/840\n",
      "Batch loss: 0.7310774326324463 batch: 412/840\n",
      "Batch loss: 0.6205249428749084 batch: 413/840\n",
      "Batch loss: 0.6583556532859802 batch: 414/840\n",
      "Batch loss: 0.8577286601066589 batch: 415/840\n",
      "Batch loss: 0.46268534660339355 batch: 416/840\n",
      "Batch loss: 0.7328569293022156 batch: 417/840\n",
      "Batch loss: 0.774833619594574 batch: 418/840\n",
      "Batch loss: 0.7086333632469177 batch: 419/840\n",
      "Batch loss: 0.7012643218040466 batch: 420/840\n",
      "Batch loss: 0.47138717770576477 batch: 421/840\n",
      "Batch loss: 0.5793026685714722 batch: 422/840\n",
      "Batch loss: 0.6418306231498718 batch: 423/840\n",
      "Batch loss: 0.6837193369865417 batch: 424/840\n",
      "Batch loss: 0.7773699164390564 batch: 425/840\n",
      "Batch loss: 0.6499770879745483 batch: 426/840\n",
      "Batch loss: 0.6087520718574524 batch: 427/840\n",
      "Batch loss: 0.723166286945343 batch: 428/840\n",
      "Batch loss: 0.5989924669265747 batch: 429/840\n",
      "Batch loss: 0.8077564835548401 batch: 430/840\n",
      "Batch loss: 0.6737662553787231 batch: 431/840\n",
      "Batch loss: 0.6299004554748535 batch: 432/840\n",
      "Batch loss: 0.4718434810638428 batch: 433/840\n",
      "Batch loss: 0.5994789600372314 batch: 434/840\n",
      "Batch loss: 0.7146170735359192 batch: 435/840\n",
      "Batch loss: 0.7190622091293335 batch: 436/840\n",
      "Batch loss: 0.7028583288192749 batch: 437/840\n",
      "Batch loss: 0.42191755771636963 batch: 438/840\n",
      "Batch loss: 0.6692763566970825 batch: 439/840\n",
      "Batch loss: 0.6835814118385315 batch: 440/840\n",
      "Batch loss: 0.6253061294555664 batch: 441/840\n",
      "Batch loss: 0.7181488275527954 batch: 442/840\n",
      "Batch loss: 0.5524372458457947 batch: 443/840\n",
      "Batch loss: 0.577187716960907 batch: 444/840\n",
      "Batch loss: 0.5305356383323669 batch: 445/840\n",
      "Batch loss: 0.7924652099609375 batch: 446/840\n",
      "Batch loss: 0.8005893230438232 batch: 447/840\n",
      "Batch loss: 0.6786934733390808 batch: 448/840\n",
      "Batch loss: 0.5896832346916199 batch: 449/840\n",
      "Batch loss: 0.6255072355270386 batch: 450/840\n",
      "Batch loss: 0.5718363523483276 batch: 451/840\n",
      "Batch loss: 0.7032983303070068 batch: 452/840\n",
      "Batch loss: 0.6017478704452515 batch: 453/840\n",
      "Batch loss: 0.6018060445785522 batch: 454/840\n",
      "Batch loss: 0.6944901347160339 batch: 455/840\n",
      "Batch loss: 0.6037296652793884 batch: 456/840\n",
      "Batch loss: 0.4834633767604828 batch: 457/840\n",
      "Batch loss: 0.5691044330596924 batch: 458/840\n",
      "Batch loss: 0.5547276139259338 batch: 459/840\n",
      "Batch loss: 0.42298758029937744 batch: 460/840\n",
      "Batch loss: 0.7000934481620789 batch: 461/840\n",
      "Batch loss: 0.616281270980835 batch: 462/840\n",
      "Batch loss: 0.7218050360679626 batch: 463/840\n",
      "Batch loss: 0.5526705384254456 batch: 464/840\n",
      "Batch loss: 0.850565493106842 batch: 465/840\n",
      "Batch loss: 0.6395118236541748 batch: 466/840\n",
      "Batch loss: 0.9431568384170532 batch: 467/840\n",
      "Batch loss: 0.5549964308738708 batch: 468/840\n",
      "Batch loss: 0.5717161893844604 batch: 469/840\n",
      "Batch loss: 0.636761486530304 batch: 470/840\n",
      "Batch loss: 0.6470252275466919 batch: 471/840\n",
      "Batch loss: 0.5970724821090698 batch: 472/840\n",
      "Batch loss: 0.6505590081214905 batch: 473/840\n",
      "Batch loss: 0.6154570579528809 batch: 474/840\n",
      "Batch loss: 0.5800915360450745 batch: 475/840\n",
      "Batch loss: 0.6573874950408936 batch: 476/840\n",
      "Batch loss: 0.5922496318817139 batch: 477/840\n",
      "Batch loss: 0.5509419441223145 batch: 478/840\n",
      "Batch loss: 0.5281808376312256 batch: 479/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6400166153907776 batch: 480/840\n",
      "Batch loss: 0.6102261543273926 batch: 481/840\n",
      "Batch loss: 0.6746479272842407 batch: 482/840\n",
      "Batch loss: 0.854095458984375 batch: 483/840\n",
      "Batch loss: 0.4890179932117462 batch: 484/840\n",
      "Batch loss: 0.6781836748123169 batch: 485/840\n",
      "Batch loss: 0.6319714784622192 batch: 486/840\n",
      "Batch loss: 0.47598525881767273 batch: 487/840\n",
      "Batch loss: 0.6770341396331787 batch: 488/840\n",
      "Batch loss: 0.6584159135818481 batch: 489/840\n",
      "Batch loss: 0.7568680047988892 batch: 490/840\n",
      "Batch loss: 0.3946806788444519 batch: 491/840\n",
      "Batch loss: 0.6279184818267822 batch: 492/840\n",
      "Batch loss: 0.6859356164932251 batch: 493/840\n",
      "Batch loss: 0.34391099214553833 batch: 494/840\n",
      "Batch loss: 0.7006009817123413 batch: 495/840\n",
      "Batch loss: 0.6318884491920471 batch: 496/840\n",
      "Batch loss: 0.8168532848358154 batch: 497/840\n",
      "Batch loss: 0.5404656529426575 batch: 498/840\n",
      "Batch loss: 0.6486827731132507 batch: 499/840\n",
      "Batch loss: 0.5450182557106018 batch: 500/840\n",
      "Batch loss: 0.5592153668403625 batch: 501/840\n",
      "Batch loss: 0.704395592212677 batch: 502/840\n",
      "Batch loss: 0.39704442024230957 batch: 503/840\n",
      "Batch loss: 0.5835890769958496 batch: 504/840\n",
      "Batch loss: 0.6823760867118835 batch: 505/840\n",
      "Batch loss: 0.4935762882232666 batch: 506/840\n",
      "Batch loss: 0.6624924540519714 batch: 507/840\n",
      "Batch loss: 0.47839340567588806 batch: 508/840\n",
      "Batch loss: 0.690491259098053 batch: 509/840\n",
      "Batch loss: 0.6803860664367676 batch: 510/840\n",
      "Batch loss: 0.5494964718818665 batch: 511/840\n",
      "Batch loss: 0.6194460391998291 batch: 512/840\n",
      "Batch loss: 0.5484060645103455 batch: 513/840\n",
      "Batch loss: 0.5938004851341248 batch: 514/840\n",
      "Batch loss: 0.42980873584747314 batch: 515/840\n",
      "Batch loss: 0.6952607035636902 batch: 516/840\n",
      "Batch loss: 0.5565450191497803 batch: 517/840\n",
      "Batch loss: 0.7180718183517456 batch: 518/840\n",
      "Batch loss: 0.7858428955078125 batch: 519/840\n",
      "Batch loss: 0.5875067710876465 batch: 520/840\n",
      "Batch loss: 0.7906810641288757 batch: 521/840\n",
      "Batch loss: 0.7209058403968811 batch: 522/840\n",
      "Batch loss: 0.4643230140209198 batch: 523/840\n",
      "Batch loss: 0.5951334238052368 batch: 524/840\n",
      "Batch loss: 0.6425721049308777 batch: 525/840\n",
      "Batch loss: 0.7842915058135986 batch: 526/840\n",
      "Batch loss: 0.6293084621429443 batch: 527/840\n",
      "Batch loss: 0.5416092872619629 batch: 528/840\n",
      "Batch loss: 0.5831548571586609 batch: 529/840\n",
      "Batch loss: 0.6274606585502625 batch: 530/840\n",
      "Batch loss: 0.5732704401016235 batch: 531/840\n",
      "Batch loss: 0.6270616054534912 batch: 532/840\n",
      "Batch loss: 0.6995614767074585 batch: 533/840\n",
      "Batch loss: 0.6238203048706055 batch: 534/840\n",
      "Batch loss: 0.6965978741645813 batch: 535/840\n",
      "Batch loss: 0.6748632192611694 batch: 536/840\n",
      "Batch loss: 0.6617642045021057 batch: 537/840\n",
      "Batch loss: 0.487862229347229 batch: 538/840\n",
      "Batch loss: 0.5442208647727966 batch: 539/840\n",
      "Batch loss: 0.7587061524391174 batch: 540/840\n",
      "Batch loss: 0.5540359616279602 batch: 541/840\n",
      "Batch loss: 0.5977858304977417 batch: 542/840\n",
      "Batch loss: 0.4832744896411896 batch: 543/840\n",
      "Batch loss: 0.6388744711875916 batch: 544/840\n",
      "Batch loss: 0.6789518594741821 batch: 545/840\n",
      "Batch loss: 0.6129878163337708 batch: 546/840\n",
      "Batch loss: 0.6022384762763977 batch: 547/840\n",
      "Batch loss: 0.4635598659515381 batch: 548/840\n",
      "Batch loss: 0.49367088079452515 batch: 549/840\n",
      "Batch loss: 0.5321460366249084 batch: 550/840\n",
      "Batch loss: 0.669858992099762 batch: 551/840\n",
      "Batch loss: 0.5870617628097534 batch: 552/840\n",
      "Batch loss: 0.7245472073554993 batch: 553/840\n",
      "Batch loss: 0.6582044363021851 batch: 554/840\n",
      "Batch loss: 0.6007446646690369 batch: 555/840\n",
      "Batch loss: 0.6792578101158142 batch: 556/840\n",
      "Batch loss: 0.6296538710594177 batch: 557/840\n",
      "Batch loss: 0.5882282853126526 batch: 558/840\n",
      "Batch loss: 0.6192449927330017 batch: 559/840\n",
      "Batch loss: 0.672254741191864 batch: 560/840\n",
      "Batch loss: 0.5947195887565613 batch: 561/840\n",
      "Batch loss: 0.5249969959259033 batch: 562/840\n",
      "Batch loss: 0.5185790061950684 batch: 563/840\n",
      "Batch loss: 0.578837513923645 batch: 564/840\n",
      "Batch loss: 0.6588268280029297 batch: 565/840\n",
      "Batch loss: 0.7145860195159912 batch: 566/840\n",
      "Batch loss: 0.64548659324646 batch: 567/840\n",
      "Batch loss: 0.5547193288803101 batch: 568/840\n",
      "Batch loss: 0.5698255300521851 batch: 569/840\n",
      "Batch loss: 0.4391598403453827 batch: 570/840\n",
      "Batch loss: 0.6838642954826355 batch: 571/840\n",
      "Batch loss: 0.6220462322235107 batch: 572/840\n",
      "Batch loss: 0.6766452193260193 batch: 573/840\n",
      "Batch loss: 0.7668163180351257 batch: 574/840\n",
      "Batch loss: 0.4624524414539337 batch: 575/840\n",
      "Batch loss: 0.5962066054344177 batch: 576/840\n",
      "Batch loss: 0.5802807807922363 batch: 577/840\n",
      "Batch loss: 0.7130531072616577 batch: 578/840\n",
      "Batch loss: 0.6771485805511475 batch: 579/840\n",
      "Batch loss: 0.8405178785324097 batch: 580/840\n",
      "Batch loss: 0.7453600168228149 batch: 581/840\n",
      "Batch loss: 0.7534856200218201 batch: 582/840\n",
      "Batch loss: 0.5842110514640808 batch: 583/840\n",
      "Batch loss: 0.7460736036300659 batch: 584/840\n",
      "Batch loss: 0.6566017866134644 batch: 585/840\n",
      "Batch loss: 0.6030992865562439 batch: 586/840\n",
      "Batch loss: 0.5768106579780579 batch: 587/840\n",
      "Batch loss: 0.6181769371032715 batch: 588/840\n",
      "Batch loss: 0.6171802878379822 batch: 589/840\n",
      "Batch loss: 0.4962034225463867 batch: 590/840\n",
      "Batch loss: 0.5244624614715576 batch: 591/840\n",
      "Batch loss: 0.5650731325149536 batch: 592/840\n",
      "Batch loss: 0.7383825182914734 batch: 593/840\n",
      "Batch loss: 0.6676669120788574 batch: 594/840\n",
      "Batch loss: 0.37350308895111084 batch: 595/840\n",
      "Batch loss: 0.5296244025230408 batch: 596/840\n",
      "Batch loss: 0.5061792731285095 batch: 597/840\n",
      "Batch loss: 0.4511631727218628 batch: 598/840\n",
      "Batch loss: 0.5059530735015869 batch: 599/840\n",
      "Batch loss: 0.6426483392715454 batch: 600/840\n",
      "Batch loss: 0.8198427557945251 batch: 601/840\n",
      "Batch loss: 0.5631794333457947 batch: 602/840\n",
      "Batch loss: 0.6413595080375671 batch: 603/840\n",
      "Batch loss: 0.7092986106872559 batch: 604/840\n",
      "Batch loss: 0.6953784823417664 batch: 605/840\n",
      "Batch loss: 0.74864661693573 batch: 606/840\n",
      "Batch loss: 0.7407167553901672 batch: 607/840\n",
      "Batch loss: 0.6345645785331726 batch: 608/840\n",
      "Batch loss: 0.5156901478767395 batch: 609/840\n",
      "Batch loss: 0.7406778931617737 batch: 610/840\n",
      "Batch loss: 0.6792089343070984 batch: 611/840\n",
      "Batch loss: 0.7626140713691711 batch: 612/840\n",
      "Batch loss: 0.6776488423347473 batch: 613/840\n",
      "Batch loss: 0.7079627513885498 batch: 614/840\n",
      "Batch loss: 0.5269591212272644 batch: 615/840\n",
      "Batch loss: 0.48758941888809204 batch: 616/840\n",
      "Batch loss: 0.5316817760467529 batch: 617/840\n",
      "Batch loss: 0.7176138162612915 batch: 618/840\n",
      "Batch loss: 0.7788301110267639 batch: 619/840\n",
      "Batch loss: 0.5722135901451111 batch: 620/840\n",
      "Batch loss: 0.6155101656913757 batch: 621/840\n",
      "Batch loss: 0.5978101491928101 batch: 622/840\n",
      "Batch loss: 0.5845263004302979 batch: 623/840\n",
      "Batch loss: 0.6753244996070862 batch: 624/840\n",
      "Batch loss: 0.6955524682998657 batch: 625/840\n",
      "Batch loss: 0.5967716574668884 batch: 626/840\n",
      "Batch loss: 0.5439770221710205 batch: 627/840\n",
      "Batch loss: 0.7463212013244629 batch: 628/840\n",
      "Batch loss: 0.6135264039039612 batch: 629/840\n",
      "Batch loss: 0.5849329829216003 batch: 630/840\n",
      "Batch loss: 0.6378041505813599 batch: 631/840\n",
      "Batch loss: 0.677885890007019 batch: 632/840\n",
      "Batch loss: 0.6129451990127563 batch: 633/840\n",
      "Batch loss: 0.6252392530441284 batch: 634/840\n",
      "Batch loss: 0.6161195635795593 batch: 635/840\n",
      "Batch loss: 0.6214549541473389 batch: 636/840\n",
      "Batch loss: 0.5188069343566895 batch: 637/840\n",
      "Batch loss: 0.6913556456565857 batch: 638/840\n",
      "Batch loss: 0.6484885215759277 batch: 639/840\n",
      "Batch loss: 0.5717871189117432 batch: 640/840\n",
      "Batch loss: 0.8873674869537354 batch: 641/840\n",
      "Batch loss: 0.5375291109085083 batch: 642/840\n",
      "Batch loss: 0.9642254114151001 batch: 643/840\n",
      "Batch loss: 0.5969851016998291 batch: 644/840\n",
      "Batch loss: 0.5115501880645752 batch: 645/840\n",
      "Batch loss: 0.665250837802887 batch: 646/840\n",
      "Batch loss: 0.6960679888725281 batch: 647/840\n",
      "Batch loss: 0.705376148223877 batch: 648/840\n",
      "Batch loss: 0.6688805222511292 batch: 649/840\n",
      "Batch loss: 0.5876144170761108 batch: 650/840\n",
      "Batch loss: 0.6140949726104736 batch: 651/840\n",
      "Batch loss: 0.6747235059738159 batch: 652/840\n",
      "Batch loss: 0.513200044631958 batch: 653/840\n",
      "Batch loss: 0.7995308041572571 batch: 654/840\n",
      "Batch loss: 0.5486242771148682 batch: 655/840\n",
      "Batch loss: 0.6685651540756226 batch: 656/840\n",
      "Batch loss: 0.5690696835517883 batch: 657/840\n",
      "Batch loss: 0.6598943471908569 batch: 658/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6479200124740601 batch: 659/840\n",
      "Batch loss: 0.6304791569709778 batch: 660/840\n",
      "Batch loss: 0.7200886011123657 batch: 661/840\n",
      "Batch loss: 0.6676404476165771 batch: 662/840\n",
      "Batch loss: 0.5243040919303894 batch: 663/840\n",
      "Batch loss: 0.5975248217582703 batch: 664/840\n",
      "Batch loss: 0.6842653155326843 batch: 665/840\n",
      "Batch loss: 0.6129787564277649 batch: 666/840\n",
      "Batch loss: 0.8465477228164673 batch: 667/840\n",
      "Batch loss: 0.6948869824409485 batch: 668/840\n",
      "Batch loss: 0.6032808423042297 batch: 669/840\n",
      "Batch loss: 0.5751833915710449 batch: 670/840\n",
      "Batch loss: 0.6816482543945312 batch: 671/840\n",
      "Batch loss: 0.5310954451560974 batch: 672/840\n",
      "Batch loss: 0.7622016668319702 batch: 673/840\n",
      "Batch loss: 0.7061046361923218 batch: 674/840\n",
      "Batch loss: 0.609058678150177 batch: 675/840\n",
      "Batch loss: 0.5835261344909668 batch: 676/840\n",
      "Batch loss: 0.6031098365783691 batch: 677/840\n",
      "Batch loss: 0.5985111594200134 batch: 678/840\n",
      "Batch loss: 0.805738091468811 batch: 679/840\n",
      "Batch loss: 0.7073810696601868 batch: 680/840\n",
      "Batch loss: 0.8023204207420349 batch: 681/840\n",
      "Batch loss: 0.6111665964126587 batch: 682/840\n",
      "Batch loss: 0.5723575353622437 batch: 683/840\n",
      "Batch loss: 0.8579539656639099 batch: 684/840\n",
      "Batch loss: 0.6646499037742615 batch: 685/840\n",
      "Batch loss: 0.6933334469795227 batch: 686/840\n",
      "Batch loss: 0.6062986254692078 batch: 687/840\n",
      "Batch loss: 0.6051098108291626 batch: 688/840\n",
      "Batch loss: 0.5512608289718628 batch: 689/840\n",
      "Batch loss: 0.6072788238525391 batch: 690/840\n",
      "Batch loss: 0.5559338331222534 batch: 691/840\n",
      "Batch loss: 0.6547913551330566 batch: 692/840\n",
      "Batch loss: 0.6361003518104553 batch: 693/840\n",
      "Batch loss: 0.8223639726638794 batch: 694/840\n",
      "Batch loss: 0.7462331652641296 batch: 695/840\n",
      "Batch loss: 0.5676196217536926 batch: 696/840\n",
      "Batch loss: 0.6716977953910828 batch: 697/840\n",
      "Batch loss: 0.588202714920044 batch: 698/840\n",
      "Batch loss: 0.6962552070617676 batch: 699/840\n",
      "Batch loss: 0.7223047018051147 batch: 700/840\n",
      "Batch loss: 0.7586857080459595 batch: 701/840\n",
      "Batch loss: 0.6042057275772095 batch: 702/840\n",
      "Batch loss: 0.582892119884491 batch: 703/840\n",
      "Batch loss: 0.6388885378837585 batch: 704/840\n",
      "Batch loss: 0.6441075801849365 batch: 705/840\n",
      "Batch loss: 0.5871047973632812 batch: 706/840\n",
      "Batch loss: 0.6269508004188538 batch: 707/840\n",
      "Batch loss: 0.7537602186203003 batch: 708/840\n",
      "Batch loss: 0.6834703087806702 batch: 709/840\n",
      "Batch loss: 0.69140625 batch: 710/840\n",
      "Batch loss: 0.48029834032058716 batch: 711/840\n",
      "Batch loss: 0.8642103672027588 batch: 712/840\n",
      "Batch loss: 0.5471847057342529 batch: 713/840\n",
      "Batch loss: 0.6095269918441772 batch: 714/840\n",
      "Batch loss: 0.7618958950042725 batch: 715/840\n",
      "Batch loss: 0.7453261613845825 batch: 716/840\n",
      "Batch loss: 0.5867282748222351 batch: 717/840\n",
      "Batch loss: 0.5337288975715637 batch: 718/840\n",
      "Batch loss: 0.41003334522247314 batch: 719/840\n",
      "Batch loss: 0.4887089133262634 batch: 720/840\n",
      "Batch loss: 0.651093065738678 batch: 721/840\n",
      "Batch loss: 0.7319099307060242 batch: 722/840\n",
      "Batch loss: 0.502285897731781 batch: 723/840\n",
      "Batch loss: 0.7033671736717224 batch: 724/840\n",
      "Batch loss: 0.5881521105766296 batch: 725/840\n",
      "Batch loss: 0.5609804391860962 batch: 726/840\n",
      "Batch loss: 0.7733530402183533 batch: 727/840\n",
      "Batch loss: 0.7807145714759827 batch: 728/840\n",
      "Batch loss: 0.7482145428657532 batch: 729/840\n",
      "Batch loss: 0.6646482944488525 batch: 730/840\n",
      "Batch loss: 0.49838733673095703 batch: 731/840\n",
      "Batch loss: 0.8587275743484497 batch: 732/840\n",
      "Batch loss: 0.6673755645751953 batch: 733/840\n",
      "Batch loss: 0.5079648494720459 batch: 734/840\n",
      "Batch loss: 0.5969047546386719 batch: 735/840\n",
      "Batch loss: 0.6648111939430237 batch: 736/840\n",
      "Batch loss: 0.7211116552352905 batch: 737/840\n",
      "Batch loss: 0.5363407731056213 batch: 738/840\n",
      "Batch loss: 0.7482039928436279 batch: 739/840\n",
      "Batch loss: 0.7033200860023499 batch: 740/840\n",
      "Batch loss: 0.5247112512588501 batch: 741/840\n",
      "Batch loss: 0.5526149868965149 batch: 742/840\n",
      "Batch loss: 0.43693894147872925 batch: 743/840\n",
      "Batch loss: 0.5529811978340149 batch: 744/840\n",
      "Batch loss: 0.6023853421211243 batch: 745/840\n",
      "Batch loss: 0.6446006894111633 batch: 746/840\n",
      "Batch loss: 0.7156597375869751 batch: 747/840\n",
      "Batch loss: 0.7544658184051514 batch: 748/840\n",
      "Batch loss: 0.5190081000328064 batch: 749/840\n",
      "Batch loss: 0.5431143045425415 batch: 750/840\n",
      "Batch loss: 0.7175548076629639 batch: 751/840\n",
      "Batch loss: 0.9057502150535583 batch: 752/840\n",
      "Batch loss: 0.47085925936698914 batch: 753/840\n",
      "Batch loss: 0.6089333295822144 batch: 754/840\n",
      "Batch loss: 0.6364467144012451 batch: 755/840\n",
      "Batch loss: 0.5906474590301514 batch: 756/840\n",
      "Batch loss: 0.7192997932434082 batch: 757/840\n",
      "Batch loss: 0.5708943009376526 batch: 758/840\n",
      "Batch loss: 0.5286414623260498 batch: 759/840\n",
      "Batch loss: 0.6024566888809204 batch: 760/840\n",
      "Batch loss: 0.5182852745056152 batch: 761/840\n",
      "Batch loss: 0.6861023902893066 batch: 762/840\n",
      "Batch loss: 0.4679015278816223 batch: 763/840\n",
      "Batch loss: 0.5966505408287048 batch: 764/840\n",
      "Batch loss: 0.46661436557769775 batch: 765/840\n",
      "Batch loss: 0.5410227179527283 batch: 766/840\n",
      "Batch loss: 0.7521384358406067 batch: 767/840\n",
      "Batch loss: 0.6488821506500244 batch: 768/840\n",
      "Batch loss: 0.6214582324028015 batch: 769/840\n",
      "Batch loss: 0.5836074352264404 batch: 770/840\n",
      "Batch loss: 0.6027745008468628 batch: 771/840\n",
      "Batch loss: 0.5540893077850342 batch: 772/840\n",
      "Batch loss: 0.4529048204421997 batch: 773/840\n",
      "Batch loss: 0.43530070781707764 batch: 774/840\n",
      "Batch loss: 0.5337888598442078 batch: 775/840\n",
      "Batch loss: 0.5035765767097473 batch: 776/840\n",
      "Batch loss: 0.5120211243629456 batch: 777/840\n",
      "Batch loss: 0.4454631805419922 batch: 778/840\n",
      "Batch loss: 0.7633280158042908 batch: 779/840\n",
      "Batch loss: 0.722367525100708 batch: 780/840\n",
      "Batch loss: 0.6323766112327576 batch: 781/840\n",
      "Batch loss: 0.5973341464996338 batch: 782/840\n",
      "Batch loss: 0.4813854694366455 batch: 783/840\n",
      "Batch loss: 0.7353065609931946 batch: 784/840\n",
      "Batch loss: 0.5492451786994934 batch: 785/840\n",
      "Batch loss: 0.5896636247634888 batch: 786/840\n",
      "Batch loss: 0.5247074365615845 batch: 787/840\n",
      "Batch loss: 0.7103655934333801 batch: 788/840\n",
      "Batch loss: 0.8111789226531982 batch: 789/840\n",
      "Batch loss: 0.6544373035430908 batch: 790/840\n",
      "Batch loss: 0.5989986658096313 batch: 791/840\n",
      "Batch loss: 0.3720971941947937 batch: 792/840\n",
      "Batch loss: 0.5687772035598755 batch: 793/840\n",
      "Batch loss: 0.46948373317718506 batch: 794/840\n",
      "Batch loss: 0.6355822086334229 batch: 795/840\n",
      "Batch loss: 0.7033043503761292 batch: 796/840\n",
      "Batch loss: 0.7200834155082703 batch: 797/840\n",
      "Batch loss: 0.6240745186805725 batch: 798/840\n",
      "Batch loss: 0.664207935333252 batch: 799/840\n",
      "Batch loss: 0.5373173356056213 batch: 800/840\n",
      "Batch loss: 0.697365403175354 batch: 801/840\n",
      "Batch loss: 0.5407512187957764 batch: 802/840\n",
      "Batch loss: 0.481623113155365 batch: 803/840\n",
      "Batch loss: 0.7082231044769287 batch: 804/840\n",
      "Batch loss: 0.5324283838272095 batch: 805/840\n",
      "Batch loss: 0.6712545156478882 batch: 806/840\n",
      "Batch loss: 0.6272537708282471 batch: 807/840\n",
      "Batch loss: 0.6466551423072815 batch: 808/840\n",
      "Batch loss: 0.7423643469810486 batch: 809/840\n",
      "Batch loss: 0.5139543414115906 batch: 810/840\n",
      "Batch loss: 0.5538565516471863 batch: 811/840\n",
      "Batch loss: 0.6091011762619019 batch: 812/840\n",
      "Batch loss: 0.606443464756012 batch: 813/840\n",
      "Batch loss: 0.5883239507675171 batch: 814/840\n",
      "Batch loss: 0.6952037811279297 batch: 815/840\n",
      "Batch loss: 0.6671690940856934 batch: 816/840\n",
      "Batch loss: 0.6767839193344116 batch: 817/840\n",
      "Batch loss: 0.6396218538284302 batch: 818/840\n",
      "Batch loss: 0.5644533634185791 batch: 819/840\n",
      "Batch loss: 0.7171074748039246 batch: 820/840\n",
      "Batch loss: 0.6634662747383118 batch: 821/840\n",
      "Batch loss: 0.674709141254425 batch: 822/840\n",
      "Batch loss: 0.7202149033546448 batch: 823/840\n",
      "Batch loss: 0.6861310005187988 batch: 824/840\n",
      "Batch loss: 0.7832096219062805 batch: 825/840\n",
      "Batch loss: 0.6008729338645935 batch: 826/840\n",
      "Batch loss: 0.5396113395690918 batch: 827/840\n",
      "Batch loss: 0.6639668345451355 batch: 828/840\n",
      "Batch loss: 0.5453647375106812 batch: 829/840\n",
      "Batch loss: 0.6432309746742249 batch: 830/840\n",
      "Batch loss: 0.646078884601593 batch: 831/840\n",
      "Batch loss: 0.649441123008728 batch: 832/840\n",
      "Batch loss: 0.6421871781349182 batch: 833/840\n",
      "Batch loss: 0.6203076839447021 batch: 834/840\n",
      "Batch loss: 0.5567871332168579 batch: 835/840\n",
      "Batch loss: 0.5497341752052307 batch: 836/840\n",
      "Batch loss: 0.6331059336662292 batch: 837/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7984111905097961 batch: 838/840\n",
      "Batch loss: 0.6110010147094727 batch: 839/840\n",
      "Batch loss: 0.6561132073402405 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 10/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.810\n",
      "Running epoch 11/15\n",
      "Batch loss: 0.48212963342666626 batch: 1/840\n",
      "Batch loss: 1.1387251615524292 batch: 2/840\n",
      "Batch loss: 0.5886335968971252 batch: 3/840\n",
      "Batch loss: 0.7219880819320679 batch: 4/840\n",
      "Batch loss: 0.6514614224433899 batch: 5/840\n",
      "Batch loss: 0.41225555539131165 batch: 6/840\n",
      "Batch loss: 0.5937109589576721 batch: 7/840\n",
      "Batch loss: 0.5579603314399719 batch: 8/840\n",
      "Batch loss: 0.5902275443077087 batch: 9/840\n",
      "Batch loss: 0.619544506072998 batch: 10/840\n",
      "Batch loss: 0.5584364533424377 batch: 11/840\n",
      "Batch loss: 0.5585454702377319 batch: 12/840\n",
      "Batch loss: 0.49924957752227783 batch: 13/840\n",
      "Batch loss: 0.6949397325515747 batch: 14/840\n",
      "Batch loss: 0.5625876784324646 batch: 15/840\n",
      "Batch loss: 0.41041359305381775 batch: 16/840\n",
      "Batch loss: 0.5144999027252197 batch: 17/840\n",
      "Batch loss: 0.6517093181610107 batch: 18/840\n",
      "Batch loss: 0.6933761835098267 batch: 19/840\n",
      "Batch loss: 0.8004652261734009 batch: 20/840\n",
      "Batch loss: 0.7746126055717468 batch: 21/840\n",
      "Batch loss: 0.5430502891540527 batch: 22/840\n",
      "Batch loss: 0.6072236895561218 batch: 23/840\n",
      "Batch loss: 0.5008646249771118 batch: 24/840\n",
      "Batch loss: 0.5199167132377625 batch: 25/840\n",
      "Batch loss: 0.7125617265701294 batch: 26/840\n",
      "Batch loss: 0.7252448797225952 batch: 27/840\n",
      "Batch loss: 0.768647313117981 batch: 28/840\n",
      "Batch loss: 0.7551062703132629 batch: 29/840\n",
      "Batch loss: 0.46983805298805237 batch: 30/840\n",
      "Batch loss: 0.5592632293701172 batch: 31/840\n",
      "Batch loss: 0.5872201323509216 batch: 32/840\n",
      "Batch loss: 0.5794694423675537 batch: 33/840\n",
      "Batch loss: 0.5439127683639526 batch: 34/840\n",
      "Batch loss: 0.5506951808929443 batch: 35/840\n",
      "Batch loss: 0.5168095231056213 batch: 36/840\n",
      "Batch loss: 0.7159707546234131 batch: 37/840\n",
      "Batch loss: 0.6366446018218994 batch: 38/840\n",
      "Batch loss: 0.6265219449996948 batch: 39/840\n",
      "Batch loss: 0.5056718587875366 batch: 40/840\n",
      "Batch loss: 0.8025867342948914 batch: 41/840\n",
      "Batch loss: 0.6640019416809082 batch: 42/840\n",
      "Batch loss: 0.6632906198501587 batch: 43/840\n",
      "Batch loss: 0.6911576986312866 batch: 44/840\n",
      "Batch loss: 0.6978386640548706 batch: 45/840\n",
      "Batch loss: 0.5651458501815796 batch: 46/840\n",
      "Batch loss: 0.5505440831184387 batch: 47/840\n",
      "Batch loss: 0.6312680244445801 batch: 48/840\n",
      "Batch loss: 0.5202031135559082 batch: 49/840\n",
      "Batch loss: 0.628359317779541 batch: 50/840\n",
      "Batch loss: 0.714043378829956 batch: 51/840\n",
      "Batch loss: 0.7080774903297424 batch: 52/840\n",
      "Batch loss: 0.532762885093689 batch: 53/840\n",
      "Batch loss: 0.5417526960372925 batch: 54/840\n",
      "Batch loss: 0.589073121547699 batch: 55/840\n",
      "Batch loss: 0.5343450307846069 batch: 56/840\n",
      "Batch loss: 0.664099931716919 batch: 57/840\n",
      "Batch loss: 0.6139915585517883 batch: 58/840\n",
      "Batch loss: 0.5390165448188782 batch: 59/840\n",
      "Batch loss: 0.49988409876823425 batch: 60/840\n",
      "Batch loss: 0.7869651317596436 batch: 61/840\n",
      "Batch loss: 0.6569085121154785 batch: 62/840\n",
      "Batch loss: 0.6224052309989929 batch: 63/840\n",
      "Batch loss: 0.5836744904518127 batch: 64/840\n",
      "Batch loss: 0.638283371925354 batch: 65/840\n",
      "Batch loss: 0.6478522419929504 batch: 66/840\n",
      "Batch loss: 0.6953718662261963 batch: 67/840\n",
      "Batch loss: 0.5985553860664368 batch: 68/840\n",
      "Batch loss: 0.7054363489151001 batch: 69/840\n",
      "Batch loss: 0.6493691205978394 batch: 70/840\n",
      "Batch loss: 0.7169927954673767 batch: 71/840\n",
      "Batch loss: 0.752501368522644 batch: 72/840\n",
      "Batch loss: 0.6823654770851135 batch: 73/840\n",
      "Batch loss: 0.6028344035148621 batch: 74/840\n",
      "Batch loss: 0.6890268921852112 batch: 75/840\n",
      "Batch loss: 0.4407024085521698 batch: 76/840\n",
      "Batch loss: 0.5686256289482117 batch: 77/840\n",
      "Batch loss: 0.8172934651374817 batch: 78/840\n",
      "Batch loss: 0.5390734672546387 batch: 79/840\n",
      "Batch loss: 0.6021066904067993 batch: 80/840\n",
      "Batch loss: 0.5197808146476746 batch: 81/840\n",
      "Batch loss: 0.6171934008598328 batch: 82/840\n",
      "Batch loss: 0.48892417550086975 batch: 83/840\n",
      "Batch loss: 0.8130548000335693 batch: 84/840\n",
      "Batch loss: 0.6640989184379578 batch: 85/840\n",
      "Batch loss: 0.9094142317771912 batch: 86/840\n",
      "Batch loss: 0.5097246766090393 batch: 87/840\n",
      "Batch loss: 0.5130137801170349 batch: 88/840\n",
      "Batch loss: 0.42213454842567444 batch: 89/840\n",
      "Batch loss: 0.5045348405838013 batch: 90/840\n",
      "Batch loss: 0.6188385486602783 batch: 91/840\n",
      "Batch loss: 0.6417184472084045 batch: 92/840\n",
      "Batch loss: 0.6172778606414795 batch: 93/840\n",
      "Batch loss: 0.55791175365448 batch: 94/840\n",
      "Batch loss: 0.5441178679466248 batch: 95/840\n",
      "Batch loss: 0.6896888613700867 batch: 96/840\n",
      "Batch loss: 0.6596980094909668 batch: 97/840\n",
      "Batch loss: 0.8840702176094055 batch: 98/840\n",
      "Batch loss: 0.514919102191925 batch: 99/840\n",
      "Batch loss: 0.5906257629394531 batch: 100/840\n",
      "Batch loss: 0.5673127770423889 batch: 101/840\n",
      "Batch loss: 0.5282472372055054 batch: 102/840\n",
      "Batch loss: 0.6924615502357483 batch: 103/840\n",
      "Batch loss: 0.511655867099762 batch: 104/840\n",
      "Batch loss: 0.6105637550354004 batch: 105/840\n",
      "Batch loss: 0.7224284410476685 batch: 106/840\n",
      "Batch loss: 0.5322980284690857 batch: 107/840\n",
      "Batch loss: 0.7810952067375183 batch: 108/840\n",
      "Batch loss: 0.7356829643249512 batch: 109/840\n",
      "Batch loss: 0.5319753289222717 batch: 110/840\n",
      "Batch loss: 0.5581657886505127 batch: 111/840\n",
      "Batch loss: 0.6597445011138916 batch: 112/840\n",
      "Batch loss: 0.6975297331809998 batch: 113/840\n",
      "Batch loss: 0.5446218252182007 batch: 114/840\n",
      "Batch loss: 0.583782970905304 batch: 115/840\n",
      "Batch loss: 0.5712478160858154 batch: 116/840\n",
      "Batch loss: 0.6456698775291443 batch: 117/840\n",
      "Batch loss: 0.44062885642051697 batch: 118/840\n",
      "Batch loss: 0.6945160031318665 batch: 119/840\n",
      "Batch loss: 0.49108776450157166 batch: 120/840\n",
      "Batch loss: 0.6688363552093506 batch: 121/840\n",
      "Batch loss: 0.845913827419281 batch: 122/840\n",
      "Batch loss: 0.5275859236717224 batch: 123/840\n",
      "Batch loss: 0.6066744923591614 batch: 124/840\n",
      "Batch loss: 0.5926982760429382 batch: 125/840\n",
      "Batch loss: 0.6152421236038208 batch: 126/840\n",
      "Batch loss: 0.7042284607887268 batch: 127/840\n",
      "Batch loss: 0.6920726299285889 batch: 128/840\n",
      "Batch loss: 0.6570947766304016 batch: 129/840\n",
      "Batch loss: 0.597707986831665 batch: 130/840\n",
      "Batch loss: 0.7142142653465271 batch: 131/840\n",
      "Batch loss: 0.958871066570282 batch: 132/840\n",
      "Batch loss: 0.6194848418235779 batch: 133/840\n",
      "Batch loss: 0.5471434593200684 batch: 134/840\n",
      "Batch loss: 0.4943316578865051 batch: 135/840\n",
      "Batch loss: 0.6974179744720459 batch: 136/840\n",
      "Batch loss: 0.6186684966087341 batch: 137/840\n",
      "Batch loss: 0.46287691593170166 batch: 138/840\n",
      "Batch loss: 0.5030199885368347 batch: 139/840\n",
      "Batch loss: 0.5758378505706787 batch: 140/840\n",
      "Batch loss: 0.4937826097011566 batch: 141/840\n",
      "Batch loss: 0.6110595464706421 batch: 142/840\n",
      "Batch loss: 0.5886669754981995 batch: 143/840\n",
      "Batch loss: 0.5649693608283997 batch: 144/840\n",
      "Batch loss: 0.7084394097328186 batch: 145/840\n",
      "Batch loss: 0.6526944637298584 batch: 146/840\n",
      "Batch loss: 0.4879588782787323 batch: 147/840\n",
      "Batch loss: 0.7186241745948792 batch: 148/840\n",
      "Batch loss: 0.7143006324768066 batch: 149/840\n",
      "Batch loss: 0.508246660232544 batch: 150/840\n",
      "Batch loss: 0.6044313311576843 batch: 151/840\n",
      "Batch loss: 0.6391510963439941 batch: 152/840\n",
      "Batch loss: 0.48625388741493225 batch: 153/840\n",
      "Batch loss: 0.7941035628318787 batch: 154/840\n",
      "Batch loss: 0.5025700330734253 batch: 155/840\n",
      "Batch loss: 0.5183013677597046 batch: 156/840\n",
      "Batch loss: 0.6225952506065369 batch: 157/840\n",
      "Batch loss: 0.5736949443817139 batch: 158/840\n",
      "Batch loss: 0.5302385687828064 batch: 159/840\n",
      "Batch loss: 0.5131946206092834 batch: 160/840\n",
      "Batch loss: 0.6595540642738342 batch: 161/840\n",
      "Batch loss: 0.6147220134735107 batch: 162/840\n",
      "Batch loss: 0.7631446123123169 batch: 163/840\n",
      "Batch loss: 0.5486522912979126 batch: 164/840\n",
      "Batch loss: 0.6516784429550171 batch: 165/840\n",
      "Batch loss: 0.5039762854576111 batch: 166/840\n",
      "Batch loss: 0.6303538680076599 batch: 167/840\n",
      "Batch loss: 0.5760409235954285 batch: 168/840\n",
      "Batch loss: 0.48307299613952637 batch: 169/840\n",
      "Batch loss: 0.7797552347183228 batch: 170/840\n",
      "Batch loss: 0.6339987516403198 batch: 171/840\n",
      "Batch loss: 0.669222891330719 batch: 172/840\n",
      "Batch loss: 0.6361620426177979 batch: 173/840\n",
      "Batch loss: 0.6285378932952881 batch: 174/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5352247357368469 batch: 175/840\n",
      "Batch loss: 0.6610172986984253 batch: 176/840\n",
      "Batch loss: 0.6838523745536804 batch: 177/840\n",
      "Batch loss: 0.6734927296638489 batch: 178/840\n",
      "Batch loss: 0.7160698771476746 batch: 179/840\n",
      "Batch loss: 0.616433322429657 batch: 180/840\n",
      "Batch loss: 0.5423415899276733 batch: 181/840\n",
      "Batch loss: 0.5703869462013245 batch: 182/840\n",
      "Batch loss: 0.6112351417541504 batch: 183/840\n",
      "Batch loss: 0.5383538007736206 batch: 184/840\n",
      "Batch loss: 0.5080475211143494 batch: 185/840\n",
      "Batch loss: 0.5157424807548523 batch: 186/840\n",
      "Batch loss: 0.674416184425354 batch: 187/840\n",
      "Batch loss: 0.5744113922119141 batch: 188/840\n",
      "Batch loss: 0.6316797137260437 batch: 189/840\n",
      "Batch loss: 0.7286738753318787 batch: 190/840\n",
      "Batch loss: 0.7214086055755615 batch: 191/840\n",
      "Batch loss: 0.4670702815055847 batch: 192/840\n",
      "Batch loss: 0.47772225737571716 batch: 193/840\n",
      "Batch loss: 0.3689665198326111 batch: 194/840\n",
      "Batch loss: 0.48119503259658813 batch: 195/840\n",
      "Batch loss: 0.7525362372398376 batch: 196/840\n",
      "Batch loss: 0.636305034160614 batch: 197/840\n",
      "Batch loss: 0.4908089339733124 batch: 198/840\n",
      "Batch loss: 0.5107195973396301 batch: 199/840\n",
      "Batch loss: 0.8036348819732666 batch: 200/840\n",
      "Batch loss: 0.5757729411125183 batch: 201/840\n",
      "Batch loss: 0.6402754783630371 batch: 202/840\n",
      "Batch loss: 0.6475813984870911 batch: 203/840\n",
      "Batch loss: 0.6731213927268982 batch: 204/840\n",
      "Batch loss: 0.6925098896026611 batch: 205/840\n",
      "Batch loss: 0.6133071184158325 batch: 206/840\n",
      "Batch loss: 0.6297067999839783 batch: 207/840\n",
      "Batch loss: 0.7528130412101746 batch: 208/840\n",
      "Batch loss: 0.545138418674469 batch: 209/840\n",
      "Batch loss: 0.5094629526138306 batch: 210/840\n",
      "Batch loss: 0.46279990673065186 batch: 211/840\n",
      "Batch loss: 0.5234050154685974 batch: 212/840\n",
      "Batch loss: 0.7487180233001709 batch: 213/840\n",
      "Batch loss: 0.801124095916748 batch: 214/840\n",
      "Batch loss: 0.595729649066925 batch: 215/840\n",
      "Batch loss: 0.6494072079658508 batch: 216/840\n",
      "Batch loss: 0.6361168026924133 batch: 217/840\n",
      "Batch loss: 0.6502296924591064 batch: 218/840\n",
      "Batch loss: 0.6518279314041138 batch: 219/840\n",
      "Batch loss: 0.8015499711036682 batch: 220/840\n",
      "Batch loss: 0.5896168351173401 batch: 221/840\n",
      "Batch loss: 0.618461549282074 batch: 222/840\n",
      "Batch loss: 0.5730424523353577 batch: 223/840\n",
      "Batch loss: 0.658729612827301 batch: 224/840\n",
      "Batch loss: 0.6740398406982422 batch: 225/840\n",
      "Batch loss: 0.6829239130020142 batch: 226/840\n",
      "Batch loss: 0.8046543002128601 batch: 227/840\n",
      "Batch loss: 0.5831114053726196 batch: 228/840\n",
      "Batch loss: 0.46200647950172424 batch: 229/840\n",
      "Batch loss: 0.5874596238136292 batch: 230/840\n",
      "Batch loss: 0.5458914637565613 batch: 231/840\n",
      "Batch loss: 0.5522717237472534 batch: 232/840\n",
      "Batch loss: 0.7099617719650269 batch: 233/840\n",
      "Batch loss: 0.5777141451835632 batch: 234/840\n",
      "Batch loss: 0.5513278841972351 batch: 235/840\n",
      "Batch loss: 0.6319887042045593 batch: 236/840\n",
      "Batch loss: 0.6185004711151123 batch: 237/840\n",
      "Batch loss: 0.6954331994056702 batch: 238/840\n",
      "Batch loss: 0.6435100436210632 batch: 239/840\n",
      "Batch loss: 0.6522231101989746 batch: 240/840\n",
      "Batch loss: 0.6164042353630066 batch: 241/840\n",
      "Batch loss: 0.5655720829963684 batch: 242/840\n",
      "Batch loss: 0.597773015499115 batch: 243/840\n",
      "Batch loss: 0.6709017157554626 batch: 244/840\n",
      "Batch loss: 0.5329881906509399 batch: 245/840\n",
      "Batch loss: 0.615744411945343 batch: 246/840\n",
      "Batch loss: 0.7233905792236328 batch: 247/840\n",
      "Batch loss: 0.7581228613853455 batch: 248/840\n",
      "Batch loss: 0.9620964527130127 batch: 249/840\n",
      "Batch loss: 0.5000712871551514 batch: 250/840\n",
      "Batch loss: 0.5524039268493652 batch: 251/840\n",
      "Batch loss: 0.5554615259170532 batch: 252/840\n",
      "Batch loss: 0.7138356566429138 batch: 253/840\n",
      "Batch loss: 0.6505328416824341 batch: 254/840\n",
      "Batch loss: 0.6168713569641113 batch: 255/840\n",
      "Batch loss: 0.7361772060394287 batch: 256/840\n",
      "Batch loss: 0.5962104797363281 batch: 257/840\n",
      "Batch loss: 0.6556529402732849 batch: 258/840\n",
      "Batch loss: 0.5002491474151611 batch: 259/840\n",
      "Batch loss: 0.523862898349762 batch: 260/840\n",
      "Batch loss: 0.6518430113792419 batch: 261/840\n",
      "Batch loss: 0.4321621358394623 batch: 262/840\n",
      "Batch loss: 0.5801606178283691 batch: 263/840\n",
      "Batch loss: 0.5740020871162415 batch: 264/840\n",
      "Batch loss: 0.6863394975662231 batch: 265/840\n",
      "Batch loss: 0.6168959140777588 batch: 266/840\n",
      "Batch loss: 0.6387074589729309 batch: 267/840\n",
      "Batch loss: 0.5454217195510864 batch: 268/840\n",
      "Batch loss: 0.5153777599334717 batch: 269/840\n",
      "Batch loss: 0.6094896793365479 batch: 270/840\n",
      "Batch loss: 0.5927059650421143 batch: 271/840\n",
      "Batch loss: 0.7046089768409729 batch: 272/840\n",
      "Batch loss: 0.7795941233634949 batch: 273/840\n",
      "Batch loss: 0.6065580248832703 batch: 274/840\n",
      "Batch loss: 0.6836289167404175 batch: 275/840\n",
      "Batch loss: 0.546527087688446 batch: 276/840\n",
      "Batch loss: 0.6325732469558716 batch: 277/840\n",
      "Batch loss: 0.7588146328926086 batch: 278/840\n",
      "Batch loss: 0.730597972869873 batch: 279/840\n",
      "Batch loss: 0.6785079836845398 batch: 280/840\n",
      "Batch loss: 0.48754647374153137 batch: 281/840\n",
      "Batch loss: 0.6216078996658325 batch: 282/840\n",
      "Batch loss: 0.5843985080718994 batch: 283/840\n",
      "Batch loss: 0.45744720101356506 batch: 284/840\n",
      "Batch loss: 0.5098998546600342 batch: 285/840\n",
      "Batch loss: 0.7211723923683167 batch: 286/840\n",
      "Batch loss: 0.42535656690597534 batch: 287/840\n",
      "Batch loss: 0.502098560333252 batch: 288/840\n",
      "Batch loss: 0.7545637488365173 batch: 289/840\n",
      "Batch loss: 0.761573076248169 batch: 290/840\n",
      "Batch loss: 0.7098955512046814 batch: 291/840\n",
      "Batch loss: 0.6332080960273743 batch: 292/840\n",
      "Batch loss: 0.7288223505020142 batch: 293/840\n",
      "Batch loss: 0.6451202630996704 batch: 294/840\n",
      "Batch loss: 0.49561968445777893 batch: 295/840\n",
      "Batch loss: 0.6643081903457642 batch: 296/840\n",
      "Batch loss: 0.7373221516609192 batch: 297/840\n",
      "Batch loss: 0.7489427328109741 batch: 298/840\n",
      "Batch loss: 0.5699107646942139 batch: 299/840\n",
      "Batch loss: 0.8364415764808655 batch: 300/840\n",
      "Batch loss: 0.7523632645606995 batch: 301/840\n",
      "Batch loss: 0.6329967975616455 batch: 302/840\n",
      "Batch loss: 0.8133912682533264 batch: 303/840\n",
      "Batch loss: 0.5524521470069885 batch: 304/840\n",
      "Batch loss: 0.5621199607849121 batch: 305/840\n",
      "Batch loss: 0.6776449680328369 batch: 306/840\n",
      "Batch loss: 0.5810739398002625 batch: 307/840\n",
      "Batch loss: 0.6892416477203369 batch: 308/840\n",
      "Batch loss: 0.6207008957862854 batch: 309/840\n",
      "Batch loss: 0.8362237811088562 batch: 310/840\n",
      "Batch loss: 0.7731156945228577 batch: 311/840\n",
      "Batch loss: 0.6510261297225952 batch: 312/840\n",
      "Batch loss: 0.7915855646133423 batch: 313/840\n",
      "Batch loss: 0.5585525631904602 batch: 314/840\n",
      "Batch loss: 0.7135074138641357 batch: 315/840\n",
      "Batch loss: 0.4858730435371399 batch: 316/840\n",
      "Batch loss: 0.6610887050628662 batch: 317/840\n",
      "Batch loss: 0.5952386260032654 batch: 318/840\n",
      "Batch loss: 0.675484299659729 batch: 319/840\n",
      "Batch loss: 0.501805305480957 batch: 320/840\n",
      "Batch loss: 0.6297560334205627 batch: 321/840\n",
      "Batch loss: 0.6187903881072998 batch: 322/840\n",
      "Batch loss: 0.7072058916091919 batch: 323/840\n",
      "Batch loss: 0.6717208027839661 batch: 324/840\n",
      "Batch loss: 0.5537462830543518 batch: 325/840\n",
      "Batch loss: 0.6470598578453064 batch: 326/840\n",
      "Batch loss: 0.45946136116981506 batch: 327/840\n",
      "Batch loss: 0.6895774602890015 batch: 328/840\n",
      "Batch loss: 0.636417031288147 batch: 329/840\n",
      "Batch loss: 0.6674756407737732 batch: 330/840\n",
      "Batch loss: 0.7723917365074158 batch: 331/840\n",
      "Batch loss: 0.6373410224914551 batch: 332/840\n",
      "Batch loss: 0.6186498999595642 batch: 333/840\n",
      "Batch loss: 0.7227056622505188 batch: 334/840\n",
      "Batch loss: 0.6543221473693848 batch: 335/840\n",
      "Batch loss: 0.7780990600585938 batch: 336/840\n",
      "Batch loss: 0.8249236345291138 batch: 337/840\n",
      "Batch loss: 0.7265445590019226 batch: 338/840\n",
      "Batch loss: 0.5028283596038818 batch: 339/840\n",
      "Batch loss: 0.7465398907661438 batch: 340/840\n",
      "Batch loss: 0.5230203866958618 batch: 341/840\n",
      "Batch loss: 0.5226600766181946 batch: 342/840\n",
      "Batch loss: 0.7689890265464783 batch: 343/840\n",
      "Batch loss: 0.5992405414581299 batch: 344/840\n",
      "Batch loss: 0.4587220251560211 batch: 345/840\n",
      "Batch loss: 0.572675347328186 batch: 346/840\n",
      "Batch loss: 0.5607019662857056 batch: 347/840\n",
      "Batch loss: 0.5998653173446655 batch: 348/840\n",
      "Batch loss: 0.6683011651039124 batch: 349/840\n",
      "Batch loss: 0.5149974226951599 batch: 350/840\n",
      "Batch loss: 0.6952240467071533 batch: 351/840\n",
      "Batch loss: 0.5068896412849426 batch: 352/840\n",
      "Batch loss: 0.7956270575523376 batch: 353/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5774341821670532 batch: 354/840\n",
      "Batch loss: 0.47021159529685974 batch: 355/840\n",
      "Batch loss: 0.5130252242088318 batch: 356/840\n",
      "Batch loss: 0.5381221175193787 batch: 357/840\n",
      "Batch loss: 0.6335855722427368 batch: 358/840\n",
      "Batch loss: 0.5527399182319641 batch: 359/840\n",
      "Batch loss: 0.7753105163574219 batch: 360/840\n",
      "Batch loss: 0.7184547185897827 batch: 361/840\n",
      "Batch loss: 0.6072542071342468 batch: 362/840\n",
      "Batch loss: 0.6265612244606018 batch: 363/840\n",
      "Batch loss: 0.70276939868927 batch: 364/840\n",
      "Batch loss: 0.5392575263977051 batch: 365/840\n",
      "Batch loss: 0.6650264859199524 batch: 366/840\n",
      "Batch loss: 0.46301379799842834 batch: 367/840\n",
      "Batch loss: 0.7149473428726196 batch: 368/840\n",
      "Batch loss: 0.6183743476867676 batch: 369/840\n",
      "Batch loss: 0.7559217214584351 batch: 370/840\n",
      "Batch loss: 0.6615894436836243 batch: 371/840\n",
      "Batch loss: 0.4207928776741028 batch: 372/840\n",
      "Batch loss: 0.6401746869087219 batch: 373/840\n",
      "Batch loss: 0.6749857068061829 batch: 374/840\n",
      "Batch loss: 0.5259241461753845 batch: 375/840\n",
      "Batch loss: 0.5407330989837646 batch: 376/840\n",
      "Batch loss: 0.6887375712394714 batch: 377/840\n",
      "Batch loss: 0.5730250477790833 batch: 378/840\n",
      "Batch loss: 0.5649513602256775 batch: 379/840\n",
      "Batch loss: 0.8885581493377686 batch: 380/840\n",
      "Batch loss: 0.9486663937568665 batch: 381/840\n",
      "Batch loss: 0.6724711656570435 batch: 382/840\n",
      "Batch loss: 0.7295798659324646 batch: 383/840\n",
      "Batch loss: 0.7144272327423096 batch: 384/840\n",
      "Batch loss: 0.5815638303756714 batch: 385/840\n",
      "Batch loss: 0.7526681423187256 batch: 386/840\n",
      "Batch loss: 0.6527717709541321 batch: 387/840\n",
      "Batch loss: 0.7031699419021606 batch: 388/840\n",
      "Batch loss: 0.5343906879425049 batch: 389/840\n",
      "Batch loss: 0.879642128944397 batch: 390/840\n",
      "Batch loss: 0.5888370871543884 batch: 391/840\n",
      "Batch loss: 0.5434045195579529 batch: 392/840\n",
      "Batch loss: 0.5489419102668762 batch: 393/840\n",
      "Batch loss: 0.7589874863624573 batch: 394/840\n",
      "Batch loss: 0.5599560141563416 batch: 395/840\n",
      "Batch loss: 0.6624257564544678 batch: 396/840\n",
      "Batch loss: 0.5388152003288269 batch: 397/840\n",
      "Batch loss: 0.630560576915741 batch: 398/840\n",
      "Batch loss: 0.4898044466972351 batch: 399/840\n",
      "Batch loss: 0.6017852425575256 batch: 400/840\n",
      "Batch loss: 0.6379911303520203 batch: 401/840\n",
      "Batch loss: 0.5613909363746643 batch: 402/840\n",
      "Batch loss: 0.5978772640228271 batch: 403/840\n",
      "Batch loss: 0.5619625449180603 batch: 404/840\n",
      "Batch loss: 0.6102907657623291 batch: 405/840\n",
      "Batch loss: 0.6686891913414001 batch: 406/840\n",
      "Batch loss: 0.5962076783180237 batch: 407/840\n",
      "Batch loss: 0.6778951287269592 batch: 408/840\n",
      "Batch loss: 0.7211300730705261 batch: 409/840\n",
      "Batch loss: 0.6676865220069885 batch: 410/840\n",
      "Batch loss: 0.7164149284362793 batch: 411/840\n",
      "Batch loss: 0.7584356069564819 batch: 412/840\n",
      "Batch loss: 0.6137763857841492 batch: 413/840\n",
      "Batch loss: 0.5258175134658813 batch: 414/840\n",
      "Batch loss: 0.8836101293563843 batch: 415/840\n",
      "Batch loss: 0.41200169920921326 batch: 416/840\n",
      "Batch loss: 0.7118211388587952 batch: 417/840\n",
      "Batch loss: 0.7943006157875061 batch: 418/840\n",
      "Batch loss: 0.6120506525039673 batch: 419/840\n",
      "Batch loss: 0.7288551330566406 batch: 420/840\n",
      "Batch loss: 0.4945109486579895 batch: 421/840\n",
      "Batch loss: 0.5372191071510315 batch: 422/840\n",
      "Batch loss: 0.7223101258277893 batch: 423/840\n",
      "Batch loss: 0.6715006232261658 batch: 424/840\n",
      "Batch loss: 0.6999607086181641 batch: 425/840\n",
      "Batch loss: 0.6254245042800903 batch: 426/840\n",
      "Batch loss: 0.5809639096260071 batch: 427/840\n",
      "Batch loss: 0.7167237997055054 batch: 428/840\n",
      "Batch loss: 0.5096691846847534 batch: 429/840\n",
      "Batch loss: 0.7330420613288879 batch: 430/840\n",
      "Batch loss: 0.5859423279762268 batch: 431/840\n",
      "Batch loss: 0.6000044941902161 batch: 432/840\n",
      "Batch loss: 0.45828714966773987 batch: 433/840\n",
      "Batch loss: 0.5675672888755798 batch: 434/840\n",
      "Batch loss: 0.6948762536048889 batch: 435/840\n",
      "Batch loss: 0.45674529671669006 batch: 436/840\n",
      "Batch loss: 0.6803572177886963 batch: 437/840\n",
      "Batch loss: 0.43782153725624084 batch: 438/840\n",
      "Batch loss: 0.5967720150947571 batch: 439/840\n",
      "Batch loss: 0.7559598684310913 batch: 440/840\n",
      "Batch loss: 0.5883198976516724 batch: 441/840\n",
      "Batch loss: 0.6928516626358032 batch: 442/840\n",
      "Batch loss: 0.6275257468223572 batch: 443/840\n",
      "Batch loss: 0.5762888193130493 batch: 444/840\n",
      "Batch loss: 0.4958713948726654 batch: 445/840\n",
      "Batch loss: 0.7757415771484375 batch: 446/840\n",
      "Batch loss: 0.6524879336357117 batch: 447/840\n",
      "Batch loss: 0.6912010312080383 batch: 448/840\n",
      "Batch loss: 0.6007521152496338 batch: 449/840\n",
      "Batch loss: 0.5712286233901978 batch: 450/840\n",
      "Batch loss: 0.6758580803871155 batch: 451/840\n",
      "Batch loss: 0.6013553738594055 batch: 452/840\n",
      "Batch loss: 0.6625661253929138 batch: 453/840\n",
      "Batch loss: 0.567706823348999 batch: 454/840\n",
      "Batch loss: 0.6908236742019653 batch: 455/840\n",
      "Batch loss: 0.5328333377838135 batch: 456/840\n",
      "Batch loss: 0.5388508439064026 batch: 457/840\n",
      "Batch loss: 0.5909692645072937 batch: 458/840\n",
      "Batch loss: 0.5912476181983948 batch: 459/840\n",
      "Batch loss: 0.47942492365837097 batch: 460/840\n",
      "Batch loss: 0.6727205514907837 batch: 461/840\n",
      "Batch loss: 0.5618679523468018 batch: 462/840\n",
      "Batch loss: 0.8347499370574951 batch: 463/840\n",
      "Batch loss: 0.5160371661186218 batch: 464/840\n",
      "Batch loss: 0.8193652629852295 batch: 465/840\n",
      "Batch loss: 0.7074258923530579 batch: 466/840\n",
      "Batch loss: 0.8404009342193604 batch: 467/840\n",
      "Batch loss: 0.6076866388320923 batch: 468/840\n",
      "Batch loss: 0.48401129245758057 batch: 469/840\n",
      "Batch loss: 0.6727699041366577 batch: 470/840\n",
      "Batch loss: 0.6895259022712708 batch: 471/840\n",
      "Batch loss: 0.6969108581542969 batch: 472/840\n",
      "Batch loss: 0.6868835687637329 batch: 473/840\n",
      "Batch loss: 0.7373639941215515 batch: 474/840\n",
      "Batch loss: 0.5797313451766968 batch: 475/840\n",
      "Batch loss: 0.5955809950828552 batch: 476/840\n",
      "Batch loss: 0.5664308667182922 batch: 477/840\n",
      "Batch loss: 0.5152543187141418 batch: 478/840\n",
      "Batch loss: 0.4400499761104584 batch: 479/840\n",
      "Batch loss: 0.6213248372077942 batch: 480/840\n",
      "Batch loss: 0.5708577036857605 batch: 481/840\n",
      "Batch loss: 0.6484124064445496 batch: 482/840\n",
      "Batch loss: 1.047716498374939 batch: 483/840\n",
      "Batch loss: 0.5013293623924255 batch: 484/840\n",
      "Batch loss: 0.6479350924491882 batch: 485/840\n",
      "Batch loss: 0.6260578632354736 batch: 486/840\n",
      "Batch loss: 0.4591911733150482 batch: 487/840\n",
      "Batch loss: 0.7086358070373535 batch: 488/840\n",
      "Batch loss: 0.6597133874893188 batch: 489/840\n",
      "Batch loss: 0.8059128522872925 batch: 490/840\n",
      "Batch loss: 0.44360870122909546 batch: 491/840\n",
      "Batch loss: 0.712211012840271 batch: 492/840\n",
      "Batch loss: 0.6798829436302185 batch: 493/840\n",
      "Batch loss: 0.4063206911087036 batch: 494/840\n",
      "Batch loss: 0.7776581048965454 batch: 495/840\n",
      "Batch loss: 0.6824923157691956 batch: 496/840\n",
      "Batch loss: 0.755309522151947 batch: 497/840\n",
      "Batch loss: 0.47847992181777954 batch: 498/840\n",
      "Batch loss: 0.6461611390113831 batch: 499/840\n",
      "Batch loss: 0.5759636163711548 batch: 500/840\n",
      "Batch loss: 0.5137537121772766 batch: 501/840\n",
      "Batch loss: 0.7019696235656738 batch: 502/840\n",
      "Batch loss: 0.511606752872467 batch: 503/840\n",
      "Batch loss: 0.6516848802566528 batch: 504/840\n",
      "Batch loss: 0.6327969431877136 batch: 505/840\n",
      "Batch loss: 0.5411402583122253 batch: 506/840\n",
      "Batch loss: 0.746734082698822 batch: 507/840\n",
      "Batch loss: 0.5622652173042297 batch: 508/840\n",
      "Batch loss: 0.6662608981132507 batch: 509/840\n",
      "Batch loss: 0.6929928064346313 batch: 510/840\n",
      "Batch loss: 0.47081294655799866 batch: 511/840\n",
      "Batch loss: 0.6651179790496826 batch: 512/840\n",
      "Batch loss: 0.5659851431846619 batch: 513/840\n",
      "Batch loss: 0.7954630851745605 batch: 514/840\n",
      "Batch loss: 0.48552796244621277 batch: 515/840\n",
      "Batch loss: 0.698771595954895 batch: 516/840\n",
      "Batch loss: 0.6555611491203308 batch: 517/840\n",
      "Batch loss: 0.5961056351661682 batch: 518/840\n",
      "Batch loss: 0.8060529828071594 batch: 519/840\n",
      "Batch loss: 0.6707257628440857 batch: 520/840\n",
      "Batch loss: 0.6692768931388855 batch: 521/840\n",
      "Batch loss: 0.6024744510650635 batch: 522/840\n",
      "Batch loss: 0.45037129521369934 batch: 523/840\n",
      "Batch loss: 0.6553751230239868 batch: 524/840\n",
      "Batch loss: 0.548714816570282 batch: 525/840\n",
      "Batch loss: 0.688291072845459 batch: 526/840\n",
      "Batch loss: 0.5734502673149109 batch: 527/840\n",
      "Batch loss: 0.7324258685112 batch: 528/840\n",
      "Batch loss: 0.47395843267440796 batch: 529/840\n",
      "Batch loss: 0.5930563807487488 batch: 530/840\n",
      "Batch loss: 0.5225529670715332 batch: 531/840\n",
      "Batch loss: 0.538625955581665 batch: 532/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6248078346252441 batch: 533/840\n",
      "Batch loss: 0.6168057918548584 batch: 534/840\n",
      "Batch loss: 0.5891121029853821 batch: 535/840\n",
      "Batch loss: 0.5479736924171448 batch: 536/840\n",
      "Batch loss: 0.5828794836997986 batch: 537/840\n",
      "Batch loss: 0.4764338433742523 batch: 538/840\n",
      "Batch loss: 0.6247014999389648 batch: 539/840\n",
      "Batch loss: 0.7235237956047058 batch: 540/840\n",
      "Batch loss: 0.6950569152832031 batch: 541/840\n",
      "Batch loss: 0.6540377736091614 batch: 542/840\n",
      "Batch loss: 0.4914195239543915 batch: 543/840\n",
      "Batch loss: 0.6903573870658875 batch: 544/840\n",
      "Batch loss: 0.6398258805274963 batch: 545/840\n",
      "Batch loss: 0.5092463493347168 batch: 546/840\n",
      "Batch loss: 0.547671377658844 batch: 547/840\n",
      "Batch loss: 0.5350499749183655 batch: 548/840\n",
      "Batch loss: 0.537991464138031 batch: 549/840\n",
      "Batch loss: 0.5111928582191467 batch: 550/840\n",
      "Batch loss: 0.671984076499939 batch: 551/840\n",
      "Batch loss: 0.5269877314567566 batch: 552/840\n",
      "Batch loss: 0.6273808479309082 batch: 553/840\n",
      "Batch loss: 0.6380963325500488 batch: 554/840\n",
      "Batch loss: 0.6137915849685669 batch: 555/840\n",
      "Batch loss: 0.6666883230209351 batch: 556/840\n",
      "Batch loss: 0.6436110138893127 batch: 557/840\n",
      "Batch loss: 0.667589545249939 batch: 558/840\n",
      "Batch loss: 0.6263022422790527 batch: 559/840\n",
      "Batch loss: 0.7021798491477966 batch: 560/840\n",
      "Batch loss: 0.5633125305175781 batch: 561/840\n",
      "Batch loss: 0.5747899413108826 batch: 562/840\n",
      "Batch loss: 0.49955517053604126 batch: 563/840\n",
      "Batch loss: 0.6699550151824951 batch: 564/840\n",
      "Batch loss: 0.8067774772644043 batch: 565/840\n",
      "Batch loss: 0.7213531732559204 batch: 566/840\n",
      "Batch loss: 0.6883195638656616 batch: 567/840\n",
      "Batch loss: 0.6107850670814514 batch: 568/840\n",
      "Batch loss: 0.6622929573059082 batch: 569/840\n",
      "Batch loss: 0.41993218660354614 batch: 570/840\n",
      "Batch loss: 0.6880878210067749 batch: 571/840\n",
      "Batch loss: 0.7831026315689087 batch: 572/840\n",
      "Batch loss: 0.6478096842765808 batch: 573/840\n",
      "Batch loss: 0.6635504961013794 batch: 574/840\n",
      "Batch loss: 0.4911515712738037 batch: 575/840\n",
      "Batch loss: 0.6217777132987976 batch: 576/840\n",
      "Batch loss: 0.6102506518363953 batch: 577/840\n",
      "Batch loss: 0.7023346424102783 batch: 578/840\n",
      "Batch loss: 0.6872687339782715 batch: 579/840\n",
      "Batch loss: 0.8482749462127686 batch: 580/840\n",
      "Batch loss: 0.5737634897232056 batch: 581/840\n",
      "Batch loss: 0.8089525699615479 batch: 582/840\n",
      "Batch loss: 0.6924209594726562 batch: 583/840\n",
      "Batch loss: 0.7430517077445984 batch: 584/840\n",
      "Batch loss: 0.631074845790863 batch: 585/840\n",
      "Batch loss: 0.6090020537376404 batch: 586/840\n",
      "Batch loss: 0.6364651918411255 batch: 587/840\n",
      "Batch loss: 0.6652446985244751 batch: 588/840\n",
      "Batch loss: 0.6983340382575989 batch: 589/840\n",
      "Batch loss: 0.5452775955200195 batch: 590/840\n",
      "Batch loss: 0.4711754620075226 batch: 591/840\n",
      "Batch loss: 0.5474410057067871 batch: 592/840\n",
      "Batch loss: 0.774263322353363 batch: 593/840\n",
      "Batch loss: 0.5977291464805603 batch: 594/840\n",
      "Batch loss: 0.36913174390792847 batch: 595/840\n",
      "Batch loss: 0.5132715702056885 batch: 596/840\n",
      "Batch loss: 0.6031216979026794 batch: 597/840\n",
      "Batch loss: 0.48223987221717834 batch: 598/840\n",
      "Batch loss: 0.5497651100158691 batch: 599/840\n",
      "Batch loss: 0.6927094459533691 batch: 600/840\n",
      "Batch loss: 0.7203418016433716 batch: 601/840\n",
      "Batch loss: 0.5221142768859863 batch: 602/840\n",
      "Batch loss: 0.6629581451416016 batch: 603/840\n",
      "Batch loss: 0.6240310668945312 batch: 604/840\n",
      "Batch loss: 0.7629945874214172 batch: 605/840\n",
      "Batch loss: 0.8055536150932312 batch: 606/840\n",
      "Batch loss: 0.6775684356689453 batch: 607/840\n",
      "Batch loss: 0.5261516571044922 batch: 608/840\n",
      "Batch loss: 0.5036051273345947 batch: 609/840\n",
      "Batch loss: 0.7142711877822876 batch: 610/840\n",
      "Batch loss: 0.5654492378234863 batch: 611/840\n",
      "Batch loss: 0.7528452277183533 batch: 612/840\n",
      "Batch loss: 0.6181116700172424 batch: 613/840\n",
      "Batch loss: 0.6887692809104919 batch: 614/840\n",
      "Batch loss: 0.5359849333763123 batch: 615/840\n",
      "Batch loss: 0.47809693217277527 batch: 616/840\n",
      "Batch loss: 0.4876502752304077 batch: 617/840\n",
      "Batch loss: 0.6221016049385071 batch: 618/840\n",
      "Batch loss: 0.8104419708251953 batch: 619/840\n",
      "Batch loss: 0.5315373539924622 batch: 620/840\n",
      "Batch loss: 0.6667008996009827 batch: 621/840\n",
      "Batch loss: 0.618478536605835 batch: 622/840\n",
      "Batch loss: 0.6343956589698792 batch: 623/840\n",
      "Batch loss: 0.7983565330505371 batch: 624/840\n",
      "Batch loss: 0.7842166423797607 batch: 625/840\n",
      "Batch loss: 0.6240938305854797 batch: 626/840\n",
      "Batch loss: 0.5860567688941956 batch: 627/840\n",
      "Batch loss: 0.6147133111953735 batch: 628/840\n",
      "Batch loss: 0.5979399681091309 batch: 629/840\n",
      "Batch loss: 0.5852211117744446 batch: 630/840\n",
      "Batch loss: 0.673064649105072 batch: 631/840\n",
      "Batch loss: 0.7035287618637085 batch: 632/840\n",
      "Batch loss: 0.6051787734031677 batch: 633/840\n",
      "Batch loss: 0.6858683228492737 batch: 634/840\n",
      "Batch loss: 0.6187995672225952 batch: 635/840\n",
      "Batch loss: 0.500257670879364 batch: 636/840\n",
      "Batch loss: 0.502443790435791 batch: 637/840\n",
      "Batch loss: 0.6585706472396851 batch: 638/840\n",
      "Batch loss: 0.6501950025558472 batch: 639/840\n",
      "Batch loss: 0.588151216506958 batch: 640/840\n",
      "Batch loss: 0.8008013367652893 batch: 641/840\n",
      "Batch loss: 0.6159433126449585 batch: 642/840\n",
      "Batch loss: 0.9209333658218384 batch: 643/840\n",
      "Batch loss: 0.6037842631340027 batch: 644/840\n",
      "Batch loss: 0.5752376317977905 batch: 645/840\n",
      "Batch loss: 0.5681985020637512 batch: 646/840\n",
      "Batch loss: 0.7153096199035645 batch: 647/840\n",
      "Batch loss: 0.6096168160438538 batch: 648/840\n",
      "Batch loss: 0.6391587257385254 batch: 649/840\n",
      "Batch loss: 0.5394704341888428 batch: 650/840\n",
      "Batch loss: 0.6226799488067627 batch: 651/840\n",
      "Batch loss: 0.6910282373428345 batch: 652/840\n",
      "Batch loss: 0.48749926686286926 batch: 653/840\n",
      "Batch loss: 0.7727555632591248 batch: 654/840\n",
      "Batch loss: 0.5997195839881897 batch: 655/840\n",
      "Batch loss: 0.6907598972320557 batch: 656/840\n",
      "Batch loss: 0.597812831401825 batch: 657/840\n",
      "Batch loss: 0.7830519080162048 batch: 658/840\n",
      "Batch loss: 0.6939258575439453 batch: 659/840\n",
      "Batch loss: 0.565787136554718 batch: 660/840\n",
      "Batch loss: 0.7147308588027954 batch: 661/840\n",
      "Batch loss: 0.652850866317749 batch: 662/840\n",
      "Batch loss: 0.5608698129653931 batch: 663/840\n",
      "Batch loss: 0.6581705212593079 batch: 664/840\n",
      "Batch loss: 0.7011933922767639 batch: 665/840\n",
      "Batch loss: 0.6022598147392273 batch: 666/840\n",
      "Batch loss: 0.7274447679519653 batch: 667/840\n",
      "Batch loss: 0.5887876749038696 batch: 668/840\n",
      "Batch loss: 0.6244484186172485 batch: 669/840\n",
      "Batch loss: 0.6637090444564819 batch: 670/840\n",
      "Batch loss: 0.6816152930259705 batch: 671/840\n",
      "Batch loss: 0.5869839787483215 batch: 672/840\n",
      "Batch loss: 0.7839730978012085 batch: 673/840\n",
      "Batch loss: 0.870576024055481 batch: 674/840\n",
      "Batch loss: 0.5920211672782898 batch: 675/840\n",
      "Batch loss: 0.5549470782279968 batch: 676/840\n",
      "Batch loss: 0.6625103950500488 batch: 677/840\n",
      "Batch loss: 0.6661130785942078 batch: 678/840\n",
      "Batch loss: 0.7649558782577515 batch: 679/840\n",
      "Batch loss: 0.654171347618103 batch: 680/840\n",
      "Batch loss: 0.7202010154724121 batch: 681/840\n",
      "Batch loss: 0.5785489082336426 batch: 682/840\n",
      "Batch loss: 0.4957887530326843 batch: 683/840\n",
      "Batch loss: 0.8070517778396606 batch: 684/840\n",
      "Batch loss: 0.6914578080177307 batch: 685/840\n",
      "Batch loss: 0.6694381833076477 batch: 686/840\n",
      "Batch loss: 0.7426594495773315 batch: 687/840\n",
      "Batch loss: 0.6128717064857483 batch: 688/840\n",
      "Batch loss: 0.5139272212982178 batch: 689/840\n",
      "Batch loss: 0.6817173957824707 batch: 690/840\n",
      "Batch loss: 0.5578391551971436 batch: 691/840\n",
      "Batch loss: 0.6967315077781677 batch: 692/840\n",
      "Batch loss: 0.5924703478813171 batch: 693/840\n",
      "Batch loss: 0.7490501403808594 batch: 694/840\n",
      "Batch loss: 0.7722417712211609 batch: 695/840\n",
      "Batch loss: 0.6602903604507446 batch: 696/840\n",
      "Batch loss: 0.5840091109275818 batch: 697/840\n",
      "Batch loss: 0.5865405201911926 batch: 698/840\n",
      "Batch loss: 0.709688127040863 batch: 699/840\n",
      "Batch loss: 0.6862548589706421 batch: 700/840\n",
      "Batch loss: 0.6672569513320923 batch: 701/840\n",
      "Batch loss: 0.5964654088020325 batch: 702/840\n",
      "Batch loss: 0.5965949892997742 batch: 703/840\n",
      "Batch loss: 0.8083702921867371 batch: 704/840\n",
      "Batch loss: 0.5853164196014404 batch: 705/840\n",
      "Batch loss: 0.5556325316429138 batch: 706/840\n",
      "Batch loss: 0.6601017117500305 batch: 707/840\n",
      "Batch loss: 0.608816385269165 batch: 708/840\n",
      "Batch loss: 0.6286625862121582 batch: 709/840\n",
      "Batch loss: 0.788809061050415 batch: 710/840\n",
      "Batch loss: 0.500723659992218 batch: 711/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6669734716415405 batch: 712/840\n",
      "Batch loss: 0.5472900867462158 batch: 713/840\n",
      "Batch loss: 0.6427754759788513 batch: 714/840\n",
      "Batch loss: 0.6929024457931519 batch: 715/840\n",
      "Batch loss: 0.6409072875976562 batch: 716/840\n",
      "Batch loss: 0.7421362400054932 batch: 717/840\n",
      "Batch loss: 0.558096706867218 batch: 718/840\n",
      "Batch loss: 0.4780447483062744 batch: 719/840\n",
      "Batch loss: 0.5471028089523315 batch: 720/840\n",
      "Batch loss: 0.6687490344047546 batch: 721/840\n",
      "Batch loss: 0.7669888138771057 batch: 722/840\n",
      "Batch loss: 0.5260406732559204 batch: 723/840\n",
      "Batch loss: 0.7502405643463135 batch: 724/840\n",
      "Batch loss: 0.6517887115478516 batch: 725/840\n",
      "Batch loss: 0.6425604820251465 batch: 726/840\n",
      "Batch loss: 0.7572811841964722 batch: 727/840\n",
      "Batch loss: 0.719482958316803 batch: 728/840\n",
      "Batch loss: 0.6163515448570251 batch: 729/840\n",
      "Batch loss: 0.5720213651657104 batch: 730/840\n",
      "Batch loss: 0.5333706736564636 batch: 731/840\n",
      "Batch loss: 0.7822756171226501 batch: 732/840\n",
      "Batch loss: 0.6251577138900757 batch: 733/840\n",
      "Batch loss: 0.5786844491958618 batch: 734/840\n",
      "Batch loss: 0.6100634932518005 batch: 735/840\n",
      "Batch loss: 0.6544278860092163 batch: 736/840\n",
      "Batch loss: 0.6222019791603088 batch: 737/840\n",
      "Batch loss: 0.5060386061668396 batch: 738/840\n",
      "Batch loss: 0.7395063638687134 batch: 739/840\n",
      "Batch loss: 0.6491712927818298 batch: 740/840\n",
      "Batch loss: 0.5938634872436523 batch: 741/840\n",
      "Batch loss: 0.6331207752227783 batch: 742/840\n",
      "Batch loss: 0.4518930912017822 batch: 743/840\n",
      "Batch loss: 0.55628502368927 batch: 744/840\n",
      "Batch loss: 0.5775545835494995 batch: 745/840\n",
      "Batch loss: 0.606928825378418 batch: 746/840\n",
      "Batch loss: 0.6543765068054199 batch: 747/840\n",
      "Batch loss: 0.7099611163139343 batch: 748/840\n",
      "Batch loss: 0.4717333912849426 batch: 749/840\n",
      "Batch loss: 0.4757290184497833 batch: 750/840\n",
      "Batch loss: 0.6659216284751892 batch: 751/840\n",
      "Batch loss: 0.8754583597183228 batch: 752/840\n",
      "Batch loss: 0.5045087337493896 batch: 753/840\n",
      "Batch loss: 0.5994372367858887 batch: 754/840\n",
      "Batch loss: 0.6415192484855652 batch: 755/840\n",
      "Batch loss: 0.5460530519485474 batch: 756/840\n",
      "Batch loss: 0.7078565359115601 batch: 757/840\n",
      "Batch loss: 0.6304905414581299 batch: 758/840\n",
      "Batch loss: 0.560441792011261 batch: 759/840\n",
      "Batch loss: 0.5471179485321045 batch: 760/840\n",
      "Batch loss: 0.5668050050735474 batch: 761/840\n",
      "Batch loss: 0.7130134105682373 batch: 762/840\n",
      "Batch loss: 0.4923572242259979 batch: 763/840\n",
      "Batch loss: 0.6646933555603027 batch: 764/840\n",
      "Batch loss: 0.5287564992904663 batch: 765/840\n",
      "Batch loss: 0.5181254744529724 batch: 766/840\n",
      "Batch loss: 0.70682692527771 batch: 767/840\n",
      "Batch loss: 0.7283497452735901 batch: 768/840\n",
      "Batch loss: 0.6316289305686951 batch: 769/840\n",
      "Batch loss: 0.5187748670578003 batch: 770/840\n",
      "Batch loss: 0.6488980054855347 batch: 771/840\n",
      "Batch loss: 0.5102744102478027 batch: 772/840\n",
      "Batch loss: 0.5196408629417419 batch: 773/840\n",
      "Batch loss: 0.49401623010635376 batch: 774/840\n",
      "Batch loss: 0.538907527923584 batch: 775/840\n",
      "Batch loss: 0.5857123136520386 batch: 776/840\n",
      "Batch loss: 0.48165589570999146 batch: 777/840\n",
      "Batch loss: 0.41252583265304565 batch: 778/840\n",
      "Batch loss: 0.7042356729507446 batch: 779/840\n",
      "Batch loss: 0.6159151792526245 batch: 780/840\n",
      "Batch loss: 0.5744382739067078 batch: 781/840\n",
      "Batch loss: 0.6096013784408569 batch: 782/840\n",
      "Batch loss: 0.49323880672454834 batch: 783/840\n",
      "Batch loss: 0.670413613319397 batch: 784/840\n",
      "Batch loss: 0.4823669195175171 batch: 785/840\n",
      "Batch loss: 0.664002537727356 batch: 786/840\n",
      "Batch loss: 0.4861821234226227 batch: 787/840\n",
      "Batch loss: 0.7840666770935059 batch: 788/840\n",
      "Batch loss: 0.7377727031707764 batch: 789/840\n",
      "Batch loss: 0.6312302350997925 batch: 790/840\n",
      "Batch loss: 0.7410916686058044 batch: 791/840\n",
      "Batch loss: 0.4208015203475952 batch: 792/840\n",
      "Batch loss: 0.6493114233016968 batch: 793/840\n",
      "Batch loss: 0.6289752721786499 batch: 794/840\n",
      "Batch loss: 0.6428574919700623 batch: 795/840\n",
      "Batch loss: 0.7370687127113342 batch: 796/840\n",
      "Batch loss: 0.6269704103469849 batch: 797/840\n",
      "Batch loss: 0.6830365657806396 batch: 798/840\n",
      "Batch loss: 0.6864765286445618 batch: 799/840\n",
      "Batch loss: 0.5177352428436279 batch: 800/840\n",
      "Batch loss: 0.6601610779762268 batch: 801/840\n",
      "Batch loss: 0.49141764640808105 batch: 802/840\n",
      "Batch loss: 0.5361680388450623 batch: 803/840\n",
      "Batch loss: 0.7106014490127563 batch: 804/840\n",
      "Batch loss: 0.556903064250946 batch: 805/840\n",
      "Batch loss: 0.5837802290916443 batch: 806/840\n",
      "Batch loss: 0.6048977375030518 batch: 807/840\n",
      "Batch loss: 0.6254823207855225 batch: 808/840\n",
      "Batch loss: 0.6552113890647888 batch: 809/840\n",
      "Batch loss: 0.5410829782485962 batch: 810/840\n",
      "Batch loss: 0.6040348410606384 batch: 811/840\n",
      "Batch loss: 0.5823773145675659 batch: 812/840\n",
      "Batch loss: 0.5301353335380554 batch: 813/840\n",
      "Batch loss: 0.6505773067474365 batch: 814/840\n",
      "Batch loss: 0.6637790203094482 batch: 815/840\n",
      "Batch loss: 0.6504641175270081 batch: 816/840\n",
      "Batch loss: 0.613717794418335 batch: 817/840\n",
      "Batch loss: 0.7171165943145752 batch: 818/840\n",
      "Batch loss: 0.5029095411300659 batch: 819/840\n",
      "Batch loss: 0.6309099197387695 batch: 820/840\n",
      "Batch loss: 0.7621666193008423 batch: 821/840\n",
      "Batch loss: 0.5560552477836609 batch: 822/840\n",
      "Batch loss: 0.7635759115219116 batch: 823/840\n",
      "Batch loss: 0.7136638164520264 batch: 824/840\n",
      "Batch loss: 0.708266019821167 batch: 825/840\n",
      "Batch loss: 0.583705723285675 batch: 826/840\n",
      "Batch loss: 0.5621187090873718 batch: 827/840\n",
      "Batch loss: 0.6698652505874634 batch: 828/840\n",
      "Batch loss: 0.6183133125305176 batch: 829/840\n",
      "Batch loss: 0.6858757734298706 batch: 830/840\n",
      "Batch loss: 0.4966627061367035 batch: 831/840\n",
      "Batch loss: 0.6701873540878296 batch: 832/840\n",
      "Batch loss: 0.660747230052948 batch: 833/840\n",
      "Batch loss: 0.5679607391357422 batch: 834/840\n",
      "Batch loss: 0.579388439655304 batch: 835/840\n",
      "Batch loss: 0.5304995775222778 batch: 836/840\n",
      "Batch loss: 0.5751087069511414 batch: 837/840\n",
      "Batch loss: 0.7243891954421997 batch: 838/840\n",
      "Batch loss: 0.5373466610908508 batch: 839/840\n",
      "Batch loss: 0.6742927432060242 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 11/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.816\n",
      "Running epoch 12/15\n",
      "Batch loss: 0.5078858733177185 batch: 1/840\n",
      "Batch loss: 1.055004596710205 batch: 2/840\n",
      "Batch loss: 0.6738668084144592 batch: 3/840\n",
      "Batch loss: 0.648699164390564 batch: 4/840\n",
      "Batch loss: 0.6116102337837219 batch: 5/840\n",
      "Batch loss: 0.45341405272483826 batch: 6/840\n",
      "Batch loss: 0.703925371170044 batch: 7/840\n",
      "Batch loss: 0.5588546991348267 batch: 8/840\n",
      "Batch loss: 0.4920863211154938 batch: 9/840\n",
      "Batch loss: 0.547751247882843 batch: 10/840\n",
      "Batch loss: 0.5555068254470825 batch: 11/840\n",
      "Batch loss: 0.5659759044647217 batch: 12/840\n",
      "Batch loss: 0.5622050762176514 batch: 13/840\n",
      "Batch loss: 0.6865777373313904 batch: 14/840\n",
      "Batch loss: 0.5681748986244202 batch: 15/840\n",
      "Batch loss: 0.5454229116439819 batch: 16/840\n",
      "Batch loss: 0.5707806944847107 batch: 17/840\n",
      "Batch loss: 0.7532150745391846 batch: 18/840\n",
      "Batch loss: 0.7337008118629456 batch: 19/840\n",
      "Batch loss: 0.72660231590271 batch: 20/840\n",
      "Batch loss: 0.6559630036354065 batch: 21/840\n",
      "Batch loss: 0.5657322406768799 batch: 22/840\n",
      "Batch loss: 0.4985848367214203 batch: 23/840\n",
      "Batch loss: 0.43953609466552734 batch: 24/840\n",
      "Batch loss: 0.5621222853660583 batch: 25/840\n",
      "Batch loss: 0.7496586441993713 batch: 26/840\n",
      "Batch loss: 0.784464418888092 batch: 27/840\n",
      "Batch loss: 0.5488311648368835 batch: 28/840\n",
      "Batch loss: 0.5666686296463013 batch: 29/840\n",
      "Batch loss: 0.4600774645805359 batch: 30/840\n",
      "Batch loss: 0.5200440287590027 batch: 31/840\n",
      "Batch loss: 0.5211588740348816 batch: 32/840\n",
      "Batch loss: 0.6125946640968323 batch: 33/840\n",
      "Batch loss: 0.5700834393501282 batch: 34/840\n",
      "Batch loss: 0.659364640712738 batch: 35/840\n",
      "Batch loss: 0.4723080098628998 batch: 36/840\n",
      "Batch loss: 0.6621981263160706 batch: 37/840\n",
      "Batch loss: 0.6379610896110535 batch: 38/840\n",
      "Batch loss: 0.65578293800354 batch: 39/840\n",
      "Batch loss: 0.5111795663833618 batch: 40/840\n",
      "Batch loss: 0.7490615248680115 batch: 41/840\n",
      "Batch loss: 0.6656551361083984 batch: 42/840\n",
      "Batch loss: 0.6147017478942871 batch: 43/840\n",
      "Batch loss: 0.6517998576164246 batch: 44/840\n",
      "Batch loss: 0.6698867678642273 batch: 45/840\n",
      "Batch loss: 0.5427138805389404 batch: 46/840\n",
      "Batch loss: 0.577460765838623 batch: 47/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5860925912857056 batch: 48/840\n",
      "Batch loss: 0.579903781414032 batch: 49/840\n",
      "Batch loss: 0.6079249382019043 batch: 50/840\n",
      "Batch loss: 0.676777720451355 batch: 51/840\n",
      "Batch loss: 0.6834861636161804 batch: 52/840\n",
      "Batch loss: 0.5628201365470886 batch: 53/840\n",
      "Batch loss: 0.677206814289093 batch: 54/840\n",
      "Batch loss: 0.6455696821212769 batch: 55/840\n",
      "Batch loss: 0.46384918689727783 batch: 56/840\n",
      "Batch loss: 0.6445857286453247 batch: 57/840\n",
      "Batch loss: 0.5905929803848267 batch: 58/840\n",
      "Batch loss: 0.559836208820343 batch: 59/840\n",
      "Batch loss: 0.6566406488418579 batch: 60/840\n",
      "Batch loss: 0.7475599050521851 batch: 61/840\n",
      "Batch loss: 0.7049005031585693 batch: 62/840\n",
      "Batch loss: 0.5744215250015259 batch: 63/840\n",
      "Batch loss: 0.6995124816894531 batch: 64/840\n",
      "Batch loss: 0.5484561324119568 batch: 65/840\n",
      "Batch loss: 0.5363996028900146 batch: 66/840\n",
      "Batch loss: 0.640399158000946 batch: 67/840\n",
      "Batch loss: 0.6737123727798462 batch: 68/840\n",
      "Batch loss: 0.7087306976318359 batch: 69/840\n",
      "Batch loss: 0.5679227709770203 batch: 70/840\n",
      "Batch loss: 0.7181552052497864 batch: 71/840\n",
      "Batch loss: 0.7963203191757202 batch: 72/840\n",
      "Batch loss: 0.6137147545814514 batch: 73/840\n",
      "Batch loss: 0.5997427701950073 batch: 74/840\n",
      "Batch loss: 0.6529029011726379 batch: 75/840\n",
      "Batch loss: 0.45703747868537903 batch: 76/840\n",
      "Batch loss: 0.5403192043304443 batch: 77/840\n",
      "Batch loss: 0.7848278284072876 batch: 78/840\n",
      "Batch loss: 0.5529543161392212 batch: 79/840\n",
      "Batch loss: 0.6481285095214844 batch: 80/840\n",
      "Batch loss: 0.5373689532279968 batch: 81/840\n",
      "Batch loss: 0.6050200462341309 batch: 82/840\n",
      "Batch loss: 0.5493042469024658 batch: 83/840\n",
      "Batch loss: 0.7558041214942932 batch: 84/840\n",
      "Batch loss: 0.542695939540863 batch: 85/840\n",
      "Batch loss: 0.8697994947433472 batch: 86/840\n",
      "Batch loss: 0.5075879096984863 batch: 87/840\n",
      "Batch loss: 0.47869911789894104 batch: 88/840\n",
      "Batch loss: 0.4630211591720581 batch: 89/840\n",
      "Batch loss: 0.48419320583343506 batch: 90/840\n",
      "Batch loss: 0.5568878650665283 batch: 91/840\n",
      "Batch loss: 0.6204941272735596 batch: 92/840\n",
      "Batch loss: 0.6210339069366455 batch: 93/840\n",
      "Batch loss: 0.6314504146575928 batch: 94/840\n",
      "Batch loss: 0.5654972195625305 batch: 95/840\n",
      "Batch loss: 0.5937183499336243 batch: 96/840\n",
      "Batch loss: 0.6624322533607483 batch: 97/840\n",
      "Batch loss: 0.6393072009086609 batch: 98/840\n",
      "Batch loss: 0.6203866004943848 batch: 99/840\n",
      "Batch loss: 0.6100143194198608 batch: 100/840\n",
      "Batch loss: 0.5303901433944702 batch: 101/840\n",
      "Batch loss: 0.5213860869407654 batch: 102/840\n",
      "Batch loss: 0.7049787640571594 batch: 103/840\n",
      "Batch loss: 0.477385938167572 batch: 104/840\n",
      "Batch loss: 0.5240910053253174 batch: 105/840\n",
      "Batch loss: 0.6466855406761169 batch: 106/840\n",
      "Batch loss: 0.5660803318023682 batch: 107/840\n",
      "Batch loss: 0.722374677658081 batch: 108/840\n",
      "Batch loss: 0.5732280611991882 batch: 109/840\n",
      "Batch loss: 0.577753484249115 batch: 110/840\n",
      "Batch loss: 0.47488486766815186 batch: 111/840\n",
      "Batch loss: 0.6360474228858948 batch: 112/840\n",
      "Batch loss: 0.710164487361908 batch: 113/840\n",
      "Batch loss: 0.5213271379470825 batch: 114/840\n",
      "Batch loss: 0.7024255990982056 batch: 115/840\n",
      "Batch loss: 0.5902411937713623 batch: 116/840\n",
      "Batch loss: 0.6357455253601074 batch: 117/840\n",
      "Batch loss: 0.4141247868537903 batch: 118/840\n",
      "Batch loss: 0.6085447669029236 batch: 119/840\n",
      "Batch loss: 0.5804646015167236 batch: 120/840\n",
      "Batch loss: 0.6477624773979187 batch: 121/840\n",
      "Batch loss: 0.8142582774162292 batch: 122/840\n",
      "Batch loss: 0.4956533908843994 batch: 123/840\n",
      "Batch loss: 0.4757557809352875 batch: 124/840\n",
      "Batch loss: 0.5371763110160828 batch: 125/840\n",
      "Batch loss: 0.6862155795097351 batch: 126/840\n",
      "Batch loss: 0.6956419348716736 batch: 127/840\n",
      "Batch loss: 0.6955790519714355 batch: 128/840\n",
      "Batch loss: 0.7407054305076599 batch: 129/840\n",
      "Batch loss: 0.4779677987098694 batch: 130/840\n",
      "Batch loss: 0.8024429082870483 batch: 131/840\n",
      "Batch loss: 1.0476856231689453 batch: 132/840\n",
      "Batch loss: 0.6529677510261536 batch: 133/840\n",
      "Batch loss: 0.5306893587112427 batch: 134/840\n",
      "Batch loss: 0.5000134110450745 batch: 135/840\n",
      "Batch loss: 0.6596812605857849 batch: 136/840\n",
      "Batch loss: 0.6152721047401428 batch: 137/840\n",
      "Batch loss: 0.4012182354927063 batch: 138/840\n",
      "Batch loss: 0.49002835154533386 batch: 139/840\n",
      "Batch loss: 0.5945455431938171 batch: 140/840\n",
      "Batch loss: 0.4819200038909912 batch: 141/840\n",
      "Batch loss: 0.5916224718093872 batch: 142/840\n",
      "Batch loss: 0.4792841076850891 batch: 143/840\n",
      "Batch loss: 0.5909359455108643 batch: 144/840\n",
      "Batch loss: 0.6838566064834595 batch: 145/840\n",
      "Batch loss: 0.5631658434867859 batch: 146/840\n",
      "Batch loss: 0.5657466053962708 batch: 147/840\n",
      "Batch loss: 0.7708255052566528 batch: 148/840\n",
      "Batch loss: 0.6588873863220215 batch: 149/840\n",
      "Batch loss: 0.6154116988182068 batch: 150/840\n",
      "Batch loss: 0.6483846306800842 batch: 151/840\n",
      "Batch loss: 0.7440237998962402 batch: 152/840\n",
      "Batch loss: 0.5171553492546082 batch: 153/840\n",
      "Batch loss: 0.6980178952217102 batch: 154/840\n",
      "Batch loss: 0.5047342777252197 batch: 155/840\n",
      "Batch loss: 0.5972023010253906 batch: 156/840\n",
      "Batch loss: 0.6109591126441956 batch: 157/840\n",
      "Batch loss: 0.44982239603996277 batch: 158/840\n",
      "Batch loss: 0.5380505919456482 batch: 159/840\n",
      "Batch loss: 0.6154554486274719 batch: 160/840\n",
      "Batch loss: 0.7193471789360046 batch: 161/840\n",
      "Batch loss: 0.6675150990486145 batch: 162/840\n",
      "Batch loss: 0.789152979850769 batch: 163/840\n",
      "Batch loss: 0.5308387875556946 batch: 164/840\n",
      "Batch loss: 0.625759482383728 batch: 165/840\n",
      "Batch loss: 0.47974473237991333 batch: 166/840\n",
      "Batch loss: 0.6617190837860107 batch: 167/840\n",
      "Batch loss: 0.5627362728118896 batch: 168/840\n",
      "Batch loss: 0.46505919098854065 batch: 169/840\n",
      "Batch loss: 0.6913007497787476 batch: 170/840\n",
      "Batch loss: 0.6743324995040894 batch: 171/840\n",
      "Batch loss: 0.6046643257141113 batch: 172/840\n",
      "Batch loss: 0.6380993127822876 batch: 173/840\n",
      "Batch loss: 0.5428820252418518 batch: 174/840\n",
      "Batch loss: 0.5404475927352905 batch: 175/840\n",
      "Batch loss: 0.6964809894561768 batch: 176/840\n",
      "Batch loss: 0.6275684237480164 batch: 177/840\n",
      "Batch loss: 0.6775177121162415 batch: 178/840\n",
      "Batch loss: 0.6391416788101196 batch: 179/840\n",
      "Batch loss: 0.5724939107894897 batch: 180/840\n",
      "Batch loss: 0.5138239860534668 batch: 181/840\n",
      "Batch loss: 0.7013383507728577 batch: 182/840\n",
      "Batch loss: 0.7408989071846008 batch: 183/840\n",
      "Batch loss: 0.5150234699249268 batch: 184/840\n",
      "Batch loss: 0.5819714069366455 batch: 185/840\n",
      "Batch loss: 0.5165325403213501 batch: 186/840\n",
      "Batch loss: 0.7404168844223022 batch: 187/840\n",
      "Batch loss: 0.5348853468894958 batch: 188/840\n",
      "Batch loss: 0.5721961855888367 batch: 189/840\n",
      "Batch loss: 0.6265196800231934 batch: 190/840\n",
      "Batch loss: 0.6770831346511841 batch: 191/840\n",
      "Batch loss: 0.5083160996437073 batch: 192/840\n",
      "Batch loss: 0.5148143172264099 batch: 193/840\n",
      "Batch loss: 0.4453507661819458 batch: 194/840\n",
      "Batch loss: 0.5690106749534607 batch: 195/840\n",
      "Batch loss: 0.7021971344947815 batch: 196/840\n",
      "Batch loss: 0.6481334567070007 batch: 197/840\n",
      "Batch loss: 0.5766311883926392 batch: 198/840\n",
      "Batch loss: 0.597041666507721 batch: 199/840\n",
      "Batch loss: 0.7789863348007202 batch: 200/840\n",
      "Batch loss: 0.5591420531272888 batch: 201/840\n",
      "Batch loss: 0.5618953108787537 batch: 202/840\n",
      "Batch loss: 0.5274844169616699 batch: 203/840\n",
      "Batch loss: 0.6430310606956482 batch: 204/840\n",
      "Batch loss: 0.7338588237762451 batch: 205/840\n",
      "Batch loss: 0.6157914996147156 batch: 206/840\n",
      "Batch loss: 0.5912761092185974 batch: 207/840\n",
      "Batch loss: 0.5601957440376282 batch: 208/840\n",
      "Batch loss: 0.5361407995223999 batch: 209/840\n",
      "Batch loss: 0.580913782119751 batch: 210/840\n",
      "Batch loss: 0.4727729558944702 batch: 211/840\n",
      "Batch loss: 0.6735657453536987 batch: 212/840\n",
      "Batch loss: 0.646751344203949 batch: 213/840\n",
      "Batch loss: 0.7153763771057129 batch: 214/840\n",
      "Batch loss: 0.5622495412826538 batch: 215/840\n",
      "Batch loss: 0.6127844452857971 batch: 216/840\n",
      "Batch loss: 0.49888771772384644 batch: 217/840\n",
      "Batch loss: 0.6371602416038513 batch: 218/840\n",
      "Batch loss: 0.6959574818611145 batch: 219/840\n",
      "Batch loss: 0.6744900345802307 batch: 220/840\n",
      "Batch loss: 0.6264640688896179 batch: 221/840\n",
      "Batch loss: 0.7960702776908875 batch: 222/840\n",
      "Batch loss: 0.5253309011459351 batch: 223/840\n",
      "Batch loss: 0.7610868811607361 batch: 224/840\n",
      "Batch loss: 0.6196381449699402 batch: 225/840\n",
      "Batch loss: 0.7118554711341858 batch: 226/840\n",
      "Batch loss: 0.6961031556129456 batch: 227/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4773269593715668 batch: 228/840\n",
      "Batch loss: 0.46884995698928833 batch: 229/840\n",
      "Batch loss: 0.598045289516449 batch: 230/840\n",
      "Batch loss: 0.48795369267463684 batch: 231/840\n",
      "Batch loss: 0.5640665292739868 batch: 232/840\n",
      "Batch loss: 0.7348002791404724 batch: 233/840\n",
      "Batch loss: 0.5155380964279175 batch: 234/840\n",
      "Batch loss: 0.6067073345184326 batch: 235/840\n",
      "Batch loss: 0.7087889909744263 batch: 236/840\n",
      "Batch loss: 0.5229277610778809 batch: 237/840\n",
      "Batch loss: 0.6974466443061829 batch: 238/840\n",
      "Batch loss: 0.6268895864486694 batch: 239/840\n",
      "Batch loss: 0.662769615650177 batch: 240/840\n",
      "Batch loss: 0.6142366528511047 batch: 241/840\n",
      "Batch loss: 0.5581411719322205 batch: 242/840\n",
      "Batch loss: 0.6437787413597107 batch: 243/840\n",
      "Batch loss: 0.6744087338447571 batch: 244/840\n",
      "Batch loss: 0.478080153465271 batch: 245/840\n",
      "Batch loss: 0.5332430005073547 batch: 246/840\n",
      "Batch loss: 0.7403622269630432 batch: 247/840\n",
      "Batch loss: 0.6882920265197754 batch: 248/840\n",
      "Batch loss: 0.9368149638175964 batch: 249/840\n",
      "Batch loss: 0.43540066480636597 batch: 250/840\n",
      "Batch loss: 0.5858629941940308 batch: 251/840\n",
      "Batch loss: 0.4874819219112396 batch: 252/840\n",
      "Batch loss: 0.6435514092445374 batch: 253/840\n",
      "Batch loss: 0.6417816877365112 batch: 254/840\n",
      "Batch loss: 0.5163751244544983 batch: 255/840\n",
      "Batch loss: 0.6889575719833374 batch: 256/840\n",
      "Batch loss: 0.5596023201942444 batch: 257/840\n",
      "Batch loss: 0.7109804749488831 batch: 258/840\n",
      "Batch loss: 0.4930587410926819 batch: 259/840\n",
      "Batch loss: 0.4898175299167633 batch: 260/840\n",
      "Batch loss: 0.5712277293205261 batch: 261/840\n",
      "Batch loss: 0.3335697650909424 batch: 262/840\n",
      "Batch loss: 0.5774818062782288 batch: 263/840\n",
      "Batch loss: 0.6350709795951843 batch: 264/840\n",
      "Batch loss: 0.5921159982681274 batch: 265/840\n",
      "Batch loss: 0.5579919815063477 batch: 266/840\n",
      "Batch loss: 0.6881041526794434 batch: 267/840\n",
      "Batch loss: 0.5739734172821045 batch: 268/840\n",
      "Batch loss: 0.5001147985458374 batch: 269/840\n",
      "Batch loss: 0.7107992768287659 batch: 270/840\n",
      "Batch loss: 0.5163523554801941 batch: 271/840\n",
      "Batch loss: 0.7518008351325989 batch: 272/840\n",
      "Batch loss: 0.7253058552742004 batch: 273/840\n",
      "Batch loss: 0.6631917357444763 batch: 274/840\n",
      "Batch loss: 0.7234178781509399 batch: 275/840\n",
      "Batch loss: 0.5562602281570435 batch: 276/840\n",
      "Batch loss: 0.5305415987968445 batch: 277/840\n",
      "Batch loss: 0.7874813079833984 batch: 278/840\n",
      "Batch loss: 0.7479070425033569 batch: 279/840\n",
      "Batch loss: 0.6355640888214111 batch: 280/840\n",
      "Batch loss: 0.5255761742591858 batch: 281/840\n",
      "Batch loss: 0.4969816207885742 batch: 282/840\n",
      "Batch loss: 0.6478201150894165 batch: 283/840\n",
      "Batch loss: 0.4885184168815613 batch: 284/840\n",
      "Batch loss: 0.6214166879653931 batch: 285/840\n",
      "Batch loss: 0.5666925311088562 batch: 286/840\n",
      "Batch loss: 0.4049116373062134 batch: 287/840\n",
      "Batch loss: 0.5336592197418213 batch: 288/840\n",
      "Batch loss: 0.7877749800682068 batch: 289/840\n",
      "Batch loss: 0.7540885210037231 batch: 290/840\n",
      "Batch loss: 0.7228567600250244 batch: 291/840\n",
      "Batch loss: 0.6873675584793091 batch: 292/840\n",
      "Batch loss: 0.7019603848457336 batch: 293/840\n",
      "Batch loss: 0.6395918130874634 batch: 294/840\n",
      "Batch loss: 0.6331131458282471 batch: 295/840\n",
      "Batch loss: 0.6607256531715393 batch: 296/840\n",
      "Batch loss: 0.5814876556396484 batch: 297/840\n",
      "Batch loss: 0.8164293169975281 batch: 298/840\n",
      "Batch loss: 0.5395269989967346 batch: 299/840\n",
      "Batch loss: 0.7369803786277771 batch: 300/840\n",
      "Batch loss: 0.733113706111908 batch: 301/840\n",
      "Batch loss: 0.6032648682594299 batch: 302/840\n",
      "Batch loss: 0.7064704895019531 batch: 303/840\n",
      "Batch loss: 0.5299069285392761 batch: 304/840\n",
      "Batch loss: 0.5422686338424683 batch: 305/840\n",
      "Batch loss: 0.6374047994613647 batch: 306/840\n",
      "Batch loss: 0.5754794478416443 batch: 307/840\n",
      "Batch loss: 0.6937443614006042 batch: 308/840\n",
      "Batch loss: 0.5803583264350891 batch: 309/840\n",
      "Batch loss: 0.8131462931632996 batch: 310/840\n",
      "Batch loss: 0.6851099133491516 batch: 311/840\n",
      "Batch loss: 0.7514159679412842 batch: 312/840\n",
      "Batch loss: 0.5761722326278687 batch: 313/840\n",
      "Batch loss: 0.5453816652297974 batch: 314/840\n",
      "Batch loss: 0.6038063168525696 batch: 315/840\n",
      "Batch loss: 0.504753828048706 batch: 316/840\n",
      "Batch loss: 0.6800634264945984 batch: 317/840\n",
      "Batch loss: 0.6271861791610718 batch: 318/840\n",
      "Batch loss: 0.71265709400177 batch: 319/840\n",
      "Batch loss: 0.5165711045265198 batch: 320/840\n",
      "Batch loss: 0.6236017942428589 batch: 321/840\n",
      "Batch loss: 0.6454260349273682 batch: 322/840\n",
      "Batch loss: 0.7941861748695374 batch: 323/840\n",
      "Batch loss: 0.6810275316238403 batch: 324/840\n",
      "Batch loss: 0.455497145652771 batch: 325/840\n",
      "Batch loss: 0.6309040784835815 batch: 326/840\n",
      "Batch loss: 0.52277010679245 batch: 327/840\n",
      "Batch loss: 0.7511200904846191 batch: 328/840\n",
      "Batch loss: 0.6133603453636169 batch: 329/840\n",
      "Batch loss: 0.6721003651618958 batch: 330/840\n",
      "Batch loss: 0.6665071249008179 batch: 331/840\n",
      "Batch loss: 0.6385632753372192 batch: 332/840\n",
      "Batch loss: 0.6156360507011414 batch: 333/840\n",
      "Batch loss: 0.6447070240974426 batch: 334/840\n",
      "Batch loss: 0.566608190536499 batch: 335/840\n",
      "Batch loss: 0.707190752029419 batch: 336/840\n",
      "Batch loss: 0.8057771921157837 batch: 337/840\n",
      "Batch loss: 0.7245209217071533 batch: 338/840\n",
      "Batch loss: 0.5529037714004517 batch: 339/840\n",
      "Batch loss: 0.7384510636329651 batch: 340/840\n",
      "Batch loss: 0.4790711998939514 batch: 341/840\n",
      "Batch loss: 0.5245950818061829 batch: 342/840\n",
      "Batch loss: 0.7435960173606873 batch: 343/840\n",
      "Batch loss: 0.6374321579933167 batch: 344/840\n",
      "Batch loss: 0.4030791223049164 batch: 345/840\n",
      "Batch loss: 0.5618811845779419 batch: 346/840\n",
      "Batch loss: 0.71349036693573 batch: 347/840\n",
      "Batch loss: 0.6041473150253296 batch: 348/840\n",
      "Batch loss: 0.6305667757987976 batch: 349/840\n",
      "Batch loss: 0.5596164464950562 batch: 350/840\n",
      "Batch loss: 0.6631383299827576 batch: 351/840\n",
      "Batch loss: 0.6361976861953735 batch: 352/840\n",
      "Batch loss: 0.7139974236488342 batch: 353/840\n",
      "Batch loss: 0.6189944744110107 batch: 354/840\n",
      "Batch loss: 0.592892050743103 batch: 355/840\n",
      "Batch loss: 0.6152547597885132 batch: 356/840\n",
      "Batch loss: 0.6042603850364685 batch: 357/840\n",
      "Batch loss: 0.7850867509841919 batch: 358/840\n",
      "Batch loss: 0.6357825398445129 batch: 359/840\n",
      "Batch loss: 0.7210673689842224 batch: 360/840\n",
      "Batch loss: 0.7305688261985779 batch: 361/840\n",
      "Batch loss: 0.5715579986572266 batch: 362/840\n",
      "Batch loss: 0.5546834468841553 batch: 363/840\n",
      "Batch loss: 0.6585351824760437 batch: 364/840\n",
      "Batch loss: 0.5751511454582214 batch: 365/840\n",
      "Batch loss: 0.6195489764213562 batch: 366/840\n",
      "Batch loss: 0.4860106408596039 batch: 367/840\n",
      "Batch loss: 0.7821192145347595 batch: 368/840\n",
      "Batch loss: 0.5608119368553162 batch: 369/840\n",
      "Batch loss: 0.6553118824958801 batch: 370/840\n",
      "Batch loss: 0.6232532262802124 batch: 371/840\n",
      "Batch loss: 0.4939485192298889 batch: 372/840\n",
      "Batch loss: 0.5703048706054688 batch: 373/840\n",
      "Batch loss: 0.7222312092781067 batch: 374/840\n",
      "Batch loss: 0.5650689601898193 batch: 375/840\n",
      "Batch loss: 0.5015692114830017 batch: 376/840\n",
      "Batch loss: 0.5947932600975037 batch: 377/840\n",
      "Batch loss: 0.6930209398269653 batch: 378/840\n",
      "Batch loss: 0.4695763885974884 batch: 379/840\n",
      "Batch loss: 0.9400700330734253 batch: 380/840\n",
      "Batch loss: 1.0033034086227417 batch: 381/840\n",
      "Batch loss: 0.640249490737915 batch: 382/840\n",
      "Batch loss: 0.6380525231361389 batch: 383/840\n",
      "Batch loss: 0.6244138479232788 batch: 384/840\n",
      "Batch loss: 0.5806334614753723 batch: 385/840\n",
      "Batch loss: 0.7280637621879578 batch: 386/840\n",
      "Batch loss: 0.5876635313034058 batch: 387/840\n",
      "Batch loss: 0.6960479617118835 batch: 388/840\n",
      "Batch loss: 0.6369596719741821 batch: 389/840\n",
      "Batch loss: 0.8316701650619507 batch: 390/840\n",
      "Batch loss: 0.5951426029205322 batch: 391/840\n",
      "Batch loss: 0.5901085138320923 batch: 392/840\n",
      "Batch loss: 0.44032588601112366 batch: 393/840\n",
      "Batch loss: 0.7182424068450928 batch: 394/840\n",
      "Batch loss: 0.5516662001609802 batch: 395/840\n",
      "Batch loss: 0.6636819243431091 batch: 396/840\n",
      "Batch loss: 0.5117454528808594 batch: 397/840\n",
      "Batch loss: 0.6943795084953308 batch: 398/840\n",
      "Batch loss: 0.4684395492076874 batch: 399/840\n",
      "Batch loss: 0.5705519914627075 batch: 400/840\n",
      "Batch loss: 0.7371229529380798 batch: 401/840\n",
      "Batch loss: 0.5631465911865234 batch: 402/840\n",
      "Batch loss: 0.6322831511497498 batch: 403/840\n",
      "Batch loss: 0.5282383561134338 batch: 404/840\n",
      "Batch loss: 0.584873378276825 batch: 405/840\n",
      "Batch loss: 0.6373178362846375 batch: 406/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5322454571723938 batch: 407/840\n",
      "Batch loss: 0.6290424466133118 batch: 408/840\n",
      "Batch loss: 0.699280321598053 batch: 409/840\n",
      "Batch loss: 0.87884122133255 batch: 410/840\n",
      "Batch loss: 0.7878567576408386 batch: 411/840\n",
      "Batch loss: 0.7522141933441162 batch: 412/840\n",
      "Batch loss: 0.672089159488678 batch: 413/840\n",
      "Batch loss: 0.5225752592086792 batch: 414/840\n",
      "Batch loss: 0.8961448669433594 batch: 415/840\n",
      "Batch loss: 0.44602280855178833 batch: 416/840\n",
      "Batch loss: 0.5941513180732727 batch: 417/840\n",
      "Batch loss: 0.7514727115631104 batch: 418/840\n",
      "Batch loss: 0.6251924633979797 batch: 419/840\n",
      "Batch loss: 0.6919043064117432 batch: 420/840\n",
      "Batch loss: 0.5423755049705505 batch: 421/840\n",
      "Batch loss: 0.5515317320823669 batch: 422/840\n",
      "Batch loss: 0.7362473011016846 batch: 423/840\n",
      "Batch loss: 0.6893961429595947 batch: 424/840\n",
      "Batch loss: 0.6998827457427979 batch: 425/840\n",
      "Batch loss: 0.5869777202606201 batch: 426/840\n",
      "Batch loss: 0.5497910976409912 batch: 427/840\n",
      "Batch loss: 0.7341428399085999 batch: 428/840\n",
      "Batch loss: 0.5130681991577148 batch: 429/840\n",
      "Batch loss: 0.7187865376472473 batch: 430/840\n",
      "Batch loss: 0.555692732334137 batch: 431/840\n",
      "Batch loss: 0.6318865418434143 batch: 432/840\n",
      "Batch loss: 0.4912225604057312 batch: 433/840\n",
      "Batch loss: 0.4782577157020569 batch: 434/840\n",
      "Batch loss: 0.7751396894454956 batch: 435/840\n",
      "Batch loss: 0.6663411855697632 batch: 436/840\n",
      "Batch loss: 0.689359724521637 batch: 437/840\n",
      "Batch loss: 0.40141844749450684 batch: 438/840\n",
      "Batch loss: 0.6580355763435364 batch: 439/840\n",
      "Batch loss: 0.7116983532905579 batch: 440/840\n",
      "Batch loss: 0.6846888065338135 batch: 441/840\n",
      "Batch loss: 0.8853726387023926 batch: 442/840\n",
      "Batch loss: 0.6224777102470398 batch: 443/840\n",
      "Batch loss: 0.6931198239326477 batch: 444/840\n",
      "Batch loss: 0.5522392988204956 batch: 445/840\n",
      "Batch loss: 0.6112095713615417 batch: 446/840\n",
      "Batch loss: 0.6228811144828796 batch: 447/840\n",
      "Batch loss: 0.6417009830474854 batch: 448/840\n",
      "Batch loss: 0.5908865928649902 batch: 449/840\n",
      "Batch loss: 0.6161062121391296 batch: 450/840\n",
      "Batch loss: 0.6174285411834717 batch: 451/840\n",
      "Batch loss: 0.6432651281356812 batch: 452/840\n",
      "Batch loss: 0.7003354430198669 batch: 453/840\n",
      "Batch loss: 0.4770389199256897 batch: 454/840\n",
      "Batch loss: 0.7082405686378479 batch: 455/840\n",
      "Batch loss: 0.5891145467758179 batch: 456/840\n",
      "Batch loss: 0.5714473128318787 batch: 457/840\n",
      "Batch loss: 0.6320388913154602 batch: 458/840\n",
      "Batch loss: 0.5740613341331482 batch: 459/840\n",
      "Batch loss: 0.4391363263130188 batch: 460/840\n",
      "Batch loss: 0.6688092947006226 batch: 461/840\n",
      "Batch loss: 0.5568661093711853 batch: 462/840\n",
      "Batch loss: 0.8135719895362854 batch: 463/840\n",
      "Batch loss: 0.5150141716003418 batch: 464/840\n",
      "Batch loss: 0.8374230265617371 batch: 465/840\n",
      "Batch loss: 0.5917041897773743 batch: 466/840\n",
      "Batch loss: 0.9203794002532959 batch: 467/840\n",
      "Batch loss: 0.5760747790336609 batch: 468/840\n",
      "Batch loss: 0.5706791281700134 batch: 469/840\n",
      "Batch loss: 0.6049445867538452 batch: 470/840\n",
      "Batch loss: 0.6623214483261108 batch: 471/840\n",
      "Batch loss: 0.6437536478042603 batch: 472/840\n",
      "Batch loss: 0.7042834758758545 batch: 473/840\n",
      "Batch loss: 0.5222921967506409 batch: 474/840\n",
      "Batch loss: 0.5830422043800354 batch: 475/840\n",
      "Batch loss: 0.677880048751831 batch: 476/840\n",
      "Batch loss: 0.48653343319892883 batch: 477/840\n",
      "Batch loss: 0.5570404529571533 batch: 478/840\n",
      "Batch loss: 0.48558157682418823 batch: 479/840\n",
      "Batch loss: 0.5792885422706604 batch: 480/840\n",
      "Batch loss: 0.6762422323226929 batch: 481/840\n",
      "Batch loss: 0.6298931837081909 batch: 482/840\n",
      "Batch loss: 0.7584133744239807 batch: 483/840\n",
      "Batch loss: 0.5640512704849243 batch: 484/840\n",
      "Batch loss: 0.656666100025177 batch: 485/840\n",
      "Batch loss: 0.6453942656517029 batch: 486/840\n",
      "Batch loss: 0.5590928196907043 batch: 487/840\n",
      "Batch loss: 0.630646824836731 batch: 488/840\n",
      "Batch loss: 0.6053840517997742 batch: 489/840\n",
      "Batch loss: 0.6382092833518982 batch: 490/840\n",
      "Batch loss: 0.3880001902580261 batch: 491/840\n",
      "Batch loss: 0.7423239946365356 batch: 492/840\n",
      "Batch loss: 0.6853811740875244 batch: 493/840\n",
      "Batch loss: 0.4157203733921051 batch: 494/840\n",
      "Batch loss: 0.7081514000892639 batch: 495/840\n",
      "Batch loss: 0.6532291173934937 batch: 496/840\n",
      "Batch loss: 0.5861707925796509 batch: 497/840\n",
      "Batch loss: 0.5022900104522705 batch: 498/840\n",
      "Batch loss: 0.5769599676132202 batch: 499/840\n",
      "Batch loss: 0.5476093292236328 batch: 500/840\n",
      "Batch loss: 0.5724483728408813 batch: 501/840\n",
      "Batch loss: 0.6793115735054016 batch: 502/840\n",
      "Batch loss: 0.4528874456882477 batch: 503/840\n",
      "Batch loss: 0.6106045842170715 batch: 504/840\n",
      "Batch loss: 0.7340139150619507 batch: 505/840\n",
      "Batch loss: 0.5603891015052795 batch: 506/840\n",
      "Batch loss: 0.6797479391098022 batch: 507/840\n",
      "Batch loss: 0.5305485725402832 batch: 508/840\n",
      "Batch loss: 0.6130673885345459 batch: 509/840\n",
      "Batch loss: 0.6615445017814636 batch: 510/840\n",
      "Batch loss: 0.46072593331336975 batch: 511/840\n",
      "Batch loss: 0.6092372536659241 batch: 512/840\n",
      "Batch loss: 0.593991219997406 batch: 513/840\n",
      "Batch loss: 0.6389920711517334 batch: 514/840\n",
      "Batch loss: 0.4751697778701782 batch: 515/840\n",
      "Batch loss: 0.7003337740898132 batch: 516/840\n",
      "Batch loss: 0.5924117565155029 batch: 517/840\n",
      "Batch loss: 0.6383491158485413 batch: 518/840\n",
      "Batch loss: 0.7018968462944031 batch: 519/840\n",
      "Batch loss: 0.7343624234199524 batch: 520/840\n",
      "Batch loss: 0.6930654644966125 batch: 521/840\n",
      "Batch loss: 0.5693054795265198 batch: 522/840\n",
      "Batch loss: 0.46109557151794434 batch: 523/840\n",
      "Batch loss: 0.6391705870628357 batch: 524/840\n",
      "Batch loss: 0.5721644759178162 batch: 525/840\n",
      "Batch loss: 0.6759772300720215 batch: 526/840\n",
      "Batch loss: 0.7159601449966431 batch: 527/840\n",
      "Batch loss: 0.6130024194717407 batch: 528/840\n",
      "Batch loss: 0.5437930822372437 batch: 529/840\n",
      "Batch loss: 0.691387951374054 batch: 530/840\n",
      "Batch loss: 0.5081195831298828 batch: 531/840\n",
      "Batch loss: 0.47150662541389465 batch: 532/840\n",
      "Batch loss: 0.727719247341156 batch: 533/840\n",
      "Batch loss: 0.6557338237762451 batch: 534/840\n",
      "Batch loss: 0.619204044342041 batch: 535/840\n",
      "Batch loss: 0.7233704924583435 batch: 536/840\n",
      "Batch loss: 0.5716590881347656 batch: 537/840\n",
      "Batch loss: 0.5236033201217651 batch: 538/840\n",
      "Batch loss: 0.6348752975463867 batch: 539/840\n",
      "Batch loss: 0.830704927444458 batch: 540/840\n",
      "Batch loss: 0.5134446024894714 batch: 541/840\n",
      "Batch loss: 0.6709269881248474 batch: 542/840\n",
      "Batch loss: 0.48126038908958435 batch: 543/840\n",
      "Batch loss: 0.5670129656791687 batch: 544/840\n",
      "Batch loss: 0.6787556409835815 batch: 545/840\n",
      "Batch loss: 0.6542748808860779 batch: 546/840\n",
      "Batch loss: 0.53883957862854 batch: 547/840\n",
      "Batch loss: 0.44796866178512573 batch: 548/840\n",
      "Batch loss: 0.5121161937713623 batch: 549/840\n",
      "Batch loss: 0.6021077632904053 batch: 550/840\n",
      "Batch loss: 0.6962472796440125 batch: 551/840\n",
      "Batch loss: 0.6412487626075745 batch: 552/840\n",
      "Batch loss: 0.6218100190162659 batch: 553/840\n",
      "Batch loss: 0.6584031581878662 batch: 554/840\n",
      "Batch loss: 0.616638720035553 batch: 555/840\n",
      "Batch loss: 0.6216123700141907 batch: 556/840\n",
      "Batch loss: 0.6779518723487854 batch: 557/840\n",
      "Batch loss: 0.6520467400550842 batch: 558/840\n",
      "Batch loss: 0.5430906414985657 batch: 559/840\n",
      "Batch loss: 0.7107716202735901 batch: 560/840\n",
      "Batch loss: 0.5549176931381226 batch: 561/840\n",
      "Batch loss: 0.5646111965179443 batch: 562/840\n",
      "Batch loss: 0.4895932078361511 batch: 563/840\n",
      "Batch loss: 0.6568499803543091 batch: 564/840\n",
      "Batch loss: 0.7979480028152466 batch: 565/840\n",
      "Batch loss: 0.7082401514053345 batch: 566/840\n",
      "Batch loss: 0.7598574161529541 batch: 567/840\n",
      "Batch loss: 0.5942183136940002 batch: 568/840\n",
      "Batch loss: 0.5962017178535461 batch: 569/840\n",
      "Batch loss: 0.4254729151725769 batch: 570/840\n",
      "Batch loss: 0.6534597873687744 batch: 571/840\n",
      "Batch loss: 0.7653396129608154 batch: 572/840\n",
      "Batch loss: 0.6088857054710388 batch: 573/840\n",
      "Batch loss: 0.7031378746032715 batch: 574/840\n",
      "Batch loss: 0.5820451974868774 batch: 575/840\n",
      "Batch loss: 0.6112561225891113 batch: 576/840\n",
      "Batch loss: 0.5256468057632446 batch: 577/840\n",
      "Batch loss: 0.738126277923584 batch: 578/840\n",
      "Batch loss: 0.630729079246521 batch: 579/840\n",
      "Batch loss: 0.7612872123718262 batch: 580/840\n",
      "Batch loss: 0.6521339416503906 batch: 581/840\n",
      "Batch loss: 0.7361297607421875 batch: 582/840\n",
      "Batch loss: 0.5905354022979736 batch: 583/840\n",
      "Batch loss: 0.8549981713294983 batch: 584/840\n",
      "Batch loss: 0.6025715470314026 batch: 585/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6413366198539734 batch: 586/840\n",
      "Batch loss: 0.6614829301834106 batch: 587/840\n",
      "Batch loss: 0.6117152571678162 batch: 588/840\n",
      "Batch loss: 0.7502248287200928 batch: 589/840\n",
      "Batch loss: 0.4205474853515625 batch: 590/840\n",
      "Batch loss: 0.4354659616947174 batch: 591/840\n",
      "Batch loss: 0.4998418390750885 batch: 592/840\n",
      "Batch loss: 0.8253275156021118 batch: 593/840\n",
      "Batch loss: 0.6586718559265137 batch: 594/840\n",
      "Batch loss: 0.30704745650291443 batch: 595/840\n",
      "Batch loss: 0.4726310968399048 batch: 596/840\n",
      "Batch loss: 0.6004575490951538 batch: 597/840\n",
      "Batch loss: 0.43487539887428284 batch: 598/840\n",
      "Batch loss: 0.6150572299957275 batch: 599/840\n",
      "Batch loss: 0.785199761390686 batch: 600/840\n",
      "Batch loss: 0.7335411310195923 batch: 601/840\n",
      "Batch loss: 0.49271923303604126 batch: 602/840\n",
      "Batch loss: 0.5401470065116882 batch: 603/840\n",
      "Batch loss: 0.6728925108909607 batch: 604/840\n",
      "Batch loss: 0.7537436485290527 batch: 605/840\n",
      "Batch loss: 0.8493638634681702 batch: 606/840\n",
      "Batch loss: 0.5750709772109985 batch: 607/840\n",
      "Batch loss: 0.6423200964927673 batch: 608/840\n",
      "Batch loss: 0.4950275719165802 batch: 609/840\n",
      "Batch loss: 0.6399682760238647 batch: 610/840\n",
      "Batch loss: 0.6422601938247681 batch: 611/840\n",
      "Batch loss: 0.740812361240387 batch: 612/840\n",
      "Batch loss: 0.7536125779151917 batch: 613/840\n",
      "Batch loss: 0.6058425307273865 batch: 614/840\n",
      "Batch loss: 0.47112154960632324 batch: 615/840\n",
      "Batch loss: 0.5629440546035767 batch: 616/840\n",
      "Batch loss: 0.540911078453064 batch: 617/840\n",
      "Batch loss: 0.5821003913879395 batch: 618/840\n",
      "Batch loss: 0.8178130388259888 batch: 619/840\n",
      "Batch loss: 0.52071213722229 batch: 620/840\n",
      "Batch loss: 0.6995669603347778 batch: 621/840\n",
      "Batch loss: 0.6207782626152039 batch: 622/840\n",
      "Batch loss: 0.6682758331298828 batch: 623/840\n",
      "Batch loss: 0.7379850149154663 batch: 624/840\n",
      "Batch loss: 0.6389577984809875 batch: 625/840\n",
      "Batch loss: 0.5161448121070862 batch: 626/840\n",
      "Batch loss: 0.550246000289917 batch: 627/840\n",
      "Batch loss: 0.7079382538795471 batch: 628/840\n",
      "Batch loss: 0.6662198901176453 batch: 629/840\n",
      "Batch loss: 0.5283733606338501 batch: 630/840\n",
      "Batch loss: 0.6407064199447632 batch: 631/840\n",
      "Batch loss: 0.5921967625617981 batch: 632/840\n",
      "Batch loss: 0.5122734904289246 batch: 633/840\n",
      "Batch loss: 0.6022642254829407 batch: 634/840\n",
      "Batch loss: 0.517062783241272 batch: 635/840\n",
      "Batch loss: 0.560640811920166 batch: 636/840\n",
      "Batch loss: 0.4987196624279022 batch: 637/840\n",
      "Batch loss: 0.6446515917778015 batch: 638/840\n",
      "Batch loss: 0.6830532550811768 batch: 639/840\n",
      "Batch loss: 0.6406967639923096 batch: 640/840\n",
      "Batch loss: 0.8255777955055237 batch: 641/840\n",
      "Batch loss: 0.5968844294548035 batch: 642/840\n",
      "Batch loss: 0.9632092118263245 batch: 643/840\n",
      "Batch loss: 0.6343184113502502 batch: 644/840\n",
      "Batch loss: 0.47930821776390076 batch: 645/840\n",
      "Batch loss: 0.6015228033065796 batch: 646/840\n",
      "Batch loss: 0.7505815625190735 batch: 647/840\n",
      "Batch loss: 0.6175368428230286 batch: 648/840\n",
      "Batch loss: 0.5970281362533569 batch: 649/840\n",
      "Batch loss: 0.5421432256698608 batch: 650/840\n",
      "Batch loss: 0.6532044410705566 batch: 651/840\n",
      "Batch loss: 0.6572248935699463 batch: 652/840\n",
      "Batch loss: 0.49922168254852295 batch: 653/840\n",
      "Batch loss: 0.8757549524307251 batch: 654/840\n",
      "Batch loss: 0.5755211114883423 batch: 655/840\n",
      "Batch loss: 0.6856259107589722 batch: 656/840\n",
      "Batch loss: 0.530108630657196 batch: 657/840\n",
      "Batch loss: 0.63470059633255 batch: 658/840\n",
      "Batch loss: 0.6625553369522095 batch: 659/840\n",
      "Batch loss: 0.5570699572563171 batch: 660/840\n",
      "Batch loss: 0.741970419883728 batch: 661/840\n",
      "Batch loss: 0.638063371181488 batch: 662/840\n",
      "Batch loss: 0.6140555143356323 batch: 663/840\n",
      "Batch loss: 0.6626144647598267 batch: 664/840\n",
      "Batch loss: 0.7665949463844299 batch: 665/840\n",
      "Batch loss: 0.6187500357627869 batch: 666/840\n",
      "Batch loss: 0.6756354570388794 batch: 667/840\n",
      "Batch loss: 0.6380197405815125 batch: 668/840\n",
      "Batch loss: 0.5836050510406494 batch: 669/840\n",
      "Batch loss: 0.6006343364715576 batch: 670/840\n",
      "Batch loss: 0.6133929491043091 batch: 671/840\n",
      "Batch loss: 0.5626880526542664 batch: 672/840\n",
      "Batch loss: 1.0222645998001099 batch: 673/840\n",
      "Batch loss: 0.6874274611473083 batch: 674/840\n",
      "Batch loss: 0.6189028024673462 batch: 675/840\n",
      "Batch loss: 0.5260934829711914 batch: 676/840\n",
      "Batch loss: 0.5852232575416565 batch: 677/840\n",
      "Batch loss: 0.6703851222991943 batch: 678/840\n",
      "Batch loss: 0.738777220249176 batch: 679/840\n",
      "Batch loss: 0.7162445783615112 batch: 680/840\n",
      "Batch loss: 0.6515851020812988 batch: 681/840\n",
      "Batch loss: 0.5594914555549622 batch: 682/840\n",
      "Batch loss: 0.5701388716697693 batch: 683/840\n",
      "Batch loss: 0.7541883587837219 batch: 684/840\n",
      "Batch loss: 0.6398715376853943 batch: 685/840\n",
      "Batch loss: 0.7824050784111023 batch: 686/840\n",
      "Batch loss: 0.6483238935470581 batch: 687/840\n",
      "Batch loss: 0.5333116054534912 batch: 688/840\n",
      "Batch loss: 0.5017670392990112 batch: 689/840\n",
      "Batch loss: 0.547770619392395 batch: 690/840\n",
      "Batch loss: 0.6755842566490173 batch: 691/840\n",
      "Batch loss: 0.5508931875228882 batch: 692/840\n",
      "Batch loss: 0.6494296193122864 batch: 693/840\n",
      "Batch loss: 0.7977785468101501 batch: 694/840\n",
      "Batch loss: 0.751020610332489 batch: 695/840\n",
      "Batch loss: 0.633201003074646 batch: 696/840\n",
      "Batch loss: 0.5432584285736084 batch: 697/840\n",
      "Batch loss: 0.5485438704490662 batch: 698/840\n",
      "Batch loss: 0.7369927167892456 batch: 699/840\n",
      "Batch loss: 0.810921847820282 batch: 700/840\n",
      "Batch loss: 0.844235897064209 batch: 701/840\n",
      "Batch loss: 0.670396625995636 batch: 702/840\n",
      "Batch loss: 0.6128900647163391 batch: 703/840\n",
      "Batch loss: 0.7186647057533264 batch: 704/840\n",
      "Batch loss: 0.6414781212806702 batch: 705/840\n",
      "Batch loss: 0.635908305644989 batch: 706/840\n",
      "Batch loss: 0.6343871355056763 batch: 707/840\n",
      "Batch loss: 0.6925700306892395 batch: 708/840\n",
      "Batch loss: 0.7156246304512024 batch: 709/840\n",
      "Batch loss: 0.7661969661712646 batch: 710/840\n",
      "Batch loss: 0.43140342831611633 batch: 711/840\n",
      "Batch loss: 0.642937421798706 batch: 712/840\n",
      "Batch loss: 0.4982008635997772 batch: 713/840\n",
      "Batch loss: 0.6785765290260315 batch: 714/840\n",
      "Batch loss: 0.5914632678031921 batch: 715/840\n",
      "Batch loss: 0.6616377830505371 batch: 716/840\n",
      "Batch loss: 0.5408468842506409 batch: 717/840\n",
      "Batch loss: 0.5442501306533813 batch: 718/840\n",
      "Batch loss: 0.5109111666679382 batch: 719/840\n",
      "Batch loss: 0.569847822189331 batch: 720/840\n",
      "Batch loss: 0.7650351524353027 batch: 721/840\n",
      "Batch loss: 0.8529992699623108 batch: 722/840\n",
      "Batch loss: 0.44808492064476013 batch: 723/840\n",
      "Batch loss: 0.7166257500648499 batch: 724/840\n",
      "Batch loss: 0.6248912811279297 batch: 725/840\n",
      "Batch loss: 0.6326414346694946 batch: 726/840\n",
      "Batch loss: 0.8157463669776917 batch: 727/840\n",
      "Batch loss: 0.7429158687591553 batch: 728/840\n",
      "Batch loss: 0.6954141855239868 batch: 729/840\n",
      "Batch loss: 0.5844292640686035 batch: 730/840\n",
      "Batch loss: 0.49715760350227356 batch: 731/840\n",
      "Batch loss: 0.7326433062553406 batch: 732/840\n",
      "Batch loss: 0.5528890490531921 batch: 733/840\n",
      "Batch loss: 0.5999974012374878 batch: 734/840\n",
      "Batch loss: 0.6222541332244873 batch: 735/840\n",
      "Batch loss: 0.7393093109130859 batch: 736/840\n",
      "Batch loss: 0.7482357621192932 batch: 737/840\n",
      "Batch loss: 0.47458842396736145 batch: 738/840\n",
      "Batch loss: 0.7574465870857239 batch: 739/840\n",
      "Batch loss: 0.802153468132019 batch: 740/840\n",
      "Batch loss: 0.5751532912254333 batch: 741/840\n",
      "Batch loss: 0.5620622038841248 batch: 742/840\n",
      "Batch loss: 0.4065576493740082 batch: 743/840\n",
      "Batch loss: 0.5117288827896118 batch: 744/840\n",
      "Batch loss: 0.5750569701194763 batch: 745/840\n",
      "Batch loss: 0.6023653149604797 batch: 746/840\n",
      "Batch loss: 0.6358316540718079 batch: 747/840\n",
      "Batch loss: 0.6256625652313232 batch: 748/840\n",
      "Batch loss: 0.48964595794677734 batch: 749/840\n",
      "Batch loss: 0.5520215034484863 batch: 750/840\n",
      "Batch loss: 0.6385996341705322 batch: 751/840\n",
      "Batch loss: 0.7684612274169922 batch: 752/840\n",
      "Batch loss: 0.5197076201438904 batch: 753/840\n",
      "Batch loss: 0.6656026244163513 batch: 754/840\n",
      "Batch loss: 0.6621477007865906 batch: 755/840\n",
      "Batch loss: 0.5325576663017273 batch: 756/840\n",
      "Batch loss: 0.6455357074737549 batch: 757/840\n",
      "Batch loss: 0.5632897019386292 batch: 758/840\n",
      "Batch loss: 0.5511366724967957 batch: 759/840\n",
      "Batch loss: 0.5777307748794556 batch: 760/840\n",
      "Batch loss: 0.5406115651130676 batch: 761/840\n",
      "Batch loss: 0.7301223278045654 batch: 762/840\n",
      "Batch loss: 0.45933136343955994 batch: 763/840\n",
      "Batch loss: 0.6179913878440857 batch: 764/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.41795018315315247 batch: 765/840\n",
      "Batch loss: 0.619662880897522 batch: 766/840\n",
      "Batch loss: 0.898394763469696 batch: 767/840\n",
      "Batch loss: 0.7107377648353577 batch: 768/840\n",
      "Batch loss: 0.6453225612640381 batch: 769/840\n",
      "Batch loss: 0.5436524748802185 batch: 770/840\n",
      "Batch loss: 0.6763499975204468 batch: 771/840\n",
      "Batch loss: 0.483007550239563 batch: 772/840\n",
      "Batch loss: 0.5038214325904846 batch: 773/840\n",
      "Batch loss: 0.50376296043396 batch: 774/840\n",
      "Batch loss: 0.6219906806945801 batch: 775/840\n",
      "Batch loss: 0.5490787029266357 batch: 776/840\n",
      "Batch loss: 0.48236599564552307 batch: 777/840\n",
      "Batch loss: 0.5030001997947693 batch: 778/840\n",
      "Batch loss: 0.7370665669441223 batch: 779/840\n",
      "Batch loss: 0.5727481245994568 batch: 780/840\n",
      "Batch loss: 0.5302107930183411 batch: 781/840\n",
      "Batch loss: 0.6718466877937317 batch: 782/840\n",
      "Batch loss: 0.4620470404624939 batch: 783/840\n",
      "Batch loss: 0.6622456312179565 batch: 784/840\n",
      "Batch loss: 0.5017804503440857 batch: 785/840\n",
      "Batch loss: 0.6025637984275818 batch: 786/840\n",
      "Batch loss: 0.6070781946182251 batch: 787/840\n",
      "Batch loss: 0.7285681366920471 batch: 788/840\n",
      "Batch loss: 0.7319190502166748 batch: 789/840\n",
      "Batch loss: 0.6576252579689026 batch: 790/840\n",
      "Batch loss: 0.6787466406822205 batch: 791/840\n",
      "Batch loss: 0.4076040983200073 batch: 792/840\n",
      "Batch loss: 0.5607892274856567 batch: 793/840\n",
      "Batch loss: 0.6331566572189331 batch: 794/840\n",
      "Batch loss: 0.6934766173362732 batch: 795/840\n",
      "Batch loss: 0.6609005928039551 batch: 796/840\n",
      "Batch loss: 0.663100004196167 batch: 797/840\n",
      "Batch loss: 0.624847948551178 batch: 798/840\n",
      "Batch loss: 0.615920901298523 batch: 799/840\n",
      "Batch loss: 0.5331419706344604 batch: 800/840\n",
      "Batch loss: 0.7074882984161377 batch: 801/840\n",
      "Batch loss: 0.5343488454818726 batch: 802/840\n",
      "Batch loss: 0.4762311577796936 batch: 803/840\n",
      "Batch loss: 0.7285926342010498 batch: 804/840\n",
      "Batch loss: 0.5508307814598083 batch: 805/840\n",
      "Batch loss: 0.5847902894020081 batch: 806/840\n",
      "Batch loss: 0.5580186247825623 batch: 807/840\n",
      "Batch loss: 0.5846338868141174 batch: 808/840\n",
      "Batch loss: 0.5958237648010254 batch: 809/840\n",
      "Batch loss: 0.6315876245498657 batch: 810/840\n",
      "Batch loss: 0.5400842428207397 batch: 811/840\n",
      "Batch loss: 0.6397668719291687 batch: 812/840\n",
      "Batch loss: 0.5495477318763733 batch: 813/840\n",
      "Batch loss: 0.5912976264953613 batch: 814/840\n",
      "Batch loss: 0.6755450367927551 batch: 815/840\n",
      "Batch loss: 0.6696791052818298 batch: 816/840\n",
      "Batch loss: 0.8227161169052124 batch: 817/840\n",
      "Batch loss: 0.6898368000984192 batch: 818/840\n",
      "Batch loss: 0.4849810004234314 batch: 819/840\n",
      "Batch loss: 0.5515204071998596 batch: 820/840\n",
      "Batch loss: 0.7222538590431213 batch: 821/840\n",
      "Batch loss: 0.5643680691719055 batch: 822/840\n",
      "Batch loss: 0.6878578066825867 batch: 823/840\n",
      "Batch loss: 0.786053478717804 batch: 824/840\n",
      "Batch loss: 0.7635923027992249 batch: 825/840\n",
      "Batch loss: 0.6444634795188904 batch: 826/840\n",
      "Batch loss: 0.5222848653793335 batch: 827/840\n",
      "Batch loss: 0.6700548529624939 batch: 828/840\n",
      "Batch loss: 0.5348095893859863 batch: 829/840\n",
      "Batch loss: 0.7286996245384216 batch: 830/840\n",
      "Batch loss: 0.5699361562728882 batch: 831/840\n",
      "Batch loss: 0.7051595449447632 batch: 832/840\n",
      "Batch loss: 0.6281506419181824 batch: 833/840\n",
      "Batch loss: 0.6106591820716858 batch: 834/840\n",
      "Batch loss: 0.47078973054885864 batch: 835/840\n",
      "Batch loss: 0.630265474319458 batch: 836/840\n",
      "Batch loss: 0.6286699175834656 batch: 837/840\n",
      "Batch loss: 0.6714129447937012 batch: 838/840\n",
      "Batch loss: 0.5562722682952881 batch: 839/840\n",
      "Batch loss: 0.5839744806289673 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 12/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.816\n",
      "Running epoch 13/15\n",
      "Batch loss: 0.5036607980728149 batch: 1/840\n",
      "Batch loss: 0.9804290533065796 batch: 2/840\n",
      "Batch loss: 0.6159551739692688 batch: 3/840\n",
      "Batch loss: 0.6258542537689209 batch: 4/840\n",
      "Batch loss: 0.5491620898246765 batch: 5/840\n",
      "Batch loss: 0.40424561500549316 batch: 6/840\n",
      "Batch loss: 0.5102386474609375 batch: 7/840\n",
      "Batch loss: 0.6457424759864807 batch: 8/840\n",
      "Batch loss: 0.5184339880943298 batch: 9/840\n",
      "Batch loss: 0.5641844272613525 batch: 10/840\n",
      "Batch loss: 0.5643116235733032 batch: 11/840\n",
      "Batch loss: 0.5190249085426331 batch: 12/840\n",
      "Batch loss: 0.542433500289917 batch: 13/840\n",
      "Batch loss: 0.6189667582511902 batch: 14/840\n",
      "Batch loss: 0.6765131950378418 batch: 15/840\n",
      "Batch loss: 0.43194279074668884 batch: 16/840\n",
      "Batch loss: 0.4812396764755249 batch: 17/840\n",
      "Batch loss: 0.6684117317199707 batch: 18/840\n",
      "Batch loss: 0.6707934737205505 batch: 19/840\n",
      "Batch loss: 0.7447673678398132 batch: 20/840\n",
      "Batch loss: 0.6327390074729919 batch: 21/840\n",
      "Batch loss: 0.5921540260314941 batch: 22/840\n",
      "Batch loss: 0.5779668092727661 batch: 23/840\n",
      "Batch loss: 0.4744459092617035 batch: 24/840\n",
      "Batch loss: 0.573089599609375 batch: 25/840\n",
      "Batch loss: 0.6618281602859497 batch: 26/840\n",
      "Batch loss: 0.5594446659088135 batch: 27/840\n",
      "Batch loss: 0.6392179727554321 batch: 28/840\n",
      "Batch loss: 0.6788428425788879 batch: 29/840\n",
      "Batch loss: 0.4903554618358612 batch: 30/840\n",
      "Batch loss: 0.6167500615119934 batch: 31/840\n",
      "Batch loss: 0.6243274211883545 batch: 32/840\n",
      "Batch loss: 0.6036558747291565 batch: 33/840\n",
      "Batch loss: 0.5205501914024353 batch: 34/840\n",
      "Batch loss: 0.5708192586898804 batch: 35/840\n",
      "Batch loss: 0.6097880005836487 batch: 36/840\n",
      "Batch loss: 0.633003294467926 batch: 37/840\n",
      "Batch loss: 0.5637083053588867 batch: 38/840\n",
      "Batch loss: 0.7452789545059204 batch: 39/840\n",
      "Batch loss: 0.5830608606338501 batch: 40/840\n",
      "Batch loss: 0.7264846563339233 batch: 41/840\n",
      "Batch loss: 0.6240135431289673 batch: 42/840\n",
      "Batch loss: 0.6399986743927002 batch: 43/840\n",
      "Batch loss: 0.7261914610862732 batch: 44/840\n",
      "Batch loss: 0.6864904761314392 batch: 45/840\n",
      "Batch loss: 0.5862768888473511 batch: 46/840\n",
      "Batch loss: 0.47258347272872925 batch: 47/840\n",
      "Batch loss: 0.6745566725730896 batch: 48/840\n",
      "Batch loss: 0.5301948189735413 batch: 49/840\n",
      "Batch loss: 0.7220573425292969 batch: 50/840\n",
      "Batch loss: 0.6334478855133057 batch: 51/840\n",
      "Batch loss: 0.8267577290534973 batch: 52/840\n",
      "Batch loss: 0.5465923547744751 batch: 53/840\n",
      "Batch loss: 0.6656922101974487 batch: 54/840\n",
      "Batch loss: 0.5564695596694946 batch: 55/840\n",
      "Batch loss: 0.576797604560852 batch: 56/840\n",
      "Batch loss: 0.6597208380699158 batch: 57/840\n",
      "Batch loss: 0.5676340460777283 batch: 58/840\n",
      "Batch loss: 0.5264391303062439 batch: 59/840\n",
      "Batch loss: 0.48545515537261963 batch: 60/840\n",
      "Batch loss: 0.738350510597229 batch: 61/840\n",
      "Batch loss: 0.5960401892662048 batch: 62/840\n",
      "Batch loss: 0.6468268632888794 batch: 63/840\n",
      "Batch loss: 0.6247274875640869 batch: 64/840\n",
      "Batch loss: 0.5536025762557983 batch: 65/840\n",
      "Batch loss: 0.6301259398460388 batch: 66/840\n",
      "Batch loss: 0.6281054615974426 batch: 67/840\n",
      "Batch loss: 0.6497933864593506 batch: 68/840\n",
      "Batch loss: 0.7119879722595215 batch: 69/840\n",
      "Batch loss: 0.5394319295883179 batch: 70/840\n",
      "Batch loss: 0.6318526268005371 batch: 71/840\n",
      "Batch loss: 0.7176962494850159 batch: 72/840\n",
      "Batch loss: 0.6988381743431091 batch: 73/840\n",
      "Batch loss: 0.7547297477722168 batch: 74/840\n",
      "Batch loss: 0.5561135411262512 batch: 75/840\n",
      "Batch loss: 0.4378223419189453 batch: 76/840\n",
      "Batch loss: 0.5701354146003723 batch: 77/840\n",
      "Batch loss: 0.8553322553634644 batch: 78/840\n",
      "Batch loss: 0.5467815399169922 batch: 79/840\n",
      "Batch loss: 0.6398679614067078 batch: 80/840\n",
      "Batch loss: 0.5594854950904846 batch: 81/840\n",
      "Batch loss: 0.6121453046798706 batch: 82/840\n",
      "Batch loss: 0.5264843106269836 batch: 83/840\n",
      "Batch loss: 0.7233469486236572 batch: 84/840\n",
      "Batch loss: 0.6251479983329773 batch: 85/840\n",
      "Batch loss: 0.9217491745948792 batch: 86/840\n",
      "Batch loss: 0.5426027178764343 batch: 87/840\n",
      "Batch loss: 0.39191004633903503 batch: 88/840\n",
      "Batch loss: 0.41579753160476685 batch: 89/840\n",
      "Batch loss: 0.5240668654441833 batch: 90/840\n",
      "Batch loss: 0.6592557430267334 batch: 91/840\n",
      "Batch loss: 0.5711017847061157 batch: 92/840\n",
      "Batch loss: 0.6396788358688354 batch: 93/840\n",
      "Batch loss: 0.5283372402191162 batch: 94/840\n",
      "Batch loss: 0.528456449508667 batch: 95/840\n",
      "Batch loss: 0.6642158031463623 batch: 96/840\n",
      "Batch loss: 0.654343843460083 batch: 97/840\n",
      "Batch loss: 0.6957263946533203 batch: 98/840\n",
      "Batch loss: 0.5752315521240234 batch: 99/840\n",
      "Batch loss: 0.6812949180603027 batch: 100/840\n",
      "Batch loss: 0.48289793729782104 batch: 101/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4575197100639343 batch: 102/840\n",
      "Batch loss: 0.6288957595825195 batch: 103/840\n",
      "Batch loss: 0.494482159614563 batch: 104/840\n",
      "Batch loss: 0.6672325134277344 batch: 105/840\n",
      "Batch loss: 0.6609136462211609 batch: 106/840\n",
      "Batch loss: 0.5712193846702576 batch: 107/840\n",
      "Batch loss: 0.9120755791664124 batch: 108/840\n",
      "Batch loss: 0.5141875147819519 batch: 109/840\n",
      "Batch loss: 0.5728437304496765 batch: 110/840\n",
      "Batch loss: 0.4922439157962799 batch: 111/840\n",
      "Batch loss: 0.7980489134788513 batch: 112/840\n",
      "Batch loss: 0.6867132782936096 batch: 113/840\n",
      "Batch loss: 0.5374689698219299 batch: 114/840\n",
      "Batch loss: 0.5895994305610657 batch: 115/840\n",
      "Batch loss: 0.560984194278717 batch: 116/840\n",
      "Batch loss: 0.7223750948905945 batch: 117/840\n",
      "Batch loss: 0.42967385053634644 batch: 118/840\n",
      "Batch loss: 0.5247234106063843 batch: 119/840\n",
      "Batch loss: 0.6045535802841187 batch: 120/840\n",
      "Batch loss: 0.6964160799980164 batch: 121/840\n",
      "Batch loss: 0.7588688731193542 batch: 122/840\n",
      "Batch loss: 0.5578256845474243 batch: 123/840\n",
      "Batch loss: 0.5515294671058655 batch: 124/840\n",
      "Batch loss: 0.5199217796325684 batch: 125/840\n",
      "Batch loss: 0.6890340447425842 batch: 126/840\n",
      "Batch loss: 0.6620494723320007 batch: 127/840\n",
      "Batch loss: 0.6871533989906311 batch: 128/840\n",
      "Batch loss: 0.7877343893051147 batch: 129/840\n",
      "Batch loss: 0.5739404559135437 batch: 130/840\n",
      "Batch loss: 0.7159053087234497 batch: 131/840\n",
      "Batch loss: 1.0222910642623901 batch: 132/840\n",
      "Batch loss: 0.6988839507102966 batch: 133/840\n",
      "Batch loss: 0.6188451051712036 batch: 134/840\n",
      "Batch loss: 0.4748421609401703 batch: 135/840\n",
      "Batch loss: 0.7792260646820068 batch: 136/840\n",
      "Batch loss: 0.5997462272644043 batch: 137/840\n",
      "Batch loss: 0.5717076659202576 batch: 138/840\n",
      "Batch loss: 0.5237694382667542 batch: 139/840\n",
      "Batch loss: 0.6595207452774048 batch: 140/840\n",
      "Batch loss: 0.4355350136756897 batch: 141/840\n",
      "Batch loss: 0.6084500551223755 batch: 142/840\n",
      "Batch loss: 0.5054313540458679 batch: 143/840\n",
      "Batch loss: 0.535926342010498 batch: 144/840\n",
      "Batch loss: 0.6250299215316772 batch: 145/840\n",
      "Batch loss: 0.6152370572090149 batch: 146/840\n",
      "Batch loss: 0.45909878611564636 batch: 147/840\n",
      "Batch loss: 0.751797616481781 batch: 148/840\n",
      "Batch loss: 0.6822227239608765 batch: 149/840\n",
      "Batch loss: 0.677002489566803 batch: 150/840\n",
      "Batch loss: 0.5003649592399597 batch: 151/840\n",
      "Batch loss: 0.6209072470664978 batch: 152/840\n",
      "Batch loss: 0.49206021428108215 batch: 153/840\n",
      "Batch loss: 0.7565870881080627 batch: 154/840\n",
      "Batch loss: 0.5606335997581482 batch: 155/840\n",
      "Batch loss: 0.6950721740722656 batch: 156/840\n",
      "Batch loss: 0.7586179375648499 batch: 157/840\n",
      "Batch loss: 0.5254136919975281 batch: 158/840\n",
      "Batch loss: 0.5391573905944824 batch: 159/840\n",
      "Batch loss: 0.559238612651825 batch: 160/840\n",
      "Batch loss: 0.6351345181465149 batch: 161/840\n",
      "Batch loss: 0.6952626705169678 batch: 162/840\n",
      "Batch loss: 0.7531952857971191 batch: 163/840\n",
      "Batch loss: 0.41991087794303894 batch: 164/840\n",
      "Batch loss: 0.7019119262695312 batch: 165/840\n",
      "Batch loss: 0.5244496464729309 batch: 166/840\n",
      "Batch loss: 0.6257568597793579 batch: 167/840\n",
      "Batch loss: 0.6680499911308289 batch: 168/840\n",
      "Batch loss: 0.505210816860199 batch: 169/840\n",
      "Batch loss: 0.6530898809432983 batch: 170/840\n",
      "Batch loss: 0.5765596032142639 batch: 171/840\n",
      "Batch loss: 0.6533472537994385 batch: 172/840\n",
      "Batch loss: 0.6130748987197876 batch: 173/840\n",
      "Batch loss: 0.5326606035232544 batch: 174/840\n",
      "Batch loss: 0.5339324474334717 batch: 175/840\n",
      "Batch loss: 0.708071231842041 batch: 176/840\n",
      "Batch loss: 0.6557153463363647 batch: 177/840\n",
      "Batch loss: 0.6452015042304993 batch: 178/840\n",
      "Batch loss: 0.7892102003097534 batch: 179/840\n",
      "Batch loss: 0.5402370691299438 batch: 180/840\n",
      "Batch loss: 0.49937117099761963 batch: 181/840\n",
      "Batch loss: 0.5890459418296814 batch: 182/840\n",
      "Batch loss: 0.6105645298957825 batch: 183/840\n",
      "Batch loss: 0.5545132160186768 batch: 184/840\n",
      "Batch loss: 0.409498393535614 batch: 185/840\n",
      "Batch loss: 0.44438502192497253 batch: 186/840\n",
      "Batch loss: 0.6039928793907166 batch: 187/840\n",
      "Batch loss: 0.5404748320579529 batch: 188/840\n",
      "Batch loss: 0.5937586426734924 batch: 189/840\n",
      "Batch loss: 0.6132490634918213 batch: 190/840\n",
      "Batch loss: 0.7285782098770142 batch: 191/840\n",
      "Batch loss: 0.5188469290733337 batch: 192/840\n",
      "Batch loss: 0.5455370545387268 batch: 193/840\n",
      "Batch loss: 0.4363400638103485 batch: 194/840\n",
      "Batch loss: 0.6034091114997864 batch: 195/840\n",
      "Batch loss: 0.7576190233230591 batch: 196/840\n",
      "Batch loss: 0.6580401062965393 batch: 197/840\n",
      "Batch loss: 0.5203490257263184 batch: 198/840\n",
      "Batch loss: 0.5667948126792908 batch: 199/840\n",
      "Batch loss: 0.7817663550376892 batch: 200/840\n",
      "Batch loss: 0.5346764922142029 batch: 201/840\n",
      "Batch loss: 0.550764262676239 batch: 202/840\n",
      "Batch loss: 0.5563246011734009 batch: 203/840\n",
      "Batch loss: 0.7845974564552307 batch: 204/840\n",
      "Batch loss: 0.799164891242981 batch: 205/840\n",
      "Batch loss: 0.6751624345779419 batch: 206/840\n",
      "Batch loss: 0.5597259998321533 batch: 207/840\n",
      "Batch loss: 0.7120383381843567 batch: 208/840\n",
      "Batch loss: 0.6061113476753235 batch: 209/840\n",
      "Batch loss: 0.530133068561554 batch: 210/840\n",
      "Batch loss: 0.47012147307395935 batch: 211/840\n",
      "Batch loss: 0.6328648328781128 batch: 212/840\n",
      "Batch loss: 0.7064104676246643 batch: 213/840\n",
      "Batch loss: 0.7190476059913635 batch: 214/840\n",
      "Batch loss: 0.562180757522583 batch: 215/840\n",
      "Batch loss: 0.5506166219711304 batch: 216/840\n",
      "Batch loss: 0.6967542767524719 batch: 217/840\n",
      "Batch loss: 0.7145792245864868 batch: 218/840\n",
      "Batch loss: 0.6403088569641113 batch: 219/840\n",
      "Batch loss: 0.8019769787788391 batch: 220/840\n",
      "Batch loss: 0.5783253312110901 batch: 221/840\n",
      "Batch loss: 0.6525318622589111 batch: 222/840\n",
      "Batch loss: 0.5842103362083435 batch: 223/840\n",
      "Batch loss: 0.7024075984954834 batch: 224/840\n",
      "Batch loss: 0.6745067834854126 batch: 225/840\n",
      "Batch loss: 0.6219552755355835 batch: 226/840\n",
      "Batch loss: 0.6728657484054565 batch: 227/840\n",
      "Batch loss: 0.43596571683883667 batch: 228/840\n",
      "Batch loss: 0.4632633328437805 batch: 229/840\n",
      "Batch loss: 0.5698246955871582 batch: 230/840\n",
      "Batch loss: 0.5459584593772888 batch: 231/840\n",
      "Batch loss: 0.690845251083374 batch: 232/840\n",
      "Batch loss: 0.7287687659263611 batch: 233/840\n",
      "Batch loss: 0.49765563011169434 batch: 234/840\n",
      "Batch loss: 0.5679263472557068 batch: 235/840\n",
      "Batch loss: 0.6139625906944275 batch: 236/840\n",
      "Batch loss: 0.6209495663642883 batch: 237/840\n",
      "Batch loss: 0.8321919441223145 batch: 238/840\n",
      "Batch loss: 0.5087875127792358 batch: 239/840\n",
      "Batch loss: 0.590808629989624 batch: 240/840\n",
      "Batch loss: 0.731777548789978 batch: 241/840\n",
      "Batch loss: 0.668211042881012 batch: 242/840\n",
      "Batch loss: 0.5715216994285583 batch: 243/840\n",
      "Batch loss: 0.7069661021232605 batch: 244/840\n",
      "Batch loss: 0.5313319563865662 batch: 245/840\n",
      "Batch loss: 0.6816354990005493 batch: 246/840\n",
      "Batch loss: 0.7017492651939392 batch: 247/840\n",
      "Batch loss: 0.7630067467689514 batch: 248/840\n",
      "Batch loss: 0.6804342865943909 batch: 249/840\n",
      "Batch loss: 0.490859717130661 batch: 250/840\n",
      "Batch loss: 0.6287223696708679 batch: 251/840\n",
      "Batch loss: 0.5720888376235962 batch: 252/840\n",
      "Batch loss: 0.612110435962677 batch: 253/840\n",
      "Batch loss: 0.6300536394119263 batch: 254/840\n",
      "Batch loss: 0.5305514335632324 batch: 255/840\n",
      "Batch loss: 0.6195023059844971 batch: 256/840\n",
      "Batch loss: 0.5538866519927979 batch: 257/840\n",
      "Batch loss: 0.671448290348053 batch: 258/840\n",
      "Batch loss: 0.5049063563346863 batch: 259/840\n",
      "Batch loss: 0.562009871006012 batch: 260/840\n",
      "Batch loss: 0.5227091312408447 batch: 261/840\n",
      "Batch loss: 0.4199252724647522 batch: 262/840\n",
      "Batch loss: 0.6014963388442993 batch: 263/840\n",
      "Batch loss: 0.5918645262718201 batch: 264/840\n",
      "Batch loss: 0.6807258129119873 batch: 265/840\n",
      "Batch loss: 0.6309789419174194 batch: 266/840\n",
      "Batch loss: 0.7123377919197083 batch: 267/840\n",
      "Batch loss: 0.5583942532539368 batch: 268/840\n",
      "Batch loss: 0.5619924664497375 batch: 269/840\n",
      "Batch loss: 0.5927507877349854 batch: 270/840\n",
      "Batch loss: 0.5516918301582336 batch: 271/840\n",
      "Batch loss: 0.8053772449493408 batch: 272/840\n",
      "Batch loss: 0.8707010746002197 batch: 273/840\n",
      "Batch loss: 0.6182580590248108 batch: 274/840\n",
      "Batch loss: 0.6557294726371765 batch: 275/840\n",
      "Batch loss: 0.5393837094306946 batch: 276/840\n",
      "Batch loss: 0.5773231387138367 batch: 277/840\n",
      "Batch loss: 0.7513315677642822 batch: 278/840\n",
      "Batch loss: 0.6599475741386414 batch: 279/840\n",
      "Batch loss: 0.6589648723602295 batch: 280/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6347368359565735 batch: 281/840\n",
      "Batch loss: 0.5841825008392334 batch: 282/840\n",
      "Batch loss: 0.6910359859466553 batch: 283/840\n",
      "Batch loss: 0.4188976585865021 batch: 284/840\n",
      "Batch loss: 0.5642722845077515 batch: 285/840\n",
      "Batch loss: 0.6625900268554688 batch: 286/840\n",
      "Batch loss: 0.44194549322128296 batch: 287/840\n",
      "Batch loss: 0.5055350661277771 batch: 288/840\n",
      "Batch loss: 0.8952553272247314 batch: 289/840\n",
      "Batch loss: 0.6730090975761414 batch: 290/840\n",
      "Batch loss: 0.6497672200202942 batch: 291/840\n",
      "Batch loss: 0.5181186199188232 batch: 292/840\n",
      "Batch loss: 0.6637270450592041 batch: 293/840\n",
      "Batch loss: 0.6071469187736511 batch: 294/840\n",
      "Batch loss: 0.5413346290588379 batch: 295/840\n",
      "Batch loss: 0.6088092923164368 batch: 296/840\n",
      "Batch loss: 0.6883724927902222 batch: 297/840\n",
      "Batch loss: 0.6573560833930969 batch: 298/840\n",
      "Batch loss: 0.5272627472877502 batch: 299/840\n",
      "Batch loss: 0.8262540698051453 batch: 300/840\n",
      "Batch loss: 0.6046696901321411 batch: 301/840\n",
      "Batch loss: 0.6811429858207703 batch: 302/840\n",
      "Batch loss: 0.831360399723053 batch: 303/840\n",
      "Batch loss: 0.5646851062774658 batch: 304/840\n",
      "Batch loss: 0.49551546573638916 batch: 305/840\n",
      "Batch loss: 0.671011209487915 batch: 306/840\n",
      "Batch loss: 0.6623392701148987 batch: 307/840\n",
      "Batch loss: 0.7048860192298889 batch: 308/840\n",
      "Batch loss: 0.6032183766365051 batch: 309/840\n",
      "Batch loss: 0.9096603393554688 batch: 310/840\n",
      "Batch loss: 0.7651520371437073 batch: 311/840\n",
      "Batch loss: 0.6469307541847229 batch: 312/840\n",
      "Batch loss: 0.7092030048370361 batch: 313/840\n",
      "Batch loss: 0.5174869298934937 batch: 314/840\n",
      "Batch loss: 0.6211822628974915 batch: 315/840\n",
      "Batch loss: 0.4526265263557434 batch: 316/840\n",
      "Batch loss: 0.6375110745429993 batch: 317/840\n",
      "Batch loss: 0.65230393409729 batch: 318/840\n",
      "Batch loss: 0.7158960700035095 batch: 319/840\n",
      "Batch loss: 0.5466465353965759 batch: 320/840\n",
      "Batch loss: 0.5978412628173828 batch: 321/840\n",
      "Batch loss: 0.6573398113250732 batch: 322/840\n",
      "Batch loss: 0.6698611974716187 batch: 323/840\n",
      "Batch loss: 0.6334651708602905 batch: 324/840\n",
      "Batch loss: 0.4413107633590698 batch: 325/840\n",
      "Batch loss: 0.615068256855011 batch: 326/840\n",
      "Batch loss: 0.47458353638648987 batch: 327/840\n",
      "Batch loss: 0.7941391468048096 batch: 328/840\n",
      "Batch loss: 0.7269712090492249 batch: 329/840\n",
      "Batch loss: 0.6962332725524902 batch: 330/840\n",
      "Batch loss: 0.7729238271713257 batch: 331/840\n",
      "Batch loss: 0.6862074136734009 batch: 332/840\n",
      "Batch loss: 0.5410665273666382 batch: 333/840\n",
      "Batch loss: 0.6449762582778931 batch: 334/840\n",
      "Batch loss: 0.5800111889839172 batch: 335/840\n",
      "Batch loss: 0.6649373769760132 batch: 336/840\n",
      "Batch loss: 0.7240064740180969 batch: 337/840\n",
      "Batch loss: 0.7483857870101929 batch: 338/840\n",
      "Batch loss: 0.4730881154537201 batch: 339/840\n",
      "Batch loss: 0.7567125558853149 batch: 340/840\n",
      "Batch loss: 0.5136144757270813 batch: 341/840\n",
      "Batch loss: 0.448735773563385 batch: 342/840\n",
      "Batch loss: 0.8737959861755371 batch: 343/840\n",
      "Batch loss: 0.6251614689826965 batch: 344/840\n",
      "Batch loss: 0.449726402759552 batch: 345/840\n",
      "Batch loss: 0.6012592911720276 batch: 346/840\n",
      "Batch loss: 0.6468419432640076 batch: 347/840\n",
      "Batch loss: 0.5861223340034485 batch: 348/840\n",
      "Batch loss: 0.7204253673553467 batch: 349/840\n",
      "Batch loss: 0.5814214944839478 batch: 350/840\n",
      "Batch loss: 0.6866165995597839 batch: 351/840\n",
      "Batch loss: 0.5967938303947449 batch: 352/840\n",
      "Batch loss: 0.7089378237724304 batch: 353/840\n",
      "Batch loss: 0.6047922372817993 batch: 354/840\n",
      "Batch loss: 0.49097731709480286 batch: 355/840\n",
      "Batch loss: 0.613430380821228 batch: 356/840\n",
      "Batch loss: 0.5123084783554077 batch: 357/840\n",
      "Batch loss: 0.822771430015564 batch: 358/840\n",
      "Batch loss: 0.5977579355239868 batch: 359/840\n",
      "Batch loss: 0.8137902021408081 batch: 360/840\n",
      "Batch loss: 0.7422086596488953 batch: 361/840\n",
      "Batch loss: 0.5307857990264893 batch: 362/840\n",
      "Batch loss: 0.6133518218994141 batch: 363/840\n",
      "Batch loss: 0.7351024746894836 batch: 364/840\n",
      "Batch loss: 0.5530338883399963 batch: 365/840\n",
      "Batch loss: 0.6449033617973328 batch: 366/840\n",
      "Batch loss: 0.4042026102542877 batch: 367/840\n",
      "Batch loss: 0.7521933913230896 batch: 368/840\n",
      "Batch loss: 0.6257582306861877 batch: 369/840\n",
      "Batch loss: 0.6866937875747681 batch: 370/840\n",
      "Batch loss: 0.641246497631073 batch: 371/840\n",
      "Batch loss: 0.45519545674324036 batch: 372/840\n",
      "Batch loss: 0.6225239038467407 batch: 373/840\n",
      "Batch loss: 0.6619026064872742 batch: 374/840\n",
      "Batch loss: 0.5559588074684143 batch: 375/840\n",
      "Batch loss: 0.4737117290496826 batch: 376/840\n",
      "Batch loss: 0.7103269100189209 batch: 377/840\n",
      "Batch loss: 0.6428906321525574 batch: 378/840\n",
      "Batch loss: 0.5315451622009277 batch: 379/840\n",
      "Batch loss: 0.9403800368309021 batch: 380/840\n",
      "Batch loss: 0.8460927605628967 batch: 381/840\n",
      "Batch loss: 0.6498625874519348 batch: 382/840\n",
      "Batch loss: 0.6089039444923401 batch: 383/840\n",
      "Batch loss: 0.5816152095794678 batch: 384/840\n",
      "Batch loss: 0.6825273036956787 batch: 385/840\n",
      "Batch loss: 0.7573367953300476 batch: 386/840\n",
      "Batch loss: 0.5772077441215515 batch: 387/840\n",
      "Batch loss: 0.7148931622505188 batch: 388/840\n",
      "Batch loss: 0.5346153378486633 batch: 389/840\n",
      "Batch loss: 0.7576436400413513 batch: 390/840\n",
      "Batch loss: 0.6574561595916748 batch: 391/840\n",
      "Batch loss: 0.5529409646987915 batch: 392/840\n",
      "Batch loss: 0.486587256193161 batch: 393/840\n",
      "Batch loss: 0.8565599322319031 batch: 394/840\n",
      "Batch loss: 0.5155708193778992 batch: 395/840\n",
      "Batch loss: 0.664919912815094 batch: 396/840\n",
      "Batch loss: 0.6386785507202148 batch: 397/840\n",
      "Batch loss: 0.7045203447341919 batch: 398/840\n",
      "Batch loss: 0.5027633905410767 batch: 399/840\n",
      "Batch loss: 0.5500088334083557 batch: 400/840\n",
      "Batch loss: 0.6479625701904297 batch: 401/840\n",
      "Batch loss: 0.48843058943748474 batch: 402/840\n",
      "Batch loss: 0.6820830702781677 batch: 403/840\n",
      "Batch loss: 0.6472915410995483 batch: 404/840\n",
      "Batch loss: 0.6348812580108643 batch: 405/840\n",
      "Batch loss: 0.683036744594574 batch: 406/840\n",
      "Batch loss: 0.5157849788665771 batch: 407/840\n",
      "Batch loss: 0.6922891736030579 batch: 408/840\n",
      "Batch loss: 0.6785176992416382 batch: 409/840\n",
      "Batch loss: 0.715877890586853 batch: 410/840\n",
      "Batch loss: 0.6920410990715027 batch: 411/840\n",
      "Batch loss: 0.7366981506347656 batch: 412/840\n",
      "Batch loss: 0.5970507264137268 batch: 413/840\n",
      "Batch loss: 0.5615819692611694 batch: 414/840\n",
      "Batch loss: 0.7674899101257324 batch: 415/840\n",
      "Batch loss: 0.46398788690567017 batch: 416/840\n",
      "Batch loss: 0.6174025535583496 batch: 417/840\n",
      "Batch loss: 0.8162781000137329 batch: 418/840\n",
      "Batch loss: 0.5310090184211731 batch: 419/840\n",
      "Batch loss: 0.5466639995574951 batch: 420/840\n",
      "Batch loss: 0.46511349081993103 batch: 421/840\n",
      "Batch loss: 0.5221843123435974 batch: 422/840\n",
      "Batch loss: 0.6721296906471252 batch: 423/840\n",
      "Batch loss: 0.6126870512962341 batch: 424/840\n",
      "Batch loss: 0.6546462178230286 batch: 425/840\n",
      "Batch loss: 0.6237757205963135 batch: 426/840\n",
      "Batch loss: 0.6877424120903015 batch: 427/840\n",
      "Batch loss: 0.7588188052177429 batch: 428/840\n",
      "Batch loss: 0.5738441944122314 batch: 429/840\n",
      "Batch loss: 0.8197635412216187 batch: 430/840\n",
      "Batch loss: 0.6406444311141968 batch: 431/840\n",
      "Batch loss: 0.6268097162246704 batch: 432/840\n",
      "Batch loss: 0.4573434591293335 batch: 433/840\n",
      "Batch loss: 0.52681565284729 batch: 434/840\n",
      "Batch loss: 0.7278614640235901 batch: 435/840\n",
      "Batch loss: 0.6193475127220154 batch: 436/840\n",
      "Batch loss: 0.7260099053382874 batch: 437/840\n",
      "Batch loss: 0.4175192713737488 batch: 438/840\n",
      "Batch loss: 0.6747228503227234 batch: 439/840\n",
      "Batch loss: 0.7819319367408752 batch: 440/840\n",
      "Batch loss: 0.5669706463813782 batch: 441/840\n",
      "Batch loss: 0.6652353405952454 batch: 442/840\n",
      "Batch loss: 0.5425652861595154 batch: 443/840\n",
      "Batch loss: 0.6244065761566162 batch: 444/840\n",
      "Batch loss: 0.5807908773422241 batch: 445/840\n",
      "Batch loss: 0.6967543959617615 batch: 446/840\n",
      "Batch loss: 0.7204495072364807 batch: 447/840\n",
      "Batch loss: 0.7176653146743774 batch: 448/840\n",
      "Batch loss: 0.5449273586273193 batch: 449/840\n",
      "Batch loss: 0.605463445186615 batch: 450/840\n",
      "Batch loss: 0.6945136785507202 batch: 451/840\n",
      "Batch loss: 0.6796191930770874 batch: 452/840\n",
      "Batch loss: 0.5846362113952637 batch: 453/840\n",
      "Batch loss: 0.5103972554206848 batch: 454/840\n",
      "Batch loss: 0.6558580994606018 batch: 455/840\n",
      "Batch loss: 0.6220605373382568 batch: 456/840\n",
      "Batch loss: 0.5269351005554199 batch: 457/840\n",
      "Batch loss: 0.5979527235031128 batch: 458/840\n",
      "Batch loss: 0.5517020225524902 batch: 459/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.4559035003185272 batch: 460/840\n",
      "Batch loss: 0.6985692381858826 batch: 461/840\n",
      "Batch loss: 0.5903876423835754 batch: 462/840\n",
      "Batch loss: 0.6932179927825928 batch: 463/840\n",
      "Batch loss: 0.5406429767608643 batch: 464/840\n",
      "Batch loss: 0.8148140907287598 batch: 465/840\n",
      "Batch loss: 0.6921818256378174 batch: 466/840\n",
      "Batch loss: 0.8492562174797058 batch: 467/840\n",
      "Batch loss: 0.5001876354217529 batch: 468/840\n",
      "Batch loss: 0.5583797693252563 batch: 469/840\n",
      "Batch loss: 0.5937869548797607 batch: 470/840\n",
      "Batch loss: 0.5899854898452759 batch: 471/840\n",
      "Batch loss: 0.7263176441192627 batch: 472/840\n",
      "Batch loss: 0.7579324841499329 batch: 473/840\n",
      "Batch loss: 0.5620824694633484 batch: 474/840\n",
      "Batch loss: 0.5539972186088562 batch: 475/840\n",
      "Batch loss: 0.5890229940414429 batch: 476/840\n",
      "Batch loss: 0.5256059169769287 batch: 477/840\n",
      "Batch loss: 0.43296322226524353 batch: 478/840\n",
      "Batch loss: 0.4556267559528351 batch: 479/840\n",
      "Batch loss: 0.6446810364723206 batch: 480/840\n",
      "Batch loss: 0.6221598386764526 batch: 481/840\n",
      "Batch loss: 0.7048999071121216 batch: 482/840\n",
      "Batch loss: 0.8119864463806152 batch: 483/840\n",
      "Batch loss: 0.44721758365631104 batch: 484/840\n",
      "Batch loss: 0.5613517165184021 batch: 485/840\n",
      "Batch loss: 0.6266236901283264 batch: 486/840\n",
      "Batch loss: 0.43929222226142883 batch: 487/840\n",
      "Batch loss: 0.6622074842453003 batch: 488/840\n",
      "Batch loss: 0.5826254487037659 batch: 489/840\n",
      "Batch loss: 0.675179660320282 batch: 490/840\n",
      "Batch loss: 0.37719330191612244 batch: 491/840\n",
      "Batch loss: 0.6617639660835266 batch: 492/840\n",
      "Batch loss: 0.6792087554931641 batch: 493/840\n",
      "Batch loss: 0.4033249616622925 batch: 494/840\n",
      "Batch loss: 0.6810980439186096 batch: 495/840\n",
      "Batch loss: 0.6374233365058899 batch: 496/840\n",
      "Batch loss: 0.7519327402114868 batch: 497/840\n",
      "Batch loss: 0.4206940531730652 batch: 498/840\n",
      "Batch loss: 0.6574695110321045 batch: 499/840\n",
      "Batch loss: 0.6043276786804199 batch: 500/840\n",
      "Batch loss: 0.522233784198761 batch: 501/840\n",
      "Batch loss: 0.7371082901954651 batch: 502/840\n",
      "Batch loss: 0.5553089380264282 batch: 503/840\n",
      "Batch loss: 0.5917670726776123 batch: 504/840\n",
      "Batch loss: 0.6529048085212708 batch: 505/840\n",
      "Batch loss: 0.47412383556365967 batch: 506/840\n",
      "Batch loss: 0.7245979905128479 batch: 507/840\n",
      "Batch loss: 0.5709931254386902 batch: 508/840\n",
      "Batch loss: 0.715624213218689 batch: 509/840\n",
      "Batch loss: 0.6726927161216736 batch: 510/840\n",
      "Batch loss: 0.49961644411087036 batch: 511/840\n",
      "Batch loss: 0.6724191308021545 batch: 512/840\n",
      "Batch loss: 0.5362192988395691 batch: 513/840\n",
      "Batch loss: 0.6239349842071533 batch: 514/840\n",
      "Batch loss: 0.4622374475002289 batch: 515/840\n",
      "Batch loss: 0.730492115020752 batch: 516/840\n",
      "Batch loss: 0.6402668952941895 batch: 517/840\n",
      "Batch loss: 0.7301598191261292 batch: 518/840\n",
      "Batch loss: 0.7605190873146057 batch: 519/840\n",
      "Batch loss: 0.6631010174751282 batch: 520/840\n",
      "Batch loss: 0.631350040435791 batch: 521/840\n",
      "Batch loss: 0.5980948805809021 batch: 522/840\n",
      "Batch loss: 0.46936455368995667 batch: 523/840\n",
      "Batch loss: 0.6872487664222717 batch: 524/840\n",
      "Batch loss: 0.5251563191413879 batch: 525/840\n",
      "Batch loss: 0.7621906995773315 batch: 526/840\n",
      "Batch loss: 0.6256245970726013 batch: 527/840\n",
      "Batch loss: 0.5552690029144287 batch: 528/840\n",
      "Batch loss: 0.49881067872047424 batch: 529/840\n",
      "Batch loss: 0.6535261273384094 batch: 530/840\n",
      "Batch loss: 0.5241398811340332 batch: 531/840\n",
      "Batch loss: 0.4910661578178406 batch: 532/840\n",
      "Batch loss: 0.7170085906982422 batch: 533/840\n",
      "Batch loss: 0.5714423656463623 batch: 534/840\n",
      "Batch loss: 0.5948826670646667 batch: 535/840\n",
      "Batch loss: 0.7203524708747864 batch: 536/840\n",
      "Batch loss: 0.6401268839836121 batch: 537/840\n",
      "Batch loss: 0.5514928698539734 batch: 538/840\n",
      "Batch loss: 0.5261951684951782 batch: 539/840\n",
      "Batch loss: 0.7093557119369507 batch: 540/840\n",
      "Batch loss: 0.6314501762390137 batch: 541/840\n",
      "Batch loss: 0.6697046756744385 batch: 542/840\n",
      "Batch loss: 0.45903250575065613 batch: 543/840\n",
      "Batch loss: 0.6588042974472046 batch: 544/840\n",
      "Batch loss: 0.5928634405136108 batch: 545/840\n",
      "Batch loss: 0.5903505682945251 batch: 546/840\n",
      "Batch loss: 0.5718152523040771 batch: 547/840\n",
      "Batch loss: 0.5083886981010437 batch: 548/840\n",
      "Batch loss: 0.5001492500305176 batch: 549/840\n",
      "Batch loss: 0.5024921894073486 batch: 550/840\n",
      "Batch loss: 0.6178650856018066 batch: 551/840\n",
      "Batch loss: 0.4987432360649109 batch: 552/840\n",
      "Batch loss: 0.7172516584396362 batch: 553/840\n",
      "Batch loss: 0.6480329036712646 batch: 554/840\n",
      "Batch loss: 0.6529022455215454 batch: 555/840\n",
      "Batch loss: 0.5917108654975891 batch: 556/840\n",
      "Batch loss: 0.6897563338279724 batch: 557/840\n",
      "Batch loss: 0.5887235403060913 batch: 558/840\n",
      "Batch loss: 0.6014201045036316 batch: 559/840\n",
      "Batch loss: 0.6391971111297607 batch: 560/840\n",
      "Batch loss: 0.6243188381195068 batch: 561/840\n",
      "Batch loss: 0.557532548904419 batch: 562/840\n",
      "Batch loss: 0.3649376332759857 batch: 563/840\n",
      "Batch loss: 0.6927154660224915 batch: 564/840\n",
      "Batch loss: 0.8055208325386047 batch: 565/840\n",
      "Batch loss: 0.6374661922454834 batch: 566/840\n",
      "Batch loss: 0.6391894221305847 batch: 567/840\n",
      "Batch loss: 0.5890471339225769 batch: 568/840\n",
      "Batch loss: 0.621655285358429 batch: 569/840\n",
      "Batch loss: 0.4187203347682953 batch: 570/840\n",
      "Batch loss: 0.8258291482925415 batch: 571/840\n",
      "Batch loss: 0.6544971466064453 batch: 572/840\n",
      "Batch loss: 0.7486458420753479 batch: 573/840\n",
      "Batch loss: 0.6883310079574585 batch: 574/840\n",
      "Batch loss: 0.5190992951393127 batch: 575/840\n",
      "Batch loss: 0.560042142868042 batch: 576/840\n",
      "Batch loss: 0.5293171405792236 batch: 577/840\n",
      "Batch loss: 0.745337963104248 batch: 578/840\n",
      "Batch loss: 0.6140998005867004 batch: 579/840\n",
      "Batch loss: 0.7845144867897034 batch: 580/840\n",
      "Batch loss: 0.6287998557090759 batch: 581/840\n",
      "Batch loss: 0.7201262712478638 batch: 582/840\n",
      "Batch loss: 0.618053138256073 batch: 583/840\n",
      "Batch loss: 0.770399272441864 batch: 584/840\n",
      "Batch loss: 0.6739243268966675 batch: 585/840\n",
      "Batch loss: 0.7095028162002563 batch: 586/840\n",
      "Batch loss: 0.5314176082611084 batch: 587/840\n",
      "Batch loss: 0.5773276686668396 batch: 588/840\n",
      "Batch loss: 0.6629116535186768 batch: 589/840\n",
      "Batch loss: 0.49788525700569153 batch: 590/840\n",
      "Batch loss: 0.4055730402469635 batch: 591/840\n",
      "Batch loss: 0.5750631093978882 batch: 592/840\n",
      "Batch loss: 0.6949238777160645 batch: 593/840\n",
      "Batch loss: 0.6346212029457092 batch: 594/840\n",
      "Batch loss: 0.36293256282806396 batch: 595/840\n",
      "Batch loss: 0.5252074599266052 batch: 596/840\n",
      "Batch loss: 0.5610987544059753 batch: 597/840\n",
      "Batch loss: 0.3853023052215576 batch: 598/840\n",
      "Batch loss: 0.5260631442070007 batch: 599/840\n",
      "Batch loss: 0.792061448097229 batch: 600/840\n",
      "Batch loss: 0.7673184275627136 batch: 601/840\n",
      "Batch loss: 0.6401754021644592 batch: 602/840\n",
      "Batch loss: 0.7118814587593079 batch: 603/840\n",
      "Batch loss: 0.5763694047927856 batch: 604/840\n",
      "Batch loss: 0.7408193945884705 batch: 605/840\n",
      "Batch loss: 0.7934651374816895 batch: 606/840\n",
      "Batch loss: 0.6147011518478394 batch: 607/840\n",
      "Batch loss: 0.6102240681648254 batch: 608/840\n",
      "Batch loss: 0.47572121024131775 batch: 609/840\n",
      "Batch loss: 0.672698974609375 batch: 610/840\n",
      "Batch loss: 0.5938866138458252 batch: 611/840\n",
      "Batch loss: 0.657716691493988 batch: 612/840\n",
      "Batch loss: 0.6686482429504395 batch: 613/840\n",
      "Batch loss: 0.5513553023338318 batch: 614/840\n",
      "Batch loss: 0.5622712969779968 batch: 615/840\n",
      "Batch loss: 0.5808941125869751 batch: 616/840\n",
      "Batch loss: 0.4953269958496094 batch: 617/840\n",
      "Batch loss: 0.5951573252677917 batch: 618/840\n",
      "Batch loss: 0.7239896655082703 batch: 619/840\n",
      "Batch loss: 0.5608450174331665 batch: 620/840\n",
      "Batch loss: 0.6289771199226379 batch: 621/840\n",
      "Batch loss: 0.630210280418396 batch: 622/840\n",
      "Batch loss: 0.5361269116401672 batch: 623/840\n",
      "Batch loss: 0.7675983309745789 batch: 624/840\n",
      "Batch loss: 0.684762179851532 batch: 625/840\n",
      "Batch loss: 0.6341497302055359 batch: 626/840\n",
      "Batch loss: 0.6066765785217285 batch: 627/840\n",
      "Batch loss: 0.6253359913825989 batch: 628/840\n",
      "Batch loss: 0.6867725849151611 batch: 629/840\n",
      "Batch loss: 0.5694056153297424 batch: 630/840\n",
      "Batch loss: 0.6284845471382141 batch: 631/840\n",
      "Batch loss: 0.7143133282661438 batch: 632/840\n",
      "Batch loss: 0.513904333114624 batch: 633/840\n",
      "Batch loss: 0.5916696786880493 batch: 634/840\n",
      "Batch loss: 0.5432551503181458 batch: 635/840\n",
      "Batch loss: 0.5241740942001343 batch: 636/840\n",
      "Batch loss: 0.4587455689907074 batch: 637/840\n",
      "Batch loss: 0.6512502431869507 batch: 638/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6829483509063721 batch: 639/840\n",
      "Batch loss: 0.5164150595664978 batch: 640/840\n",
      "Batch loss: 0.832904040813446 batch: 641/840\n",
      "Batch loss: 0.5813828110694885 batch: 642/840\n",
      "Batch loss: 0.8516293168067932 batch: 643/840\n",
      "Batch loss: 0.5536300539970398 batch: 644/840\n",
      "Batch loss: 0.5381994843482971 batch: 645/840\n",
      "Batch loss: 0.5818721652030945 batch: 646/840\n",
      "Batch loss: 0.6366590857505798 batch: 647/840\n",
      "Batch loss: 0.627524197101593 batch: 648/840\n",
      "Batch loss: 0.6363242864608765 batch: 649/840\n",
      "Batch loss: 0.5277549028396606 batch: 650/840\n",
      "Batch loss: 0.6065276861190796 batch: 651/840\n",
      "Batch loss: 0.7135367393493652 batch: 652/840\n",
      "Batch loss: 0.46122032403945923 batch: 653/840\n",
      "Batch loss: 0.9576066732406616 batch: 654/840\n",
      "Batch loss: 0.5838545560836792 batch: 655/840\n",
      "Batch loss: 0.5272199511528015 batch: 656/840\n",
      "Batch loss: 0.5417273044586182 batch: 657/840\n",
      "Batch loss: 0.5683385729789734 batch: 658/840\n",
      "Batch loss: 0.7289000153541565 batch: 659/840\n",
      "Batch loss: 0.5465776324272156 batch: 660/840\n",
      "Batch loss: 0.7092772722244263 batch: 661/840\n",
      "Batch loss: 0.6724178791046143 batch: 662/840\n",
      "Batch loss: 0.5974462032318115 batch: 663/840\n",
      "Batch loss: 0.6528881788253784 batch: 664/840\n",
      "Batch loss: 0.7103700041770935 batch: 665/840\n",
      "Batch loss: 0.6993297338485718 batch: 666/840\n",
      "Batch loss: 0.7461307644844055 batch: 667/840\n",
      "Batch loss: 0.5746815204620361 batch: 668/840\n",
      "Batch loss: 0.6023868918418884 batch: 669/840\n",
      "Batch loss: 0.6432543992996216 batch: 670/840\n",
      "Batch loss: 0.6802797913551331 batch: 671/840\n",
      "Batch loss: 0.5071035623550415 batch: 672/840\n",
      "Batch loss: 0.8530562520027161 batch: 673/840\n",
      "Batch loss: 0.7338521480560303 batch: 674/840\n",
      "Batch loss: 0.5664899349212646 batch: 675/840\n",
      "Batch loss: 0.6100568175315857 batch: 676/840\n",
      "Batch loss: 0.66742342710495 batch: 677/840\n",
      "Batch loss: 0.7012967467308044 batch: 678/840\n",
      "Batch loss: 0.8191478848457336 batch: 679/840\n",
      "Batch loss: 0.6580959558486938 batch: 680/840\n",
      "Batch loss: 0.6508958339691162 batch: 681/840\n",
      "Batch loss: 0.663603663444519 batch: 682/840\n",
      "Batch loss: 0.5346840023994446 batch: 683/840\n",
      "Batch loss: 0.8063697814941406 batch: 684/840\n",
      "Batch loss: 0.6315262317657471 batch: 685/840\n",
      "Batch loss: 0.7500048279762268 batch: 686/840\n",
      "Batch loss: 0.8155550360679626 batch: 687/840\n",
      "Batch loss: 0.5774939060211182 batch: 688/840\n",
      "Batch loss: 0.5327341556549072 batch: 689/840\n",
      "Batch loss: 0.5106289386749268 batch: 690/840\n",
      "Batch loss: 0.6981445550918579 batch: 691/840\n",
      "Batch loss: 0.6002148389816284 batch: 692/840\n",
      "Batch loss: 0.6366092562675476 batch: 693/840\n",
      "Batch loss: 0.7981806397438049 batch: 694/840\n",
      "Batch loss: 0.7313670516014099 batch: 695/840\n",
      "Batch loss: 0.6061763167381287 batch: 696/840\n",
      "Batch loss: 0.6119241714477539 batch: 697/840\n",
      "Batch loss: 0.5789543390274048 batch: 698/840\n",
      "Batch loss: 0.7268092632293701 batch: 699/840\n",
      "Batch loss: 0.7093021869659424 batch: 700/840\n",
      "Batch loss: 0.7356856465339661 batch: 701/840\n",
      "Batch loss: 0.6353018283843994 batch: 702/840\n",
      "Batch loss: 0.5808401107788086 batch: 703/840\n",
      "Batch loss: 0.6463239789009094 batch: 704/840\n",
      "Batch loss: 0.5819241404533386 batch: 705/840\n",
      "Batch loss: 0.6003208160400391 batch: 706/840\n",
      "Batch loss: 0.6768446564674377 batch: 707/840\n",
      "Batch loss: 0.7221214771270752 batch: 708/840\n",
      "Batch loss: 0.7099047899246216 batch: 709/840\n",
      "Batch loss: 0.8720389008522034 batch: 710/840\n",
      "Batch loss: 0.43140166997909546 batch: 711/840\n",
      "Batch loss: 0.7756777405738831 batch: 712/840\n",
      "Batch loss: 0.5460973381996155 batch: 713/840\n",
      "Batch loss: 0.643028974533081 batch: 714/840\n",
      "Batch loss: 0.7962911128997803 batch: 715/840\n",
      "Batch loss: 0.6516765356063843 batch: 716/840\n",
      "Batch loss: 0.6575716137886047 batch: 717/840\n",
      "Batch loss: 0.6466549634933472 batch: 718/840\n",
      "Batch loss: 0.5128429532051086 batch: 719/840\n",
      "Batch loss: 0.5961570143699646 batch: 720/840\n",
      "Batch loss: 0.7966455221176147 batch: 721/840\n",
      "Batch loss: 0.8440302014350891 batch: 722/840\n",
      "Batch loss: 0.4541102647781372 batch: 723/840\n",
      "Batch loss: 0.7526270151138306 batch: 724/840\n",
      "Batch loss: 0.6334419846534729 batch: 725/840\n",
      "Batch loss: 0.4872608184814453 batch: 726/840\n",
      "Batch loss: 0.6961987018585205 batch: 727/840\n",
      "Batch loss: 0.7519493699073792 batch: 728/840\n",
      "Batch loss: 0.6222428679466248 batch: 729/840\n",
      "Batch loss: 0.5946080684661865 batch: 730/840\n",
      "Batch loss: 0.5192051529884338 batch: 731/840\n",
      "Batch loss: 0.9016870856285095 batch: 732/840\n",
      "Batch loss: 0.6427620649337769 batch: 733/840\n",
      "Batch loss: 0.5524643659591675 batch: 734/840\n",
      "Batch loss: 0.6654998064041138 batch: 735/840\n",
      "Batch loss: 0.5807082653045654 batch: 736/840\n",
      "Batch loss: 0.6421259045600891 batch: 737/840\n",
      "Batch loss: 0.4556953012943268 batch: 738/840\n",
      "Batch loss: 0.7183523774147034 batch: 739/840\n",
      "Batch loss: 0.6486084461212158 batch: 740/840\n",
      "Batch loss: 0.514064371585846 batch: 741/840\n",
      "Batch loss: 0.4752561151981354 batch: 742/840\n",
      "Batch loss: 0.46666625142097473 batch: 743/840\n",
      "Batch loss: 0.5210162997245789 batch: 744/840\n",
      "Batch loss: 0.5552288293838501 batch: 745/840\n",
      "Batch loss: 0.6464323401451111 batch: 746/840\n",
      "Batch loss: 0.7105973958969116 batch: 747/840\n",
      "Batch loss: 0.6223199367523193 batch: 748/840\n",
      "Batch loss: 0.4951223134994507 batch: 749/840\n",
      "Batch loss: 0.478508859872818 batch: 750/840\n",
      "Batch loss: 0.6384923458099365 batch: 751/840\n",
      "Batch loss: 0.9119808077812195 batch: 752/840\n",
      "Batch loss: 0.5271556377410889 batch: 753/840\n",
      "Batch loss: 0.6159495711326599 batch: 754/840\n",
      "Batch loss: 0.5853685736656189 batch: 755/840\n",
      "Batch loss: 0.5146971344947815 batch: 756/840\n",
      "Batch loss: 0.7506248354911804 batch: 757/840\n",
      "Batch loss: 0.5817458629608154 batch: 758/840\n",
      "Batch loss: 0.5184056162834167 batch: 759/840\n",
      "Batch loss: 0.528980016708374 batch: 760/840\n",
      "Batch loss: 0.5129719972610474 batch: 761/840\n",
      "Batch loss: 0.7205377817153931 batch: 762/840\n",
      "Batch loss: 0.47291508316993713 batch: 763/840\n",
      "Batch loss: 0.6025996208190918 batch: 764/840\n",
      "Batch loss: 0.45019692182540894 batch: 765/840\n",
      "Batch loss: 0.5818818211555481 batch: 766/840\n",
      "Batch loss: 0.6903079748153687 batch: 767/840\n",
      "Batch loss: 0.6365746855735779 batch: 768/840\n",
      "Batch loss: 0.6355825066566467 batch: 769/840\n",
      "Batch loss: 0.5372568368911743 batch: 770/840\n",
      "Batch loss: 0.5753286480903625 batch: 771/840\n",
      "Batch loss: 0.529055118560791 batch: 772/840\n",
      "Batch loss: 0.5220451951026917 batch: 773/840\n",
      "Batch loss: 0.4308375120162964 batch: 774/840\n",
      "Batch loss: 0.5965903997421265 batch: 775/840\n",
      "Batch loss: 0.4809993505477905 batch: 776/840\n",
      "Batch loss: 0.5827023983001709 batch: 777/840\n",
      "Batch loss: 0.43673405051231384 batch: 778/840\n",
      "Batch loss: 0.7772752642631531 batch: 779/840\n",
      "Batch loss: 0.5574645400047302 batch: 780/840\n",
      "Batch loss: 0.677864134311676 batch: 781/840\n",
      "Batch loss: 0.602199375629425 batch: 782/840\n",
      "Batch loss: 0.46263357996940613 batch: 783/840\n",
      "Batch loss: 0.6563425660133362 batch: 784/840\n",
      "Batch loss: 0.5343401432037354 batch: 785/840\n",
      "Batch loss: 0.5673997402191162 batch: 786/840\n",
      "Batch loss: 0.6038674712181091 batch: 787/840\n",
      "Batch loss: 0.7175063490867615 batch: 788/840\n",
      "Batch loss: 0.7046661972999573 batch: 789/840\n",
      "Batch loss: 0.6989863514900208 batch: 790/840\n",
      "Batch loss: 0.5982017517089844 batch: 791/840\n",
      "Batch loss: 0.37963563203811646 batch: 792/840\n",
      "Batch loss: 0.5815944671630859 batch: 793/840\n",
      "Batch loss: 0.5772024393081665 batch: 794/840\n",
      "Batch loss: 0.5227156281471252 batch: 795/840\n",
      "Batch loss: 0.6825637817382812 batch: 796/840\n",
      "Batch loss: 0.6087549924850464 batch: 797/840\n",
      "Batch loss: 0.5992975234985352 batch: 798/840\n",
      "Batch loss: 0.6216323375701904 batch: 799/840\n",
      "Batch loss: 0.5444793701171875 batch: 800/840\n",
      "Batch loss: 0.758574903011322 batch: 801/840\n",
      "Batch loss: 0.47308576107025146 batch: 802/840\n",
      "Batch loss: 0.46111592650413513 batch: 803/840\n",
      "Batch loss: 0.6278155446052551 batch: 804/840\n",
      "Batch loss: 0.5474757552146912 batch: 805/840\n",
      "Batch loss: 0.6094765663146973 batch: 806/840\n",
      "Batch loss: 0.7319166660308838 batch: 807/840\n",
      "Batch loss: 0.5526302456855774 batch: 808/840\n",
      "Batch loss: 0.600649356842041 batch: 809/840\n",
      "Batch loss: 0.6031404137611389 batch: 810/840\n",
      "Batch loss: 0.5075522065162659 batch: 811/840\n",
      "Batch loss: 0.619204044342041 batch: 812/840\n",
      "Batch loss: 0.47737061977386475 batch: 813/840\n",
      "Batch loss: 0.6208605766296387 batch: 814/840\n",
      "Batch loss: 0.695960521697998 batch: 815/840\n",
      "Batch loss: 0.5930094718933105 batch: 816/840\n",
      "Batch loss: 0.6817011833190918 batch: 817/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.7062437534332275 batch: 818/840\n",
      "Batch loss: 0.526105523109436 batch: 819/840\n",
      "Batch loss: 0.6460859179496765 batch: 820/840\n",
      "Batch loss: 0.6787360310554504 batch: 821/840\n",
      "Batch loss: 0.5791730284690857 batch: 822/840\n",
      "Batch loss: 0.7560252547264099 batch: 823/840\n",
      "Batch loss: 0.8068123459815979 batch: 824/840\n",
      "Batch loss: 0.6423029899597168 batch: 825/840\n",
      "Batch loss: 0.5552993416786194 batch: 826/840\n",
      "Batch loss: 0.5475264191627502 batch: 827/840\n",
      "Batch loss: 0.6749385595321655 batch: 828/840\n",
      "Batch loss: 0.5850094556808472 batch: 829/840\n",
      "Batch loss: 0.7395409345626831 batch: 830/840\n",
      "Batch loss: 0.5961737632751465 batch: 831/840\n",
      "Batch loss: 0.7108189463615417 batch: 832/840\n",
      "Batch loss: 0.6469309329986572 batch: 833/840\n",
      "Batch loss: 0.5631629824638367 batch: 834/840\n",
      "Batch loss: 0.47077882289886475 batch: 835/840\n",
      "Batch loss: 0.6336544156074524 batch: 836/840\n",
      "Batch loss: 0.6452784538269043 batch: 837/840\n",
      "Batch loss: 0.6852951049804688 batch: 838/840\n",
      "Batch loss: 0.5256032943725586 batch: 839/840\n",
      "Batch loss: 0.5588181018829346 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 13/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.822\n",
      "Running epoch 14/15\n",
      "Batch loss: 0.5434228181838989 batch: 1/840\n",
      "Batch loss: 1.0146644115447998 batch: 2/840\n",
      "Batch loss: 0.6566750407218933 batch: 3/840\n",
      "Batch loss: 0.6102261543273926 batch: 4/840\n",
      "Batch loss: 0.5964108109474182 batch: 5/840\n",
      "Batch loss: 0.5179630517959595 batch: 6/840\n",
      "Batch loss: 0.5236395597457886 batch: 7/840\n",
      "Batch loss: 0.5314561724662781 batch: 8/840\n",
      "Batch loss: 0.609652042388916 batch: 9/840\n",
      "Batch loss: 0.6756606101989746 batch: 10/840\n",
      "Batch loss: 0.5671285390853882 batch: 11/840\n",
      "Batch loss: 0.5279728174209595 batch: 12/840\n",
      "Batch loss: 0.5210738778114319 batch: 13/840\n",
      "Batch loss: 0.6120873093605042 batch: 14/840\n",
      "Batch loss: 0.605975866317749 batch: 15/840\n",
      "Batch loss: 0.49399295449256897 batch: 16/840\n",
      "Batch loss: 0.44361647963523865 batch: 17/840\n",
      "Batch loss: 0.7337782979011536 batch: 18/840\n",
      "Batch loss: 0.6965024471282959 batch: 19/840\n",
      "Batch loss: 0.8052114844322205 batch: 20/840\n",
      "Batch loss: 0.6924617886543274 batch: 21/840\n",
      "Batch loss: 0.5885708332061768 batch: 22/840\n",
      "Batch loss: 0.5316839814186096 batch: 23/840\n",
      "Batch loss: 0.558861494064331 batch: 24/840\n",
      "Batch loss: 0.554677426815033 batch: 25/840\n",
      "Batch loss: 0.6814932823181152 batch: 26/840\n",
      "Batch loss: 0.6315906047821045 batch: 27/840\n",
      "Batch loss: 0.655712902545929 batch: 28/840\n",
      "Batch loss: 0.6163175702095032 batch: 29/840\n",
      "Batch loss: 0.5720410346984863 batch: 30/840\n",
      "Batch loss: 0.5651705861091614 batch: 31/840\n",
      "Batch loss: 0.591777503490448 batch: 32/840\n",
      "Batch loss: 0.5696592330932617 batch: 33/840\n",
      "Batch loss: 0.5482008457183838 batch: 34/840\n",
      "Batch loss: 0.6275604963302612 batch: 35/840\n",
      "Batch loss: 0.5867372751235962 batch: 36/840\n",
      "Batch loss: 0.7018560767173767 batch: 37/840\n",
      "Batch loss: 0.6909288167953491 batch: 38/840\n",
      "Batch loss: 0.7064767479896545 batch: 39/840\n",
      "Batch loss: 0.5449248552322388 batch: 40/840\n",
      "Batch loss: 0.7829082012176514 batch: 41/840\n",
      "Batch loss: 0.6024878025054932 batch: 42/840\n",
      "Batch loss: 0.546842634677887 batch: 43/840\n",
      "Batch loss: 0.6570965647697449 batch: 44/840\n",
      "Batch loss: 0.6286100149154663 batch: 45/840\n",
      "Batch loss: 0.5600574016571045 batch: 46/840\n",
      "Batch loss: 0.48440030217170715 batch: 47/840\n",
      "Batch loss: 0.6953128576278687 batch: 48/840\n",
      "Batch loss: 0.6862406134605408 batch: 49/840\n",
      "Batch loss: 0.6186681985855103 batch: 50/840\n",
      "Batch loss: 0.6746307611465454 batch: 51/840\n",
      "Batch loss: 0.7381964325904846 batch: 52/840\n",
      "Batch loss: 0.544450044631958 batch: 53/840\n",
      "Batch loss: 0.6323606371879578 batch: 54/840\n",
      "Batch loss: 0.6681698560714722 batch: 55/840\n",
      "Batch loss: 0.5555040240287781 batch: 56/840\n",
      "Batch loss: 0.7446720004081726 batch: 57/840\n",
      "Batch loss: 0.5662447810173035 batch: 58/840\n",
      "Batch loss: 0.5309808850288391 batch: 59/840\n",
      "Batch loss: 0.5688895583152771 batch: 60/840\n",
      "Batch loss: 0.7906685471534729 batch: 61/840\n",
      "Batch loss: 0.6144466400146484 batch: 62/840\n",
      "Batch loss: 0.5645824670791626 batch: 63/840\n",
      "Batch loss: 0.7351258993148804 batch: 64/840\n",
      "Batch loss: 0.574110746383667 batch: 65/840\n",
      "Batch loss: 0.720734715461731 batch: 66/840\n",
      "Batch loss: 0.6220135688781738 batch: 67/840\n",
      "Batch loss: 0.6437782049179077 batch: 68/840\n",
      "Batch loss: 0.6653033494949341 batch: 69/840\n",
      "Batch loss: 0.58377605676651 batch: 70/840\n",
      "Batch loss: 0.6823458075523376 batch: 71/840\n",
      "Batch loss: 0.6793676018714905 batch: 72/840\n",
      "Batch loss: 0.6451705098152161 batch: 73/840\n",
      "Batch loss: 0.6346768736839294 batch: 74/840\n",
      "Batch loss: 0.6274751424789429 batch: 75/840\n",
      "Batch loss: 0.3584495484828949 batch: 76/840\n",
      "Batch loss: 0.5686748623847961 batch: 77/840\n",
      "Batch loss: 0.7450085282325745 batch: 78/840\n",
      "Batch loss: 0.5948692560195923 batch: 79/840\n",
      "Batch loss: 0.5692108273506165 batch: 80/840\n",
      "Batch loss: 0.5314992666244507 batch: 81/840\n",
      "Batch loss: 0.6825621128082275 batch: 82/840\n",
      "Batch loss: 0.516953706741333 batch: 83/840\n",
      "Batch loss: 0.73406982421875 batch: 84/840\n",
      "Batch loss: 0.6021485328674316 batch: 85/840\n",
      "Batch loss: 0.9059431552886963 batch: 86/840\n",
      "Batch loss: 0.4343537390232086 batch: 87/840\n",
      "Batch loss: 0.45001423358917236 batch: 88/840\n",
      "Batch loss: 0.3697466254234314 batch: 89/840\n",
      "Batch loss: 0.5285038948059082 batch: 90/840\n",
      "Batch loss: 0.6068570017814636 batch: 91/840\n",
      "Batch loss: 0.614423394203186 batch: 92/840\n",
      "Batch loss: 0.593168318271637 batch: 93/840\n",
      "Batch loss: 0.5430366396903992 batch: 94/840\n",
      "Batch loss: 0.5544496774673462 batch: 95/840\n",
      "Batch loss: 0.700164258480072 batch: 96/840\n",
      "Batch loss: 0.6394885182380676 batch: 97/840\n",
      "Batch loss: 0.7865153551101685 batch: 98/840\n",
      "Batch loss: 0.5991502404212952 batch: 99/840\n",
      "Batch loss: 0.6017789244651794 batch: 100/840\n",
      "Batch loss: 0.56051105260849 batch: 101/840\n",
      "Batch loss: 0.6944308280944824 batch: 102/840\n",
      "Batch loss: 0.5942488312721252 batch: 103/840\n",
      "Batch loss: 0.5270311236381531 batch: 104/840\n",
      "Batch loss: 0.5136454105377197 batch: 105/840\n",
      "Batch loss: 0.636063277721405 batch: 106/840\n",
      "Batch loss: 0.5745035409927368 batch: 107/840\n",
      "Batch loss: 0.7639236450195312 batch: 108/840\n",
      "Batch loss: 0.6834562420845032 batch: 109/840\n",
      "Batch loss: 0.5803247690200806 batch: 110/840\n",
      "Batch loss: 0.4460260272026062 batch: 111/840\n",
      "Batch loss: 0.6413333415985107 batch: 112/840\n",
      "Batch loss: 0.6607221364974976 batch: 113/840\n",
      "Batch loss: 0.5686892867088318 batch: 114/840\n",
      "Batch loss: 0.5166299939155579 batch: 115/840\n",
      "Batch loss: 0.6596108078956604 batch: 116/840\n",
      "Batch loss: 0.6431522369384766 batch: 117/840\n",
      "Batch loss: 0.4624316394329071 batch: 118/840\n",
      "Batch loss: 0.6675025820732117 batch: 119/840\n",
      "Batch loss: 0.5428414344787598 batch: 120/840\n",
      "Batch loss: 0.7335765361785889 batch: 121/840\n",
      "Batch loss: 0.7293987274169922 batch: 122/840\n",
      "Batch loss: 0.636457085609436 batch: 123/840\n",
      "Batch loss: 0.5447871685028076 batch: 124/840\n",
      "Batch loss: 0.6018778681755066 batch: 125/840\n",
      "Batch loss: 0.5830787420272827 batch: 126/840\n",
      "Batch loss: 0.6681369543075562 batch: 127/840\n",
      "Batch loss: 0.594120442867279 batch: 128/840\n",
      "Batch loss: 0.7402793169021606 batch: 129/840\n",
      "Batch loss: 0.5439771413803101 batch: 130/840\n",
      "Batch loss: 0.7124481201171875 batch: 131/840\n",
      "Batch loss: 0.9604586958885193 batch: 132/840\n",
      "Batch loss: 0.7012699842453003 batch: 133/840\n",
      "Batch loss: 0.5608540177345276 batch: 134/840\n",
      "Batch loss: 0.5108143091201782 batch: 135/840\n",
      "Batch loss: 0.7144440412521362 batch: 136/840\n",
      "Batch loss: 0.5708867907524109 batch: 137/840\n",
      "Batch loss: 0.5363368391990662 batch: 138/840\n",
      "Batch loss: 0.5564932823181152 batch: 139/840\n",
      "Batch loss: 0.6170703768730164 batch: 140/840\n",
      "Batch loss: 0.4579004943370819 batch: 141/840\n",
      "Batch loss: 0.620582103729248 batch: 142/840\n",
      "Batch loss: 0.5673085451126099 batch: 143/840\n",
      "Batch loss: 0.5736448168754578 batch: 144/840\n",
      "Batch loss: 0.6447995901107788 batch: 145/840\n",
      "Batch loss: 0.7549266219139099 batch: 146/840\n",
      "Batch loss: 0.4661344885826111 batch: 147/840\n",
      "Batch loss: 0.7113397121429443 batch: 148/840\n",
      "Batch loss: 0.6288538575172424 batch: 149/840\n",
      "Batch loss: 0.6933133602142334 batch: 150/840\n",
      "Batch loss: 0.5060579180717468 batch: 151/840\n",
      "Batch loss: 0.6460875868797302 batch: 152/840\n",
      "Batch loss: 0.5001895427703857 batch: 153/840\n",
      "Batch loss: 0.8777002096176147 batch: 154/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.500082790851593 batch: 155/840\n",
      "Batch loss: 0.579103410243988 batch: 156/840\n",
      "Batch loss: 0.6690755486488342 batch: 157/840\n",
      "Batch loss: 0.4972093999385834 batch: 158/840\n",
      "Batch loss: 0.5225533843040466 batch: 159/840\n",
      "Batch loss: 0.5444604158401489 batch: 160/840\n",
      "Batch loss: 0.6820705533027649 batch: 161/840\n",
      "Batch loss: 0.6696226596832275 batch: 162/840\n",
      "Batch loss: 0.6340273022651672 batch: 163/840\n",
      "Batch loss: 0.45905840396881104 batch: 164/840\n",
      "Batch loss: 0.6680819988250732 batch: 165/840\n",
      "Batch loss: 0.5232229232788086 batch: 166/840\n",
      "Batch loss: 0.6236520409584045 batch: 167/840\n",
      "Batch loss: 0.633539080619812 batch: 168/840\n",
      "Batch loss: 0.4581613540649414 batch: 169/840\n",
      "Batch loss: 0.6246985793113708 batch: 170/840\n",
      "Batch loss: 0.6537085175514221 batch: 171/840\n",
      "Batch loss: 0.6630408763885498 batch: 172/840\n",
      "Batch loss: 0.6177719235420227 batch: 173/840\n",
      "Batch loss: 0.6088497638702393 batch: 174/840\n",
      "Batch loss: 0.5580502152442932 batch: 175/840\n",
      "Batch loss: 0.7439872026443481 batch: 176/840\n",
      "Batch loss: 0.7598273754119873 batch: 177/840\n",
      "Batch loss: 0.6266701221466064 batch: 178/840\n",
      "Batch loss: 0.6279127597808838 batch: 179/840\n",
      "Batch loss: 0.5068793296813965 batch: 180/840\n",
      "Batch loss: 0.5197913646697998 batch: 181/840\n",
      "Batch loss: 0.5453570485115051 batch: 182/840\n",
      "Batch loss: 0.6743299961090088 batch: 183/840\n",
      "Batch loss: 0.48645949363708496 batch: 184/840\n",
      "Batch loss: 0.4028746783733368 batch: 185/840\n",
      "Batch loss: 0.4931938648223877 batch: 186/840\n",
      "Batch loss: 0.6955485343933105 batch: 187/840\n",
      "Batch loss: 0.6293899416923523 batch: 188/840\n",
      "Batch loss: 0.6329358816146851 batch: 189/840\n",
      "Batch loss: 0.6703990697860718 batch: 190/840\n",
      "Batch loss: 0.7902382612228394 batch: 191/840\n",
      "Batch loss: 0.5473521947860718 batch: 192/840\n",
      "Batch loss: 0.48152026534080505 batch: 193/840\n",
      "Batch loss: 0.3812355399131775 batch: 194/840\n",
      "Batch loss: 0.46842655539512634 batch: 195/840\n",
      "Batch loss: 0.7147849798202515 batch: 196/840\n",
      "Batch loss: 0.645285964012146 batch: 197/840\n",
      "Batch loss: 0.4852569103240967 batch: 198/840\n",
      "Batch loss: 0.6118861436843872 batch: 199/840\n",
      "Batch loss: 0.8158531188964844 batch: 200/840\n",
      "Batch loss: 0.604811429977417 batch: 201/840\n",
      "Batch loss: 0.5639132857322693 batch: 202/840\n",
      "Batch loss: 0.5127109885215759 batch: 203/840\n",
      "Batch loss: 0.7146012783050537 batch: 204/840\n",
      "Batch loss: 0.6330256462097168 batch: 205/840\n",
      "Batch loss: 0.576623260974884 batch: 206/840\n",
      "Batch loss: 0.6059650182723999 batch: 207/840\n",
      "Batch loss: 0.6775639057159424 batch: 208/840\n",
      "Batch loss: 0.6256041526794434 batch: 209/840\n",
      "Batch loss: 0.5253512859344482 batch: 210/840\n",
      "Batch loss: 0.491054505109787 batch: 211/840\n",
      "Batch loss: 0.5542595982551575 batch: 212/840\n",
      "Batch loss: 0.7654190063476562 batch: 213/840\n",
      "Batch loss: 0.7897831201553345 batch: 214/840\n",
      "Batch loss: 0.5768377184867859 batch: 215/840\n",
      "Batch loss: 0.6130542159080505 batch: 216/840\n",
      "Batch loss: 0.5466140508651733 batch: 217/840\n",
      "Batch loss: 0.6268365383148193 batch: 218/840\n",
      "Batch loss: 0.623337984085083 batch: 219/840\n",
      "Batch loss: 0.7998200058937073 batch: 220/840\n",
      "Batch loss: 0.6246489882469177 batch: 221/840\n",
      "Batch loss: 0.6223487257957458 batch: 222/840\n",
      "Batch loss: 0.5551787614822388 batch: 223/840\n",
      "Batch loss: 0.6752100586891174 batch: 224/840\n",
      "Batch loss: 0.6827126145362854 batch: 225/840\n",
      "Batch loss: 0.6218644976615906 batch: 226/840\n",
      "Batch loss: 0.7608181834220886 batch: 227/840\n",
      "Batch loss: 0.4763798117637634 batch: 228/840\n",
      "Batch loss: 0.5283304452896118 batch: 229/840\n",
      "Batch loss: 0.5747380256652832 batch: 230/840\n",
      "Batch loss: 0.45425090193748474 batch: 231/840\n",
      "Batch loss: 0.6611031889915466 batch: 232/840\n",
      "Batch loss: 0.7718625664710999 batch: 233/840\n",
      "Batch loss: 0.6428854465484619 batch: 234/840\n",
      "Batch loss: 0.6757808923721313 batch: 235/840\n",
      "Batch loss: 0.6643469929695129 batch: 236/840\n",
      "Batch loss: 0.5057214498519897 batch: 237/840\n",
      "Batch loss: 0.7344843149185181 batch: 238/840\n",
      "Batch loss: 0.522347629070282 batch: 239/840\n",
      "Batch loss: 0.6261637806892395 batch: 240/840\n",
      "Batch loss: 0.6261369585990906 batch: 241/840\n",
      "Batch loss: 0.5865603685379028 batch: 242/840\n",
      "Batch loss: 0.5974574089050293 batch: 243/840\n",
      "Batch loss: 0.6986653804779053 batch: 244/840\n",
      "Batch loss: 0.4701172709465027 batch: 245/840\n",
      "Batch loss: 0.5953604578971863 batch: 246/840\n",
      "Batch loss: 0.659411609172821 batch: 247/840\n",
      "Batch loss: 0.7031552195549011 batch: 248/840\n",
      "Batch loss: 0.8538950085639954 batch: 249/840\n",
      "Batch loss: 0.5196691751480103 batch: 250/840\n",
      "Batch loss: 0.5121750235557556 batch: 251/840\n",
      "Batch loss: 0.4876929223537445 batch: 252/840\n",
      "Batch loss: 0.6583220958709717 batch: 253/840\n",
      "Batch loss: 0.646500825881958 batch: 254/840\n",
      "Batch loss: 0.5697101950645447 batch: 255/840\n",
      "Batch loss: 0.6533088684082031 batch: 256/840\n",
      "Batch loss: 0.6544140577316284 batch: 257/840\n",
      "Batch loss: 0.7667118310928345 batch: 258/840\n",
      "Batch loss: 0.48904815316200256 batch: 259/840\n",
      "Batch loss: 0.4448833763599396 batch: 260/840\n",
      "Batch loss: 0.5247042179107666 batch: 261/840\n",
      "Batch loss: 0.3284870982170105 batch: 262/840\n",
      "Batch loss: 0.6524749994277954 batch: 263/840\n",
      "Batch loss: 0.5354492664337158 batch: 264/840\n",
      "Batch loss: 0.6610608696937561 batch: 265/840\n",
      "Batch loss: 0.5824761390686035 batch: 266/840\n",
      "Batch loss: 0.6906070113182068 batch: 267/840\n",
      "Batch loss: 0.5949495434761047 batch: 268/840\n",
      "Batch loss: 0.4683483839035034 batch: 269/840\n",
      "Batch loss: 0.5562444925308228 batch: 270/840\n",
      "Batch loss: 0.5619240403175354 batch: 271/840\n",
      "Batch loss: 0.7327542901039124 batch: 272/840\n",
      "Batch loss: 0.6559127569198608 batch: 273/840\n",
      "Batch loss: 0.60331130027771 batch: 274/840\n",
      "Batch loss: 0.7168757915496826 batch: 275/840\n",
      "Batch loss: 0.48835527896881104 batch: 276/840\n",
      "Batch loss: 0.5860775113105774 batch: 277/840\n",
      "Batch loss: 0.7410463690757751 batch: 278/840\n",
      "Batch loss: 0.7459718585014343 batch: 279/840\n",
      "Batch loss: 0.6653091311454773 batch: 280/840\n",
      "Batch loss: 0.5219674706459045 batch: 281/840\n",
      "Batch loss: 0.5505234003067017 batch: 282/840\n",
      "Batch loss: 0.615120530128479 batch: 283/840\n",
      "Batch loss: 0.4230318069458008 batch: 284/840\n",
      "Batch loss: 0.5319818258285522 batch: 285/840\n",
      "Batch loss: 0.7284862995147705 batch: 286/840\n",
      "Batch loss: 0.47111260890960693 batch: 287/840\n",
      "Batch loss: 0.5470535755157471 batch: 288/840\n",
      "Batch loss: 0.7162296175956726 batch: 289/840\n",
      "Batch loss: 0.7302055358886719 batch: 290/840\n",
      "Batch loss: 0.7320147156715393 batch: 291/840\n",
      "Batch loss: 0.5684145092964172 batch: 292/840\n",
      "Batch loss: 0.7339783310890198 batch: 293/840\n",
      "Batch loss: 0.5816679000854492 batch: 294/840\n",
      "Batch loss: 0.4520012140274048 batch: 295/840\n",
      "Batch loss: 0.6892561316490173 batch: 296/840\n",
      "Batch loss: 0.6905168890953064 batch: 297/840\n",
      "Batch loss: 0.7102023363113403 batch: 298/840\n",
      "Batch loss: 0.5304093360900879 batch: 299/840\n",
      "Batch loss: 0.6533600687980652 batch: 300/840\n",
      "Batch loss: 0.6736562848091125 batch: 301/840\n",
      "Batch loss: 0.5720511078834534 batch: 302/840\n",
      "Batch loss: 0.6803748607635498 batch: 303/840\n",
      "Batch loss: 0.5190896391868591 batch: 304/840\n",
      "Batch loss: 0.5615551471710205 batch: 305/840\n",
      "Batch loss: 0.6476691365242004 batch: 306/840\n",
      "Batch loss: 0.499863862991333 batch: 307/840\n",
      "Batch loss: 0.8315671682357788 batch: 308/840\n",
      "Batch loss: 0.5049313902854919 batch: 309/840\n",
      "Batch loss: 0.9053609371185303 batch: 310/840\n",
      "Batch loss: 0.588532567024231 batch: 311/840\n",
      "Batch loss: 0.6970455050468445 batch: 312/840\n",
      "Batch loss: 0.6611437797546387 batch: 313/840\n",
      "Batch loss: 0.5787277221679688 batch: 314/840\n",
      "Batch loss: 0.6259291172027588 batch: 315/840\n",
      "Batch loss: 0.5134378671646118 batch: 316/840\n",
      "Batch loss: 0.6507917642593384 batch: 317/840\n",
      "Batch loss: 0.635310173034668 batch: 318/840\n",
      "Batch loss: 0.6082335710525513 batch: 319/840\n",
      "Batch loss: 0.4748266339302063 batch: 320/840\n",
      "Batch loss: 0.527509331703186 batch: 321/840\n",
      "Batch loss: 0.5979211926460266 batch: 322/840\n",
      "Batch loss: 0.7088600993156433 batch: 323/840\n",
      "Batch loss: 0.6454238295555115 batch: 324/840\n",
      "Batch loss: 0.5405431985855103 batch: 325/840\n",
      "Batch loss: 0.5353599786758423 batch: 326/840\n",
      "Batch loss: 0.5348628759384155 batch: 327/840\n",
      "Batch loss: 0.7914277911186218 batch: 328/840\n",
      "Batch loss: 0.6827390789985657 batch: 329/840\n",
      "Batch loss: 0.6168255805969238 batch: 330/840\n",
      "Batch loss: 0.7079256176948547 batch: 331/840\n",
      "Batch loss: 0.6529772281646729 batch: 332/840\n",
      "Batch loss: 0.6286103129386902 batch: 333/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.676219642162323 batch: 334/840\n",
      "Batch loss: 0.5374742150306702 batch: 335/840\n",
      "Batch loss: 0.7275542616844177 batch: 336/840\n",
      "Batch loss: 0.8822558522224426 batch: 337/840\n",
      "Batch loss: 0.740756094455719 batch: 338/840\n",
      "Batch loss: 0.5489345192909241 batch: 339/840\n",
      "Batch loss: 0.7556161284446716 batch: 340/840\n",
      "Batch loss: 0.5028223991394043 batch: 341/840\n",
      "Batch loss: 0.46881648898124695 batch: 342/840\n",
      "Batch loss: 0.7942386865615845 batch: 343/840\n",
      "Batch loss: 0.564178466796875 batch: 344/840\n",
      "Batch loss: 0.4889373779296875 batch: 345/840\n",
      "Batch loss: 0.6293520331382751 batch: 346/840\n",
      "Batch loss: 0.7809052467346191 batch: 347/840\n",
      "Batch loss: 0.63685542345047 batch: 348/840\n",
      "Batch loss: 0.6130476593971252 batch: 349/840\n",
      "Batch loss: 0.53387451171875 batch: 350/840\n",
      "Batch loss: 0.7601810693740845 batch: 351/840\n",
      "Batch loss: 0.6361535787582397 batch: 352/840\n",
      "Batch loss: 0.726309061050415 batch: 353/840\n",
      "Batch loss: 0.6223248839378357 batch: 354/840\n",
      "Batch loss: 0.5553798675537109 batch: 355/840\n",
      "Batch loss: 0.576300323009491 batch: 356/840\n",
      "Batch loss: 0.5361530780792236 batch: 357/840\n",
      "Batch loss: 0.7373523712158203 batch: 358/840\n",
      "Batch loss: 0.6173052787780762 batch: 359/840\n",
      "Batch loss: 0.7253578901290894 batch: 360/840\n",
      "Batch loss: 0.7537611126899719 batch: 361/840\n",
      "Batch loss: 0.6013035774230957 batch: 362/840\n",
      "Batch loss: 0.5438396334648132 batch: 363/840\n",
      "Batch loss: 0.7037264108657837 batch: 364/840\n",
      "Batch loss: 0.629504919052124 batch: 365/840\n",
      "Batch loss: 0.6116386651992798 batch: 366/840\n",
      "Batch loss: 0.41947323083877563 batch: 367/840\n",
      "Batch loss: 0.7181267738342285 batch: 368/840\n",
      "Batch loss: 0.5994436740875244 batch: 369/840\n",
      "Batch loss: 0.7059076428413391 batch: 370/840\n",
      "Batch loss: 0.59710294008255 batch: 371/840\n",
      "Batch loss: 0.43283310532569885 batch: 372/840\n",
      "Batch loss: 0.6082074046134949 batch: 373/840\n",
      "Batch loss: 0.7554945349693298 batch: 374/840\n",
      "Batch loss: 0.5169870853424072 batch: 375/840\n",
      "Batch loss: 0.4369804859161377 batch: 376/840\n",
      "Batch loss: 0.6715614199638367 batch: 377/840\n",
      "Batch loss: 0.5355713367462158 batch: 378/840\n",
      "Batch loss: 0.5557812452316284 batch: 379/840\n",
      "Batch loss: 0.9323446154594421 batch: 380/840\n",
      "Batch loss: 0.8840396404266357 batch: 381/840\n",
      "Batch loss: 0.7376193404197693 batch: 382/840\n",
      "Batch loss: 0.6650537848472595 batch: 383/840\n",
      "Batch loss: 0.5354195237159729 batch: 384/840\n",
      "Batch loss: 0.7802238464355469 batch: 385/840\n",
      "Batch loss: 0.7731094360351562 batch: 386/840\n",
      "Batch loss: 0.6010236740112305 batch: 387/840\n",
      "Batch loss: 0.6905174255371094 batch: 388/840\n",
      "Batch loss: 0.6068141460418701 batch: 389/840\n",
      "Batch loss: 0.8023223280906677 batch: 390/840\n",
      "Batch loss: 0.6075165867805481 batch: 391/840\n",
      "Batch loss: 0.5787353515625 batch: 392/840\n",
      "Batch loss: 0.4903806447982788 batch: 393/840\n",
      "Batch loss: 0.7389135956764221 batch: 394/840\n",
      "Batch loss: 0.5810390710830688 batch: 395/840\n",
      "Batch loss: 0.6223352551460266 batch: 396/840\n",
      "Batch loss: 0.5588265061378479 batch: 397/840\n",
      "Batch loss: 0.670208215713501 batch: 398/840\n",
      "Batch loss: 0.43723341822624207 batch: 399/840\n",
      "Batch loss: 0.49313899874687195 batch: 400/840\n",
      "Batch loss: 0.697706937789917 batch: 401/840\n",
      "Batch loss: 0.5105940103530884 batch: 402/840\n",
      "Batch loss: 0.6364908814430237 batch: 403/840\n",
      "Batch loss: 0.6326229572296143 batch: 404/840\n",
      "Batch loss: 0.5362449288368225 batch: 405/840\n",
      "Batch loss: 0.7871946692466736 batch: 406/840\n",
      "Batch loss: 0.5730146169662476 batch: 407/840\n",
      "Batch loss: 0.685886025428772 batch: 408/840\n",
      "Batch loss: 0.802620530128479 batch: 409/840\n",
      "Batch loss: 0.7918369174003601 batch: 410/840\n",
      "Batch loss: 0.7062209248542786 batch: 411/840\n",
      "Batch loss: 0.732158362865448 batch: 412/840\n",
      "Batch loss: 0.6177668571472168 batch: 413/840\n",
      "Batch loss: 0.5697880387306213 batch: 414/840\n",
      "Batch loss: 0.8636430501937866 batch: 415/840\n",
      "Batch loss: 0.4725542366504669 batch: 416/840\n",
      "Batch loss: 0.7350360751152039 batch: 417/840\n",
      "Batch loss: 0.7493749856948853 batch: 418/840\n",
      "Batch loss: 0.6176608800888062 batch: 419/840\n",
      "Batch loss: 0.7105987071990967 batch: 420/840\n",
      "Batch loss: 0.526593029499054 batch: 421/840\n",
      "Batch loss: 0.5429587960243225 batch: 422/840\n",
      "Batch loss: 0.6015427708625793 batch: 423/840\n",
      "Batch loss: 0.6620270609855652 batch: 424/840\n",
      "Batch loss: 0.6756578683853149 batch: 425/840\n",
      "Batch loss: 0.6317527294158936 batch: 426/840\n",
      "Batch loss: 0.5282789468765259 batch: 427/840\n",
      "Batch loss: 0.8686708211898804 batch: 428/840\n",
      "Batch loss: 0.5022487044334412 batch: 429/840\n",
      "Batch loss: 0.7947998642921448 batch: 430/840\n",
      "Batch loss: 0.537221372127533 batch: 431/840\n",
      "Batch loss: 0.6060991883277893 batch: 432/840\n",
      "Batch loss: 0.4666135013103485 batch: 433/840\n",
      "Batch loss: 0.48665446043014526 batch: 434/840\n",
      "Batch loss: 0.7009114027023315 batch: 435/840\n",
      "Batch loss: 0.6539507508277893 batch: 436/840\n",
      "Batch loss: 0.7085855007171631 batch: 437/840\n",
      "Batch loss: 0.38463252782821655 batch: 438/840\n",
      "Batch loss: 0.5525672435760498 batch: 439/840\n",
      "Batch loss: 0.7473223209381104 batch: 440/840\n",
      "Batch loss: 0.6983915567398071 batch: 441/840\n",
      "Batch loss: 0.7239072918891907 batch: 442/840\n",
      "Batch loss: 0.6649685502052307 batch: 443/840\n",
      "Batch loss: 0.7560506463050842 batch: 444/840\n",
      "Batch loss: 0.595939040184021 batch: 445/840\n",
      "Batch loss: 0.6153905391693115 batch: 446/840\n",
      "Batch loss: 0.6047555804252625 batch: 447/840\n",
      "Batch loss: 0.6457678079605103 batch: 448/840\n",
      "Batch loss: 0.532366156578064 batch: 449/840\n",
      "Batch loss: 0.6715058088302612 batch: 450/840\n",
      "Batch loss: 0.6906469464302063 batch: 451/840\n",
      "Batch loss: 0.7555400133132935 batch: 452/840\n",
      "Batch loss: 0.6245026588439941 batch: 453/840\n",
      "Batch loss: 0.5754498243331909 batch: 454/840\n",
      "Batch loss: 0.6715438961982727 batch: 455/840\n",
      "Batch loss: 0.5982654094696045 batch: 456/840\n",
      "Batch loss: 0.46896064281463623 batch: 457/840\n",
      "Batch loss: 0.5627157092094421 batch: 458/840\n",
      "Batch loss: 0.5010339021682739 batch: 459/840\n",
      "Batch loss: 0.48482388257980347 batch: 460/840\n",
      "Batch loss: 0.786817729473114 batch: 461/840\n",
      "Batch loss: 0.589102566242218 batch: 462/840\n",
      "Batch loss: 0.6843095421791077 batch: 463/840\n",
      "Batch loss: 0.572454035282135 batch: 464/840\n",
      "Batch loss: 0.8491867780685425 batch: 465/840\n",
      "Batch loss: 0.6582213640213013 batch: 466/840\n",
      "Batch loss: 0.8399180769920349 batch: 467/840\n",
      "Batch loss: 0.6182288527488708 batch: 468/840\n",
      "Batch loss: 0.5055999755859375 batch: 469/840\n",
      "Batch loss: 0.6516063213348389 batch: 470/840\n",
      "Batch loss: 0.6176545023918152 batch: 471/840\n",
      "Batch loss: 0.7206306457519531 batch: 472/840\n",
      "Batch loss: 0.639219343662262 batch: 473/840\n",
      "Batch loss: 0.6224815249443054 batch: 474/840\n",
      "Batch loss: 0.5145986676216125 batch: 475/840\n",
      "Batch loss: 0.5450862646102905 batch: 476/840\n",
      "Batch loss: 0.5871643424034119 batch: 477/840\n",
      "Batch loss: 0.6176324486732483 batch: 478/840\n",
      "Batch loss: 0.4697718024253845 batch: 479/840\n",
      "Batch loss: 0.6454680562019348 batch: 480/840\n",
      "Batch loss: 0.7114211916923523 batch: 481/840\n",
      "Batch loss: 0.5970136523246765 batch: 482/840\n",
      "Batch loss: 0.5771339535713196 batch: 483/840\n",
      "Batch loss: 0.5808314681053162 batch: 484/840\n",
      "Batch loss: 0.6162593364715576 batch: 485/840\n",
      "Batch loss: 0.6069095134735107 batch: 486/840\n",
      "Batch loss: 0.47300949692726135 batch: 487/840\n",
      "Batch loss: 0.6657093167304993 batch: 488/840\n",
      "Batch loss: 0.5517643690109253 batch: 489/840\n",
      "Batch loss: 0.6717708110809326 batch: 490/840\n",
      "Batch loss: 0.4555032253265381 batch: 491/840\n",
      "Batch loss: 0.6150957345962524 batch: 492/840\n",
      "Batch loss: 0.5839841961860657 batch: 493/840\n",
      "Batch loss: 0.4141913652420044 batch: 494/840\n",
      "Batch loss: 0.6139199137687683 batch: 495/840\n",
      "Batch loss: 0.6303128004074097 batch: 496/840\n",
      "Batch loss: 0.8037828803062439 batch: 497/840\n",
      "Batch loss: 0.41611626744270325 batch: 498/840\n",
      "Batch loss: 0.7455827593803406 batch: 499/840\n",
      "Batch loss: 0.6284587979316711 batch: 500/840\n",
      "Batch loss: 0.4776490330696106 batch: 501/840\n",
      "Batch loss: 0.6586090326309204 batch: 502/840\n",
      "Batch loss: 0.5118610858917236 batch: 503/840\n",
      "Batch loss: 0.6648271083831787 batch: 504/840\n",
      "Batch loss: 0.6951308250427246 batch: 505/840\n",
      "Batch loss: 0.454137921333313 batch: 506/840\n",
      "Batch loss: 0.7673022747039795 batch: 507/840\n",
      "Batch loss: 0.5172027349472046 batch: 508/840\n",
      "Batch loss: 0.69987952709198 batch: 509/840\n",
      "Batch loss: 0.6462929248809814 batch: 510/840\n",
      "Batch loss: 0.5038703083992004 batch: 511/840\n",
      "Batch loss: 0.6251270771026611 batch: 512/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5871518850326538 batch: 513/840\n",
      "Batch loss: 0.6323200464248657 batch: 514/840\n",
      "Batch loss: 0.38930976390838623 batch: 515/840\n",
      "Batch loss: 0.660062313079834 batch: 516/840\n",
      "Batch loss: 0.5781707167625427 batch: 517/840\n",
      "Batch loss: 0.7400498390197754 batch: 518/840\n",
      "Batch loss: 0.9457343816757202 batch: 519/840\n",
      "Batch loss: 0.5971339344978333 batch: 520/840\n",
      "Batch loss: 0.5823533535003662 batch: 521/840\n",
      "Batch loss: 0.5669263005256653 batch: 522/840\n",
      "Batch loss: 0.5207718014717102 batch: 523/840\n",
      "Batch loss: 0.6216354370117188 batch: 524/840\n",
      "Batch loss: 0.5415240526199341 batch: 525/840\n",
      "Batch loss: 0.6962172985076904 batch: 526/840\n",
      "Batch loss: 0.7221055030822754 batch: 527/840\n",
      "Batch loss: 0.7299468517303467 batch: 528/840\n",
      "Batch loss: 0.5005067586898804 batch: 529/840\n",
      "Batch loss: 0.5308778285980225 batch: 530/840\n",
      "Batch loss: 0.5702365636825562 batch: 531/840\n",
      "Batch loss: 0.3818659484386444 batch: 532/840\n",
      "Batch loss: 0.6996857523918152 batch: 533/840\n",
      "Batch loss: 0.6518881916999817 batch: 534/840\n",
      "Batch loss: 0.584604799747467 batch: 535/840\n",
      "Batch loss: 0.62775057554245 batch: 536/840\n",
      "Batch loss: 0.661192774772644 batch: 537/840\n",
      "Batch loss: 0.5879675149917603 batch: 538/840\n",
      "Batch loss: 0.5686830282211304 batch: 539/840\n",
      "Batch loss: 0.7038653492927551 batch: 540/840\n",
      "Batch loss: 0.6101893782615662 batch: 541/840\n",
      "Batch loss: 0.6447113752365112 batch: 542/840\n",
      "Batch loss: 0.5361217260360718 batch: 543/840\n",
      "Batch loss: 0.6660280823707581 batch: 544/840\n",
      "Batch loss: 0.6025073528289795 batch: 545/840\n",
      "Batch loss: 0.5464833974838257 batch: 546/840\n",
      "Batch loss: 0.5699522495269775 batch: 547/840\n",
      "Batch loss: 0.5153961777687073 batch: 548/840\n",
      "Batch loss: 0.5019190907478333 batch: 549/840\n",
      "Batch loss: 0.46608299016952515 batch: 550/840\n",
      "Batch loss: 0.6273699998855591 batch: 551/840\n",
      "Batch loss: 0.55306077003479 batch: 552/840\n",
      "Batch loss: 0.6909062266349792 batch: 553/840\n",
      "Batch loss: 0.6681967377662659 batch: 554/840\n",
      "Batch loss: 0.677198588848114 batch: 555/840\n",
      "Batch loss: 0.5987534523010254 batch: 556/840\n",
      "Batch loss: 0.6166261434555054 batch: 557/840\n",
      "Batch loss: 0.5944342613220215 batch: 558/840\n",
      "Batch loss: 0.4914061725139618 batch: 559/840\n",
      "Batch loss: 0.5791844725608826 batch: 560/840\n",
      "Batch loss: 0.6661115288734436 batch: 561/840\n",
      "Batch loss: 0.5260601043701172 batch: 562/840\n",
      "Batch loss: 0.32206401228904724 batch: 563/840\n",
      "Batch loss: 0.6692163944244385 batch: 564/840\n",
      "Batch loss: 0.6782090067863464 batch: 565/840\n",
      "Batch loss: 0.6534122228622437 batch: 566/840\n",
      "Batch loss: 0.7235315442085266 batch: 567/840\n",
      "Batch loss: 0.5761296153068542 batch: 568/840\n",
      "Batch loss: 0.5630121827125549 batch: 569/840\n",
      "Batch loss: 0.44473230838775635 batch: 570/840\n",
      "Batch loss: 0.743385374546051 batch: 571/840\n",
      "Batch loss: 0.6430859565734863 batch: 572/840\n",
      "Batch loss: 0.5854107737541199 batch: 573/840\n",
      "Batch loss: 0.667919397354126 batch: 574/840\n",
      "Batch loss: 0.45905858278274536 batch: 575/840\n",
      "Batch loss: 0.7219913005828857 batch: 576/840\n",
      "Batch loss: 0.5731759071350098 batch: 577/840\n",
      "Batch loss: 0.649070680141449 batch: 578/840\n",
      "Batch loss: 0.5543990135192871 batch: 579/840\n",
      "Batch loss: 0.7631928324699402 batch: 580/840\n",
      "Batch loss: 0.6872274875640869 batch: 581/840\n",
      "Batch loss: 0.792815089225769 batch: 582/840\n",
      "Batch loss: 0.5171042084693909 batch: 583/840\n",
      "Batch loss: 0.7538299560546875 batch: 584/840\n",
      "Batch loss: 0.6178674101829529 batch: 585/840\n",
      "Batch loss: 0.682895302772522 batch: 586/840\n",
      "Batch loss: 0.6392226219177246 batch: 587/840\n",
      "Batch loss: 0.578021764755249 batch: 588/840\n",
      "Batch loss: 0.6661379933357239 batch: 589/840\n",
      "Batch loss: 0.5697019100189209 batch: 590/840\n",
      "Batch loss: 0.5090216994285583 batch: 591/840\n",
      "Batch loss: 0.553955078125 batch: 592/840\n",
      "Batch loss: 0.6491584181785583 batch: 593/840\n",
      "Batch loss: 0.6256996989250183 batch: 594/840\n",
      "Batch loss: 0.4078179895877838 batch: 595/840\n",
      "Batch loss: 0.4694978594779968 batch: 596/840\n",
      "Batch loss: 0.6762406229972839 batch: 597/840\n",
      "Batch loss: 0.344462513923645 batch: 598/840\n",
      "Batch loss: 0.5760067105293274 batch: 599/840\n",
      "Batch loss: 0.6863808631896973 batch: 600/840\n",
      "Batch loss: 0.7256749868392944 batch: 601/840\n",
      "Batch loss: 0.6307058930397034 batch: 602/840\n",
      "Batch loss: 0.6246170401573181 batch: 603/840\n",
      "Batch loss: 0.6399763226509094 batch: 604/840\n",
      "Batch loss: 0.8319773077964783 batch: 605/840\n",
      "Batch loss: 0.8094985485076904 batch: 606/840\n",
      "Batch loss: 0.7597669959068298 batch: 607/840\n",
      "Batch loss: 0.5461746454238892 batch: 608/840\n",
      "Batch loss: 0.4760666787624359 batch: 609/840\n",
      "Batch loss: 0.639951229095459 batch: 610/840\n",
      "Batch loss: 0.5621934533119202 batch: 611/840\n",
      "Batch loss: 0.7125011682510376 batch: 612/840\n",
      "Batch loss: 0.6234686970710754 batch: 613/840\n",
      "Batch loss: 0.652239203453064 batch: 614/840\n",
      "Batch loss: 0.5793484449386597 batch: 615/840\n",
      "Batch loss: 0.5159639716148376 batch: 616/840\n",
      "Batch loss: 0.49039188027381897 batch: 617/840\n",
      "Batch loss: 0.5908780694007874 batch: 618/840\n",
      "Batch loss: 0.8176378011703491 batch: 619/840\n",
      "Batch loss: 0.5222575068473816 batch: 620/840\n",
      "Batch loss: 0.5742634534835815 batch: 621/840\n",
      "Batch loss: 0.6579767465591431 batch: 622/840\n",
      "Batch loss: 0.6003434062004089 batch: 623/840\n",
      "Batch loss: 0.7249157428741455 batch: 624/840\n",
      "Batch loss: 0.6271501779556274 batch: 625/840\n",
      "Batch loss: 0.5494120121002197 batch: 626/840\n",
      "Batch loss: 0.548887312412262 batch: 627/840\n",
      "Batch loss: 0.6196151971817017 batch: 628/840\n",
      "Batch loss: 0.6111351251602173 batch: 629/840\n",
      "Batch loss: 0.6260139346122742 batch: 630/840\n",
      "Batch loss: 0.6390783786773682 batch: 631/840\n",
      "Batch loss: 0.6880074143409729 batch: 632/840\n",
      "Batch loss: 0.5986843705177307 batch: 633/840\n",
      "Batch loss: 0.6227086782455444 batch: 634/840\n",
      "Batch loss: 0.5835034847259521 batch: 635/840\n",
      "Batch loss: 0.5940688848495483 batch: 636/840\n",
      "Batch loss: 0.44034403562545776 batch: 637/840\n",
      "Batch loss: 0.6031391620635986 batch: 638/840\n",
      "Batch loss: 0.7063944935798645 batch: 639/840\n",
      "Batch loss: 0.5702681541442871 batch: 640/840\n",
      "Batch loss: 0.8353266716003418 batch: 641/840\n",
      "Batch loss: 0.6044111847877502 batch: 642/840\n",
      "Batch loss: 0.872016966342926 batch: 643/840\n",
      "Batch loss: 0.6514652967453003 batch: 644/840\n",
      "Batch loss: 0.5465191602706909 batch: 645/840\n",
      "Batch loss: 0.6034573912620544 batch: 646/840\n",
      "Batch loss: 0.6272298097610474 batch: 647/840\n",
      "Batch loss: 0.6348474621772766 batch: 648/840\n",
      "Batch loss: 0.586098849773407 batch: 649/840\n",
      "Batch loss: 0.4539017975330353 batch: 650/840\n",
      "Batch loss: 0.5916130542755127 batch: 651/840\n",
      "Batch loss: 0.6967926025390625 batch: 652/840\n",
      "Batch loss: 0.497819185256958 batch: 653/840\n",
      "Batch loss: 0.8845117688179016 batch: 654/840\n",
      "Batch loss: 0.5372388958930969 batch: 655/840\n",
      "Batch loss: 0.6695082187652588 batch: 656/840\n",
      "Batch loss: 0.5788385272026062 batch: 657/840\n",
      "Batch loss: 0.5986414551734924 batch: 658/840\n",
      "Batch loss: 0.7289347052574158 batch: 659/840\n",
      "Batch loss: 0.5578842759132385 batch: 660/840\n",
      "Batch loss: 0.6813848614692688 batch: 661/840\n",
      "Batch loss: 0.5924882888793945 batch: 662/840\n",
      "Batch loss: 0.5570168495178223 batch: 663/840\n",
      "Batch loss: 0.632051944732666 batch: 664/840\n",
      "Batch loss: 0.6774359345436096 batch: 665/840\n",
      "Batch loss: 0.6818607449531555 batch: 666/840\n",
      "Batch loss: 0.7125282287597656 batch: 667/840\n",
      "Batch loss: 0.5082995891571045 batch: 668/840\n",
      "Batch loss: 0.6460691690444946 batch: 669/840\n",
      "Batch loss: 0.5737528800964355 batch: 670/840\n",
      "Batch loss: 0.7095494866371155 batch: 671/840\n",
      "Batch loss: 0.547793447971344 batch: 672/840\n",
      "Batch loss: 0.8449463844299316 batch: 673/840\n",
      "Batch loss: 0.8561840057373047 batch: 674/840\n",
      "Batch loss: 0.558552086353302 batch: 675/840\n",
      "Batch loss: 0.5638185143470764 batch: 676/840\n",
      "Batch loss: 0.6434939503669739 batch: 677/840\n",
      "Batch loss: 0.6541454792022705 batch: 678/840\n",
      "Batch loss: 0.7765635848045349 batch: 679/840\n",
      "Batch loss: 0.723930835723877 batch: 680/840\n",
      "Batch loss: 0.6256083250045776 batch: 681/840\n",
      "Batch loss: 0.6062396168708801 batch: 682/840\n",
      "Batch loss: 0.5206640958786011 batch: 683/840\n",
      "Batch loss: 0.8626837134361267 batch: 684/840\n",
      "Batch loss: 0.6669989228248596 batch: 685/840\n",
      "Batch loss: 0.6814048290252686 batch: 686/840\n",
      "Batch loss: 0.6772311329841614 batch: 687/840\n",
      "Batch loss: 0.5471054315567017 batch: 688/840\n",
      "Batch loss: 0.5205010771751404 batch: 689/840\n",
      "Batch loss: 0.48536568880081177 batch: 690/840\n",
      "Batch loss: 0.7000871300697327 batch: 691/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6288873553276062 batch: 692/840\n",
      "Batch loss: 0.6288907527923584 batch: 693/840\n",
      "Batch loss: 0.7972927093505859 batch: 694/840\n",
      "Batch loss: 0.6923161149024963 batch: 695/840\n",
      "Batch loss: 0.6135925650596619 batch: 696/840\n",
      "Batch loss: 0.562557578086853 batch: 697/840\n",
      "Batch loss: 0.5995562076568604 batch: 698/840\n",
      "Batch loss: 0.6688772439956665 batch: 699/840\n",
      "Batch loss: 0.7191662788391113 batch: 700/840\n",
      "Batch loss: 0.8126928210258484 batch: 701/840\n",
      "Batch loss: 0.691344678401947 batch: 702/840\n",
      "Batch loss: 0.5445941686630249 batch: 703/840\n",
      "Batch loss: 0.8110744953155518 batch: 704/840\n",
      "Batch loss: 0.6062442660331726 batch: 705/840\n",
      "Batch loss: 0.5800601243972778 batch: 706/840\n",
      "Batch loss: 0.7102557420730591 batch: 707/840\n",
      "Batch loss: 0.6095046401023865 batch: 708/840\n",
      "Batch loss: 0.6768530011177063 batch: 709/840\n",
      "Batch loss: 0.7482885122299194 batch: 710/840\n",
      "Batch loss: 0.503308117389679 batch: 711/840\n",
      "Batch loss: 0.7782816886901855 batch: 712/840\n",
      "Batch loss: 0.4531058073043823 batch: 713/840\n",
      "Batch loss: 0.6122187972068787 batch: 714/840\n",
      "Batch loss: 0.7673240900039673 batch: 715/840\n",
      "Batch loss: 0.6242961883544922 batch: 716/840\n",
      "Batch loss: 0.6666055917739868 batch: 717/840\n",
      "Batch loss: 0.5192804932594299 batch: 718/840\n",
      "Batch loss: 0.45053547620773315 batch: 719/840\n",
      "Batch loss: 0.4906650483608246 batch: 720/840\n",
      "Batch loss: 0.6870281100273132 batch: 721/840\n",
      "Batch loss: 0.8219525814056396 batch: 722/840\n",
      "Batch loss: 0.4912257492542267 batch: 723/840\n",
      "Batch loss: 0.6976081132888794 batch: 724/840\n",
      "Batch loss: 0.7060788869857788 batch: 725/840\n",
      "Batch loss: 0.537290096282959 batch: 726/840\n",
      "Batch loss: 0.66047203540802 batch: 727/840\n",
      "Batch loss: 0.7045086622238159 batch: 728/840\n",
      "Batch loss: 0.6948326230049133 batch: 729/840\n",
      "Batch loss: 0.6488586664199829 batch: 730/840\n",
      "Batch loss: 0.48795756697654724 batch: 731/840\n",
      "Batch loss: 0.8366498351097107 batch: 732/840\n",
      "Batch loss: 0.6570034027099609 batch: 733/840\n",
      "Batch loss: 0.593604564666748 batch: 734/840\n",
      "Batch loss: 0.6592600345611572 batch: 735/840\n",
      "Batch loss: 0.626448929309845 batch: 736/840\n",
      "Batch loss: 0.5955674052238464 batch: 737/840\n",
      "Batch loss: 0.4733894467353821 batch: 738/840\n",
      "Batch loss: 0.6546578407287598 batch: 739/840\n",
      "Batch loss: 0.6860595941543579 batch: 740/840\n",
      "Batch loss: 0.4515896737575531 batch: 741/840\n",
      "Batch loss: 0.5408003330230713 batch: 742/840\n",
      "Batch loss: 0.4290908873081207 batch: 743/840\n",
      "Batch loss: 0.4867631196975708 batch: 744/840\n",
      "Batch loss: 0.6311394572257996 batch: 745/840\n",
      "Batch loss: 0.6030593514442444 batch: 746/840\n",
      "Batch loss: 0.7342594861984253 batch: 747/840\n",
      "Batch loss: 0.707928478717804 batch: 748/840\n",
      "Batch loss: 0.5122149586677551 batch: 749/840\n",
      "Batch loss: 0.5230022668838501 batch: 750/840\n",
      "Batch loss: 0.5734915733337402 batch: 751/840\n",
      "Batch loss: 0.9177488684654236 batch: 752/840\n",
      "Batch loss: 0.5210528373718262 batch: 753/840\n",
      "Batch loss: 0.615459680557251 batch: 754/840\n",
      "Batch loss: 0.5938770771026611 batch: 755/840\n",
      "Batch loss: 0.5120543837547302 batch: 756/840\n",
      "Batch loss: 0.6571048498153687 batch: 757/840\n",
      "Batch loss: 0.4998916983604431 batch: 758/840\n",
      "Batch loss: 0.4878213405609131 batch: 759/840\n",
      "Batch loss: 0.5611961483955383 batch: 760/840\n",
      "Batch loss: 0.5569649338722229 batch: 761/840\n",
      "Batch loss: 0.717897355556488 batch: 762/840\n",
      "Batch loss: 0.43728476762771606 batch: 763/840\n",
      "Batch loss: 0.609836757183075 batch: 764/840\n",
      "Batch loss: 0.4694799780845642 batch: 765/840\n",
      "Batch loss: 0.5535840392112732 batch: 766/840\n",
      "Batch loss: 0.7215271592140198 batch: 767/840\n",
      "Batch loss: 0.6430392265319824 batch: 768/840\n",
      "Batch loss: 0.5634744167327881 batch: 769/840\n",
      "Batch loss: 0.4762943983078003 batch: 770/840\n",
      "Batch loss: 0.6552064418792725 batch: 771/840\n",
      "Batch loss: 0.5667259097099304 batch: 772/840\n",
      "Batch loss: 0.5102220773696899 batch: 773/840\n",
      "Batch loss: 0.4359223246574402 batch: 774/840\n",
      "Batch loss: 0.5450590252876282 batch: 775/840\n",
      "Batch loss: 0.5498209595680237 batch: 776/840\n",
      "Batch loss: 0.5572091937065125 batch: 777/840\n",
      "Batch loss: 0.4579685926437378 batch: 778/840\n",
      "Batch loss: 0.7120427489280701 batch: 779/840\n",
      "Batch loss: 0.6163183450698853 batch: 780/840\n",
      "Batch loss: 0.5775561332702637 batch: 781/840\n",
      "Batch loss: 0.5741493105888367 batch: 782/840\n",
      "Batch loss: 0.45409417152404785 batch: 783/840\n",
      "Batch loss: 0.6794794201850891 batch: 784/840\n",
      "Batch loss: 0.5711485743522644 batch: 785/840\n",
      "Batch loss: 0.5718070864677429 batch: 786/840\n",
      "Batch loss: 0.5451628565788269 batch: 787/840\n",
      "Batch loss: 0.6055648922920227 batch: 788/840\n",
      "Batch loss: 0.6603925228118896 batch: 789/840\n",
      "Batch loss: 0.7606157064437866 batch: 790/840\n",
      "Batch loss: 0.5217701196670532 batch: 791/840\n",
      "Batch loss: 0.3636399507522583 batch: 792/840\n",
      "Batch loss: 0.6168176531791687 batch: 793/840\n",
      "Batch loss: 0.573108434677124 batch: 794/840\n",
      "Batch loss: 0.6583845615386963 batch: 795/840\n",
      "Batch loss: 0.7665239572525024 batch: 796/840\n",
      "Batch loss: 0.5959212183952332 batch: 797/840\n",
      "Batch loss: 0.5588639974594116 batch: 798/840\n",
      "Batch loss: 0.5878786444664001 batch: 799/840\n",
      "Batch loss: 0.4622865915298462 batch: 800/840\n",
      "Batch loss: 0.6487221717834473 batch: 801/840\n",
      "Batch loss: 0.4679984152317047 batch: 802/840\n",
      "Batch loss: 0.4127255976200104 batch: 803/840\n",
      "Batch loss: 0.6740542650222778 batch: 804/840\n",
      "Batch loss: 0.5768120884895325 batch: 805/840\n",
      "Batch loss: 0.6324569582939148 batch: 806/840\n",
      "Batch loss: 0.6364322304725647 batch: 807/840\n",
      "Batch loss: 0.6001479625701904 batch: 808/840\n",
      "Batch loss: 0.6289697885513306 batch: 809/840\n",
      "Batch loss: 0.5946261286735535 batch: 810/840\n",
      "Batch loss: 0.5676143765449524 batch: 811/840\n",
      "Batch loss: 0.5753506422042847 batch: 812/840\n",
      "Batch loss: 0.5575874447822571 batch: 813/840\n",
      "Batch loss: 0.5970896482467651 batch: 814/840\n",
      "Batch loss: 0.676240861415863 batch: 815/840\n",
      "Batch loss: 0.6378946304321289 batch: 816/840\n",
      "Batch loss: 0.7318201661109924 batch: 817/840\n",
      "Batch loss: 0.618467390537262 batch: 818/840\n",
      "Batch loss: 0.5015500783920288 batch: 819/840\n",
      "Batch loss: 0.7355952262878418 batch: 820/840\n",
      "Batch loss: 0.6116433143615723 batch: 821/840\n",
      "Batch loss: 0.6797624826431274 batch: 822/840\n",
      "Batch loss: 0.6835792064666748 batch: 823/840\n",
      "Batch loss: 0.6162757277488708 batch: 824/840\n",
      "Batch loss: 0.703370213508606 batch: 825/840\n",
      "Batch loss: 0.5765421986579895 batch: 826/840\n",
      "Batch loss: 0.5536050796508789 batch: 827/840\n",
      "Batch loss: 0.6472856402397156 batch: 828/840\n",
      "Batch loss: 0.6376706957817078 batch: 829/840\n",
      "Batch loss: 0.7306322455406189 batch: 830/840\n",
      "Batch loss: 0.5463257431983948 batch: 831/840\n",
      "Batch loss: 0.5763610601425171 batch: 832/840\n",
      "Batch loss: 0.7693608999252319 batch: 833/840\n",
      "Batch loss: 0.5796348452568054 batch: 834/840\n",
      "Batch loss: 0.49456536769866943 batch: 835/840\n",
      "Batch loss: 0.5077386498451233 batch: 836/840\n",
      "Batch loss: 0.5825925469398499 batch: 837/840\n",
      "Batch loss: 0.6804670691490173 batch: 838/840\n",
      "Batch loss: 0.5507738590240479 batch: 839/840\n",
      "Batch loss: 0.7478116750717163 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 14/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.816\n",
      "Running epoch 15/15\n",
      "Batch loss: 0.45811232924461365 batch: 1/840\n",
      "Batch loss: 1.1083197593688965 batch: 2/840\n",
      "Batch loss: 0.6226295232772827 batch: 3/840\n",
      "Batch loss: 0.6512033939361572 batch: 4/840\n",
      "Batch loss: 0.6108394861221313 batch: 5/840\n",
      "Batch loss: 0.4445784389972687 batch: 6/840\n",
      "Batch loss: 0.5534914135932922 batch: 7/840\n",
      "Batch loss: 0.5392876267433167 batch: 8/840\n",
      "Batch loss: 0.4923807382583618 batch: 9/840\n",
      "Batch loss: 0.5751879811286926 batch: 10/840\n",
      "Batch loss: 0.5374080538749695 batch: 11/840\n",
      "Batch loss: 0.5383071303367615 batch: 12/840\n",
      "Batch loss: 0.5275496244430542 batch: 13/840\n",
      "Batch loss: 0.6070115566253662 batch: 14/840\n",
      "Batch loss: 0.5548355579376221 batch: 15/840\n",
      "Batch loss: 0.4673941731452942 batch: 16/840\n",
      "Batch loss: 0.4524903893470764 batch: 17/840\n",
      "Batch loss: 0.6738365292549133 batch: 18/840\n",
      "Batch loss: 0.6279027462005615 batch: 19/840\n",
      "Batch loss: 0.7342907190322876 batch: 20/840\n",
      "Batch loss: 0.6132281422615051 batch: 21/840\n",
      "Batch loss: 0.5432897806167603 batch: 22/840\n",
      "Batch loss: 0.7178775072097778 batch: 23/840\n",
      "Batch loss: 0.43447577953338623 batch: 24/840\n",
      "Batch loss: 0.6223680377006531 batch: 25/840\n",
      "Batch loss: 0.5654935240745544 batch: 26/840\n",
      "Batch loss: 0.63151615858078 batch: 27/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6021156907081604 batch: 28/840\n",
      "Batch loss: 0.5735129117965698 batch: 29/840\n",
      "Batch loss: 0.5109794735908508 batch: 30/840\n",
      "Batch loss: 0.5904938578605652 batch: 31/840\n",
      "Batch loss: 0.5981833338737488 batch: 32/840\n",
      "Batch loss: 0.6113447546958923 batch: 33/840\n",
      "Batch loss: 0.4487757384777069 batch: 34/840\n",
      "Batch loss: 0.494591623544693 batch: 35/840\n",
      "Batch loss: 0.4872990548610687 batch: 36/840\n",
      "Batch loss: 0.6875902414321899 batch: 37/840\n",
      "Batch loss: 0.7168633937835693 batch: 38/840\n",
      "Batch loss: 0.7149865031242371 batch: 39/840\n",
      "Batch loss: 0.5362414717674255 batch: 40/840\n",
      "Batch loss: 0.8019624352455139 batch: 41/840\n",
      "Batch loss: 0.5821481943130493 batch: 42/840\n",
      "Batch loss: 0.5890933275222778 batch: 43/840\n",
      "Batch loss: 0.682228147983551 batch: 44/840\n",
      "Batch loss: 0.5858602523803711 batch: 45/840\n",
      "Batch loss: 0.5124677419662476 batch: 46/840\n",
      "Batch loss: 0.5184600353240967 batch: 47/840\n",
      "Batch loss: 0.5900648832321167 batch: 48/840\n",
      "Batch loss: 0.6455181837081909 batch: 49/840\n",
      "Batch loss: 0.6943686604499817 batch: 50/840\n",
      "Batch loss: 0.6112581491470337 batch: 51/840\n",
      "Batch loss: 0.7183350920677185 batch: 52/840\n",
      "Batch loss: 0.5282990336418152 batch: 53/840\n",
      "Batch loss: 0.6635856628417969 batch: 54/840\n",
      "Batch loss: 0.5914559960365295 batch: 55/840\n",
      "Batch loss: 0.599764883518219 batch: 56/840\n",
      "Batch loss: 0.5914096236228943 batch: 57/840\n",
      "Batch loss: 0.502642035484314 batch: 58/840\n",
      "Batch loss: 0.6232933402061462 batch: 59/840\n",
      "Batch loss: 0.528312623500824 batch: 60/840\n",
      "Batch loss: 0.7223654389381409 batch: 61/840\n",
      "Batch loss: 0.605108380317688 batch: 62/840\n",
      "Batch loss: 0.6581575870513916 batch: 63/840\n",
      "Batch loss: 0.7201799750328064 batch: 64/840\n",
      "Batch loss: 0.5627424716949463 batch: 65/840\n",
      "Batch loss: 0.5735978484153748 batch: 66/840\n",
      "Batch loss: 0.6637431383132935 batch: 67/840\n",
      "Batch loss: 0.6019957065582275 batch: 68/840\n",
      "Batch loss: 0.7184947729110718 batch: 69/840\n",
      "Batch loss: 0.5289148092269897 batch: 70/840\n",
      "Batch loss: 0.6767242550849915 batch: 71/840\n",
      "Batch loss: 0.6964318156242371 batch: 72/840\n",
      "Batch loss: 0.7423313856124878 batch: 73/840\n",
      "Batch loss: 0.5865061283111572 batch: 74/840\n",
      "Batch loss: 0.6549205780029297 batch: 75/840\n",
      "Batch loss: 0.4098970890045166 batch: 76/840\n",
      "Batch loss: 0.5664315223693848 batch: 77/840\n",
      "Batch loss: 0.7985435724258423 batch: 78/840\n",
      "Batch loss: 0.6084314584732056 batch: 79/840\n",
      "Batch loss: 0.6736888885498047 batch: 80/840\n",
      "Batch loss: 0.5519720911979675 batch: 81/840\n",
      "Batch loss: 0.5673079490661621 batch: 82/840\n",
      "Batch loss: 0.5437675714492798 batch: 83/840\n",
      "Batch loss: 0.6975885033607483 batch: 84/840\n",
      "Batch loss: 0.6439808011054993 batch: 85/840\n",
      "Batch loss: 0.9581282734870911 batch: 86/840\n",
      "Batch loss: 0.5046485662460327 batch: 87/840\n",
      "Batch loss: 0.40764090418815613 batch: 88/840\n",
      "Batch loss: 0.44796082377433777 batch: 89/840\n",
      "Batch loss: 0.5065372586250305 batch: 90/840\n",
      "Batch loss: 0.5739279389381409 batch: 91/840\n",
      "Batch loss: 0.6156134009361267 batch: 92/840\n",
      "Batch loss: 0.6284353733062744 batch: 93/840\n",
      "Batch loss: 0.47464805841445923 batch: 94/840\n",
      "Batch loss: 0.6532154679298401 batch: 95/840\n",
      "Batch loss: 0.5047198534011841 batch: 96/840\n",
      "Batch loss: 0.637610673904419 batch: 97/840\n",
      "Batch loss: 0.7737067937850952 batch: 98/840\n",
      "Batch loss: 0.5402426719665527 batch: 99/840\n",
      "Batch loss: 0.6700270771980286 batch: 100/840\n",
      "Batch loss: 0.5214547514915466 batch: 101/840\n",
      "Batch loss: 0.4305437207221985 batch: 102/840\n",
      "Batch loss: 0.5966559052467346 batch: 103/840\n",
      "Batch loss: 0.5246259570121765 batch: 104/840\n",
      "Batch loss: 0.5770728588104248 batch: 105/840\n",
      "Batch loss: 0.7206098437309265 batch: 106/840\n",
      "Batch loss: 0.5398903489112854 batch: 107/840\n",
      "Batch loss: 0.8196612000465393 batch: 108/840\n",
      "Batch loss: 0.5979653596878052 batch: 109/840\n",
      "Batch loss: 0.4936859607696533 batch: 110/840\n",
      "Batch loss: 0.5446556210517883 batch: 111/840\n",
      "Batch loss: 0.6814711093902588 batch: 112/840\n",
      "Batch loss: 0.6420598030090332 batch: 113/840\n",
      "Batch loss: 0.5506348013877869 batch: 114/840\n",
      "Batch loss: 0.6911467909812927 batch: 115/840\n",
      "Batch loss: 0.5991480350494385 batch: 116/840\n",
      "Batch loss: 0.5730461478233337 batch: 117/840\n",
      "Batch loss: 0.49594226479530334 batch: 118/840\n",
      "Batch loss: 0.732176661491394 batch: 119/840\n",
      "Batch loss: 0.5392971634864807 batch: 120/840\n",
      "Batch loss: 0.7084080576896667 batch: 121/840\n",
      "Batch loss: 0.7715887427330017 batch: 122/840\n",
      "Batch loss: 0.49259239435195923 batch: 123/840\n",
      "Batch loss: 0.5728116631507874 batch: 124/840\n",
      "Batch loss: 0.5490445494651794 batch: 125/840\n",
      "Batch loss: 0.5913078784942627 batch: 126/840\n",
      "Batch loss: 0.682318925857544 batch: 127/840\n",
      "Batch loss: 0.6761289834976196 batch: 128/840\n",
      "Batch loss: 0.6729873418807983 batch: 129/840\n",
      "Batch loss: 0.6343296766281128 batch: 130/840\n",
      "Batch loss: 0.6247191429138184 batch: 131/840\n",
      "Batch loss: 0.938626766204834 batch: 132/840\n",
      "Batch loss: 0.694658100605011 batch: 133/840\n",
      "Batch loss: 0.6043897867202759 batch: 134/840\n",
      "Batch loss: 0.5619468092918396 batch: 135/840\n",
      "Batch loss: 0.6880490779876709 batch: 136/840\n",
      "Batch loss: 0.5341567397117615 batch: 137/840\n",
      "Batch loss: 0.609834611415863 batch: 138/840\n",
      "Batch loss: 0.5895158052444458 batch: 139/840\n",
      "Batch loss: 0.7574436664581299 batch: 140/840\n",
      "Batch loss: 0.4511687755584717 batch: 141/840\n",
      "Batch loss: 0.5521855354309082 batch: 142/840\n",
      "Batch loss: 0.5262014865875244 batch: 143/840\n",
      "Batch loss: 0.6043587923049927 batch: 144/840\n",
      "Batch loss: 0.6950498819351196 batch: 145/840\n",
      "Batch loss: 0.6469001173973083 batch: 146/840\n",
      "Batch loss: 0.4858131408691406 batch: 147/840\n",
      "Batch loss: 0.7057952284812927 batch: 148/840\n",
      "Batch loss: 0.6889013051986694 batch: 149/840\n",
      "Batch loss: 0.6178810000419617 batch: 150/840\n",
      "Batch loss: 0.5708010196685791 batch: 151/840\n",
      "Batch loss: 0.5991243124008179 batch: 152/840\n",
      "Batch loss: 0.5511414408683777 batch: 153/840\n",
      "Batch loss: 0.737783670425415 batch: 154/840\n",
      "Batch loss: 0.5157290697097778 batch: 155/840\n",
      "Batch loss: 0.656087338924408 batch: 156/840\n",
      "Batch loss: 0.648597002029419 batch: 157/840\n",
      "Batch loss: 0.4791492223739624 batch: 158/840\n",
      "Batch loss: 0.47350776195526123 batch: 159/840\n",
      "Batch loss: 0.5580700039863586 batch: 160/840\n",
      "Batch loss: 0.6473807692527771 batch: 161/840\n",
      "Batch loss: 0.6486113667488098 batch: 162/840\n",
      "Batch loss: 0.7545526623725891 batch: 163/840\n",
      "Batch loss: 0.43263280391693115 batch: 164/840\n",
      "Batch loss: 0.6675505042076111 batch: 165/840\n",
      "Batch loss: 0.49276724457740784 batch: 166/840\n",
      "Batch loss: 0.6355986595153809 batch: 167/840\n",
      "Batch loss: 0.7043858170509338 batch: 168/840\n",
      "Batch loss: 0.48999109864234924 batch: 169/840\n",
      "Batch loss: 0.707563579082489 batch: 170/840\n",
      "Batch loss: 0.6087126731872559 batch: 171/840\n",
      "Batch loss: 0.7442494034767151 batch: 172/840\n",
      "Batch loss: 0.590583860874176 batch: 173/840\n",
      "Batch loss: 0.5996540188789368 batch: 174/840\n",
      "Batch loss: 0.6130767464637756 batch: 175/840\n",
      "Batch loss: 0.7061548829078674 batch: 176/840\n",
      "Batch loss: 0.5901138782501221 batch: 177/840\n",
      "Batch loss: 0.6515862345695496 batch: 178/840\n",
      "Batch loss: 0.6686532497406006 batch: 179/840\n",
      "Batch loss: 0.46786367893218994 batch: 180/840\n",
      "Batch loss: 0.525535523891449 batch: 181/840\n",
      "Batch loss: 0.5873168706893921 batch: 182/840\n",
      "Batch loss: 0.7538831830024719 batch: 183/840\n",
      "Batch loss: 0.4875122308731079 batch: 184/840\n",
      "Batch loss: 0.4216795265674591 batch: 185/840\n",
      "Batch loss: 0.5124651193618774 batch: 186/840\n",
      "Batch loss: 0.6106393933296204 batch: 187/840\n",
      "Batch loss: 0.5134781002998352 batch: 188/840\n",
      "Batch loss: 0.5842614769935608 batch: 189/840\n",
      "Batch loss: 0.7086212635040283 batch: 190/840\n",
      "Batch loss: 0.8658574819564819 batch: 191/840\n",
      "Batch loss: 0.4449169933795929 batch: 192/840\n",
      "Batch loss: 0.4480072259902954 batch: 193/840\n",
      "Batch loss: 0.4187833368778229 batch: 194/840\n",
      "Batch loss: 0.5582166314125061 batch: 195/840\n",
      "Batch loss: 0.730170488357544 batch: 196/840\n",
      "Batch loss: 0.6134916543960571 batch: 197/840\n",
      "Batch loss: 0.4929153323173523 batch: 198/840\n",
      "Batch loss: 0.5597830414772034 batch: 199/840\n",
      "Batch loss: 0.8042430877685547 batch: 200/840\n",
      "Batch loss: 0.5894637107849121 batch: 201/840\n",
      "Batch loss: 0.5423139333724976 batch: 202/840\n",
      "Batch loss: 0.5355672836303711 batch: 203/840\n",
      "Batch loss: 0.6681519150733948 batch: 204/840\n",
      "Batch loss: 0.7418822646141052 batch: 205/840\n",
      "Batch loss: 0.6681916117668152 batch: 206/840\n",
      "Batch loss: 0.6297212243080139 batch: 207/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5972554087638855 batch: 208/840\n",
      "Batch loss: 0.5354229807853699 batch: 209/840\n",
      "Batch loss: 0.6074554324150085 batch: 210/840\n",
      "Batch loss: 0.49014610052108765 batch: 211/840\n",
      "Batch loss: 0.5661182999610901 batch: 212/840\n",
      "Batch loss: 0.7376562356948853 batch: 213/840\n",
      "Batch loss: 0.6857926249504089 batch: 214/840\n",
      "Batch loss: 0.5795580148696899 batch: 215/840\n",
      "Batch loss: 0.6052008271217346 batch: 216/840\n",
      "Batch loss: 0.7540960907936096 batch: 217/840\n",
      "Batch loss: 0.6341010928153992 batch: 218/840\n",
      "Batch loss: 0.606706440448761 batch: 219/840\n",
      "Batch loss: 0.7404935956001282 batch: 220/840\n",
      "Batch loss: 0.502988874912262 batch: 221/840\n",
      "Batch loss: 0.7262333631515503 batch: 222/840\n",
      "Batch loss: 0.5107777118682861 batch: 223/840\n",
      "Batch loss: 0.7443498969078064 batch: 224/840\n",
      "Batch loss: 0.6478846073150635 batch: 225/840\n",
      "Batch loss: 0.7010459899902344 batch: 226/840\n",
      "Batch loss: 0.7102625966072083 batch: 227/840\n",
      "Batch loss: 0.5508115887641907 batch: 228/840\n",
      "Batch loss: 0.46950867772102356 batch: 229/840\n",
      "Batch loss: 0.5847160220146179 batch: 230/840\n",
      "Batch loss: 0.4987185597419739 batch: 231/840\n",
      "Batch loss: 0.5933867692947388 batch: 232/840\n",
      "Batch loss: 0.7380827069282532 batch: 233/840\n",
      "Batch loss: 0.560271680355072 batch: 234/840\n",
      "Batch loss: 0.555297315120697 batch: 235/840\n",
      "Batch loss: 0.6655780076980591 batch: 236/840\n",
      "Batch loss: 0.5263563394546509 batch: 237/840\n",
      "Batch loss: 0.6942012310028076 batch: 238/840\n",
      "Batch loss: 0.6066264510154724 batch: 239/840\n",
      "Batch loss: 0.5850980877876282 batch: 240/840\n",
      "Batch loss: 0.754574716091156 batch: 241/840\n",
      "Batch loss: 0.5950651168823242 batch: 242/840\n",
      "Batch loss: 0.5940989851951599 batch: 243/840\n",
      "Batch loss: 0.7496497631072998 batch: 244/840\n",
      "Batch loss: 0.5282310843467712 batch: 245/840\n",
      "Batch loss: 0.6469603776931763 batch: 246/840\n",
      "Batch loss: 0.691882312297821 batch: 247/840\n",
      "Batch loss: 0.7508229613304138 batch: 248/840\n",
      "Batch loss: 0.9411064982414246 batch: 249/840\n",
      "Batch loss: 0.5394818782806396 batch: 250/840\n",
      "Batch loss: 0.49266374111175537 batch: 251/840\n",
      "Batch loss: 0.5295614004135132 batch: 252/840\n",
      "Batch loss: 0.637263298034668 batch: 253/840\n",
      "Batch loss: 0.6818622350692749 batch: 254/840\n",
      "Batch loss: 0.619996964931488 batch: 255/840\n",
      "Batch loss: 0.6406422257423401 batch: 256/840\n",
      "Batch loss: 0.48577815294265747 batch: 257/840\n",
      "Batch loss: 0.7653711438179016 batch: 258/840\n",
      "Batch loss: 0.45616206526756287 batch: 259/840\n",
      "Batch loss: 0.49218448996543884 batch: 260/840\n",
      "Batch loss: 0.5245451331138611 batch: 261/840\n",
      "Batch loss: 0.3905567526817322 batch: 262/840\n",
      "Batch loss: 0.565608024597168 batch: 263/840\n",
      "Batch loss: 0.5377100110054016 batch: 264/840\n",
      "Batch loss: 0.5551292300224304 batch: 265/840\n",
      "Batch loss: 0.570193350315094 batch: 266/840\n",
      "Batch loss: 0.6620549559593201 batch: 267/840\n",
      "Batch loss: 0.5514234304428101 batch: 268/840\n",
      "Batch loss: 0.5548726320266724 batch: 269/840\n",
      "Batch loss: 0.5944116115570068 batch: 270/840\n",
      "Batch loss: 0.5051957964897156 batch: 271/840\n",
      "Batch loss: 0.7745897769927979 batch: 272/840\n",
      "Batch loss: 0.8320685029029846 batch: 273/840\n",
      "Batch loss: 0.6113373637199402 batch: 274/840\n",
      "Batch loss: 0.7782652378082275 batch: 275/840\n",
      "Batch loss: 0.5278048515319824 batch: 276/840\n",
      "Batch loss: 0.5943477153778076 batch: 277/840\n",
      "Batch loss: 0.8312634825706482 batch: 278/840\n",
      "Batch loss: 0.702339768409729 batch: 279/840\n",
      "Batch loss: 0.7254237532615662 batch: 280/840\n",
      "Batch loss: 0.5284101366996765 batch: 281/840\n",
      "Batch loss: 0.5144178867340088 batch: 282/840\n",
      "Batch loss: 0.6411024332046509 batch: 283/840\n",
      "Batch loss: 0.46735772490501404 batch: 284/840\n",
      "Batch loss: 0.5399102568626404 batch: 285/840\n",
      "Batch loss: 0.6189377307891846 batch: 286/840\n",
      "Batch loss: 0.4189673662185669 batch: 287/840\n",
      "Batch loss: 0.5018985271453857 batch: 288/840\n",
      "Batch loss: 0.7077987790107727 batch: 289/840\n",
      "Batch loss: 0.8372603058815002 batch: 290/840\n",
      "Batch loss: 0.6445760130882263 batch: 291/840\n",
      "Batch loss: 0.590519368648529 batch: 292/840\n",
      "Batch loss: 0.616605818271637 batch: 293/840\n",
      "Batch loss: 0.5924672484397888 batch: 294/840\n",
      "Batch loss: 0.5111612677574158 batch: 295/840\n",
      "Batch loss: 0.7204611897468567 batch: 296/840\n",
      "Batch loss: 0.5773171186447144 batch: 297/840\n",
      "Batch loss: 0.6784302592277527 batch: 298/840\n",
      "Batch loss: 0.5661265850067139 batch: 299/840\n",
      "Batch loss: 0.6837051510810852 batch: 300/840\n",
      "Batch loss: 0.7080201506614685 batch: 301/840\n",
      "Batch loss: 0.6039088368415833 batch: 302/840\n",
      "Batch loss: 0.640472948551178 batch: 303/840\n",
      "Batch loss: 0.49669745564460754 batch: 304/840\n",
      "Batch loss: 0.5121185779571533 batch: 305/840\n",
      "Batch loss: 0.6168881058692932 batch: 306/840\n",
      "Batch loss: 0.5741708874702454 batch: 307/840\n",
      "Batch loss: 0.7058767080307007 batch: 308/840\n",
      "Batch loss: 0.5556126236915588 batch: 309/840\n",
      "Batch loss: 0.8173702359199524 batch: 310/840\n",
      "Batch loss: 0.5519742369651794 batch: 311/840\n",
      "Batch loss: 0.6935550570487976 batch: 312/840\n",
      "Batch loss: 0.6891629099845886 batch: 313/840\n",
      "Batch loss: 0.433669775724411 batch: 314/840\n",
      "Batch loss: 0.5951154828071594 batch: 315/840\n",
      "Batch loss: 0.4526495635509491 batch: 316/840\n",
      "Batch loss: 0.6458932757377625 batch: 317/840\n",
      "Batch loss: 0.6349151134490967 batch: 318/840\n",
      "Batch loss: 0.7317905426025391 batch: 319/840\n",
      "Batch loss: 0.5578371286392212 batch: 320/840\n",
      "Batch loss: 0.5674157738685608 batch: 321/840\n",
      "Batch loss: 0.5948849320411682 batch: 322/840\n",
      "Batch loss: 0.7586277723312378 batch: 323/840\n",
      "Batch loss: 0.6106854677200317 batch: 324/840\n",
      "Batch loss: 0.49504268169403076 batch: 325/840\n",
      "Batch loss: 0.6422235369682312 batch: 326/840\n",
      "Batch loss: 0.5566940903663635 batch: 327/840\n",
      "Batch loss: 0.6617343425750732 batch: 328/840\n",
      "Batch loss: 0.7450305819511414 batch: 329/840\n",
      "Batch loss: 0.6127148270606995 batch: 330/840\n",
      "Batch loss: 0.6972867846488953 batch: 331/840\n",
      "Batch loss: 0.6597552299499512 batch: 332/840\n",
      "Batch loss: 0.5389736294746399 batch: 333/840\n",
      "Batch loss: 0.6399287581443787 batch: 334/840\n",
      "Batch loss: 0.5208539366722107 batch: 335/840\n",
      "Batch loss: 0.6037229299545288 batch: 336/840\n",
      "Batch loss: 0.7541481256484985 batch: 337/840\n",
      "Batch loss: 0.7181038856506348 batch: 338/840\n",
      "Batch loss: 0.5287973284721375 batch: 339/840\n",
      "Batch loss: 0.7857170104980469 batch: 340/840\n",
      "Batch loss: 0.577848494052887 batch: 341/840\n",
      "Batch loss: 0.42005324363708496 batch: 342/840\n",
      "Batch loss: 0.8149873614311218 batch: 343/840\n",
      "Batch loss: 0.5677974820137024 batch: 344/840\n",
      "Batch loss: 0.4748934805393219 batch: 345/840\n",
      "Batch loss: 0.6228579878807068 batch: 346/840\n",
      "Batch loss: 0.6173164248466492 batch: 347/840\n",
      "Batch loss: 0.5383410453796387 batch: 348/840\n",
      "Batch loss: 0.646533727645874 batch: 349/840\n",
      "Batch loss: 0.5483815670013428 batch: 350/840\n",
      "Batch loss: 0.7126424312591553 batch: 351/840\n",
      "Batch loss: 0.5823574662208557 batch: 352/840\n",
      "Batch loss: 0.6570212841033936 batch: 353/840\n",
      "Batch loss: 0.6606947183609009 batch: 354/840\n",
      "Batch loss: 0.5874472260475159 batch: 355/840\n",
      "Batch loss: 0.4488002061843872 batch: 356/840\n",
      "Batch loss: 0.6035298109054565 batch: 357/840\n",
      "Batch loss: 0.6421039700508118 batch: 358/840\n",
      "Batch loss: 0.5922119617462158 batch: 359/840\n",
      "Batch loss: 0.7852170467376709 batch: 360/840\n",
      "Batch loss: 0.7477223873138428 batch: 361/840\n",
      "Batch loss: 0.5271193981170654 batch: 362/840\n",
      "Batch loss: 0.6237422227859497 batch: 363/840\n",
      "Batch loss: 0.6315290331840515 batch: 364/840\n",
      "Batch loss: 0.48774388432502747 batch: 365/840\n",
      "Batch loss: 0.6339418292045593 batch: 366/840\n",
      "Batch loss: 0.42181068658828735 batch: 367/840\n",
      "Batch loss: 0.7011731863021851 batch: 368/840\n",
      "Batch loss: 0.5465574264526367 batch: 369/840\n",
      "Batch loss: 0.7798868417739868 batch: 370/840\n",
      "Batch loss: 0.6123000383377075 batch: 371/840\n",
      "Batch loss: 0.5477502942085266 batch: 372/840\n",
      "Batch loss: 0.5932220220565796 batch: 373/840\n",
      "Batch loss: 0.7840862274169922 batch: 374/840\n",
      "Batch loss: 0.5103150010108948 batch: 375/840\n",
      "Batch loss: 0.5269532203674316 batch: 376/840\n",
      "Batch loss: 0.6591143012046814 batch: 377/840\n",
      "Batch loss: 0.5308864712715149 batch: 378/840\n",
      "Batch loss: 0.5357276201248169 batch: 379/840\n",
      "Batch loss: 0.8561905026435852 batch: 380/840\n",
      "Batch loss: 0.7771059274673462 batch: 381/840\n",
      "Batch loss: 0.6093423962593079 batch: 382/840\n",
      "Batch loss: 0.6564793586730957 batch: 383/840\n",
      "Batch loss: 0.6064603924751282 batch: 384/840\n",
      "Batch loss: 0.6998807787895203 batch: 385/840\n",
      "Batch loss: 0.7041065096855164 batch: 386/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5868216156959534 batch: 387/840\n",
      "Batch loss: 0.7090156674385071 batch: 388/840\n",
      "Batch loss: 0.5429076552391052 batch: 389/840\n",
      "Batch loss: 0.8035603165626526 batch: 390/840\n",
      "Batch loss: 0.5088576674461365 batch: 391/840\n",
      "Batch loss: 0.6249111294746399 batch: 392/840\n",
      "Batch loss: 0.44435226917266846 batch: 393/840\n",
      "Batch loss: 0.7629169225692749 batch: 394/840\n",
      "Batch loss: 0.5455337762832642 batch: 395/840\n",
      "Batch loss: 0.6442421078681946 batch: 396/840\n",
      "Batch loss: 0.599124550819397 batch: 397/840\n",
      "Batch loss: 0.7209210395812988 batch: 398/840\n",
      "Batch loss: 0.501073956489563 batch: 399/840\n",
      "Batch loss: 0.590226948261261 batch: 400/840\n",
      "Batch loss: 0.7158079743385315 batch: 401/840\n",
      "Batch loss: 0.4878758192062378 batch: 402/840\n",
      "Batch loss: 0.6109613180160522 batch: 403/840\n",
      "Batch loss: 0.5311827063560486 batch: 404/840\n",
      "Batch loss: 0.5416290760040283 batch: 405/840\n",
      "Batch loss: 0.6825850605964661 batch: 406/840\n",
      "Batch loss: 0.541194498538971 batch: 407/840\n",
      "Batch loss: 0.6545137763023376 batch: 408/840\n",
      "Batch loss: 0.6991758942604065 batch: 409/840\n",
      "Batch loss: 0.6822594404220581 batch: 410/840\n",
      "Batch loss: 0.7268694043159485 batch: 411/840\n",
      "Batch loss: 0.736175537109375 batch: 412/840\n",
      "Batch loss: 0.6417230367660522 batch: 413/840\n",
      "Batch loss: 0.6251525282859802 batch: 414/840\n",
      "Batch loss: 0.9051380157470703 batch: 415/840\n",
      "Batch loss: 0.48210322856903076 batch: 416/840\n",
      "Batch loss: 0.5462442636489868 batch: 417/840\n",
      "Batch loss: 0.7458499670028687 batch: 418/840\n",
      "Batch loss: 0.5724313855171204 batch: 419/840\n",
      "Batch loss: 0.6577833294868469 batch: 420/840\n",
      "Batch loss: 0.4222729504108429 batch: 421/840\n",
      "Batch loss: 0.5383245944976807 batch: 422/840\n",
      "Batch loss: 0.6770138740539551 batch: 423/840\n",
      "Batch loss: 0.5836667418479919 batch: 424/840\n",
      "Batch loss: 0.6835038661956787 batch: 425/840\n",
      "Batch loss: 0.5870009064674377 batch: 426/840\n",
      "Batch loss: 0.6013579368591309 batch: 427/840\n",
      "Batch loss: 0.7368980646133423 batch: 428/840\n",
      "Batch loss: 0.5290585160255432 batch: 429/840\n",
      "Batch loss: 0.761981725692749 batch: 430/840\n",
      "Batch loss: 0.6069988012313843 batch: 431/840\n",
      "Batch loss: 0.5852348804473877 batch: 432/840\n",
      "Batch loss: 0.4155879616737366 batch: 433/840\n",
      "Batch loss: 0.5336276292800903 batch: 434/840\n",
      "Batch loss: 0.6642064452171326 batch: 435/840\n",
      "Batch loss: 0.5918504595756531 batch: 436/840\n",
      "Batch loss: 0.6727287173271179 batch: 437/840\n",
      "Batch loss: 0.3847738206386566 batch: 438/840\n",
      "Batch loss: 0.6346426010131836 batch: 439/840\n",
      "Batch loss: 0.6175182461738586 batch: 440/840\n",
      "Batch loss: 0.6510849595069885 batch: 441/840\n",
      "Batch loss: 0.5762301087379456 batch: 442/840\n",
      "Batch loss: 0.5770943760871887 batch: 443/840\n",
      "Batch loss: 0.6079592704772949 batch: 444/840\n",
      "Batch loss: 0.5525555610656738 batch: 445/840\n",
      "Batch loss: 0.6798714399337769 batch: 446/840\n",
      "Batch loss: 0.6728543639183044 batch: 447/840\n",
      "Batch loss: 0.684811532497406 batch: 448/840\n",
      "Batch loss: 0.4680596590042114 batch: 449/840\n",
      "Batch loss: 0.6048129200935364 batch: 450/840\n",
      "Batch loss: 0.5680590271949768 batch: 451/840\n",
      "Batch loss: 0.7046110033988953 batch: 452/840\n",
      "Batch loss: 0.6388681530952454 batch: 453/840\n",
      "Batch loss: 0.5592718720436096 batch: 454/840\n",
      "Batch loss: 0.5670613646507263 batch: 455/840\n",
      "Batch loss: 0.5061460137367249 batch: 456/840\n",
      "Batch loss: 0.5325531363487244 batch: 457/840\n",
      "Batch loss: 0.6074894666671753 batch: 458/840\n",
      "Batch loss: 0.5249370336532593 batch: 459/840\n",
      "Batch loss: 0.3869294822216034 batch: 460/840\n",
      "Batch loss: 0.7836107015609741 batch: 461/840\n",
      "Batch loss: 0.6272377967834473 batch: 462/840\n",
      "Batch loss: 0.7419019937515259 batch: 463/840\n",
      "Batch loss: 0.41248103976249695 batch: 464/840\n",
      "Batch loss: 0.8706890940666199 batch: 465/840\n",
      "Batch loss: 0.580897867679596 batch: 466/840\n",
      "Batch loss: 0.9329431653022766 batch: 467/840\n",
      "Batch loss: 0.5399682521820068 batch: 468/840\n",
      "Batch loss: 0.5695666074752808 batch: 469/840\n",
      "Batch loss: 0.5953652858734131 batch: 470/840\n",
      "Batch loss: 0.6248725056648254 batch: 471/840\n",
      "Batch loss: 0.8161422610282898 batch: 472/840\n",
      "Batch loss: 0.7097030878067017 batch: 473/840\n",
      "Batch loss: 0.5251952409744263 batch: 474/840\n",
      "Batch loss: 0.665160596370697 batch: 475/840\n",
      "Batch loss: 0.6198986172676086 batch: 476/840\n",
      "Batch loss: 0.42710068821907043 batch: 477/840\n",
      "Batch loss: 0.4687286615371704 batch: 478/840\n",
      "Batch loss: 0.49688920378685 batch: 479/840\n",
      "Batch loss: 0.6106258034706116 batch: 480/840\n",
      "Batch loss: 0.5854296684265137 batch: 481/840\n",
      "Batch loss: 0.6136499643325806 batch: 482/840\n",
      "Batch loss: 0.8226121664047241 batch: 483/840\n",
      "Batch loss: 0.514097273349762 batch: 484/840\n",
      "Batch loss: 0.49319398403167725 batch: 485/840\n",
      "Batch loss: 0.5686481595039368 batch: 486/840\n",
      "Batch loss: 0.4740261733531952 batch: 487/840\n",
      "Batch loss: 0.6147465109825134 batch: 488/840\n",
      "Batch loss: 0.5546398162841797 batch: 489/840\n",
      "Batch loss: 0.5924856662750244 batch: 490/840\n",
      "Batch loss: 0.37632256746292114 batch: 491/840\n",
      "Batch loss: 0.6550464630126953 batch: 492/840\n",
      "Batch loss: 0.6121532917022705 batch: 493/840\n",
      "Batch loss: 0.43152302503585815 batch: 494/840\n",
      "Batch loss: 0.8320987820625305 batch: 495/840\n",
      "Batch loss: 0.6156984567642212 batch: 496/840\n",
      "Batch loss: 0.6419810652732849 batch: 497/840\n",
      "Batch loss: 0.4369092583656311 batch: 498/840\n",
      "Batch loss: 0.6874517798423767 batch: 499/840\n",
      "Batch loss: 0.5703195929527283 batch: 500/840\n",
      "Batch loss: 0.5399722456932068 batch: 501/840\n",
      "Batch loss: 0.6542296409606934 batch: 502/840\n",
      "Batch loss: 0.4900375306606293 batch: 503/840\n",
      "Batch loss: 0.6445146799087524 batch: 504/840\n",
      "Batch loss: 0.6783971190452576 batch: 505/840\n",
      "Batch loss: 0.5311009883880615 batch: 506/840\n",
      "Batch loss: 0.6875464916229248 batch: 507/840\n",
      "Batch loss: 0.517839789390564 batch: 508/840\n",
      "Batch loss: 0.7074503302574158 batch: 509/840\n",
      "Batch loss: 0.5864798426628113 batch: 510/840\n",
      "Batch loss: 0.4723464846611023 batch: 511/840\n",
      "Batch loss: 0.6087992191314697 batch: 512/840\n",
      "Batch loss: 0.602628767490387 batch: 513/840\n",
      "Batch loss: 0.710349440574646 batch: 514/840\n",
      "Batch loss: 0.4516608715057373 batch: 515/840\n",
      "Batch loss: 0.7020012736320496 batch: 516/840\n",
      "Batch loss: 0.6140272617340088 batch: 517/840\n",
      "Batch loss: 0.6744216680526733 batch: 518/840\n",
      "Batch loss: 0.7878504991531372 batch: 519/840\n",
      "Batch loss: 0.6444138288497925 batch: 520/840\n",
      "Batch loss: 0.6895850300788879 batch: 521/840\n",
      "Batch loss: 0.591420590877533 batch: 522/840\n",
      "Batch loss: 0.4875677227973938 batch: 523/840\n",
      "Batch loss: 0.7214210033416748 batch: 524/840\n",
      "Batch loss: 0.5564210414886475 batch: 525/840\n",
      "Batch loss: 0.6900248527526855 batch: 526/840\n",
      "Batch loss: 0.680833637714386 batch: 527/840\n",
      "Batch loss: 0.6256269216537476 batch: 528/840\n",
      "Batch loss: 0.5154610872268677 batch: 529/840\n",
      "Batch loss: 0.5558111667633057 batch: 530/840\n",
      "Batch loss: 0.5310511589050293 batch: 531/840\n",
      "Batch loss: 0.4486702084541321 batch: 532/840\n",
      "Batch loss: 0.6728498935699463 batch: 533/840\n",
      "Batch loss: 0.6305618286132812 batch: 534/840\n",
      "Batch loss: 0.5931811332702637 batch: 535/840\n",
      "Batch loss: 0.6287310719490051 batch: 536/840\n",
      "Batch loss: 0.643649697303772 batch: 537/840\n",
      "Batch loss: 0.5053192973136902 batch: 538/840\n",
      "Batch loss: 0.609330415725708 batch: 539/840\n",
      "Batch loss: 0.7145398855209351 batch: 540/840\n",
      "Batch loss: 0.5520023107528687 batch: 541/840\n",
      "Batch loss: 0.6430734395980835 batch: 542/840\n",
      "Batch loss: 0.39857304096221924 batch: 543/840\n",
      "Batch loss: 0.6254681944847107 batch: 544/840\n",
      "Batch loss: 0.673715353012085 batch: 545/840\n",
      "Batch loss: 0.5421239137649536 batch: 546/840\n",
      "Batch loss: 0.5091658234596252 batch: 547/840\n",
      "Batch loss: 0.471024751663208 batch: 548/840\n",
      "Batch loss: 0.5010433793067932 batch: 549/840\n",
      "Batch loss: 0.5018201470375061 batch: 550/840\n",
      "Batch loss: 0.696021556854248 batch: 551/840\n",
      "Batch loss: 0.4263855814933777 batch: 552/840\n",
      "Batch loss: 0.6857188940048218 batch: 553/840\n",
      "Batch loss: 0.6829026937484741 batch: 554/840\n",
      "Batch loss: 0.6079176068305969 batch: 555/840\n",
      "Batch loss: 0.6269418001174927 batch: 556/840\n",
      "Batch loss: 0.5819898843765259 batch: 557/840\n",
      "Batch loss: 0.6505609154701233 batch: 558/840\n",
      "Batch loss: 0.5107420086860657 batch: 559/840\n",
      "Batch loss: 0.6197854280471802 batch: 560/840\n",
      "Batch loss: 0.6162340641021729 batch: 561/840\n",
      "Batch loss: 0.49941059947013855 batch: 562/840\n",
      "Batch loss: 0.4406820237636566 batch: 563/840\n",
      "Batch loss: 0.5787259936332703 batch: 564/840\n",
      "Batch loss: 0.7510669231414795 batch: 565/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.6115856766700745 batch: 566/840\n",
      "Batch loss: 0.8163964152336121 batch: 567/840\n",
      "Batch loss: 0.5675003528594971 batch: 568/840\n",
      "Batch loss: 0.6457807421684265 batch: 569/840\n",
      "Batch loss: 0.5076826810836792 batch: 570/840\n",
      "Batch loss: 0.769838809967041 batch: 571/840\n",
      "Batch loss: 0.6338231563568115 batch: 572/840\n",
      "Batch loss: 0.6582450270652771 batch: 573/840\n",
      "Batch loss: 0.7656224966049194 batch: 574/840\n",
      "Batch loss: 0.5773225426673889 batch: 575/840\n",
      "Batch loss: 0.570605456829071 batch: 576/840\n",
      "Batch loss: 0.4971667230129242 batch: 577/840\n",
      "Batch loss: 0.681968629360199 batch: 578/840\n",
      "Batch loss: 0.583021342754364 batch: 579/840\n",
      "Batch loss: 0.8553261756896973 batch: 580/840\n",
      "Batch loss: 0.6238713264465332 batch: 581/840\n",
      "Batch loss: 0.7455951571464539 batch: 582/840\n",
      "Batch loss: 0.5325659513473511 batch: 583/840\n",
      "Batch loss: 0.7433427572250366 batch: 584/840\n",
      "Batch loss: 0.5405387282371521 batch: 585/840\n",
      "Batch loss: 0.596453070640564 batch: 586/840\n",
      "Batch loss: 0.6466764807701111 batch: 587/840\n",
      "Batch loss: 0.6038901805877686 batch: 588/840\n",
      "Batch loss: 0.6473907232284546 batch: 589/840\n",
      "Batch loss: 0.6382755041122437 batch: 590/840\n",
      "Batch loss: 0.35428521037101746 batch: 591/840\n",
      "Batch loss: 0.5107002258300781 batch: 592/840\n",
      "Batch loss: 0.7576454877853394 batch: 593/840\n",
      "Batch loss: 0.5961301922798157 batch: 594/840\n",
      "Batch loss: 0.3269703984260559 batch: 595/840\n",
      "Batch loss: 0.39877110719680786 batch: 596/840\n",
      "Batch loss: 0.573017418384552 batch: 597/840\n",
      "Batch loss: 0.44916820526123047 batch: 598/840\n",
      "Batch loss: 0.6934231519699097 batch: 599/840\n",
      "Batch loss: 0.6001988053321838 batch: 600/840\n",
      "Batch loss: 0.6716780066490173 batch: 601/840\n",
      "Batch loss: 0.47190016508102417 batch: 602/840\n",
      "Batch loss: 0.6075832843780518 batch: 603/840\n",
      "Batch loss: 0.6602815985679626 batch: 604/840\n",
      "Batch loss: 0.8008551597595215 batch: 605/840\n",
      "Batch loss: 0.8510645031929016 batch: 606/840\n",
      "Batch loss: 0.6786004900932312 batch: 607/840\n",
      "Batch loss: 0.5650021433830261 batch: 608/840\n",
      "Batch loss: 0.5849172472953796 batch: 609/840\n",
      "Batch loss: 0.6742687225341797 batch: 610/840\n",
      "Batch loss: 0.6694832444190979 batch: 611/840\n",
      "Batch loss: 0.710560142993927 batch: 612/840\n",
      "Batch loss: 0.691088080406189 batch: 613/840\n",
      "Batch loss: 0.5850109457969666 batch: 614/840\n",
      "Batch loss: 0.5308511257171631 batch: 615/840\n",
      "Batch loss: 0.47779038548469543 batch: 616/840\n",
      "Batch loss: 0.5442243218421936 batch: 617/840\n",
      "Batch loss: 0.6014792919158936 batch: 618/840\n",
      "Batch loss: 0.7267778515815735 batch: 619/840\n",
      "Batch loss: 0.5247629880905151 batch: 620/840\n",
      "Batch loss: 0.6035275459289551 batch: 621/840\n",
      "Batch loss: 0.6017756462097168 batch: 622/840\n",
      "Batch loss: 0.585462212562561 batch: 623/840\n",
      "Batch loss: 0.768311083316803 batch: 624/840\n",
      "Batch loss: 0.6503797769546509 batch: 625/840\n",
      "Batch loss: 0.6111810207366943 batch: 626/840\n",
      "Batch loss: 0.5570045113563538 batch: 627/840\n",
      "Batch loss: 0.5892650485038757 batch: 628/840\n",
      "Batch loss: 0.5993655920028687 batch: 629/840\n",
      "Batch loss: 0.53597092628479 batch: 630/840\n",
      "Batch loss: 0.6380742788314819 batch: 631/840\n",
      "Batch loss: 0.6569502353668213 batch: 632/840\n",
      "Batch loss: 0.4858400821685791 batch: 633/840\n",
      "Batch loss: 0.6662734746932983 batch: 634/840\n",
      "Batch loss: 0.5582931041717529 batch: 635/840\n",
      "Batch loss: 0.5196669101715088 batch: 636/840\n",
      "Batch loss: 0.5352943539619446 batch: 637/840\n",
      "Batch loss: 0.6525521874427795 batch: 638/840\n",
      "Batch loss: 0.6651278138160706 batch: 639/840\n",
      "Batch loss: 0.5796352028846741 batch: 640/840\n",
      "Batch loss: 0.7594344615936279 batch: 641/840\n",
      "Batch loss: 0.5963388681411743 batch: 642/840\n",
      "Batch loss: 0.8585491180419922 batch: 643/840\n",
      "Batch loss: 0.5682386159896851 batch: 644/840\n",
      "Batch loss: 0.5359650254249573 batch: 645/840\n",
      "Batch loss: 0.5690069794654846 batch: 646/840\n",
      "Batch loss: 0.6332535147666931 batch: 647/840\n",
      "Batch loss: 0.5998218059539795 batch: 648/840\n",
      "Batch loss: 0.601966142654419 batch: 649/840\n",
      "Batch loss: 0.5270782113075256 batch: 650/840\n",
      "Batch loss: 0.5941241979598999 batch: 651/840\n",
      "Batch loss: 0.7504913210868835 batch: 652/840\n",
      "Batch loss: 0.516394853591919 batch: 653/840\n",
      "Batch loss: 0.7891575694084167 batch: 654/840\n",
      "Batch loss: 0.6076741218566895 batch: 655/840\n",
      "Batch loss: 0.6209293603897095 batch: 656/840\n",
      "Batch loss: 0.5564825534820557 batch: 657/840\n",
      "Batch loss: 0.6336123943328857 batch: 658/840\n",
      "Batch loss: 0.6278128027915955 batch: 659/840\n",
      "Batch loss: 0.5989031791687012 batch: 660/840\n",
      "Batch loss: 0.7882199287414551 batch: 661/840\n",
      "Batch loss: 0.6101253032684326 batch: 662/840\n",
      "Batch loss: 0.6548088192939758 batch: 663/840\n",
      "Batch loss: 0.6437728404998779 batch: 664/840\n",
      "Batch loss: 0.6600924730300903 batch: 665/840\n",
      "Batch loss: 0.6254845261573792 batch: 666/840\n",
      "Batch loss: 0.6774287223815918 batch: 667/840\n",
      "Batch loss: 0.5305777192115784 batch: 668/840\n",
      "Batch loss: 0.6030817031860352 batch: 669/840\n",
      "Batch loss: 0.5947596430778503 batch: 670/840\n",
      "Batch loss: 0.6563836932182312 batch: 671/840\n",
      "Batch loss: 0.5778344869613647 batch: 672/840\n",
      "Batch loss: 0.7565372586250305 batch: 673/840\n",
      "Batch loss: 0.6704297661781311 batch: 674/840\n",
      "Batch loss: 0.6451169848442078 batch: 675/840\n",
      "Batch loss: 0.5306954383850098 batch: 676/840\n",
      "Batch loss: 0.6419148445129395 batch: 677/840\n",
      "Batch loss: 0.6799435615539551 batch: 678/840\n",
      "Batch loss: 0.8309096693992615 batch: 679/840\n",
      "Batch loss: 0.6684606671333313 batch: 680/840\n",
      "Batch loss: 0.6689434051513672 batch: 681/840\n",
      "Batch loss: 0.6420881152153015 batch: 682/840\n",
      "Batch loss: 0.553988516330719 batch: 683/840\n",
      "Batch loss: 0.8469083905220032 batch: 684/840\n",
      "Batch loss: 0.612356424331665 batch: 685/840\n",
      "Batch loss: 0.7734052538871765 batch: 686/840\n",
      "Batch loss: 0.690568745136261 batch: 687/840\n",
      "Batch loss: 0.623232364654541 batch: 688/840\n",
      "Batch loss: 0.5718336701393127 batch: 689/840\n",
      "Batch loss: 0.5374205112457275 batch: 690/840\n",
      "Batch loss: 0.6371331810951233 batch: 691/840\n",
      "Batch loss: 0.6357640624046326 batch: 692/840\n",
      "Batch loss: 0.6657404899597168 batch: 693/840\n",
      "Batch loss: 0.6935780048370361 batch: 694/840\n",
      "Batch loss: 0.6202605962753296 batch: 695/840\n",
      "Batch loss: 0.6267274022102356 batch: 696/840\n",
      "Batch loss: 0.5851894617080688 batch: 697/840\n",
      "Batch loss: 0.595486044883728 batch: 698/840\n",
      "Batch loss: 0.7068057060241699 batch: 699/840\n",
      "Batch loss: 0.6749449372291565 batch: 700/840\n",
      "Batch loss: 0.6868634223937988 batch: 701/840\n",
      "Batch loss: 0.6484792828559875 batch: 702/840\n",
      "Batch loss: 0.6994680762290955 batch: 703/840\n",
      "Batch loss: 0.7092059850692749 batch: 704/840\n",
      "Batch loss: 0.613995373249054 batch: 705/840\n",
      "Batch loss: 0.5951858162879944 batch: 706/840\n",
      "Batch loss: 0.6631699204444885 batch: 707/840\n",
      "Batch loss: 0.6219282150268555 batch: 708/840\n",
      "Batch loss: 0.7271666526794434 batch: 709/840\n",
      "Batch loss: 0.8156127333641052 batch: 710/840\n",
      "Batch loss: 0.44199156761169434 batch: 711/840\n",
      "Batch loss: 0.7835512757301331 batch: 712/840\n",
      "Batch loss: 0.5345831513404846 batch: 713/840\n",
      "Batch loss: 0.598852813243866 batch: 714/840\n",
      "Batch loss: 0.816577672958374 batch: 715/840\n",
      "Batch loss: 0.6559327840805054 batch: 716/840\n",
      "Batch loss: 0.762544572353363 batch: 717/840\n",
      "Batch loss: 0.5878655910491943 batch: 718/840\n",
      "Batch loss: 0.5472426414489746 batch: 719/840\n",
      "Batch loss: 0.5954152345657349 batch: 720/840\n",
      "Batch loss: 0.683834433555603 batch: 721/840\n",
      "Batch loss: 0.7764973640441895 batch: 722/840\n",
      "Batch loss: 0.5012838244438171 batch: 723/840\n",
      "Batch loss: 0.8202220797538757 batch: 724/840\n",
      "Batch loss: 0.7103267908096313 batch: 725/840\n",
      "Batch loss: 0.5698848366737366 batch: 726/840\n",
      "Batch loss: 0.6504359245300293 batch: 727/840\n",
      "Batch loss: 0.6917517185211182 batch: 728/840\n",
      "Batch loss: 0.6704090237617493 batch: 729/840\n",
      "Batch loss: 0.6986468434333801 batch: 730/840\n",
      "Batch loss: 0.500704288482666 batch: 731/840\n",
      "Batch loss: 0.7054010033607483 batch: 732/840\n",
      "Batch loss: 0.5713416934013367 batch: 733/840\n",
      "Batch loss: 0.5493916273117065 batch: 734/840\n",
      "Batch loss: 0.6803291440010071 batch: 735/840\n",
      "Batch loss: 0.5960351228713989 batch: 736/840\n",
      "Batch loss: 0.618638277053833 batch: 737/840\n",
      "Batch loss: 0.5143008828163147 batch: 738/840\n",
      "Batch loss: 0.6781225800514221 batch: 739/840\n",
      "Batch loss: 0.7644713521003723 batch: 740/840\n",
      "Batch loss: 0.49746426939964294 batch: 741/840\n",
      "Batch loss: 0.581377387046814 batch: 742/840\n",
      "Batch loss: 0.42611953616142273 batch: 743/840\n",
      "Batch loss: 0.5299518704414368 batch: 744/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss: 0.5446745753288269 batch: 745/840\n",
      "Batch loss: 0.6182487607002258 batch: 746/840\n",
      "Batch loss: 0.7370836734771729 batch: 747/840\n",
      "Batch loss: 0.6698896884918213 batch: 748/840\n",
      "Batch loss: 0.542156457901001 batch: 749/840\n",
      "Batch loss: 0.4097430109977722 batch: 750/840\n",
      "Batch loss: 0.6536552906036377 batch: 751/840\n",
      "Batch loss: 0.8325798511505127 batch: 752/840\n",
      "Batch loss: 0.4543101489543915 batch: 753/840\n",
      "Batch loss: 0.6355700492858887 batch: 754/840\n",
      "Batch loss: 0.5765249729156494 batch: 755/840\n",
      "Batch loss: 0.554629385471344 batch: 756/840\n",
      "Batch loss: 0.7365243434906006 batch: 757/840\n",
      "Batch loss: 0.5000913739204407 batch: 758/840\n",
      "Batch loss: 0.5045077204704285 batch: 759/840\n",
      "Batch loss: 0.569338321685791 batch: 760/840\n",
      "Batch loss: 0.489003449678421 batch: 761/840\n",
      "Batch loss: 0.8078245520591736 batch: 762/840\n",
      "Batch loss: 0.4633783996105194 batch: 763/840\n",
      "Batch loss: 0.5938063263893127 batch: 764/840\n",
      "Batch loss: 0.47184890508651733 batch: 765/840\n",
      "Batch loss: 0.5797761678695679 batch: 766/840\n",
      "Batch loss: 0.6228775382041931 batch: 767/840\n",
      "Batch loss: 0.6489959955215454 batch: 768/840\n",
      "Batch loss: 0.593310534954071 batch: 769/840\n",
      "Batch loss: 0.6225935220718384 batch: 770/840\n",
      "Batch loss: 0.6114181876182556 batch: 771/840\n",
      "Batch loss: 0.4666973054409027 batch: 772/840\n",
      "Batch loss: 0.5232582092285156 batch: 773/840\n",
      "Batch loss: 0.4356083273887634 batch: 774/840\n",
      "Batch loss: 0.49974390864372253 batch: 775/840\n",
      "Batch loss: 0.4995076358318329 batch: 776/840\n",
      "Batch loss: 0.4266093969345093 batch: 777/840\n",
      "Batch loss: 0.4290367066860199 batch: 778/840\n",
      "Batch loss: 0.7406404614448547 batch: 779/840\n",
      "Batch loss: 0.5718177556991577 batch: 780/840\n",
      "Batch loss: 0.6707674264907837 batch: 781/840\n",
      "Batch loss: 0.5640389919281006 batch: 782/840\n",
      "Batch loss: 0.45383113622665405 batch: 783/840\n",
      "Batch loss: 0.7043517827987671 batch: 784/840\n",
      "Batch loss: 0.5343902111053467 batch: 785/840\n",
      "Batch loss: 0.7011523246765137 batch: 786/840\n",
      "Batch loss: 0.509574830532074 batch: 787/840\n",
      "Batch loss: 0.6524469256401062 batch: 788/840\n",
      "Batch loss: 0.7047359347343445 batch: 789/840\n",
      "Batch loss: 0.6330021619796753 batch: 790/840\n",
      "Batch loss: 0.6097828149795532 batch: 791/840\n",
      "Batch loss: 0.37941333651542664 batch: 792/840\n",
      "Batch loss: 0.5718061923980713 batch: 793/840\n",
      "Batch loss: 0.6272184252738953 batch: 794/840\n",
      "Batch loss: 0.6657535433769226 batch: 795/840\n",
      "Batch loss: 0.6577233672142029 batch: 796/840\n",
      "Batch loss: 0.6358896493911743 batch: 797/840\n",
      "Batch loss: 0.732444167137146 batch: 798/840\n",
      "Batch loss: 0.5902425050735474 batch: 799/840\n",
      "Batch loss: 0.5233396291732788 batch: 800/840\n",
      "Batch loss: 0.7103937268257141 batch: 801/840\n",
      "Batch loss: 0.4915095269680023 batch: 802/840\n",
      "Batch loss: 0.5313786864280701 batch: 803/840\n",
      "Batch loss: 0.7207344174385071 batch: 804/840\n",
      "Batch loss: 0.5200048089027405 batch: 805/840\n",
      "Batch loss: 0.7755113244056702 batch: 806/840\n",
      "Batch loss: 0.7053383588790894 batch: 807/840\n",
      "Batch loss: 0.6676795482635498 batch: 808/840\n",
      "Batch loss: 0.6103734374046326 batch: 809/840\n",
      "Batch loss: 0.5744026303291321 batch: 810/840\n",
      "Batch loss: 0.5325297117233276 batch: 811/840\n",
      "Batch loss: 0.5840043425559998 batch: 812/840\n",
      "Batch loss: 0.6014295816421509 batch: 813/840\n",
      "Batch loss: 0.6247525215148926 batch: 814/840\n",
      "Batch loss: 0.6849014163017273 batch: 815/840\n",
      "Batch loss: 0.6573808789253235 batch: 816/840\n",
      "Batch loss: 0.662889838218689 batch: 817/840\n",
      "Batch loss: 0.7417044639587402 batch: 818/840\n",
      "Batch loss: 0.5389643311500549 batch: 819/840\n",
      "Batch loss: 0.6523484587669373 batch: 820/840\n",
      "Batch loss: 0.6709242463111877 batch: 821/840\n",
      "Batch loss: 0.5825122594833374 batch: 822/840\n",
      "Batch loss: 0.6984928846359253 batch: 823/840\n",
      "Batch loss: 0.7298630475997925 batch: 824/840\n",
      "Batch loss: 0.6629869937896729 batch: 825/840\n",
      "Batch loss: 0.6550341844558716 batch: 826/840\n",
      "Batch loss: 0.5255725383758545 batch: 827/840\n",
      "Batch loss: 0.6470170617103577 batch: 828/840\n",
      "Batch loss: 0.57442706823349 batch: 829/840\n",
      "Batch loss: 0.7178085446357727 batch: 830/840\n",
      "Batch loss: 0.5599928498268127 batch: 831/840\n",
      "Batch loss: 0.7927252054214478 batch: 832/840\n",
      "Batch loss: 0.6887117028236389 batch: 833/840\n",
      "Batch loss: 0.57737135887146 batch: 834/840\n",
      "Batch loss: 0.511448085308075 batch: 835/840\n",
      "Batch loss: 0.4500316381454468 batch: 836/840\n",
      "Batch loss: 0.6513654589653015 batch: 837/840\n",
      "Batch loss: 0.7558839321136475 batch: 838/840\n",
      "Batch loss: 0.600027859210968 batch: 839/840\n",
      "Batch loss: 0.7432034015655518 batch: 840/840\n",
      "Running evaluation loop...\n",
      "batch: 1/5\n",
      "batch: 2/5\n",
      "batch: 3/5\n",
      "batch: 4/5\n",
      "batch: 5/5\n",
      "Epoch: 15/15..  Training Loss: 0.006..  Test Loss: 0.005..  Test Accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.Adam(torchModel.parameters(), lr=0.003)\n",
    "#optimizer = optim.Adam(torchModel.parameters(), lr=0.0015)\n",
    "optimizer = optim.Adam(torchModel.TorchModel.parameters(), lr=0.0015)\n",
    "\n",
    "epochs = 15\n",
    "steps = 0\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "torchModel.to(device)\n",
    "#optimizer.to(device)\n",
    "criterion.to(device)\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f'Running epoch {e+1}/{epochs}')\n",
    "    for images, labels in mMiniBatcherTrain.getBatchIterator():\n",
    "        #print('Training batch...')\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        #print(f'labels.shape: {labels.shape}')\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        \n",
    "        #print('Running torch')\n",
    "        output = torchModel(images)\n",
    "        \n",
    "        #print(f'output.shape: {output.shape}')\n",
    "        #print('Calculating loss')\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        #print('Back prop.')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss = loss.item()\n",
    "        print(f'Batch loss: {batch_loss} {mMiniBatcherTrain.getBatchInfo()}')\n",
    "        running_loss += batch_loss\n",
    "        \n",
    "        del labels\n",
    "        del images\n",
    "        del output\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "   \n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    print('Running evaluation loop...')\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        torchModel.eval()\n",
    "        for images, labels in mMiniBatcherTest.getBatchIterator():\n",
    "            #print('Validation batch...')\n",
    "            #labels = torch.from_numpy(labels.values).type(torch.FloatTensor)\n",
    "            labels = torch.from_numpy(labels).to(device)\n",
    "            output = torchModel(images)\n",
    "            test_loss += criterion(output, labels).to('cpu') #Want the loss on CPU\n",
    "\n",
    "            top_p, top_class = output.topk(1, dim=1)\n",
    "            #print(top_p)\n",
    "            #top_p_target, top_class_target = labels.topk(1, dim=1)\n",
    "            #equals = top_class == top_class_target\n",
    "            equals = top_class == labels.view(top_class.shape)\n",
    "            accuracy += torch.sum(equals.type(torch.FloatTensor)).to('cpu')\n",
    "            print(mMiniBatcherTest.getBatchInfo())\n",
    "            \n",
    "            del labels\n",
    "            del images\n",
    "            del output\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() \n",
    "\n",
    "    torchModel.train()\n",
    "\n",
    "    train_losses.append(running_loss/len(mMiniBatcherTrain.X))\n",
    "    test_losses.append(test_loss/len(mMiniBatcherTest.X))\n",
    "\n",
    "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "          \"Training Loss: {:.5f}.. \".format(train_losses[-1]),\n",
    "          \"Test Loss: {:.5f}.. \".format(test_losses[-1]),\n",
    "          \"Test Accuracy: {:.5f}\".format(accuracy/len(mMiniBatcherTest.X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "specialized-transition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchNLP(\n",
       "  (Bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50325, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (TorchModel): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchModel.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "divided-reason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchModel.Bert.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "subject-coral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Called\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4626, -1.5387, -1.6581,  5.5220, -1.1519, -1.9339, -0.4945],\n",
       "        [-2.4869, -2.2417, -0.1596,  5.9095, -0.8429, -2.5162, -0.1103],\n",
       "        [-2.2634, -1.6346,  0.2437,  5.0969, -1.4610, -0.9196,  0.4518],\n",
       "        [-1.4876, -2.0991, -1.3574,  5.9189, -0.3710, -3.3775, -0.0616],\n",
       "        [-1.9867, -2.1058, -0.8074,  5.6330, -0.1044, -3.1820, -0.1562],\n",
       "        [-2.2982, -2.8106, -0.7049,  5.6376, -0.9757, -1.1950, -0.0750],\n",
       "        [-1.3132, -0.8251, -0.8038,  4.2634, -0.0722, -2.5963,  0.0976],\n",
       "        [-1.7288, -1.0971, -0.2127,  5.4893, -1.3920, -1.5332,  0.2278],\n",
       "        [-2.0425, -1.3268, -1.0715,  5.6121, -0.3769, -1.5521,  0.0577],\n",
       "        [-1.2958, -1.1475, -0.4645,  3.3163, -0.1179, -1.1458, -0.0809],\n",
       "        [-2.9055, -2.2610, -0.9447,  6.2404, -0.8267, -2.1755,  0.2848],\n",
       "        [-2.0705, -1.8187, -1.2083,  5.5571, -0.0396, -2.7881,  0.2048],\n",
       "        [-1.7872, -2.5268, -1.0414,  4.4269, -0.7507, -0.7875,  0.1495],\n",
       "        [-2.2243, -2.2796, -0.6230,  5.6976, -1.0154, -1.8560,  0.7660],\n",
       "        [-2.0808, -2.6473, -0.5863,  5.9454, -0.7398, -2.9108, -0.1682],\n",
       "        [-1.1619, -0.5904, -0.0986,  3.2129, -1.1888, -0.2766,  0.5494],\n",
       "        [-2.0235, -1.0664, -0.0868,  5.7351, -0.2674, -1.5629,  0.4473],\n",
       "        [-2.3887, -2.2231,  0.0325,  5.7711, -0.7572, -2.3600, -0.0986],\n",
       "        [-1.7973, -2.4545, -1.1000,  5.7570, -0.9475, -1.9836, -0.5926],\n",
       "        [-1.5662, -1.6547, -1.4485,  3.7843, -1.4425, -1.8323,  0.2154],\n",
       "        [-0.4742, -1.3688, -0.9707,  4.1654,  0.0107, -1.2688, -0.6327],\n",
       "        [-3.1214, -3.0270, -0.7252,  6.3862, -1.0545, -1.4159,  0.7737],\n",
       "        [-2.4808, -2.2597, -0.4638,  6.2402, -0.3606, -2.1318,  0.1496],\n",
       "        [-2.0498, -2.1140, -1.6466,  5.2267, -2.0208, -1.0849, -0.2112],\n",
       "        [-2.6305, -2.2619, -0.7004,  6.7615, -0.9292, -2.4601, -0.0890]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchModel(X_test[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "maritime-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "targets = []\n",
    "for X_batch, labels in mMiniBatcherTest.getBatchIterator():\n",
    "  \n",
    "        output = torchModel.predict(X_batch)\n",
    "        Y_pred.extend(output)\n",
    "        targets.extend(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "outer-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(Y_pred)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "upset-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 0, 4, 5, 4, 5, 4, 3, 0, 5, 4, 0, 4, 2, 6, 4, 4, 4, 0, 2, 6,\n",
       "       2, 6, 4, 6, 1, 2, 5, 3, 2, 6, 3, 5, 3, 4, 4, 0, 2, 3, 0, 6, 1, 3,\n",
       "       3, 5, 4, 5, 5, 5, 6, 2, 0, 3, 2, 6, 6, 1, 1, 0, 3, 6, 1, 2, 4, 2,\n",
       "       1, 1, 4, 4, 0, 0, 1, 2, 2, 6, 1, 0, 5, 1, 3, 3, 6, 6, 0, 2, 0, 1,\n",
       "       6, 5, 5, 0, 2, 3, 1, 1, 4, 5, 2, 4, 3, 5, 2, 6, 6, 3, 6, 1, 3, 5,\n",
       "       1, 5, 6, 1, 0, 4, 5, 5, 1, 1, 5, 3, 3, 5, 3, 5, 3, 3, 5, 4, 0, 0,\n",
       "       3, 2, 0, 4, 1, 2, 4, 1, 3, 3, 2, 2, 3, 0, 1, 4, 0, 4, 2, 2, 4, 4,\n",
       "       3, 5, 0, 2, 2, 5, 1, 4, 0, 5, 4, 3, 4, 4, 4, 5, 0, 1, 1, 2, 1, 0,\n",
       "       4, 0, 4, 5, 3, 2, 5, 3, 2, 2, 6, 2, 6, 1, 0, 1, 1, 4, 5, 5, 5, 0,\n",
       "       5, 6, 5, 2, 3, 3, 4, 6, 1, 6, 6, 1, 1, 4, 3, 2, 4, 5, 4, 1, 4, 0,\n",
       "       4, 4, 5, 6, 4, 2, 1, 2, 4, 1, 5, 5, 1, 6, 1, 4, 1, 5, 4, 5, 1, 2,\n",
       "       3, 3, 1, 2, 1, 6, 4, 1, 6, 4, 2, 2, 4, 3, 4, 6, 4, 0, 0, 5, 3, 5,\n",
       "       4, 4, 3, 3, 4, 3, 1, 4, 6, 1, 6, 3, 5, 3, 4, 0, 5, 2, 5, 4, 4, 6,\n",
       "       3, 1, 1, 3, 1, 4, 5, 2, 3, 5, 4, 4, 0, 4, 2, 0, 3, 4, 1, 3, 5, 1,\n",
       "       3, 1, 4, 3, 1, 1, 3, 0, 1, 0, 0, 2, 0, 5, 1, 5, 1, 3, 6, 0, 3, 5,\n",
       "       4, 2, 1, 1, 5, 0, 2, 5, 0, 4, 2, 1, 2, 4, 6, 5, 2, 0, 1, 5, 1, 2,\n",
       "       2, 4, 6, 3, 1, 4, 6, 1, 5, 2, 1, 4, 4, 3, 6, 1, 1, 6, 4, 5, 1, 3,\n",
       "       3, 5, 5, 1, 2, 4, 4, 3, 3, 1, 1, 2, 1, 1, 6, 5, 4, 0, 1, 1, 5, 0,\n",
       "       6, 5, 6, 5, 0, 3, 2, 1, 1, 5, 1, 0, 4, 2, 6, 6, 5, 5, 0, 3, 4, 0,\n",
       "       6, 6, 0, 2, 6, 6, 5, 0, 0, 3, 0, 6, 3, 5, 2, 3, 0, 6, 6, 4, 0, 2,\n",
       "       0, 3, 4, 2, 5, 1, 3, 2, 2, 4, 5, 2, 0, 4, 6, 4, 1, 5, 1, 1, 1, 2,\n",
       "       4, 5, 4, 5, 0, 5, 5, 1, 4, 4, 5, 6, 1, 3, 0, 4, 3, 1, 4, 4, 0, 1,\n",
       "       5, 2, 4, 2, 1, 4, 4, 0, 6, 0, 1, 3, 4, 3, 6, 5], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y_pred = torchModel.predict(X_test[200:300])\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "neutral-mexico",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 0, 4, 5, 4, 6, 4, 3, 0, 5, 4, 0, 4, 2, 0, 4, 4, 0, 0, 2, 6,\n",
       "       6, 2, 6, 6, 1, 2, 5, 3, 2, 2, 3, 0, 3, 4, 2, 6, 2, 3, 0, 2, 1, 3,\n",
       "       3, 5, 6, 5, 5, 5, 4, 2, 0, 3, 2, 1, 6, 1, 1, 6, 3, 6, 1, 6, 4, 2,\n",
       "       1, 1, 4, 5, 0, 0, 1, 2, 4, 6, 1, 0, 5, 1, 3, 3, 0, 6, 2, 2, 0, 1,\n",
       "       6, 4, 0, 0, 2, 3, 1, 1, 6, 4, 2, 4, 3, 5, 2, 6, 6, 3, 6, 1, 3, 5,\n",
       "       4, 5, 4, 1, 4, 4, 5, 5, 1, 1, 0, 3, 3, 5, 3, 6, 3, 3, 5, 5, 0, 0,\n",
       "       3, 2, 0, 4, 1, 2, 4, 1, 3, 3, 6, 2, 3, 4, 1, 0, 0, 4, 2, 6, 4, 0,\n",
       "       3, 5, 0, 0, 2, 5, 1, 2, 0, 5, 4, 3, 4, 2, 4, 5, 6, 1, 1, 2, 4, 2,\n",
       "       0, 0, 4, 5, 3, 2, 4, 3, 2, 2, 6, 2, 0, 1, 0, 1, 1, 4, 5, 5, 5, 0,\n",
       "       4, 4, 5, 2, 3, 3, 4, 6, 1, 6, 6, 1, 1, 4, 3, 2, 4, 5, 4, 1, 4, 0,\n",
       "       4, 6, 0, 6, 1, 2, 1, 2, 2, 1, 4, 5, 1, 0, 1, 4, 1, 5, 1, 5, 1, 4,\n",
       "       3, 3, 1, 2, 1, 4, 1, 1, 2, 4, 2, 1, 4, 3, 6, 6, 4, 6, 0, 5, 3, 5,\n",
       "       4, 3, 3, 3, 4, 3, 1, 4, 6, 1, 4, 3, 5, 3, 4, 0, 5, 2, 5, 4, 4, 2,\n",
       "       3, 1, 1, 3, 1, 6, 5, 2, 3, 5, 4, 4, 0, 4, 2, 0, 3, 4, 1, 3, 5, 1,\n",
       "       3, 1, 4, 3, 1, 1, 3, 0, 1, 0, 0, 2, 0, 5, 1, 5, 1, 3, 6, 0, 3, 5,\n",
       "       0, 2, 1, 1, 5, 6, 2, 5, 0, 4, 6, 1, 2, 1, 6, 5, 2, 0, 1, 5, 1, 4,\n",
       "       2, 4, 6, 3, 1, 4, 0, 1, 5, 2, 1, 4, 4, 3, 6, 1, 1, 6, 4, 5, 1, 3,\n",
       "       3, 0, 4, 1, 2, 4, 3, 3, 3, 1, 1, 2, 1, 1, 6, 5, 4, 0, 1, 1, 5, 0,\n",
       "       0, 5, 6, 5, 0, 3, 2, 1, 1, 5, 1, 0, 4, 2, 2, 4, 5, 5, 0, 3, 5, 0,\n",
       "       6, 4, 0, 2, 0, 2, 5, 0, 0, 3, 4, 6, 3, 5, 2, 3, 0, 6, 6, 4, 0, 2,\n",
       "       6, 3, 4, 2, 4, 1, 3, 2, 2, 4, 5, 1, 2, 4, 6, 6, 1, 2, 1, 1, 1, 2,\n",
       "       4, 0, 4, 5, 0, 5, 5, 1, 4, 4, 5, 6, 1, 3, 0, 4, 3, 1, 4, 4, 0, 1,\n",
       "       5, 2, 4, 2, 1, 4, 2, 6, 6, 0, 1, 3, 4, 3, 6, 5], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#targets = Y_test[200:300]\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "played-dodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "000000. kund ringer in, har råkat dubbelköpa en biljett i appen. xxxx biljett-id: pj0wpwz. gör ett återköp av den ena biljetten. kund får återköpskvitto.\n"
     ]
    }
   ],
   "source": [
    "testId = 21\n",
    "print(Y_test[testId])\n",
    "print(X_test.iloc[testId])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "invisible-instrument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.818\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(Y_pred == targets)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dangerous-municipality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x26b5d095e80>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(targets, Y_pred, normalize='true', labels=list(set(Y_test)))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(Y_test))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "gross-communist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x26bf4440c10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(targets, Y_pred, normalize=None, labels=list(set(Y_test)))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(Y_test))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "social-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torchModel, 'torchModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "distinguished-light",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight \t torch.Size([256, 768])\n",
      "0.bias \t torch.Size([256])\n",
      "3.weight \t torch.Size([128, 256])\n",
      "3.bias \t torch.Size([128])\n",
      "6.weight \t torch.Size([7, 128])\n",
      "6.bias \t torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in (torchModel.TorchModel.state_dict()):\n",
    "    print(param_tensor, \"\\t\", torchModel.TorchModel.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sapphire-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(torchModel.TorchModel.state_dict(),'torchModel_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hollow-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchModel =  torch.load('torchModel.pth', map_location=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "necessary-virgin",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_classes = 7\n",
    "torchModel = TorchNLP(n_classes)\n",
    "torchModel.TorchModel.load_state_dict(torch.load('torchModel_state_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fitted-garden",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 5, 6, 4, 6, 5, 6, 6, 1, 6, 4, 4, 6, 2], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchModel.eval()\n",
    "torchModel.predict(X[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "catholic-upgrade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 5, 4, 2, 6, 5, 4, 6, 4, 2, 6, 6, 6, 6], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "piano-andrew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "type(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "essential-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.Series(data={'Beskrivning_Anonymized':'När kommer nästa tågtabell ut?'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "therapeutic-watts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torchModel.predict(testing)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "digital-rouge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fråga'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[output[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "present-stranger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 6, ..., 1, 0, 4], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "official-thunder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Avvikelse', 'Beröm', 'Fråga', 'Förseningsersättning', 'Klagomål',\n",
       "       'Skada', 'Synpunkt/Önskemål'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "induced-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate loaded model on full dataset\n",
    "mMiniBatcherFull = MiniBatcher(X[:50000], Y[:50000], batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-valentine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1/500\n",
      "batch: 2/500\n",
      "batch: 3/500\n",
      "batch: 4/500\n",
      "batch: 5/500\n",
      "batch: 6/500\n",
      "batch: 7/500\n",
      "batch: 8/500\n",
      "batch: 9/500\n",
      "batch: 10/500\n",
      "batch: 11/500\n",
      "batch: 12/500\n",
      "batch: 13/500\n",
      "batch: 14/500\n",
      "batch: 15/500\n",
      "batch: 16/500\n",
      "batch: 17/500\n",
      "batch: 18/500\n",
      "batch: 19/500\n",
      "batch: 20/500\n",
      "batch: 21/500\n",
      "batch: 22/500\n",
      "batch: 23/500\n",
      "batch: 24/500\n",
      "batch: 25/500\n",
      "batch: 26/500\n",
      "batch: 27/500\n",
      "batch: 28/500\n"
     ]
    }
   ],
   "source": [
    "Y_pred = []\n",
    "targets = []\n",
    "for X_batch, labels in mMiniBatcherFull.getBatchIterator():\n",
    "  \n",
    "        output = torchModel.predict(X_batch)\n",
    "        Y_pred.extend(output)\n",
    "        targets.extend(labels)\n",
    "        print(mMiniBatcherFull.getBatchInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(Y_pred == targets)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(targets, Y_pred, normalize='true', labels=list(set(Y_test)))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(Y_test))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(targets, Y_pred, normalize=None, labels=list(set(Y_test)))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(set(Y_test))).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "encouraging-journalism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(range(Y_test.columns.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-marshall",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
